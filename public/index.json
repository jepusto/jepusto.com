[{"authors":["admin"],"categories":null,"content":"I am a statistician and associate professor in the School of Education at the University of Wisconsin-Madison, where I teach in the Educational Psychology Department and the graduate program in Quantitative Methods. My research involves developing statistical methods for problems in education, psychology, and other areas of social science research, with a focus on methods related to research synthesis and meta-analysis.\n","date":1632441600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1632441600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a statistician and associate professor in the School of Education at the University of Wisconsin-Madison, where I teach in the Educational Psychology Department and the graduate program in Quantitative Methods.","tags":null,"title":"James E. Pustejovsky","type":"authors"},{"authors":["megha"],"categories":null,"content":"Megha Joshi Megha completed her PhD in the Quantitative Methods program at The University of Texas at Austin. Her dissertation work examined cluster wild bootstrapping to handle dependent effect sizes in meta-analyses with a small number of studies. She now works as quantitative researcher at the American Institutes for Research. Her research interests include causal inference, meta-analysis, and missing data analysis.\n","date":1632441600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1632441600,"objectID":"893f425741200ca4f20cc0c5d11d5add","permalink":"/authors/megha-joshi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/megha-joshi/","section":"authors","summary":"Megha Joshi Megha completed her PhD in the Quantitative Methods program at The University of Texas at Austin. Her dissertation work examined cluster wild bootstrapping to handle dependent effect sizes in meta-analyses with a small number of studies.","tags":null,"title":"Megha Joshi","type":"authors"},{"authors":["Man-Chen"],"categories":null,"content":"Man Chen Man Chen is a doctoral student in the Quantitative Methods area in the Educational Psychology Department at UW-Madison. Her research interests include meta-analysis, single-case experimental designs, and hierarchical linear modeling. She is currently working as a project assistant at the Wisconsin Center for Educational Research (WCER) with Dr. Pustejovsky.\n","date":1625702400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1625702400,"objectID":"1cf4efd04210c2fa913fd2d4c84c9b4b","permalink":"/authors/man-chen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/man-chen/","section":"authors","summary":"Man Chen Man Chen is a doctoral student in the Quantitative Methods area in the Educational Psychology Department at UW-Madison. Her research interests include meta-analysis, single-case experimental designs, and hierarchical linear modeling.","tags":null,"title":"Man Chen","type":"authors"},{"authors":["Christoper-Runyon"],"categories":null,"content":"Chris Runyon Chris did his doctoral work on causal inference at UT Austin. He is currently a measurement scientist in the Center for Advanced Assessment at the National Board of Medical Examiners, where his research focuses on developing automated scoring systems for constructed item responses.\n","date":1406851200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1406851200,"objectID":"e2b98fa6b3eb987af8b9c5c8cd32ddc7","permalink":"/authors/christopher-runyon/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/christopher-runyon/","section":"authors","summary":"Chris Runyon Chris did his doctoral work on causal inference at UT Austin. He is currently a measurement scientist in the Center for Advanced Assessment at the National Board of Medical Examiners, where his research focuses on developing automated scoring systems for constructed item responses.","tags":null,"title":"Christopher Runyon","type":"authors"},{"authors":["What-Evs-Donkey"],"categories":null,"content":"W. E. Donkey W. E. is our lab mascot. His name is also his philosophy in life: \u0026ldquo;Whatev\u0026rsquo;s, Donkey!\u0026rdquo;\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"6bdf03f287c50c1f3656566d88050a84","permalink":"/authors/what-evs-donkey/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/what-evs-donkey/","section":"authors","summary":"W. E. Donkey W. E. is our lab mascot. His name is also his philosophy in life: \u0026ldquo;Whatev\u0026rsquo;s, Donkey!\u0026rdquo;","tags":null,"title":"Whatev's Donkey","type":"authors"},{"authors":["Young-Ri-Lee"],"categories":null,"content":"Young Ri Lee Young Ri is a doctoral student in the Quantitative Methods program at The University of Texas at Austin. Her research interests include Hierarchical Linear Modeling (HLM), Robust Variance Estimation methods (RVE), and meta-analysis. She is currently an assistant statistician in the SMARTER group and assistant data analyst in UT\u0026rsquo;s Gender Equity Council.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"8701efa5e33ca735ad76255f7ec99a41","permalink":"/authors/young-ri-lee/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/young-ri-lee/","section":"authors","summary":"Young Ri Lee Young Ri is a doctoral student in the Quantitative Methods program at The University of Texas at Austin. Her research interests include Hierarchical Linear Modeling (HLM), Robust Variance Estimation methods (RVE), and meta-analysis.","tags":null,"title":"Young Ri Lee","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]]\rname = \u0026quot;Courses\u0026quot;\rurl = \u0026quot;courses/\u0026quot;\rweight = 50\r Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]]\rname = \u0026quot;Docs\u0026quot;\rurl = \u0026quot;docs/\u0026quot;\rweight = 50\r Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":["Megha Joshi","James E. Pustejovsky","S. Natasha Beretvas"],"categories":null,"content":"","date":1632441600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632441600,"objectID":"df806dc4eec21bebe1f1989dea72de33","permalink":"/publication/cluster-wild-bootstrap-for-meta-analysis/","publishdate":"2021-09-24T00:00:00Z","relpermalink":"/publication/cluster-wild-bootstrap-for-meta-analysis/","section":"publication","summary":"The most common and well-known meta-regression models work under the assumption that there is only one effect size estimate per study and that the estimates are independent. However, meta-analytic reviews of social science research often include multiple effect size estimates per primary study, leading to dependence in the estimates. Some meta-analyses also include multiple studies conducted by the same lab or investigator, creating another potential source of dependence. An increasingly popular method to handle dependence is robust variance estimation (RVE), but this method can result in inflated Type I error rates when the number of studies is small. Small-sample correction methods for RVE have been shown to control Type I error rates adequately but may be overly conservative, especially for tests of multiple-contrast hypotheses. We evaluated an alternative method for handling dependence, cluster wild bootstrapping, which has been examined in the econometrics literature but not in the context of meta-analysis. Results from two simulation studies indicate that cluster wild bootstrapping maintains adequate Type I error rates and provides more power than extant small sample correction methods, particularly for multiple-contrast hypothesis tests. We recommend using cluster wild bootstrapping to conduct hypothesis tests for meta-analyses with a small number of studies. We have also created an R package that implements such tests.","tags":["meta-analysis","robust variance estimation","bootstrap"],"title":"Cluster wild bootstrapping to handle dependent effect sizes in meta-analysis with a small number of studies","type":"publication"},{"authors":null,"categories":null,"content":"In many fields, randomized experiments are often considered the gold standard approach for learning about the causal effects of an intervention, program, or policy. However, randomized experiments are not always feasible or ethical. Furthermore, the increasing availability of large-scale observational datasets presents opportunities to investigate causal effects outside of the realm of designed experiments. This course surveys contemporary research design strategies for investigating questions about causal effects, focusing on the theory and application of quasi-experimental methods that can, under some conditions, provide strong warrants for drawing causal inferences. The focus of the course is on causal description of point-in-time interventions (“Is this intervention effective?”) rather than causal explanation (“Why is this intervention effective?”).\nThe course begins with an introduction to the potential outcomes framework for expressing causal quantities, followed by an examination of (idealized) simple and block randomized experiments as prototypes for learning about causal effects. The remainder of the course covers theory and data-analysis strategies for drawing causal inferences from four quasi-experimental designs: instrumental variables approaches, regression discontinuity designs, non-equivalent control group designs (using techniques such as matching and propensity score weighting), and comparative interrupted time series designs. For each design, we will consider (i) the core strategy for identifying a causal effect, (ii) corresponding statistical approaches for estimating the effect, and (iii) strategies and design elements for strengthening the design. Further, advanced topics will be covered based on student interest.\n \r2021 (Fall) syllabus and reading list  ","date":1631077200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631077200,"objectID":"0a431820112478d62fe786057ce877eb","permalink":"/teaching/quasi-experimental/","publishdate":"2021-09-08T00:00:00-05:00","relpermalink":"/teaching/quasi-experimental/","section":"teaching","summary":"In many fields, randomized experiments are often considered the gold standard approach for learning about the causal effects of an intervention, program, or policy. However, randomized experiments are not always feasible or ethical.","tags":[],"title":"Design \u0026 Analysis of Quasi-Experiments for Causal Inference","type":"teaching"},{"authors":[],"categories":null,"content":"","date":1630576800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630576800,"objectID":"e5eb97ffaa8dbb89ca5c2e448848c08c","permalink":"/talk/oslorug-2021-rve-with-metafor-and-clubsandwich/","publishdate":"2021-09-02T12:00:00Z","relpermalink":"/talk/oslorug-2021-rve-with-metafor-and-clubsandwich/","section":"talk","summary":"Large meta-analyses often involve dependent effect sizes, but where the exact form of the dependence is unknown. Meta-analysis with robust variance estimation handles this problem through specification of a working model for the dependence, which need not be correct. However, the two currently available working models are limited to each describing a single type of dependence. James will demonstrate a workflow for implementing an expanded set of working models by combining the metafor and clubSandwich R packages.","tags":[],"title":"Synthesis of dependent effect sizes: Robust variance estimation with clubSandwich","type":"talk"},{"authors":["Man Chen","James E. Pustejovsky"],"categories":null,"content":"","date":1625702400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625702400,"objectID":"55d1d151a2aa96c8918a1a9760bd9cb7","permalink":"/publication/sced-mlma-rve/","publishdate":"2021-07-08T00:00:00Z","relpermalink":"/publication/sced-mlma-rve/","section":"publication","summary":"Single-case experimental designs (SCEDs) are used to study the effects of interventions on the behavior of individual cases, by making comparisons between repeated measurements of an outcome under different conditions. In research areas where SCEDs are prevalent, there is a need for methods to synthesize results across multiple studies. One approach to synthesis uses a multi-level meta-analysis (MLMA) model to describe the distribution of effect sizes across studies and across cases within studies. However, MLMA relies on having accurate sampling variances of effect size estimates for each case, which may not be possible due to auto-correlation in the raw data series. One possible solution is to combine MLMA with robust variance estimation (RVE), which provides valid assessments of uncertainty even if the sampling variances of effect size estimates are inaccurate. Another possible solution is to forgo MLMA and use simpler, ordinary least squares (OLS) methods with RVE. This study evaluates the performance of effect size estimators and methods of synthesizing SCEDs in the presence of auto-correlation, for several different effect size metrics, via a Monte Carlo simulation designed to emulate the features of real data series. Results demonstrate that the MLMA model with RVE performs properly in terms of bias, accuracy, and confidence interval coverage for estimating overall average log response ratios. The OLS estimator corrected with RVE performs the best in estimating overall average Tau effect sizes. None of the available methods perform adequately for meta-analysis of within-case standardized mean differences.","tags":["single-case design","meta-analysis","robust variance estimation","response ratio","non-overlap measures","standardized mean difference"],"title":"Multi-level meta-analysis of single-case experimental designs using robust variance estimation","type":"publication"},{"authors":["Maya B. Mathur","James E. Pustejovsky"],"categories":null,"content":"This workshop will cover methods to investigate selective reporting in meta-analysis of statistically dependent effect sizes, which are a common feature of systematic reviews in psychology. The workshop is organized into two sections. In the first section, we will describe situations where dependent effect sizes occur and review methods for summarizing findings in the presence of dependent effects. We will then describe methods for creating and interpreting funnel plots, including tests of asymmetry, with dependent effect sizes. In the second section, we will present new statistical sensitivity analyses for publication bias, which perform well in small meta-analyses, those with non-normal or dependent effect sizes, and those with heterogeneity. The sensitivity analyses enable statements such as \u0026ldquo;For publication bias to shift the observed point estimate to the null, \u0026lsquo;significant\u0026rsquo; results would need to be at least 10-fold more likely to be published than negative or \u0026lsquo;non-significant\u0026rsquo; results\u0026rdquo; or \u0026ldquo;no amount of publication bias could explain away the average effect.\u0026rdquo; In both sections, we will demonstrate methods using R code and examples from real meta-analyses.\n","date":1624451400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624451400,"objectID":"eabfe5a23b0e5ebc76fe2dda56ce987a","permalink":"/talk/sips-2021-statistical-frontiers-for-selective-reporting/","publishdate":"2021-06-23T12:30:00Z","relpermalink":"/talk/sips-2021-statistical-frontiers-for-selective-reporting/","section":"talk","summary":"This workshop will cover methods to investigate selective reporting in meta-analysis of statistically dependent effect sizes, which are a common feature of systematic reviews in psychology. The workshop is organized into two sections.","tags":[],"title":"Statistical frontiers for selective reporting and publication bias","type":"talk"},{"authors":["James E. Pustejovsky","Elizabeth Tipton"],"categories":null,"content":"","date":1620345600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620345600,"objectID":"74d5d7f20e24d23dedd218e3102a59f7","permalink":"/publication/rve-meta-analysis-expanding-the-range/","publishdate":"2021-05-07T00:00:00Z","relpermalink":"/publication/rve-meta-analysis-expanding-the-range/","section":"publication","summary":"In prevention science and related fields, large meta-analyses are common, and these analyses often involve dependent effect size estimates. Robust variance estimation (RVE) methods provide a way to include all dependent effect sizes in a single meta-regression model, even when the nature of the dependence is unknown. RVE uses a working model of the dependence structure, but the two currently available working models are limited to each describing a single type of dependence. Drawing on flexible tools from multivariate meta-analysis, this paper describes an expanded range of working models, along with accompanying estimation methods, which offer benefits in terms of better capturing the types of data structures that occur in practice and improving the efficiency of meta-regression estimates. We describe how the methods can be implemented using existing software (the 'metafor' and 'clubSandwich' packages for R) and illustrate the approach in a meta-analysis of randomized trials examining the effects of brief alcohol interventions for adolescents and young adults.","tags":["meta-analysis","meta-regression","dependent effect sizes","robust variance estimation"],"title":"Meta-Analysis with robust variance estimation: Expanding the range of working models","type":"publication"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\r\rIn basic meta-analysis, where each study contributes just a single effect size estimate, there has been a lot of work devoted to developing models for selective reporting. Most of these models formulate the selection process as a function of the statistical significance of the effect size estimate; some also allow for the possibility that the precision of the study’s effect influences the probability of selection (i.e., bigger studies are more likely to be reported, regardless of statistical significance).\nA problem that I’ve been mulling recently is how to think about selective reporting in meta-analyses that include some studies with multiple effect size estimates. This setting is quite a bit more complicated than basic meta-analysis because there are several different ways that selective reporting could happen. It could be that each effect size estimate is selected (or censored) individually, on the basis of its statistical significance. However, it seems just as plausible that the pattern of statistical significance across the full set of results could influence whether any of the results get selected.\nIn pondering this stuff, I’m trying to find ways to simplify the space of possibilities or formulate stylized (or “toy”) problems that are more tractable. Here is one such problem. I’ll write it as a question such as you might find in a problem set from a course on statistical distribution theory.\nThe general problem\rConsider a study that assesses some effect size across \\(m\\) different outcomes. Let \\(T_i\\) denote the effect size estimate for outcome \\(i\\), let \\(V_i\\) denote the sampling variance of the effect size estimate for outcome \\(i\\), and let \\(\\theta_i\\) denote the true effect size parameter for corresponding to outcome \\(i\\). Assume that\r\\[T_i \\sim N(\\theta_i, V_i),\\]\rwhere \\(V_i\\) is known. Define \\(A_i\\) as an indicator that is equal to one if \\(T_i\\) is statistically significant at level \\(\\alpha\\) based on a one-sided test, and otherwise equal to zero. (Equivalently, let \\(A_i\\) be equal to one if the effect is statistically significant at level \\(2 \\alpha\\) and in the theoretically expected direction.) Formally,\r\\[A_i = I\\left(\\frac{T_i}{\\sqrt{V_i}} \u0026gt; q_\\alpha \\right)\\]\rwhere \\(q_\\alpha = \\Phi^{-1}(1 - \\alpha)\\) is the critical value from a standard normal distribution (e.g., \\(q_{.05} = 1.645\\), \\(q_{.025} = 1.96\\)). Let \\(N_A = \\sum_{i=1}^m A_i\\) denote the total number of statistically significant effect sizes in the study. Our general interest is in the distribution of \\(N_A\\).\nCompound symmetry\rIn general, the distribution of \\(N_A\\) will depend on the joint distribution of \\((T_1,...,T_m)\\), so we will need to make some further assumptions regarding that joint distribution in order to make progress here. One simplifying assumption that seems worth considering is that the effect size estimates follow a compound symmetric distribution. Specifically, assume that all of the effect size estimates have equal sampling variance, \\(V_1 = V_2 = \\cdots = V_m = V\\), and that there is a constant correlation between every pair of effect size estimates:\r\\[\\text{Cov}(T_h, T_i) = \\rho V\\]\rfor some correlation \\(\\rho\\). Further, assume that the true effect sizes vary based on a compound symmetric distribution where\r\\[\r\\theta_i \\sim N(\\mu, \\omega^2).\r\\]\rAll of this implies that the joint distribution of the effect size estimates is compound symmetric:\r\\[\r\\left(\\begin{array}{c} T_1 \\\\ T_2 \\\\ \\vdots \\\\ T_m \\end{array}\\right) \\sim N\\left[ \\mu \\mathbf{1}_m, \\ \\left(\\omega^2 + \\rho V\\right)\\mathbf{J}_m + (1 - \\rho) V \\mathbf{I}_m \\right],\r\\]\rwhere \\(\\mathbf{1}_m\\) is an \\(m \\times 1\\) vector of 1’s, \\(\\mathbf{J}_m\\) is an \\(m \\times m\\) matrix of 1’s, and \\(\\mathbf{I}_m\\) is an \\(m \\times m\\) identity matrix.\nGiven the above assumptions, What is the distribution of \\(N_A\\)?\nPlease write to me if you’d like to discuss the theory or implications of this problem.\n\r\r","date":1619481600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619481600,"objectID":"992f863234b954f1dca1f73347f02ca9","permalink":"/number-of-significant-effects/","publishdate":"2021-04-27T00:00:00Z","relpermalink":"/number-of-significant-effects/","section":"post","summary":"In basic meta-analysis, where each study contributes just a single effect size estimate, there has been a lot of work devoted to developing models for selective reporting. Most of these models formulate the selection process as a function of the statistical significance of the effect size estimate; some also allow for the possibility that the precision of the study’s effect influences the probability of selection (i.","tags":["effect size","distribution theory"],"title":"Finding the distribution of significant effect sizes","type":"post"},{"authors":["James E. Pustejovsky","Megha Joshi"],"categories":null,"content":"","date":1618876800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618876800,"objectID":"5a1d579b5894fae8620b1b39dc965ca0","permalink":"/publication/transition-to-college-mathematics-year-3/","publishdate":"2021-04-20T00:00:00Z","relpermalink":"/publication/transition-to-college-mathematics-year-3/","section":"publication","summary":"Texas House Bill 5 introduced requirements that school districts partner with institutions of higher education to provide college preparatory courses in mathematics and English language arts for high school seniors who are not yet college ready. In response to House Bill 5 requirements, the Charles A. Dana Center developed a model college preparatory mathematics course, Transition to College Mathematics Course (TCMC), which has been adopted by dozens of school districts across Texas over the past several school years. In prior work, we examined the effects of TCMC on students’ progress into post-secondary education by comparing two student cohorts who participated in TCMC to observationally similar students from the same cohort but who did not enroll in the course. In this report, we investigate the extent of heterogeneity in the effects of participating in TCMC. We find little evidence that the program was differentially effective for students from different socio-economic backgrounds, nor do we find evidence that program effects varied by the number of years that it had been offered. However, for key outcomes such as rates of passing a college-level math course, the effects of participating in TCMC may have varied across the schools that offered the course. Just as in prior work, these findings must be interpreted cautiously because we were unable to fully assess and account for students' college-readiness status at the start of their senior year.","tags":["causal inference","evaluation","Transition to College Mathematics","heterogeneity"],"title":"Evaluating the Transition to College Mathematics Course in Texas high schools: Examining heterogeneity across schools and student characteristics","type":"publication"},{"authors":["Laurie E. McLouth","C. Graham Ford","James E. Pustejovsky","Crystal Park","Allen C. Sherman","Kelly Trevino","John A. Salsman"],"categories":null,"content":"","date":1612137600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612137600,"objectID":"35f3fa734504c73158288248a0c57aa5","permalink":"/publication/psychosocial-interventions-for-spiritual-well-being/","publishdate":"2021-02-01T00:00:00Z","relpermalink":"/publication/psychosocial-interventions-for-spiritual-well-being/","section":"publication","summary":"__Objective__ Spiritual well‐being (SpWb) is an important dimension of health‐related quality of life for many cancer patients. Accordingly, an increasing number of psychosocial intervention studies have included SpWb as a study endpoint, and may improve SpWb even if not designed explicitly to do so. This meta‐analysis of randomized controlled trials (RCTs) evaluated effects of psychosocial interventions on SpWb in adults with cancer and tested potential moderators of intervention effects. __Methods__ Six literature databases were systematically searched to identify RCTs of psychosocial interventions in which SpWb was an outcome. Doctoral‐level rater pairs extracted data using Covidence following Preferred Reporting Items for Systematic reviews and Meta‐Analyses guidelines. Standard meta‐analytic techniques were applied, including meta‐regression with robust variance estimation and risk‐of‐bias sensitivity analysis. __Results__ Forty‐one RCTs were identified, encompassing 88 treatment effects among 3883 survivors. Interventions were associated with significant improvements in SpWb ($g = 0.22$, 95% CI [0.14, 0.29], $p ","tags":["cancer","meta-analysis","psycho-social intervention","systematic review"],"title":"A systematic review and meta‐analysis of effects of psychosocial interventions on spiritual well‐being in adults with cancer","type":"publication"},{"authors":[],"categories":null,"content":"","date":1611219600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611219600,"objectID":"621fb0f86d223034dc373451fa175485","permalink":"/talk/esmarconf2021-rve-with-metafor-and-clubsandwich/","publishdate":"2021-01-21T12:00:00Z","relpermalink":"/talk/esmarconf2021-rve-with-metafor-and-clubsandwich/","section":"talk","summary":"Across scientific fields, large meta-analyses often involve dependent effect size estimates. Robust variance estimation (RVE) methods provide a way to include all dependent effect sizes in a single meta-analysis model, even when the nature of the dependence is unknown. RVE uses a working model of the dependence structure, but the two currently available working models (available in the robumeta package) are limited to each describing a single type of dependence. We describe a workflow combining two existing packages, metafor and clubSandwich, that can be used to implement an expanded set of working models, offering benefits in terms of better capturing the types of data structures that occur in practice and improving the efficiency of meta-analytic model estimates.","tags":[],"title":"Synthesis of dependent effect sizes: Versatile models through metafor and clubSandwich","type":"talk"},{"authors":null,"categories":null,"content":"Meta-analysis is the set of statistical methods and practices for synthesizing evidence collected from multiple sources, such as multiple studies on the same topic. Often conducted as part of a systematic literature review, meta-analyses play an increasingly prominent role in education research, psychology, and many other areas of social and behavior science. This course introduces the stages of the research synthesis process and the statistical methods used for conducting quantitative syntheses of social-scientific research. The focus of the course is on practical application and interpretation of meta-analytic methods, enriched with discussion of underlying statistical theory. Major topics include scope of research syntheses, systematic search and screening procedures, effect size calculations, summary meta-analysis, meta-regression, dependent effect sizes, selective reporting and publication bias analysis. Computational exercises use the R statistical computing environment.\n \r2021 (Spring) syllabus and reading list  ","date":1611208800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611208800,"objectID":"b5c50e75f836ecafaf99287dbb4b3554","permalink":"/teaching/meta-analysis/","publishdate":"2021-01-21T00:00:00-06:00","relpermalink":"/teaching/meta-analysis/","section":"teaching","summary":"Meta-analysis is the set of statistical methods and practices for synthesizing evidence collected from multiple sources, such as multiple studies on the same topic. Often conducted as part of a systematic literature review, meta-analyses play an increasingly prominent role in education research, psychology, and many other areas of social and behavior science.","tags":[],"title":"Meta-analysis","type":"teaching"},{"authors":["Jennifer R. Ledford","James E. Pustejovsky"],"categories":null,"content":"","date":1609804800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609804800,"objectID":"1a41d4a07c5d29da8ea5efd999d4f98c","permalink":"/publication/stay-play-talk-meta-analysis/","publishdate":"2021-01-05T00:00:00Z","relpermalink":"/publication/stay-play-talk-meta-analysis/","section":"publication","summary":"Stay-play-talk (SPT) is a peer-mediated intervention which involves training peer implementers to stay in proximity to, play with, and talk to a focal child who has disabilities or lower social competence. This systematic review and meta-analysis investigated the contexts in which SPT interventions have been conducted, the methodological adequacy of the research assessing its effects, and the outcomes for both peer implementers and focal children. Studies have primarily occurred in inclusive preschool settings during free play activities, with researchers serving as facilitators. Average effects were positive for both peer implementers and focal children, although considerable heterogeneity across studies was observed. Additional research is needed to determine what peer implementer and focal child characteristics moderate intervention success, what modifications are needed for children who have complex communication needs, and optimal procedural variations (e.g., group size, training time).","tags":["meta-analysis","response ratio","single-case design","systematic review"],"title":"Systematic review and meta-analysis of stay-play-talk interventions for improving social behaviors of young children","type":"publication"},{"authors":["James E. Pustejovsky","Megha Joshi"],"categories":null,"content":"","date":1608076800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608076800,"objectID":"f2e493b61c122220998e8906114dc1e4","permalink":"/publication/transition-to-college-mathematics-year-2/","publishdate":"2020-12-16T00:00:00Z","relpermalink":"/publication/transition-to-college-mathematics-year-2/","section":"publication","summary":"Texas House Bill 5 introduced requirements that school districts partner with institutions of higher education to provide college preparatory courses in mathematics and English for high school seniors who are not yet college ready. As districts and college partners begin to respond to these provisions, there is a need for empirical research on the effects of different approaches to implementing the college preparatory courses. In response to House Bill 5 requirements, the Charles A. Dana Center has developed a model college preparatory mathematics course, Transition to College Mathematics Course (TCMC), which has been adopted by dozens of school districts across Texas over the past several school years. We examine the effects of TCMC on students’ progress into post-secondary education by comparing students who participated in TCMC during the 2017-18 school year (the second year of implementation) to observationally similar students from the same cohort but who did not enroll in the course. We find that students who took TCMC graduated at higher rates than comparison students. They had similar rates of overall enrollment in post-secondary education, but enrolled in community colleges at higher rates and in 4-year colleges or universities at lower rates than did comparison students. Enrollment tended to increase over the course of four semesters after high school graduation. Relative to comparison students, students who took TCMC were also less likely to take and less likely to pass college-level math coursework. These results must be interpreted cautiously because we were unable to fully assess and account for students’ college-readiness status at the start of their senior year.","tags":["causal inference","evaluation","Transition to College Mathematics"],"title":"Evaluating the Transition to College Mathematics Course in Texas high schools: Findings from the second year of implementation","type":"publication"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rAs in many parts of life, statistics is full of little bits of knowledge that are useful if you happen to know them, but which hardly anybody ever bothers to mention. You would think, if something is so useful, perhaps your professors would spend a fair bit of time explaining it to you. But maybe the stuff seems trivial, obvious, or simple to them, so they don’t bother.\nOne example of this is Excel keyboard shortcuts. In a previous life, I was an Excel jockey so I learned all the keyboard shortcuts, such as how to move the cursor to the last cell in a continuous block of entries (ctrl + an arrow key). Whenever I do this while sharing a screen in a meeting, someone is invariably astounded and wants to know what dark sorcery I’m conjuring. It’s a simple trick, but a useful one—especially if you’re working with a really large dataset with thousands of rows. But it’s also something that there’s no reason to expect anyone to figure out on their own, and that no stats or quant methods professor is going to spend class time demonstrating.\nLet me explain another, slightly more involved example, involving one of my favorite pieces of matrix algebra. There’s a thing called the Woodbury identity, also known as the Sherman-Morrison-Woodbury identity, that is a little life hack for inverting certain types of matrices. It has a Wikipedia page, which I have visited many times. It is a very handy bit of math, if you happen to be a statistics student working with hierarchical models (such as meta-analytic models). I’ll give a statement of the identity, then explain a bit about the connection to hierarchical models.\nThe Woodbury identity\rSay that you’ve got four matrices, an \\(n \\times n\\) matrix \\(\\mathbf{A}\\), a \\(k \\times k\\) matrix \\(\\mathbf{C}\\), an \\(n \\times k\\) matrix \\(\\mathbf{U}\\), and a \\(k \\times n\\) matrix \\(\\mathbf{V}\\). Assume that \\(\\mathbf{A}\\) and \\(\\mathbf{C}\\) are invertible. The Woodbury identity tells you how to get the inverse of a certain combination of these matrices:\r\\[\r\\left(\\mathbf{A} + \\mathbf{U} \\mathbf{C} \\mathbf{V}\\right)^{-1} = \\mathbf{A}^{-1} - \\mathbf{A}^{-1} \\mathbf{U} \\left(\\mathbf{C}^{-1} + \\mathbf{V} \\mathbf{A}^{-1} \\mathbf{U} \\right)^{-1} \\mathbf{V} \\mathbf{A}^{-1}.\r\\]\rAdmit it, you’re impressed. “Dude! Mind. Blown.” you’re probably saying to yourself right now.\nOr perhaps you’re still a touch skeptical that this formula is worth knowing. Let me explain the connection to hierarchical models.\n\rHierarchical models\rHierarchical linear models are a mainstay of statistical analysis in many, many areas of application, including education research, where we often deal with data collected on individuals (students, teachers) nested within larger aggregate units (like schools). In meta-analysis, these models come up if we’re dealing with samples that have more than one relevant outcome, so that we have multiple effect size estimates nested within a given sample or study.\nSuppose we have a hierarchical structure with \\(J\\) clusters, where cluster \\(j\\) has \\(n_j\\) individual observations. A quite general way of expressing a hierarchical model for such a data structure is\r\\[\r\\mathbf{Y}_j = \\mathbf{X}_j \\boldsymbol\\beta + \\mathbf{Z}_j \\boldsymbol\\eta_j + \\boldsymbol\\epsilon_j,\r\\]\rfor \\(j = 1,...,J\\), where, for cluster \\(j\\):\n\r\\(\\mathbf{Y}_j\\) is an \\(n_j \\times 1\\) vector of outcomes,\r\\(\\mathbf{X}_j\\) is an \\(n_j \\times p\\) design matrix for the fixed effects,\r\\(\\boldsymbol\\beta\\) is a \\(p \\times 1\\) vector of fixed effect coefficients,\r\\(\\mathbf{Z}_j\\) is an \\(n_j \\times q\\) design matrix for the random effects,\r\\(\\boldsymbol\\eta_j\\) is a \\(q \\times 1\\) vector of random effects, and\r\\(\\boldsymbol\\epsilon_j\\) is an \\(n_j \\times 1\\) vector of level-1 errors.\r\rIn this model, we assume that the random effects have mean zero and unknown variance-covariance matrix \\(\\mathbf{T}\\), often assumed to be an unstructured, symmetric and invertible matrix; we assume that the level-1 errors are also mean zero with variance-covariance matrix \\(\\boldsymbol\\Sigma_j\\); and we assume that \\(\\boldsymbol\\eta_j\\) is independent of \\(\\boldsymbol\\epsilon_j\\). In many instances, we might assume that the entries of \\(\\mathbf{e}_j\\) are all independent, so \\(\\boldsymbol\\Sigma_j\\) will be a multiple of an identity matrix, \\(\\boldsymbol\\Sigma_j = \\sigma^2 \\mathbf{I}_j\\). In other instances (such as models for longitudinal data), \\(\\boldsymbol\\Sigma\\) might be a patterned matrix that includes off-diagonal terms, such as an auto-regressive structure.\nWhat is the marginal variance of \\(\\mathbf{Y}_j | \\mathbf{X}_j\\) in this model? In other words, if we combine the variance due to the random effects and the variance of the level-1 errors, what do we get? We get\r\\[\r\\text{Var}\\left(\\mathbf{Y}_j | \\mathbf{X}_j \\right) = \\mathbf{V}_j = \\mathbf{Z}_j \\mathbf{T} \\mathbf{Z}_j\u0026#39; + \\boldsymbol\\Sigma_j,\r\\]\ra matrix that, if you reverse the terms, looks like\r\\[\r\\mathbf{V}_j = \\boldsymbol\\Sigma_j + \\mathbf{Z}_j \\mathbf{T} \\mathbf{Z}_j\u0026#39;\r\\]\ra simple form of the combination of matrices in the left-hand side of the Woodbury identity. Thus, the identity tells us how we can invert this matrix.\nBut why would we care about inverting this variance-covariance matrix, you might ask? One good reason is that the fixed effect coefficients in the hierarchical model are estimated by weighted least squares, where the weight matrices are the inverse of an estimate of \\(\\mathbf{V}_j\\). Thus, to understand how the weights in a hierarchical model work, it’s quite useful to be able to invert \\(\\mathbf{V}_j\\). Another good (related) reason is that the sampling variance of the fixed effect estimates is approximately\r\\[\r\\text{Var}(\\boldsymbol{\\hat\\beta}) \\approx \\left(\\sum_{j=1}^J \\mathbf{X}_j\u0026#39;\\mathbf{V}_j^{-1} \\mathbf{X}_j \\right)^{-1}\r\\]\r(it would be exact if we knew the parameters of \\(\\mathbf{V}_j\\) with certainty). So if we want to understand the precision of \\(\\boldsymbol{\\hat\\beta}\\) or the power of a hypothesis test involving \\(\\boldsymbol{\\hat\\beta}\\), then we we won’t be able to get very far without inverting \\(\\mathbf{V}_j\\).\nDirectly applying the identity, we get\r\\[\r\\mathbf{V}_j^{-1} = \\boldsymbol\\Sigma_j^{-1} - \\boldsymbol\\Sigma_j^{-1} \\mathbf{Z}_j \\left(\\mathbf{T}^{-1} + \\mathbf{Z}_j\u0026#39;\\boldsymbol\\Sigma_j^{-1}\\mathbf{Z}_j \\right)^{-1} \\mathbf{Z}_j\u0026#39; \\boldsymbol\\Sigma_j^{-1}\r\\]\rThis expression looks like a bit of a mess, I’ll admit, but it can be useful. Things simplify quite a bit of \\(\\boldsymbol\\Sigma_j^{-1}\\) has a form that is easy to invert (like a multiple of an identity matrix) and if the dimension of the random effects \\(q\\) is small. Under these conditions, \\(\\boldsymbol\\Sigma_j^{-1}\\) is easy to work with, \\(\\mathbf{T}^{-1}\\) is manageable because it has small dimensions, and \\(\\mathbf{Z}_j\u0026#39;\\boldsymbol\\Sigma_j^{-1}\\mathbf{Z}_j\\) becomes manageable because it also has small dimensions (\\(q \\times q\\), in both cases).\nRandom intercepts\rAs an example, consider a very simple model that includes only random intercepts, so \\(\\mathbf{Z}_j = \\mathbf{1}_j\\), an \\(n_j \\times 1\\) vector with every entry equal to 1, and \\(\\mathbf{T}\\) is simply \\(\\tau^2\\), the variance of the random intercepts. For simplicity, let’s also assume that the level-1 errors are independent, so \\(\\boldsymbol\\Sigma_j = \\sigma^2 \\mathbf{I}_j\\) and \\(\\boldsymbol\\Sigma_j^{-1} = \\sigma^{-2} \\mathbf{I}_j\\). Applying the Woodbury identity,\r\\[\r\\begin{aligned}\r\\mathbf{V}_j^{-1} \u0026amp;= \\boldsymbol\\Sigma_j^{-1} - \\boldsymbol\\Sigma_j^{-1} \\mathbf{1}_j \\left(\\mathbf{T}^{-1} + \\mathbf{1}_j\u0026#39;\\boldsymbol\\Sigma_j^{-1}\\mathbf{1}_j \\right)^{-1} \\mathbf{1}_j\u0026#39; \\boldsymbol\\Sigma_j^{-1} \\\\\r\u0026amp;= \\sigma^{-2} \\mathbf{I}_j - \\sigma^{-4} \\mathbf{1}_j \\left(\\tau^{-2} + \\sigma^{-2} \\mathbf{1}_j\u0026#39;\\mathbf{1}_j \\right)^{-1} \\mathbf{1}_j\u0026#39; \\\\\r\u0026amp;= \\sigma^{-2} \\mathbf{I}_j - \\sigma^{-4} \\left(\\tau^{-2} + \\sigma^{-2} n_j \\right)^{-1} \\mathbf{1}_j \\mathbf{1}_j\u0026#39; \\\\\r\u0026amp;= \\sigma^{-2} \\left(\\mathbf{I}_j - \\frac{\\tau^2} {\\sigma^2 + n_j \\tau^2} \\mathbf{1}_j \\mathbf{1}_j\u0026#39;\\right).\r\\end{aligned}\r\\]\rTry checking this for yourself by carrying through the matrix algebra for \\(\\mathbf{V}_j \\mathbf{V}_j^{-1}\\), which should come out equal to \\(\\mathbf{I}_j\\).\nNow suppose that the design matrix is also quite simple, consisting of just an intercept term \\(\\mathbf{X}_j = \\mathbf{1}_j\\), so that \\(\\boldsymbol\\beta = \\beta\\) is simply a population mean. How precise is the estimate of the population mean from this hierarchical model? Well, the sampling variance of the estimator \\(\\hat\\beta\\) is approximately\r\\[\r\\begin{aligned}\r\\text{Var}(\\hat\\beta) \u0026amp;\\approx \\left(\\sum_{j=1}^J \\mathbf{1}_j\u0026#39;\\mathbf{V}_j^{-1} \\mathbf{1}_j \\right)^{-1} \\\\\r\u0026amp;= \\left(\\sigma^{-2}\\sum_{j=1}^J \\mathbf{1}_j\u0026#39; \\left(\\mathbf{I}_j - \\frac{\\tau^2} {\\sigma^2 + n_j \\tau^2} \\mathbf{1}_j \\mathbf{1}_j\u0026#39;\\right) \\mathbf{1}_j \\right)^{-1} \\\\\r\u0026amp;= \\left(\\sigma^{-2} \\sum_{j=1}^J n_j \\left(1 - \\frac{n_j \\tau^2} {\\sigma^2 + n_j \\tau^2} \\right) \\right)^{-1} \\\\ \u0026amp;= \\left( \\sigma^{-2} \\sum_{j=1}^J \\frac{n_j \\sigma^2} {\\sigma^2 + n_j \\tau^2} \\right)^{-1} \\\\ \u0026amp;= \\left(\\sum_{j=1}^J \\frac{n_j} {\\sigma^2 + n_j \\tau^2} \\right)^{-1} \\\\\r\u0026amp;= \\left(\\sigma^2 + \\tau^2\\right) \\left(\\sum_{j=1}^J \\frac{n_j} {1 + (n_j - 1) \\rho} \\right)^{-1},\r\\end{aligned}\r\\]\rwhere \\(\\rho = \\tau^2 / (\\tau^2 + \\sigma^2)\\) is the intra-class correlation. Squint at this expression for a bit and you can see how the ICC influences the varince. If \\(\\rho\\) is near zero, then the sampling variance will be close to \\(\\left(\\sigma^2 + \\tau^2\\right) / N\\), which is what you would get if you treated every observation as independent. If \\(\\rho\\) is near 1, then the sampling variance ends up being nearly \\(\\left(\\sigma^2 + \\tau^2\\right) / J\\), which is what you would get if you treated every cluster as a single observation. For intermediate ICCs, the sample size from cluster \\(j\\) (in the numerator of the fraction inside the summation) gets cut down to size accordingly.\nThe estimator of the population mean is a weighted average of the outcomes. Specifically,\r\\[\r\\hat\\beta = \\left(\\sum_{j=1}^J \\mathbf{1}_j\u0026#39;\\mathbf{\\hat{V}}_j^{-1} \\mathbf{1}_j \\right)^{-1} \\sum_{j=1}^J \\mathbf{1}_j\u0026#39;\\mathbf{\\hat{V}}_j^{-1} \\mathbf{Y}_j,\r\\]\rwhere \\(\\mathbf{\\hat{V}}_j\\) is an estimator of \\(\\mathbf{V}_j\\). If you carry through the matrix algebra, you’ll find that\r\\[\r\\begin{aligned}\r\\hat\\beta \u0026amp;= \\left(\\sum_{j=1}^J \\frac{n_j} {\\sigma^2 + n_j \\tau^2} \\right)^{-1} \\sum_{j=1}^J \\frac{\\mathbf{1}_j\u0026#39;\\mathbf{Y}_j}{\\sigma^2 + n_j \\tau^2} \\\\\r\u0026amp;= \\frac{1}{W} \\sum_{j=1}^J \\sum_{i=1}^{n_j} w_j y_{ij},\r\\end{aligned}\r\\]\rwhere \\(w_j = \\frac{1}{1 + (n_j - 1) \\rho}\\) and \\(\\displaystyle{W = \\sum_{j=1}^J n_j w_j}\\). From this, we can see that the weight of a given observation depends on the ICC and the size of the cluster. If the ICC is low, then weights will all be close to 1. For higher ICCs, observations in smaller clusters get proportionately more weight than observations in larger clusters.\n\rA meta-analysis example\rIn a previous post on multi-variate meta-analysis, I examined how weighting works in some multi-variate meta-analysis models, where you have multiple effect size estimates nested within a study. Letting \\(T_{ij}\\) denote effect size estimate \\(i\\) in study \\(j\\), for \\(i = 1,...,n_j\\) and \\(j = 1,...,J\\). The first model I considered in the previous post was\r\\[\rT_{ij} = \\mu + \\eta_j + \\nu_{ij} + e_{ij},\r\\]\rwhere \\(\\text{Var}(\\eta_j) = \\tau^2\\), \\(\\text{Var}(\\nu_{ij}) = \\omega^2\\), \\(\\text{Var}(e_{ij}) = V_j\\), treated as known, and \\(\\text{cor}(e_{hj}, e_{ij}) = \\rho\\) for some specified value of \\(\\rho\\).1 This model makes the simplifying assumptions that the effect sizes within a given study all have the same sampling variance, \\(V_j\\), and that there is a single correlation between pairs of outcomes from the same study, that is constant across all pairs of outcomes and across all studies.\nYou can write this model in matrix form as\r\\[\r\\mathbf{T}_j = \\mu \\mathbf{1}_j + \\eta_j \\mathbf{1}_j + \\boldsymbol\\nu_j + \\mathbf{e}_j,\r\\]\rwhere \\(\\text{Var}(\\boldsymbol\\nu_j) = \\omega^2 \\mathbf{I}_j\\) and \\(\\text{Var}(\\mathbf{e}_j) = V_j \\left[\\rho \\mathbf{1}_j \\mathbf{1}_j\u0026#39; + (1 - \\rho) \\mathbf{I}_j\\right]\\). It follows that\r\\[\r\\text{Var}(\\mathbf{T}_j) = (\\tau^2 + V_j\\rho) \\mathbf{1}_j \\mathbf{1}_j\u0026#39; + [\\omega^2 + V_j (1 - \\rho)] \\mathbf{I}_j.\r\\]\rThe Woodbury identity comes in handy here again, if we want to examine the weights implied by this model or the sampling variance of the overall average effect size estimator.2 I’ll leave it as an exercise to find an expression for the weight assigned to effect size \\(T_{ij}\\) under this model.3 You could also try finding an expression for the variance of the overall average effect size estimator \\(\\hat\\mu\\), based on inverse-variance weighting, when the model is correctly specified.\n\rAnother meta-analysis example\rIn the previous post, I also covered weighting in a bit more general model, where the sampling variances and correlations are no longer quite so constrained. As before, we have\r\\[\r\\mathbf{T}_j = \\mu \\mathbf{1}_j + \\eta_j \\mathbf{1}_j + \\boldsymbol\\nu_j + \\mathbf{e}_j,\r\\]\rwhere \\(\\text{Var}(\\eta_j) = \\tau^2\\) and \\(\\text{Var}(\\boldsymbol\\nu_j) = \\omega^2 \\mathbf{I}_j\\). But now let \\(\\text{Var}(\\mathbf{e}_j) = \\boldsymbol\\Sigma_j\\) for some arbitrary, symmetric, invertible matrix \\(\\boldsymbol\\Sigma_j\\). The marginal variance of \\(\\mathbf{T}_j\\) is therefore\r\\[\r\\text{Var}(\\mathbf{T}_j) = \\tau^2\\mathbf{1}_j \\mathbf{1}_j\u0026#39; + \\omega^2 \\mathbf{I}_j + \\boldsymbol\\Sigma_j.\r\\]\rLet \\(\\mathbf{S}_j = \\left(\\omega^2 \\mathbf{I}_j + \\boldsymbol\\Sigma_j\\right)^{-1}\\). Try applying the Woodbury identity to invert \\(\\text{Var}(\\mathbf{T}_j)\\) in terms of \\(\\tau^2\\), \\(n_j\\), and \\(\\mathbf{S}_j\\). Then see if you can derive the weight assigned to effect \\(i\\) in study \\(j\\) under this model. See the previous post for the solution.4\n\r\r\rThis model is what we call the “correlated-and-hierarchical effects model” in my paper (with Beth Tipton) on extending working models for robust variance estimation.↩︎\n\rOr squint hard at the formula for the variance of \\(\\mathbf{T}_j\\), and you’ll see that it has the same form as the random intercepts model in the previous example. Just replace the \\(\\tau^2\\) in that model with \\(\\tau^2 + V_j \\rho\\) and replace the \\(\\sigma^2\\) in that model with \\(\\omega^2 + V_j (1 - \\rho)\\).↩︎\n\rSee the previous post for the answer.↩︎\n\rIn the previous post, I expressed the weights in terms of \\(s_{ij}\\), the sum of the entries in row \\(i\\) of the \\(\\mathbf{S}_j\\) matrix. In vector form, \\(\\mathbf{s}_j = \\left(s_{1j} \\ s_{2j} \\ \\cdots \\ s_{n_j j}\\right)\u0026#39; = \\mathbf{S}_j \\mathbf{1}_j\\).↩︎\n\r\r\r","date":1607040000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607040000,"objectID":"5e29babcd4c0d6f354e0ad97ec03b218","permalink":"/woodbury-identity/","publishdate":"2020-12-04T00:00:00Z","relpermalink":"/woodbury-identity/","section":"post","summary":"As in many parts of life, statistics is full of little bits of knowledge that are useful if you happen to know them, but which hardly anybody ever bothers to mention.","tags":["hierarchical models","matrix algebra"],"title":"The Woodbury identity","type":"post"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rDoing effect size calculations for meta-analysis is a good way to lose your faith in humanity—or at least your faith in researchers’ abilities to do anything like sensible statistical inference. Try it, and you’re surely encounter head-scratchingly weird ways that authors have reported even simple analyses, like basic group comparisons. When you encounter this sort of thing, you have two paths: you can despair, curse, and/or throw things, or you can view the studies as curious little puzzles—brain-teasers, if you will—to keep you awake and prevent you from losing track of those notes you took during your stats courses, back when. Here’s one of those curious little puzzles, which I recently encountered in helping a colleague with a meta-analysis project.\nA researcher conducts a randomized experiment, assigning participants to each of \\(G\\) groups. Each participant is assessed on a variable \\(Y\\) at pre-test and at post-test (we can assume there’s no attrition). In their study write-up, the researcher reports sample sizes for each group, means and standard deviations for each group at pre-test and at post-test, and adjusted means at post-test, where the adjustment is done using a basic analysis of covariance, controlling for pre-test scores only. The data layout looks like this:\n\r\rGroup\r\\(N\\)\rPre-test \\(M\\)\rPre-test \\(SD\\)\rPost-test \\(M\\)\rPost-test \\(SD\\)\rAdjusted post-test \\(M\\)\r\r\r\rGroup A\r\\(n_A\\)\r\\(\\bar{x}_{A}\\)\r\\(s_{A0}\\)\r\\(\\bar{y}_{A}\\)\r\\(s_{A1}\\)\r\\(\\tilde{y}_A\\)\r\rGroup B\r\\(n_B\\)\r\\(\\bar{x}_{B}\\)\r\\(s_{B0}\\)\r\\(\\bar{y}_{B}\\)\r\\(s_{B1}\\)\r\\(\\tilde{y}_B\\)\r\r\\(\\vdots\\)\r\\(\\vdots\\)\r\\(\\vdots\\)\r\\(\\vdots\\)\r\\(\\vdots\\)\r\\(\\vdots\\)\r\\(\\vdots\\)\r\r\r\rNote that the write-up does not provide an estimate of the correlation between the pre-test and the post-test, nor does it report a standard deviation or standard error for the mean change-score between pre-test and post-test within each group. All we have are the summary statistics, plus the adjusted post-test scores. We can assume that the adjustment was done according to the basic ANCOVA model, assuming a common slope across groups as well as homoskedasticity and so on. The model is then\r\\[\ry_{ig} = \\alpha_g + \\beta x_{ig} + e_{ig},\r\\]\rfor \\(i = 1,...,n_g\\) and \\(g = 1,...,G\\), where \\(e_{ig}\\) is an independent error term that is assumed to have constant variance across groups.\nFor realz?\rHere’s an example with real data, drawn from Table 2 of Murawski (2006):\n\r\rGroup\r\\(N\\)\rPre-test \\(M\\)\rPre-test \\(SD\\)\rPost-test \\(M\\)\rPost-test \\(SD\\)\rAdjusted post-test \\(M\\)\r\r\r\rGroup A\r25\r37.48\r4.64\r37.96\r4.35\r37.84\r\rGroup B\r26\r36.85\r5.18\r36.46\r3.86\r36.66\r\rGroup C\r16\r37.88\r3.88\r37.38\r4.76\r36.98\r\r\r\rThat study reported this information for each of several outcomes, with separate analyses for each of two sub-groups (LD and NLD). The text also reports that they used a two-level hierarchical linear model for the ANCOVA adjustment. For simplicity, let’s just ignore the hierarchical linear model aspect and assume that it’s a straight, one-level ANCOVA.\n\rThe puzzler\rCalculate an estimate of the standardized mean difference between group \\(B\\) and group \\(A\\), along with the sampling variance of the SMD estimate, that adjusts for pre-test differences between groups. Candidates for numerator of the SMD include the adjusted mean difference, \\(\\tilde{y}_B - \\tilde{y}_A\\) or the difference-in-differences, \\(\\left(\\bar{y}_B - \\bar{x}_B\\right) - \\left(\\bar{y}_A - \\bar{x}_A\\right)\\). In either case, the tricky bit is finding the sampling variance of this quantity, which involves the pre-post correlation. For the denominator of the SMD, you use the post-test SD, either pooled across just groups \\(A\\) and \\(B\\) or pooled across all \\(G\\) groups, assuming a common population variance.\nHave an idea for how to solve this? Post it in the comments or email it to me. Need the solution because you have a study like this in your meta-analysis? Contact me and I’ll share it with you directly. I’m being coy because I’m teaching meta-analysis next semester, and I feel like this would make a good extra credit problem…\n\r","date":1606176000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606176000,"objectID":"232f20496aeb9fc4ff55e45fb6c58d7b","permalink":"/ancova-puzzler/","publishdate":"2020-11-24T00:00:00Z","relpermalink":"/ancova-puzzler/","section":"post","summary":"Doing effect size calculations for meta-analysis is a good way to lose your faith in humanity—or at least your faith in researchers’ abilities to do anything like sensible statistical inference.","tags":["meta-analysis","effect size","standardized mean difference"],"title":"An ANCOVA puzzler","type":"post"},{"authors":["John Protzko","Jon Krosnick","Leif Nelson","Brian Nosek","Jordan Axt","Matt Berent","Nicholas Buttrick","Matthew DeBell","Charles R. Ebersole","Sebastian Lundmark","Bo MacInnis","Michael O'Donnell","Hannah Perfecto","James E. Pustejovsky","Scott Roeder","Jan Walleczek","Jonathan W. Schooler"],"categories":null,"content":"","date":1599696000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1599696000,"objectID":"98aaf035f8b01ffc044dae70c9bcbd7f","permalink":"/publication/decline-effects/","publishdate":"2020-09-10T00:00:00Z","relpermalink":"/publication/decline-effects/","section":"publication","summary":"Failures to replicate evidence of new discoveries have forced scientists to ask whether this unreliability is due to suboptimal implementation of optimal methods or whether presumptively optimal methods are not, in fact, optimal. This paper reports an investigation by four coordinated laboratories of the prospective replicability of 16 novel experimental findings using current optimal practices: high statistical power, preregistration, and complete methodological transparency. In contrast to past systematic replication efforts that reported replication rates averaging 50%, replication attempts here produced the expected effects with significance testing (p","tags":["replication","meta-analysis"],"title":"High replicability of newly-discovered social-behavioral findings is achievable.","type":"publication"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rIt’s taken me a while to finally get around to updating my website with some personal news. I’ve moved from UT Austin to the UW Madison School of Education, where I am now an associate professor in the Educational Psychology Department’s Quantitative Methods program. We left Austin at the very end of July, arriving in Madison on August 1st. Our moving truck took a bit longer to arrive, but we’re now more or less installed in our new (or rather old–1950’s era) home. I grew up in Wisconsin (in the Milwaukee area), so this move brings us much closer to my family, who have already come to visit. We’ve also already been enjoying the fantastic bike paths and facilities that Madison has to offer.\nOn a professional level, I’m very much looking forward to the opportunities that the School of Education and Educational Psychology Department present, especially to opportunities for collaboration with new colleagues and students. I’m planning to offer a course on research synthesis and meta-analysis this coming Spring semester—something I’ve never had the opportunity to teach in a semester-long format, actually—and I’m looking forward to offering my own pedagogical perspective on material that I think about constantly in a research context. Gene Glass, who is credited as the originator of the term meta-analysis and who conducted some of the first meta-analyses within the social sciences, received his Ph.D. in Educational Psychology from UW Madison in 1965, so perhaps I’ll be able to channel a bit of his spirit in my course.\nEven as I’m excited to get started at Madison, I will also very much miss my colleagues at UT Austin, who were so supportive during my pre-tenure phase. Because of COVID, I didn’t really get to say a proper farewell before we skipped town. I am continuing to advise several doctoral students in the Quantitative Methods program though, so we will likely get to connect in video meetings, at least.\nMoving during the COVID pandemic has presented some challenges (logistical, emotional, and family-related) for me, though I know many others have had to deal with far worse. As we all weather this together, please feel free to leave a comment or drop me a line if you’d like to talk about stats, meta-analysis, R programming, or what-not.\n","date":1598572800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598572800,"objectID":"1305db4a3dd69ce6e9ff75054eb61210","permalink":"/from-longhorn-to-badger/","publishdate":"2020-08-28T00:00:00Z","relpermalink":"/from-longhorn-to-badger/","section":"post","summary":"It’s taken me a while to finally get around to updating my website with some personal news. I’ve moved from UT Austin to the UW Madison School of Education, where I am now an associate professor in the Educational Psychology Department’s Quantitative Methods program.","tags":[],"title":"From Longhorn to Badger","type":"post"},{"authors":["Melissa A. Rodgers","James E. Pustejovsky"],"categories":null,"content":"","date":1594512000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1594512000,"objectID":"bded9c825b641b84f6e7fe8d5932e275","permalink":"/publication/selective-reporting-with-dependent-effects/","publishdate":"2021-04-01T00:00:00Z","relpermalink":"/publication/selective-reporting-with-dependent-effects/","section":"publication","summary":"Meta-analysis is a set of statistical tools used to synthesize results from multiple studies evaluating a common research question. Two methodological challenges when conducting meta-analysis include selective reporting and correlated dependent effect sizes. Selective reporting is often a result of selective publication practices based on the statistical significance of study findings, which threatens the validity of meta-analytic results. One of the main sources of dependent effect sizes is the inclusion of multiple outcome measures from a primary study. This violates conventional, univariate meta-analytic techniques. Meta-analysts lack validated methods to detect the presence of selective reporting while incorporating methods to handle dependent effect sizes. This study evaluates currently available univariate selective reporting methods, when ignoring dependence, selecting one effect size per study, or aggregating dependent correlated effect sizes. This study also proposes and examines an Egger’s Regression variant incorporated with Robust Variance Estimation (RVE) to handle within-study dependence. A Monte Carlo simulation study assess the performance of the methods for Type I error rates in the absence of selective reporting, and power to detect selective reporting when introduced. Ignoring dependence inflates Type I error rates for all univariate detection methods. Type I error rates are maintained with regression tests when dependent effect sizes are sampled, aggregated or modeled using RVE. However, all selective reporting methods evaluated in this study have little to no power to detect selection bias, except under strong selection censoring.","tags":["meta-analysis","publication bias","dependent effect sizes"],"title":"Evaluating meta-analytic methods to detect selective reporting in the presence of dependent effect sizes","type":"publication"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rIf you’ve ever had class with me or attended one of my presentations, you’ve probably heard me grouse about how statisticians are mostly awful about naming things.1 A lot of the terminology in our field is pretty bad and ineloquent. As a leading example, look no further than Rubin and Little’s classification of missing data mechanisms as missing completely at random (MCAR), missing at random (MAR), or missing not at random (MNAR). Clear as mud, and the last one sounds like something you’d see on a handmade sign with a picture of someone’s pet puppy who wandered off last week.\nAs another example, consider that introductory statistics students always struggle to distinguish between no less than three different concepts that are all called “variance”: population variance, sample variance, and sampling variance.2 Unless the instructor also took diction training from the Royal Shakespeare Company, it’s no wonder that a fair number of students are left confused.\nIn this post, I will try to clarify (at least a little bit) another mess of terminology that crops up a lot in my work on meta-analysis: what do we mean when we say a model or method is “multivariate”? In the context of meta-analysis methods, I think there are at least three distinct senses in which this term is used:\n\rAs an umbrella term for models/methods where there is more than one effect size estimate per study,\rAs a description for a class of methods within that broad umbrella, where certain aspects of the model are treated as known, or\rAs a description for a class of models for multivariate effect size estimates, where each effect size estimate from a study falls into one of a set of distinct categories.\r\rLet me explain what I mean by each of these.\nMultivariate handwaving\rIn the context of meta-analysis, the broadest meaning of “multivariate” is any method used for modeling data that includes more than one effect size estimate in some or all of the included studies. Formally, the term would apply to any model appropriate for a set of \\(k\\) studies, where study \\(j\\) includes \\(n_j\\) effect size estimates, and where the effect size estimates would be denoted \\(T_{ij}\\), for \\(i = 1,...,n_j\\) and \\(j = 1,...,k\\).\nAs it is used here, “multivariate” is really an umbrella term that could encompass a wide variety of methods and models, including multi-level meta-analysis or meta-regression models, multivariate methods in the narrower senses I will describe subsequently, and even robust variance estimation methods. It would also encompass techniques for handling this sort of data structure that aren’t strictly models, such as aggregating effect size estimates to the level of the study or using Harris Cooper’s “shifting unit-of-analysis” method (Cooper, 1998).\rThis usage of “multivariate” involves a bit too much hand-waving for my taste (although I’ve been guilty of using the term this way in the past). I think a better, clearer term for this broad class of methods would be to call them methods for meta-analysis of dependent effect sizes.\n\rMultivariate sampling errors\rAnother sense in which “multivariate” is used pertains to a certain class of models for dependent effect sizes. In particular, “multivariate meta-analysis” sometimes means a model where the sampling variances and covariances of the effect size estimates are treated as fully known. Say that each effect size estimate \\(T_{ij}\\) has a corresponding true effect size parameter \\(\\theta_{ij}\\), so that the sampling error is \\(e_{ij} = T_{ij} - \\theta_{ij}\\), or\r\\[\rT_{ij} = \\theta_{ij} + e_{ij}.\r\\]\rTypically, meta-analysis techniques treat the sampling errors as having known variances, \\(\\text{Var}(e_{ij}) = \\sigma_{ij}^2\\) for known \\(\\sigma_{ij}^2\\).\rHere, a multivariate meta-analysis would go a step further and make assumptions that \\(\\text{Cov}(e_{hj}, e_{ij}) = \\rho_{hij}\\sigma_{hj} \\sigma_{ij}\\) for known correlations \\(\\rho_{hij}\\), \\(h,i = 1,...,n_j\\) and \\(j=1,...,k\\).\rTypically, the sampling variances and covariances would play into how the model is estimated and how one conducts inference and gets standard errors on things, etc.\nBecker (2000) and Gleser \u0026amp; Olkin (2009) describe a whole slew of different situations where meta-analysts will encounter multiple effect size estimates within a given study, and both provide formulas for the covariances between those effect sizes.\rIn some situations, these covariances can be calculated just based on primary study sample sizes or other information readily available from study reports.\rIn other situations (such as when one calculates standardized mean differences for each of several outcomes on a common set of participants), the information needed to calculate covariances might not be available, which is where methods like robust variance estimation come in.\rWith this meaning of the term, multivariate meta-analysis methods are those that both directly model the dependent effects structure and that treat the sampling covariances as known. They are therefore distinct from methods, such as robust variance estimation, that do not rely on knowing the exact variance-covariance structure of the sampling errors.\rIn my own work, I find it helpful to be able to draw this distinction, so I rather like this usage of “multivariate.” This will surely irritate some statisticians, though, who prefer the third, stricter meaning of the term.\n\rStrictly multivariate models\rA third meaning of multivariate is to denote a class of models for multivariate data, meaning data where each unit is measured on several dimensions or characteristics. In the meta-analysis context, multivariate effect sizes are ones where, for each included study or sample, we have effect sizes describing outcomes (e.g., treatment effects) on one or more dimensions.\rFor example, say that we have a bunch of studies examining some sort of educational intervention, and each study reports effect sizes describing the intervention’s impact on a) reading performance, b) social studies achievement, and/or c) language arts achievement. What differentiates this sort of multivariate data from the first, “umbrella” sense of the term is that with strictly multivariate data, no study has more than one effect size within a given dimension. In contrast, meta-analysis of dependent effect sizes deal with data structures that are not necessarily so tidy and organized, such that we might not be able to classify each effect size into one of a finite and exhaustive set of categories.\nWhen working with strictly multivariate data like this, a multivariate meta-analysis (or meta-regression) model would entail estimating average effects (or regression coefficients) for each dimension rather than aggregating across dimensions. This class of models was discussed extensively in an excellent article by Jackson et al. (2011).3 With my example of educational intervention studies, we would estimate average impacts on reading performance, social studies achievement, and language arts achievement. Estimating an overall aggregate effect on academic achievement would make little sense here, because we’d be mixing apples, oranges, and kiwis.\nFormally, this sort of data structure and model can be described as follows. As previously, say that we have a set of \\(k\\) studies, where study \\(j\\) has \\(n_j\\) effect sizes, \\(T_{ij}\\), and correspoding sampling variances \\(\\sigma_{ij}^2\\), both for \\(i = 1,...,n_j\\) and \\(j = 1,...k\\). Effect size \\(i\\) from study \\(j\\) can be classified into one of \\(C\\) dimensions. Let \\(d^c_{ij}\\) be an indicator for whether effect \\(i\\) falls into dimension \\(c\\), for \\(c = 1,...,C\\). With a strictly multivariate structure, there is never more than one effect per category, so \\(\\sum_{i=1}^{n_j} d^c_{ij} \\leq 1\\) for each \\(c = 1,...,C\\) and \\(j = 1,...,k\\). A typical multivariate random effects model would then be\r\\[\rT_{ij} = \\sum_{c=1}^C \\left(\\mu_c + v_{cj}\\right) d^c_{ij} + e_{ij},\r\\]\rwhere \\(\\mu_c\\) is the average effect size for category \\(c\\), \\(v_{cj}\\) is a random effect for category \\(c\\) in study \\(j\\), and \\(e_{ij}\\) is the sampling error term. The classic assumption about the random effects is that they are dependent within study, so\r\\[\r\\text{Var}(v_{cj}) = \\tau^2_c \\qquad \\text{and} \\qquad \\text{Cov}(v_{bj}, v_{cj}) = \\tau_{bc}\r\\]\rfor \\(b,c = 1,...,C\\). Typically, these sorts of models would also rely on assumptions about the correlations between the sampling errors, just as with the second meaning of multivariate. Thus, to complete the model, we would have \\(\\text{Cov}(e_{hj}, e_{ij}) = \\rho_{hij}\\sigma_{hj}\\sigma_{ij}\\) for known \\(\\rho_{hij}\\). In practice, we might want to impose some common structure to the correlations across studies, such as using \\(\\rho_{hij}\\)’s that depend on the dimensions being correlated but are common across studies. Formally, we would have\r\\[\r\\rho_{hij} = \\sum_{b=1}^C \\sum_{c=1}^C d^b_{ij} \\ d^c_{ij} \\ \\rho_{bc}.\r\\]\rOf course, even getting this level of detail about correlations between effect sizes might often be pretty challenging.\nIn a strictly multivariate meta-regression model, we would also allow the coefficients for each predictor to be specific to each category, so that\r\\[\rT_{ij} = \\sum_{c=1}^C \\left(\\mathbf{x}_{ij}\\boldsymbol\\beta_c + v_{cj}\\right) d^c_{ij} + e_{ij},\r\\]\rIn my example of educational intervention impact studies, say that are interested in whether the effects differ between quasi-experimental studies and true randomized control trials, and whether the effects differ based on the proportion of the sample that was economically disadvantaged. The strictly multivariate model would always involve interacting these predictors with the outcome category. In R’s equation notation, the meta-regression specification would be\nES ~ 0 + Cat + Cat:RCT + Cat:disadvantaged_pct\rIn contrast, in a generic meta-regression for dependent effect sizes, we might not include all of the interactions, and instead assume that the associations of the predictors were constant across outcome dimensions, as in\nES ~ 0 + outcome_cat + RCT + college_pct\rIn the strict sense of the term, the model without interactions is no longer really a multivariate meta-regression.\n\rRemarks\rAn interesting property of strict multivariate meta-analysis models is that they involve partial pooling—or “borrowing of strength”—across dimensions (Riley et al., 2007, 2017). Even though the model has separate coefficients for each dimension, the estimates for a given dimension are influenced by the available effect sizes for all dimensions. For instance, in the meta-analysis of educational intervention studies, the average impact on reading performance outcomes is based in part on the effect size estimates for the social studies and language arts performance. This happens because the model treats all of the dimensions as correlated—through the correlated sampling errors and, potentially, through the correlated random effects structure. Copas et al. (2018) examine how this works and propose a diagnostic plot to understand how it happens in application. Kirkham et al. (2012) also show that the borrowing of strength phenomenon can partially mitigate bias from selective outcome reporting. These concepts could be quite relevant even beyond the “strict” multivariate meta-analysis context in which they have been explored. It strikes me that it would be useful to investigate them in the more general context of meta-analysis with dependent effect sizes—that is, multivariate meta-analysis in the first, broadest sense.\n\rReferences\rBecker, B. J. (2000). Multivariate meta-analysis. In S. D. Brown \u0026amp; H. E. A. Tinsley (Eds.), Handbook of applied multivariate statistics and mathematical modeling (pp. 499–525). Academic Press. https://doi.org/10.1016/B978-012691360-6/50018-5\n\rCooper, H. M. (1998). Synthesizing Research: A Guide for Literature Reviews (3rd ed.). Sage Publications, Inc.\n\rCopas, J. B., Jackson, D., White, I. R., \u0026amp; Riley, R. D. (2018). The role of secondary outcomes in multivariate meta-analysis. Journal of the Royal Statistical Society: Series C (Applied Statistics), 67(5), 1177–1205. https://doi.org/10.1111/rssc.12274\n\rGleser, L. J., \u0026amp; Olkin, I. (2009). Stochastically dependent effect sizes. In H. Cooper, L. V. Hedges, \u0026amp; J. C. Valentine (Eds.), The handbook of research synthesis and meta-analysis (2nd ed., pp. 357–376). Russell Sage Foundation.\n\rJackson, D., Riley, R. D., \u0026amp; White, I. R. (2011). Multivariate meta-analysis: Potential and promise. Statistics in Medicine. https://doi.org/10.1002/sim.4172\n\rKirkham, J. J., Riley, R. D., \u0026amp; Williamson, P. R. (2012). A multivariate meta-analysis approach for reducing the impact of outcome reporting bias in systematic reviews. Statistics in Medicine, 31(20), 2179–2195. https://doi.org/10.1002/sim.5356\n\rRiley, R. D., Abrams, K. R., Lambert, P. C., Sutton, A. J., \u0026amp; Thompson, J. R. (2007). An evaluation of bivariate random-effects meta-analysis for the joint synthesis of two correlated outcomes. Statistics in Medicine, 26(1), 78–97. https://doi.org/10.1002/sim.2524\n\rRiley, R. D., Jackson, D., Salanti, G., Burke, D. L., Price, M., Kirkham, J., \u0026amp; White, I. R. (2017). Multivariate and network meta-analysis of multiple outcomes and multiple treatments: Rationale, concepts, and examples. BMJ, j3932. https://doi.org/10.1136/bmj.j3932\n\r\r\r\r“Mostly” rather than “uniformly” due to exceptions like Brad Efron (a.k.a. Mr. Bootstrap) and Rob Tibshirani (a.k.a. Mr. Lasso).↩︎\n\rAnd then consider the square roots of these quantities, respectively: population standard deviation, sample standard deviation, and standard error. WTF?↩︎\n\rRead this article! It’s essential. And it comes with pages and pages of commentary by other statisticans.↩︎\n\r\r\r","date":1593216000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593216000,"objectID":"2447bf938fcedc74976a3241c8b587ea","permalink":"/what-does-multivariate-mean/","publishdate":"2020-06-27T00:00:00Z","relpermalink":"/what-does-multivariate-mean/","section":"post","summary":"If you’ve ever had class with me or attended one of my presentations, you’ve probably heard me grouse about how statisticians are mostly awful about naming things.1 A lot of the terminology in our field is pretty bad and ineloquent.","tags":["meta-analysis","multivariate","dependent effect sizes"],"title":"What do meta-analysts mean by 'multivariate' meta-analysis?","type":"post"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rOne common question about multivariate/multi-level meta-analysis is how such models assign weight to individual effect size estimates. When a version of the question came up recently on the R-sig-meta-analysis listserv, Dr. Wolfgang Viechtbauer offered a whole blog post in reply, demonstrating how weights work in simpler fixed effect and random effects meta-analysis and then how things get more complicated in multivariate models. I started thumb-typing my own reply as well, but then decided it would be better to write up a post so that I could use a bit of math notation (and to give my thumbs a break). So, in this post I’ll try to add some further intuition on how weights work in certain multivariate meta-analysis models. Most of the discussion will apply to models that include multiple level of random effects, but no predictors. I’ll also comment briefly on meta-regression models with only study-level predictor variables, and finally give some pointers to work on more complicated models.\nA little background\rIt’s helpful to start by looking briefly at the basic fixed effect and random effects models, assuming that we’ve got a set of studies that each contribute a single effect size estimate so everything’s independent. Letting \\(T_j\\) be the effect size from study \\(j\\), with sampling variance \\(V_j\\), both for \\(j = 1,...,k\\), the basic random effects model is:\r\\[\rT_j = \\mu + \\eta_j + e_j\r\\]\rwhere \\(\\mu\\) is the overall average effect size, \\(v_j\\) is a random effect with variance \\(\\text{Var}(\\eta_j) = \\tau^2\\) and \\(e_j\\) is a sampling error with known variance \\(V_j\\). The first step in estimating this model is to estimate \\(\\tau^2\\). There’s lots of methods for doing so, but let’s not worry about those details—just pick one and call the estimate \\(\\hat\\tau^2\\). Then, to estimate \\(\\mu\\), we take a weighted average of the effect size estimates:\r\\[\r\\hat\\mu = \\frac{1}{W} \\sum_{j=1}^k w_j T_j, \\qquad \\text{where} \\quad W = \\sum_{j=1}^k w_j.\r\\]\rThe weights used in the weighted average are chosen to make the overall estimate as precise as possible (i.e., having the smallest possible sampling variance or standard error). Mathematically, the best possible weights are inverse variance weights, that is, setting the weight for each effect size estimate proportional to the inverse of how much variance there is in each estimate. With inverse variance weights, larger studies with more precise effect size estimates will tend to get more weight and smaller, noisier studies will tend to get less weight.\nIn the basic random effects model, the weights for each study are proportional to\r\\[\rw_j = \\frac{1}{\\hat\\tau^2 + V_j},\r\\]\rfor \\(j = 1,...,k\\). The denominator term here includes both the (estimated) between-study heterogeneity and the sampling variance because both terms contribute to how noisy the effect size estimate is. In the fixed effect model, we ignore between-study heterogeneity so the weights are inversely proportional to the sampling variances, with \\(w_j = 1 / V_j\\). In the random effects model, larger between-study heterogeneity will make the weights closer to equal, while smaller between-study heterogeneity will lead to weights that tend to emphasize larger studies with more precise estimates. In the remainder, I’ll show that there are some similar dynamics at work in a more complicated, multivariate meta-analysis model\n\rA multivariate meta-analysis\rNow let’s consider the case where some or all studies in our synthesis contribute more than one effect size estimate. Say that we have effect sizes \\(T_{ij}\\), where \\(i = 1,...,n_j\\) indexes effect size estimates within study \\(j\\) and \\(j\\) indexes studies, for \\(j = 1,...,k\\). Say that effect size estimate \\(T_{ij}\\) has sampling variance \\(V_{ij}\\), and there is some sampling correlation between effect sizes \\(h\\) and \\(i\\) within study \\(j\\), denoted \\(r_{hij}\\).\nThere are many models that a meta-analyst might consider for this data structure. A fairly common one would be a model that includes random effects not only for between-study heterogeneity (as in the basic random effects model) but also random effects capturing within-study heterogeneity in true effect sizes. Let me write this model heirarchically, as\r\\[\r\\begin{align}\rT_{ij} \u0026amp;= \\theta_j + \\nu_{ij} + e_{ij} \\\\\r\\theta_j \u0026amp;= \\mu + \\eta_j\r\\end{align}\r\\]\rIn the first line of the model, \\(\\theta_j\\) denotes the average effect size parameter for study \\(j\\), \\(\\nu_{ij}\\) captures within-study heterogeneity in the true effect size parameters and \\(e_{ij}\\) is a sampling error. Above, I’ve assumed that we know the structure of the sampling errors, so \\(\\text{Var}(e_{ij}) = V_{ij}\\) and \\(\\text{Cov}(e_{hj}, e_{ij}) = r_{hij} \\sqrt{V_{hj} V_{ij}}\\). Let’s also denote the within-study variance as \\(\\omega^2\\), so \\(\\text{Var}(\\nu_{ij}) = \\omega^2\\).\rIn the second line of the model, \\(\\mu\\) is still the overall average effect size across all studies and effect sizes within studies and \\(\\eta_j\\) is a between-study error, with \\(\\text{Var}(\\eta_j) = \\tau^2\\), capturing the degree of heterogeneity in the average effect sizes (the \\(\\theta_j\\)’s) across studies.1\nOne thing to note about this model is that it treats all of the effect sizes as coming from a population with a common mean \\(\\mu\\). Some statisticians might object to calling it a multivariate model because we’re not distinguishing averages for different dimensions (or variates) of the effect sizes. To this I say: whatev’s, donkey! I’m calling it multivariate because you have to use the rma.mv() function from the metafor package to estimate it. I will acknowledge, though, that there will often be reason to use more complicated models, for example by replacing the overall average \\(\\mu\\) with some meta-regression \\(\\mathbf{x}_{ij} \\boldsymbol\\beta\\). That’s a discussion for another day. For now, we’re only going to consider the model with an overall average effect size parameter \\(\\mu\\). The question is, how do the individual effect size estimates \\(T_{ij}\\) contribute to the estimate of this overall average effect?\n\rEqually precise effect size estimates\rTo make some headway, it is helpful to first consider an even more specific model where, within a given study, all effect size estimates are equally precise and equally correlated. In particular, let’s assume that for each study \\(j\\), the sampling variances are all equal, with \\(V_{ij} = V_j\\) for \\(i = 1,...,n_j\\), and the correlations between the sampling errors are also all equal, with \\(r_{hij} = r_j\\) for \\(h,i = 1,...,n_j\\).\nThese assumptions might not be all that far-fetched. Within a given study, if the effect size estimates are for different measures of a common construct, it’s not unlikely that they would all be based on similar sample sizes (+/- a bit of item non-response). It might be a bit less likely if the effect size estimates are for treatment effects from different follow-up times (since drop-out/non-response tends to increase over time) or different treatment groups compared to a common control group—but still perhaps not entirely unreasonable. Further, it’s rather uncommon to have good information about the correlations between effect size estimates from a given study (because primary studies don’t often report all of the information needed to calculate these correlations). In practice, meta-analysts might need to simply make a rough guess about the correlations and then use robust variance estimation and/or sensitivity analysis to check themselves. And if we’re just ball-parking, then we’ll probably assume a single correlation for all of the studies.\nThe handy thing about this particular scenario is that, because all of the effect size estimates within a study are equally precise and equally correlated, the most efficient way to estimate an average effect for a given study is to just take the simple average (and, intuitively, this seems like the only sensible thing to do). To be precise, consider how we would estimate \\(\\theta_j\\) for a given study \\(j\\). The most precise possible estimate is simply\r\\[\r\\hat\\theta_j = \\frac{1}{n_j} \\sum_{i=1}^{n_j} T_{ij}.\r\\]\rAnd we could do the same for each of the other studies, \\(j = 1,...,k\\).\rIt turns out that the estimate of the overall average effect size is a weighted average of these study-specific average effect sizes:\r\\[\r\\hat\\mu = \\frac{1}{W} \\sum_{j=1}^k w_j \\hat\\theta_j,\r\\]\rfor some weights \\(w_1,...,w_k\\). But what are these weights? Just like in the basic random effects model, they are inverse-variance weights. It’s just that the variance is a little bit more complicated.\nConsider how precise each of the study-specific estimates are, relative to the true effects in their respective studies. Conditional on the true effect \\(\\theta_j\\),\r\\[\r\\text{Var}(\\hat\\theta_j | \\theta_j) = \\frac{1}{n_j}\\left(\\omega^2 + (n_j - 1) r_j V_j + V_j\\right).\r\\]\rWithout conditioning on \\(\\theta_j\\), the variance of the \\(\\hat\\theta_j\\) estimates also includes a term for variation in the true study-specific average effect sizes, becoming\r\\[\r\\text{Var}(\\hat\\theta_j) = \\tau^2 + \\frac{1}{n_j}\\left(\\omega^2 + (n_j - 1) r_j V_j + V_j\\right).\r\\]\rThe weights used in estimating \\(\\mu\\) are the inverse of this quantity:\r\\[\rw_j = \\frac{1}{\\tau^2 + \\frac{1}{n_j}\\left(\\omega^2 + (n_j - 1) r_j V_j + V_j\\right)}.\r\\]\rWithin a study, each individual effect size gets an \\(n_j^{th}\\) of this study-level weight. We can therefore write the overall average as\r\\[\r\\hat\\mu = \\frac{1}{W} \\sum_{j=1}^k \\sum_{i=1}^{n_j} w_{ij} T_{ij},\r\\]\rwhere\r\\[\rw_{ij} = \\frac{1}{n_j \\tau^2 + \\omega^2 + (n_j - 1) r_j V_j + V_j}.\r\\]\n\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; HEAD\rThere are several things worth noting about this expression for the weights. First, suppose that there is little between-study or within-study heterogeneity, so \\(\\tau^2\\) and \\(\\omega^2\\) are both close to zero. Then the weights are driven by the number of effect sizes within the study (\\(n_j\\)), the sampling variance of those effect sizes (\\(V_j\\)) and their correlation \\(r_j\\). If \\(r_j\\) is near one, then averaging together a bunch of highly correlated estimates doesn’t improve precision much, relative to just using one of the effect sizes. The study-specific average effect estimate will therefore have variance close to \\(V_j\\) (i.e., the variance of a single effect size estimate). If \\(r_j\\) is below one, then averaging yields a more precise estimate than any of the individual effect sizes, and averaging together more effect sizes will yield more precise estimate at the study level. If the assumed correlations are reasonably accurate, the weights used in the multi-variate meta-analysis will appropriately take into account the number of effect sizes within each study and the precision of those effect sizes.\r=======\rThere are several things worth noting about this expression for the weights. First, suppose that there is little between-study or within-study heterogeneity, so \\(\\tau^2\\) and \\(\\omega^2\\) are both close to zero. Then the weights are driven by the number of effect sizes within the study (\\(n_j\\)), the sampling variance of those effect sizes (\\(V_j\\)) and their correlation \\(r_j\\). If \\(r_j\\) is near one, then averaging together a bunch of highly correlated estimates doesn’t improve precision much, relative to just using one of the effect sizes. The study-specific average effect estimate will therefore have variance close to \\(V_j\\) (i.e., the variance of a single effect size estimate). If \\(r_j\\) is below one, then averaging yields a more precise estimate than any of the individual effect sizes, and averaging together more effect sizes will yield more precise estimate at the study level. If the assumed correlations are reasonably accurate, the weights used in the multivariate meta-analysis will appropriately take into account the number of effect sizes within each study and the precision of those effect sizes.\r\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; origin/master\nSecond, now suppose that there is no between-study heterogeneity (\\(\\tau^2 = 0\\)) but there is positive within-study heterogeneity. Larger degrees of within-study heterogeneity will tend to equalize the weights at the effect size level, regardless of how effect size estimates are nested within studies. When there is within-study heterogeneity, averaging together a bunch of estimates will yield a more precise estimate of study-specific average effects. Therefore, when \\(\\omega^2\\) is larger, studies with more effect sizes will tend to get a relatively larger share of the weight.2\nThird and finally, between-study heterogeneity will tend to equalize the weights at the study level, so that the overall average is pulled closer to a simple average of the study-specific average effects. This works very much like in basic random effects meta-analysis, where increased heterogeneity will lead to weights that are closer to equal and an average effect size estimate that is closer to a simple average.\n\rA computational example\rI think it’s useful to verify algebraic results like the ones I’ve given above by checking that you can reproduce them with real data. I’ll use the corrdat dataset from the robumeta package for illustration. The dataset has one duplicated row in it (I have no idea why!), which I’ll remove before analyzing further.\nlibrary(dplyr)\rdata(corrdat, package = \u0026quot;robumeta\u0026quot;)\rcorrdat \u0026lt;- corrdat %\u0026gt;%\rdistinct(studyid, esid, .keep_all = TRUE)\rThis dataset included a total of 171 effect size estimates from 39 unique studies. For each study, between 1 and 18 eligible effect size estimates were reported. Here is a histogram depicting the number of studies by the number of reported effect size estimates:\nHere is the plot of the variances of each effect size versus the study IDs:\nFor most of the studies, the effect sizes have very similar sampling variances. One exception is study 9, where two of the effect sizes have variances of under 0.20 and the other two effect sizes have variances in excess of 0.35. Another exception is study 30, which has one effect size with much larger variance than the others.\nJust for sake of illustration, I’m going to enforce my assumption that effect sizes have equal variances within each study by recomputing the sampling variances as the average sampling variance within each study. I will then impute a sampling variance-covariance matrix for the effect sizes, assuming a correlation of 0.7 for effects from the same study:\nlibrary(clubSandwich)\rcorrdat \u0026lt;- corrdat %\u0026gt;%\rgroup_by(studyid) %\u0026gt;%\rmutate(V_bar = mean(var)) %\u0026gt;%\rungroup()\rV_mat \u0026lt;- impute_covariance_matrix(vi = corrdat$V_bar, cluster = corrdat$studyid,\rr = 0.7)\rWith this variance-covariance matrix, I can then estimate the multivariate meta-analysis model:\nlibrary(metafor)\rMVMA_fit \u0026lt;- rma.mv(yi = effectsize, V = V_mat, random = ~ 1 | studyid / esid,\rdata = corrdat)\rsummary(MVMA_fit)\r## ## Multivariate Meta-Analysis Model (k = 171; method: REML)\r## ## logLik Deviance AIC BIC AICc ## -94.7852 189.5703 195.5703 204.9777 195.7149 ## ## Variance Components:\r## ## estim sqrt nlvls fixed factor ## sigma^2.1 0.0466 0.2159 39 no studyid ## sigma^2.2 0.1098 0.3314 171 no studyid/esid ## ## Test for Heterogeneity:\r## Q(df = 170) = 1141.4235, p-val \u0026lt; .0001\r## ## Model Results:\r## ## estimate se zval pval ci.lb ci.ub ## 0.2263 0.0589 3.8413 0.0001 0.1108 0.3417 *** ## ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rBased on this model, between-study heterogeneity is estimated as \\(\\hat\\tau = 0.216\\) and within-study heterogeneity is estimated as \\(\\hat\\omega = 0.331\\), both of which are quite high. The overall average effect size estimate is 0.226, with a standard error of 0.059.\nI’ll first get the weights used in rma.mv to compute the overall average. The weights are represented as an \\(N \\times N\\) matrix. Taking the row or column sums, then rescaling by the total, gives the weight assigned to each effect size estimate:\nW_mat \u0026lt;- weights(MVMA_fit, type = \u0026quot;matrix\u0026quot;)\rcorrdat$w_ij_metafor \u0026lt;- colSums(W_mat) / sum(W_mat)\rTo verify that the formulas above are correct, I’ll use them to directly compute weights:\nr \u0026lt;- 0.7\rtau_sq \u0026lt;- MVMA_fit$sigma2[1]\romega_sq \u0026lt;- MVMA_fit$sigma2[2]\rcorrdat_weights \u0026lt;- corrdat %\u0026gt;%\rgroup_by(studyid) %\u0026gt;%\rmutate(\rn_j = n(),\rw_ij = 1 / (n_j * tau_sq + omega_sq + (n_j - 1) * r * V_bar + V_bar)\r) %\u0026gt;%\rungroup() %\u0026gt;%\rmutate(\rw_ij = w_ij / sum(w_ij)\r)\rThe weights I computed are perfectly correlated with the weights used rma.mv, as can be seen in the plot below.\nggplot(corrdat_weights, aes(w_ij, w_ij_metafor)) + geom_point() + theme_minimal()\rIf we remove the within-study random effect term from the model, the weights will be equivalent to setting \\(\\omega^2\\) to zero, but with a different estimate of \\(\\tau^2\\).\nMVMA_no_omega \u0026lt;- rma.mv(yi = effectsize, V = V_mat, random = ~ 1 | studyid,\rdata = corrdat)\rMVMA_no_omega\r## ## Multivariate Meta-Analysis Model (k = 171; method: REML)\r## ## Variance Components:\r## ## estim sqrt nlvls fixed factor ## sigma^2 0.0951 0.3084 39 no studyid ## ## Test for Heterogeneity:\r## Q(df = 170) = 1141.4235, p-val \u0026lt; .0001\r## ## Model Results:\r## ## estimate se zval pval ci.lb ci.ub ## 0.2235 0.0619 3.6122 0.0003 0.1022 0.3448 *** ## ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; HEAD\rRe-fitting the model with rma.mv() gives an between-study heterogeneity estimate of \\(\\hat\\tau = 0.308\\) and an overall average effect size estimate of \\(\\hat\\mu = 0\\). Using this estimate, I’ll compute the weights based on the formula and then use those weights to determine the overall average effect size estimate.\r=======\rRe-fitting the model with rma.mv() gives an between-study heterogeneity estimate of \\(\\hat\\tau = 0.308\\) and an overall average effect size estimate of \\(\\hat\\mu = 0.224\\). Using this estimate, I’ll compute the weights based on the formula and then use those weights to determine the overall average effect size estimate.\r\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; origin/master\ntau_sq \u0026lt;- MVMA_no_omega$sigma2\rcorrdat_weights \u0026lt;- corrdat_weights %\u0026gt;%\rmutate(\rw_ij_no_omega = 1 / (n_j * tau_sq + (n_j - 1) * r * V_bar + V_bar),\rw_ij_no_omega = w_ij_no_omega / sum(w_ij_no_omega)\r)\rwith(corrdat_weights, weighted.mean(effectsize, w = w_ij_no_omega))\r## [1] 0.2235231\rThis matches the output of rma.mv().\nHere is a plot showing the weights of individual effect sizes for each study. In blue are the weights under the assumption that \\(\\omega^2 = 0\\). In green are the weights allowing for \\(\\omega^2 \u0026gt; 0\\). It’s notable here that introducing the within-study heterogeneity term leads to pretty big changes in the weights for some studies. In particular, studies that have only a single effect size estimate (e.g., studys 7, 8, 22, 25, 28) lose a lot of weight when \\(\\omega^2 \u0026gt; 0\\). That’s partially because \\(\\omega^2\\) tends to pull weight towards studies with more effect sizes, and partially because of the change in the estimate of \\(\\tau^2\\), which tends to equalize the weight assigned to each study.\nBelow is a plot illustrating the changes in study-level weights (i.e., aggregating the weight assigned to each study). The bar color corresponds to the number of effect size estimates in each study; light grey studies have just one effect size, while studies with more effect sizes are more intensly purple. The notable drops in weight for studies with a single effect size estimate (light grey) are visible here too. Studies with more effect sizes (e.g., studies 2, 15, 30, with dark purple bars) gain weight when we allow \\(\\omega^2\\) to be greater than zero.\n## `summarise()` ungrouping output (override with `.groups` argument)\r\rNow without compound symmetry\rIf we remove the restrictions that effect sizes from the same study have the same sampling variance and are equi-correlated, then the weights get a little bit more complicated. However, the general intuitions carry through. Let’s now consider the model with arbitrary sampling variance \\(V_{ij}\\) and sampling correlations within studies \\(r_{hij}\\). The most efficient estimate of the study-specific average effect is now a weighted average, with weights that depend on both the variances and covariances of the effect size estimates within each study. Let\r\\[\r\\boldsymbol{\\hat\\Sigma}_j = \\hat\\omega^2 \\mathbf{I}_j + \\mathbf{V}_j,\r\\]\rwhere \\(\\mathbf{I}_j\\) is an \\(n_j \\times n_j\\) identity matrix and \\(\\mathbf{V}_j\\) is the sampling variance-covariance matrix of the effect size estimates, with entry \\((h,i)\\) equal to \\(\\left[\\mathbf{V}_j\\right]_{h,i} = r_{hij} \\sqrt{V_{hj} V_{ij}}\\). The estimate of the study-specific average effect size for study \\(j\\) is still a weighted average:\r\\[\r\\hat\\theta_j = \\frac{\\sum_{i=1}^{n_j} s_{ij} T_{ij}}{\\sum_{i=1}^{n_j} s_{ij}},\r\\]\rwhere\r\\[\rs_{ij} = \\displaystyle{\\sum_{h=1}^{n_j} \\left[\\boldsymbol{\\hat\\Sigma}^{-1}\\right]_{hi}},\r\\]\rand \\(\\left[\\boldsymbol{\\hat\\Sigma}^{-1}\\right]_{hi}\\) denotes entry \\((h,i)\\) in the inverse of the matrix \\(\\boldsymbol{\\hat\\Sigma}\\). Let \\(V^C_j\\) denote the variance of the study-specific average effect size estimate, conditional on the true \\(\\theta_j\\):\r\\[\rV^C_j = \\text{Var}(\\hat\\theta_j | \\theta_j) = \\left(\\sum_{i=1}^{n_j} s_{ij} \\right)^{-1}\r\\]\rThe unconditional variance of \\(\\hat\\theta_j\\) is then\r\\[\r\\text{Var}(\\hat\\theta_j) = \\tau^2 + V^C_j.\r\\]\rBecause the overall average effect size estimate is (still) the inverse-variance weighted average, the weight assigned at the study level is equal to\r\\[\rw_j = \\frac{1}{\\hat\\tau^2 + V^C_j}\r\\]\rand the weight assigned to individual effect sizes is\r\\[\rw_{ij} = \\frac{s_{ij} V^C_j}{\\hat\\tau^2 + V^C_j}.\r\\]\rHow do \\(\\omega^2\\) and \\(\\tau^2\\) affect these more general weights? The intuitions that I described earlier still mostly hold. Increasing \\(\\omega^2\\) will tend to equalize the weights at the effect size level (i.e., equalize the \\(s_{ij}\\) across \\(i\\) and \\(j\\)), pulling weight towards studies with more effect size estimates. Increasing \\(\\tau^2\\) will tend to equalize the weights at the study-level.\nOne wrinkle with the more general form of the weights is that the effect-size level weights can sometimes be negative (i.e., negative \\(s_{ij}\\)). This will tend to happen when the sampling variances within a study are discrepant, such as when one \\(V_{ij}\\) is much smaller than the others in study \\(j\\), when the (assumed or estimated) sampling correlation is high, and when \\(\\omega^2\\) is zero or small. This is something that warrants further investigation.\n\rWhat about meta-regression?\rSome of the foregoing analysis also applies to models that include predictors. In particular, the formulas I’ve given for the weights will still hold for meta-regression models that include only study-level predictors. In other words, they work for models of the following form:\r\\[\rT_{ij} = \\mathbf{x}_j \\boldsymbol\\beta + \\eta_j + \\nu_{ij} + e_{ij},\r\\]\rwhere \\(\\mathbf{x}_j\\) is a row-vector of one or more predictors for study \\(j\\) (including a constant intercept). Introducing these predictors will alter the variance component estimates \\(\\hat\\tau^2\\) and \\(\\hat\\omega^2\\), but the form of the weights will remain the same as above, and the intuitions still hold. This is because, for purposes of estimating \\(\\boldsymbol\\beta\\), the model is essentially the same as a meta-regression at the study level, using the study-specific average effect size estimates as input:\r\\[\r\\hat\\theta_j = \\mathbf{x}_j \\boldsymbol\\beta + \\eta_j + \\tilde{e}_j\r\\]\rwhere \\(\\text{Var}(\\tilde{e}_j) = \\text{Var}(\\hat\\theta_j | \\theta_j)\\).3\nHere is an illustration with the corrdat meta-analysis. In these data, the variable college indicates whether the effect size comes from a college-age sample; it varies only at the study level. The variable males, binge, and followup have some within-study variation, which I’ll by taking the average of each of these predictors at the study level:\ncorrdat \u0026lt;- corrdat %\u0026gt;%\rgroup_by(studyid) %\u0026gt;%\rmutate(\rmales_M = mean(males),\rbinge_M = mean(binge),\rfollowup_M = mean(followup)\r)\rNow let’s fit a meta-regression model using all of the study-level predictors:\nMVMR_fit \u0026lt;- rma.mv(yi = effectsize, V = V_mat,\rmods = ~ college + males_M + binge_M + followup_M, random = ~ 1 | studyid / esid,\rdata = corrdat)\rsummary(MVMR_fit)\r## ## Multivariate Meta-Analysis Model (k = 171; method: REML)\r## ## logLik Deviance AIC BIC AICc ## -86.6244 173.2488 187.2488 209.0327 187.9577 ## ## Variance Components:\r## ## estim sqrt nlvls fixed factor ## sigma^2.1 0.0297 0.1723 39 no studyid ## sigma^2.2 0.1068 0.3268 171 no studyid/esid ## ## Test for Residual Heterogeneity:\r## QE(df = 166) = 1083.6655, p-val \u0026lt; .0001\r## ## Test of Moderators (coefficients 2:5):\r## QM(df = 4) = 13.0787, p-val = 0.0109\r## ## Model Results:\r## ## estimate se zval pval ci.lb ci.ub ## intrcpt -0.0361 0.3678 -0.0982 0.9218 -0.7571 0.6849 ## college 0.2660 0.1384 1.9215 0.0547 -0.0053 0.5373 . ## males_M 0.0023 0.0048 0.4753 0.6346 -0.0072 0.0118 ## binge_M 0.3441 0.1570 2.1927 0.0283 0.0365 0.6518 * ## followup_M -0.0023 0.0011 -2.0379 0.0416 -0.0044 -0.0001 * ## ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rAs you might expect, between-study heterogeneity is reduced a bit by the inclusion of these predictors.4\nWe can check my claim of computational equivalence by fitting the meta-regression model at the study level. Here I’ll aggregate everything up to the study level and compute the study-level weights:\ntau_sq_reg \u0026lt;- MVMR_fit$sigma2[1]\romega_sq_reg \u0026lt;- MVMR_fit$sigma2[2]\rcorrdat_studylevel \u0026lt;- corrdat %\u0026gt;%\rgroup_by(studyid) %\u0026gt;%\rmutate(n_j = n()) %\u0026gt;%\rsummarize_at(vars(effectsize, n_j, V_bar, college, binge_M, followup_M, males_M), mean\r) %\u0026gt;%\rmutate(\rV_cond = (omega_sq_reg + (n_j - 1) * r * V_bar + V_bar) / n_j,\rw_j = 1 / (tau_sq_reg + V_cond)\r)\rNow I can fit a study-level meta-regression model. I use the weights argument to ensure that the meta-regression is estimated using the \\(w_j\\) weights:\nMR_study_fit \u0026lt;- rma(yi = effectsize, vi = V_cond, mods = ~ college + males_M + binge_M + followup_M, weights = w_j, data = corrdat_studylevel)\rsummary(MR_study_fit)\r## ## Mixed-Effects Model (k = 39; tau^2 estimator: REML)\r## ## logLik deviance AIC BIC AICc ## -13.0651 26.1303 38.1303 47.2884 41.2414 ## ## tau^2 (estimated amount of residual heterogeneity): 0.0297 (SE = 0.0264)\r## tau (square root of estimated tau^2 value): 0.1723\r## I^2 (residual heterogeneity / unaccounted variability): 26.89%\r## H^2 (unaccounted variability / sampling variability): 1.37\r## R^2 (amount of heterogeneity accounted for): 37.90%\r## ## Test for Residual Heterogeneity:\r## QE(df = 34) = 46.5050, p-val = 0.0748\r## ## Test of Moderators (coefficients 2:5):\r## QM(df = 4) = 13.0787, p-val = 0.0109\r## ## Model Results:\r## ## estimate se zval pval ci.lb ci.ub ## intrcpt -0.0361 0.3678 -0.0982 0.9218 -0.7571 0.6849 ## college 0.2660 0.1384 1.9215 0.0547 -0.0053 0.5373 . ## males_M 0.0023 0.0048 0.4753 0.6346 -0.0072 0.0118 ## binge_M 0.3441 0.1570 2.1927 0.0283 0.0365 0.6518 * ## followup_M -0.0023 0.0011 -2.0379 0.0416 -0.0044 -0.0001 * ## ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; HEAD\rThe meta-regression coefficient estimates are essentially identical to those from the multi-variate meta-regression, although the between-study heterogeneity estimate differs slightly because it is based on maximizing the single-level model, conditional on an estimate of \\(\\omega^2\\).\n\rAnd beyond!\r\rIn true multi-variate models, the meta-regression specification would typically include indicators for each dimension of the model. More generally, we might have a model that includes predictors varying within study, encoding characteristics of the outcome measures, sub-groups, or treatment conditions corresponding to each effect size estimate. The weights in these model get substantially more complicated, not in the least because the weights are specific to the predictors. For instance, in a model with four within-study predictors, a different set of weights is used in estimating the coefficients corresponding to each predictor. As Dr. Richard Riley noted on Twitter, relevant work on more complicated models includes this great paper by Dan Jackson and colleagues and this paper by Riley and colleagues. The latter paper demonstrates how multivariate models entail partial “borrowing of strength” across dimensions of the effect sizes, which is very helpful for building intuition about how these models work. I would encourage you to check out both papers if you are grappling with understanding how weights work in complex meta-regression models.\rThe meta-regression coefficient estimates are essentially identical to those from the multivariate meta-regression, although the between-study heterogeneity estimate differs slightly because it is based on maximizing the single-level model, conditional on an estimate of \\(\\omega^2\\).\n\rAnd beyond!\rIn true multivariate models, the meta-regression specification would typically include indicators for each dimension of the model. More generally, we might have a model that includes predictors varying within study, encoding characteristics of the outcome measures, sub-groups, or treatment conditions corresponding to each effect size estimate. The weights in these model get substantially more complicated, not in the least because the weights are specific to the predictors. For instance, in a model with four within-study predictors, a different set of weights is used in estimating the coefficients corresponding to each predictor. As Dr. Richard Riley noted on Twitter, relevant work on more complicated models includes this great paper by Dan Jackson and colleagues and this paper by Riley and colleagues. The latter paper demonstrates how multivariate models entail partial “borrowing of strength” across dimensions of the effect sizes, which is very helpful for building intuition about how these models work. I would encourage you to check out both papers if you are grappling with understanding how weights work in complex meta-regression models.\r\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; origin/master\n\r\rNote that this model also encompasses the multi-level meta-analysis described by Konstantopoulos (2011) and Van den Noortgate, et al. (2013) as a special case, with \\(r_{hij} = 0\\) for all \\(h,i=1,...,n_j\\) and \\(j = 1,...,k\\).↩︎\n\rPerhaps that makes sense, if you’ve carefully selected the set of effect sizes for inclusion in your meta-analysis. However, it seems to me that it could sometimes lead to perverse results. Say that all studies but one include just a single effect size estimate, each using the absolute gold standard approach to assessing the outcome, but that one study took a “kitchen sink” approach and assessed the outcome a bunch of different ways, including the gold standard plus a bunch of junky scales. Inclusion of the junky scales will lead to within-study heterogeneity, which in turn will pull the overall average effect size towards this study—the one with all the junk! That seems less than ideal, and the sort of situation where it would be better to select from the study with multiple outcomes the single effect size estimate based on the outcome assessment that most closely aligns with the other studies.↩︎\n\rThings get even simpler if the model does not include within-study random effects, as I discussed in a previous post.↩︎\n\rHowever, this need not be the case—it’s possible that introducing between-study predictors could increase the estimate of between-study heterogeneity. Yes, that’s totally counter-intuitive. Multi-level models can be weird.↩︎\n\r\r\r","date":1591660800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591660800,"objectID":"d4f4a81cf671f04fa25daa7f0bc3484d","permalink":"/weighting-in-multivariate-meta-analysis/","publishdate":"2020-06-09T00:00:00Z","relpermalink":"/weighting-in-multivariate-meta-analysis/","section":"post","summary":"One common question about multivariate/multi-level meta-analysis is how such models assign weight to individual effect size estimates. When a version of the question came up recently on the R-sig-meta-analysis listserv, Dr.","tags":["meta-analysis","weighting"],"title":"Weighting in multivariate meta-analysis","type":"post"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rUPDATED November 21, 2020. Thanks to Allen O’Brien for pointing out a bug in the codefolding code, which led to the last code chunk defaulting to hidden rather than open. Allen sent along a simple fix to the codefolding.js file.\nAbout a year ago I added a code-folding feature to my site, following an approach developed by Sébastien Rochette. I recently updated my site to work with the latest version of the Academic theme for Hugo, and it turns out that this broke my code-folding implementation. It took a bit of putzing and some help from a freelance web developer to fix it, but it’s now working again, and I’m again doing my happy robot dance:\nIn this post, I’ll provide instructions on how to reproduce the approach with the current version of the Academic theme, which is 4.8 (March 2020). Credit where credit is due:\n\rSébastien Rochette worked out the earlier implementation.\rWeb developer Max B. worked out the kinks to get it working with the latest version of Academic. We connected through Upwork. Hire him there if you have web dev work!\rAs I’ve said before, I couldn’t write javascript to save my life, and my only contribution here is to write down the instructions.\r\rCode folding with the Academic theme\rYou’ll first need to add the codefolding javascript assets. Create a folder called js under the /static directory of your site. Add the file codefolding.js.\n\rCreate a folder called css under the /static directory of your site. Add the file codefolding.css. This is the css for the buttons that will appear on your posts.\n\rAdd the file article_footer_js.html to the /layouts/partials directory of your site.\n\rAdd the file header_maincodefolding.html to the /layouts/partials directory of your site.\n\rIf you do not already have a file head_custom.html in the /layouts/partials directory, create it. Add the following lines of code to the file:\n{{ if not site.Params.disable_codefolding }}\r\u0026lt;script src=\u0026quot;https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt;\r\u0026lt;link rel=\u0026quot;stylesheet\u0026quot; href=\u0026quot;{{ \u0026quot;css/codefolding.css\u0026quot; | relURL }}\u0026quot; /\u0026gt;\r{{ end }}\rIf you do not already have a file site_footer.html in the /layouts/partials directory, copy it over from /themes/hugo-academic/layouts/partials. Add the following lines of code to it, somewhere towards the bottom (see my version for example):\n\u0026lt;!-- Init code folding --\u0026gt;\r{{ partial \u0026quot;article_footer_js.html\u0026quot; . }}\rIf you do not already have the file page_header.html in the /layouts/partials directory, copy it over from /themes/hugo-academic/layouts/partials. Add the following line of code at appropriate points so that your posts will include the “Show/hide code” button:\n {{ partial \u0026quot;header_maincodefolding\u0026quot; . }}\rNote that you’ll likely need to add it twice due do conditionals in page_header.html. For example, my version of the file includes the partial at lines 62 and 91.\n\rModify your params.toml file (in the directory /config/_default) to include the following lines:\n############################\r## Code folding\r############################\r# Set to true to disable code folding\rdisable_codefolding = false\r# Set to \u0026quot;hide\u0026quot; or \u0026quot;show\u0026quot; all codes by default\rcodefolding_show = \u0026quot;show\u0026quot;\r# Set to true to exclude the \u0026quot;Show/hide all\u0026quot; button\rcodefolding_nobutton = false\r\r\rUsing the codefolding parameters\rThe params.toml file now has three parameters that control code folding:\n\rdisable_codefolding controls whether to load the code folding scripts on your site. Set it to true to disable code folding globally.\rcodefolding_show controls whether code blocks will be shown or hidden by default. If your previous posts have lots of code in them, set the default to show to minimize changes in the appearance of your site.\rcodefolding_nobutton controls whether the “Show/hide code” button will appear at the top of posts that include code blocks. Set it to true to disable the button but keep the other code folding functionality.\r\rThe above parameters are defaults for your entire site. To over-ride the defaults, you can also set the parameters in the YAML header of any post:\n\rSet disable_codefolding: true to turn off code folding for the post.\rSet codefolding_show: hide to hide the code blocks in the post (as in this post).\rSet codefolding_nobutton: true to turn off the “Show/hide code” button at the top of the post (as in the present post).\r\rI hope these instructions work for you. If not, questions, corrections, and clarifications are welcome. Happy blogging, y’all!\n\r","date":1588464000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605916800,"objectID":"6674f78666f5d99aecdf9b954fd6f639","permalink":"/code-folding-update/","publishdate":"2020-05-03T00:00:00Z","relpermalink":"/code-folding-update/","section":"post","summary":"UPDATED November 21, 2020. Thanks to Allen O’Brien for pointing out a bug in the codefolding code, which led to the last code chunk defaulting to hidden rather than open.","tags":["programming","Rstats"],"title":"An update on code folding with blogdown + Academic theme","type":"post"},{"authors":["James E. Pustejovsky","Man Chen"],"categories":null,"content":"lmeInfo provides analytic derivatives and information matrices for fitted linear mixed effects models and generalized least squares models estimated using nlme::lme() and nlme::gls(), respectively. The package includes functions for estimating the sampling variance-covariance of variance component parameters using the inverse Fisher information. The variance components include the parameters of the random effects structure (for lme models), the variance structure, and the correlation structure. The expected and average forms of the Fisher information matrix are used in the calculations, and models estimated by full maximum likelihood or restricted maximum likelihood are supported. The package also includes a function for estimating standardized mean difference effect sizes (\rPustejovsky et al., 2014) based on fitted lme or gls models.\n R package available on the Comprehensive R Archive Network \rR source code on Github  ","date":1587686400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587686400,"objectID":"6fc509ba7d8de676563d46489460c4b4","permalink":"/software/lmeinfo/","publishdate":"2020-04-24T00:00:00Z","relpermalink":"/software/lmeinfo/","section":"software","summary":"Information Matrices for 'lmeStruct' and 'glsStruct' Objects","tags":["hierarchical models","effect size","Rstats","nlme"],"title":"lmeInfo","type":"software"},{"authors":["Megha Joshi","James E. Pustejovsky"],"categories":null,"content":"Monte Carlo simulations are computer experiments designed to study the performance of statistical methods under known data-generating conditions. The goal of simhelpers is to assist in running such simulation studies. The main tools in the package consist of functions to calculate measures of estimator performance, such as bias, root mean squared error, rejection rates. The functions also calculate the associated Monte Carlo standard errors (MCSE) of the performance measures. The functions use the tidyeval principles, so that they play well with dplyr and fit easily into a %\u0026gt;%-centric workflow.\n \rAvailable on the Comprehensive R Archive Network \rSource code and installation instructions on Github  ","date":1585612800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585612800,"objectID":"8b9c60311ade30202d26b746a0235655","permalink":"/software/simhelpers/","publishdate":"2020-03-31T00:00:00Z","relpermalink":"/software/simhelpers/","section":"software","summary":"Helper package to assist in running simulation studies","tags":["simulation","Rstats"],"title":"simhelpers","type":"software"},{"authors":["Daniel M. Swan","James E. Pustejovsky","S. Natasha Beretvas"],"categories":null,"content":"","date":1585180800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585180800,"objectID":"659d4719c9a7d160a0687bb4347e5957","permalink":"/publication/response-guided-designs-in-sced-baselines/","publishdate":"2020-03-26T00:00:00Z","relpermalink":"/publication/response-guided-designs-in-sced-baselines/","section":"publication","summary":"In single-case experimental design (SCED) research, researchers often choose when to start treatment based on whether the baseline data collected so far are stable, using what is called a response-guided design. There is evidence that response-guided designs are common, and researchers have described a variety of criteria for assessing stability. With many of these criteria, making judgments about stability could yield data with limited variability, which may have consequences for statistical inference and effect size estimates. However, little research has examined the impact of response-guided design on the resulting data. Drawing on both applied and methodological research, we describe several algorithms as models for response-guided design. We use simulation methods to assess how using a response-guided design impacts the baseline data pattern. The simulations generate baseline data in the form of frequency counts, a common type of outcome in SCEDs. Most of the response-guided algorithms we identified lead to baselines with approximately unbiased mean levels, but nearly all of them lead to underestimates in the baseline variance. We discuss implications for the use of response-guided designs in practice and for the plausibility of specific algorithms as representations of actual research practice.","tags":["single-case design","response-guided design"],"title":"The impact of response-guided designs on count outcomes in single-case experimental design baselines","type":"publication"},{"authors":["John A. Salsman","James E. Pustejovsky","Stephen M. Schueller","Rosalba Hernandez","Mark Berendsen","Laurie E. Steffen McLouth","Judith T. Moskowitz"],"categories":null,"content":"","date":1574121600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574121600,"objectID":"e0a2a936c6badbd0ae7460948af7c8a8","permalink":"/publication/psychosocial-interventions-for-positive-affect/","publishdate":"2019-11-19T00:00:00Z","relpermalink":"/publication/psychosocial-interventions-for-positive-affect/","section":"publication","summary":"__Purpose__\nPositive affect has demonstrated unique benefits in the context of health-related stress and is emerging as an important target for psychosocial interventions. The primary objective of this meta-analysis was to determine whether psychosocial interventions increase positive affect in cancer survivors.\n__Methods__\nWe coded 28 randomized controlled trials of psychosocial interventions assessing 2082 cancer survivors from six electronic databases. We calculated 76 effect sizes for positive affect and conducted synthesis using random effects models with robust variance estimation. Tests for moderation included demographic, clinical, and intervention characteristics.\n__Results__\nInterventions had a modest effect on positive affect (g = 0.35, 95% CI [0.16, 0.54]) with substantial heterogeneity of effects across studies ($\\hat\\tau^2$ = 0.40; $I^2$ = 78%). Three significant moderators were identified: in-person interventions outperformed remote interventions (P = .046), effects were larger when evaluated against standard of care or wait list control conditions versus attentional, educational, or component controls (P = .009), and trials with survivors of early-stage cancer diagnoses yielded larger effects than those with advanced-stage diagnoses (P = .046). We did not detect differential benefits of psychosocial interventions across samples varying in sex, age, on-treatment versus off-treatment status, or cancer type. Although no conclusive evidence suggested outcome reporting biases (P = .370), effects were smaller in studies with lower risk of bias.\n__Conclusions__ In-person interventions with survivors of early-stage cancers hold promise for enhancing positive affect, but more methodological rigor is needed.\n__Implications for Cancer Survivors__ Positive affect strategies can be an explicit target in evidence-based medicine and have a role in patient-centered survivorship care, providing tools to uniquely mobilize human strengths.","tags":["cancer","meta-analysis","positive affect","psycho-social intervention","systematic review"],"title":"Psychosocial interventions for cancer survivors: A meta-analysis of effects on positive affect","type":"publication"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rAs I’ve discussed in previous posts, meta-analyses in psychology, education, and other areas often include studies that contribute multiple, statistically dependent effect size estimates.\rI’m interested in methods for meta-analyzing and meta-regressing effect sizes from data structures like this, and studying this sort of thing often entails conducting Monte Carlo simulations.\rMonte Carlo simulations involve generating artificial data—in this case, a set of studies, each of which has one or more dependent effect size estimates—that follows a certain distributional model, applying different analytic methods to the artificial data, and then repeating the process a bunch of times.\rBecause we know the true parameters that govern the data-generating process, we can evaluate the performance of the analytic methods in terms of bias, accuracy, hypothesis test calibration and power, confidence interval coverage, and the like.\nIn this post, I’ll discuss two alternative methods to simulate meta-analytic datasets that include studies with multiple, dependent effect size estimates: simulating individual participant-level data or simulating summary statistics. I’ll focus on the case of the standardized mean difference (SMD) because it is so common in meta-analyses of intervention studies. For simplicity, I’ll assume that the effect sizes all come from simple, two-group comparisons (without any covariate adjustment or anything like that) and that the individual observations are multi-variate normally distributed within each group. Our goal will be to simulate a set of \\(K\\) studies, where study \\(k\\) is based on measuring \\(J_k\\) outcomes on a sample of \\(N_k\\) participants, all for \\(k = 1,...,K\\).\rLet \\(\\boldsymbol\\delta_k = (\\delta_{1k} \\cdots \\delta_{J_k k})\u0026#39;\\) be the \\(J_k \\times 1\\) vector of true standardized mean differences for study \\(k\\).\rI’ll assume that we know these true effect size parameters for all \\(K\\) studies, so that I can avoid committing to any particular form of random effects model.\nSimulating individual participant-level data\rThe most direct way to simulate this sort of effect size data is to generate outcome data for every artificial participant in every artificial study. Let \\(\\mathbf{Y}_{ik}^T\\) be the \\(J_k \\times 1\\) vector of outcomes for treatment group participant \\(i\\) in study \\(k\\), and let \\(\\mathbf{Y}_{ik}^C\\) be the \\(J_k \\times 1\\) vector outcomes for control group participant \\(i\\) in study \\(k\\), for \\(i=1,...,N_k / 2\\) and \\(k = 1,...,K\\). Assuming multi-variate normality of the outcomes, we can generate these outcome vectors as\r\\[\r\\mathbf{Y}_{ik}^T \\sim N\\left(\\boldsymbol\\delta_k, \\boldsymbol\\Psi_k\\right) \\qquad \\text{and}\\qquad \\mathbf{Y}_{ik}^C \\sim N\\left(\\mathbf{0}, \\boldsymbol\\Psi_k\\right),\r\\]\rwhere \\(\\boldsymbol\\Psi_k\\) is the population correlation matrix of the outcomes in study \\(k\\).\rNote that I am setting the mean outcomes of the control group participants to zero and also specifying that the outcomes all have unit variance within each group.\rAfter simulating data based on these distributions, the effect size estimates for each outcome can be calculated directly, following standard formulas.\nHere’s what this approach looks like in code.\rIt is helpful to simplify things by focusing on simulating just a single study with multiple, correlated effect sizes.\rFocusing first on just the input parameters, a function might look like the following:\nr_SMDs_raw \u0026lt;- function(delta, J, N, Psi) {\r# stuff\rreturn(ES_data) }\rIn the above function skeleton, delta would be the true effect size parameter \\(\\boldsymbol\\delta_k\\), J would be the number of effect sizes to generate \\((J_k)\\), N is the total number of participants \\((N_k)\\), and Psi is a matrix of correlations between the outcomes \\((\\Psi_k)\\).\rFrom these parameters, we’ll generate raw data, calculate effect size estimates and standard errors, and return the results in a little dataset.\nTo make the function a little bit easier to use, I’m going overload the Psi argument so that it can be a single number, indicating a common correlation between the outcomes. Thus, instead of having to feed in a \\(J_k \\times J_k\\) matrix, you can specify a single correlation \\(r_k\\), and the function will assume that all of the outcomes are equicorrelated. In code, the logic is:\nif (!is.matrix(Psi)) Psi \u0026lt;- Psi + diag(1 - Psi, nrow = J)\rHere’s the function with the innards:\nr_SMDs_raw \u0026lt;- function(delta, J, N, Psi) {\rrequire(mvtnorm) # for simulating multi-variate normal data\r# create Psi matrix assuming equicorrelation\rif (!is.matrix(Psi)) Psi \u0026lt;- Psi + diag(1 - Psi, nrow = J)\r# generate control group summary statistics\rY_C \u0026lt;- rmvnorm(n = N / 2, mean = rep(0, J), sigma = Psi)\rybar_C \u0026lt;- colMeans(Y_C)\rsd_C \u0026lt;- apply(Y_C, 2, sd)\r# generate treatment group summary statistics\rdelta \u0026lt;- rep(delta, length.out = J)\rY_T \u0026lt;- rmvnorm(n = N / 2, mean = delta, sigma = Psi)\rybar_T \u0026lt;- colMeans(Y_T)\rsd_T \u0026lt;- apply(Y_T, 2, sd)\r# calculate Cohen\u0026#39;s d\rsd_pool \u0026lt;- sqrt((sd_C^2 + sd_T^2) / 2)\rES \u0026lt;- (ybar_T - ybar_C) / sd_pool\r# calculate SE of d\rSE \u0026lt;- sqrt(4 / N + ES^2 / (2 * (N - 2)))\rdata.frame(ES = ES, SE = SE, N = N)\r}\rIn action:\ndelta \u0026lt;- rnorm(4, mean = 0.2, sd = 0.1)\rr_SMDs_raw(delta = delta, J = 4, N = 40, Psi = 0.6)\r## Loading required package: mvtnorm\r## ES SE N\r## 1 -0.19106514 0.3169863 40\r## 2 0.18427227 0.3169334 40\r## 3 0.25646209 0.3175932 40\r## 4 0.00210429 0.3162279 40\rOr if you’d rather specify the full \\(\\Psi_k\\) matrix yourself:\nPsi_k \u0026lt;- 0.6 + diag(0.4, nrow = 4)\rPsi_k\r## [,1] [,2] [,3] [,4]\r## [1,] 1.0 0.6 0.6 0.6\r## [2,] 0.6 1.0 0.6 0.6\r## [3,] 0.6 0.6 1.0 0.6\r## [4,] 0.6 0.6 0.6 1.0\rr_SMDs_raw(delta = delta, J = 4, N = 40, Psi = Psi_k)\r## ES SE N\r## 1 -0.1597097 0.3167580 40\r## 2 -0.1717717 0.3168410 40\r## 3 -0.4369032 0.3201744 40\r## 4 0.0657410 0.3163177 40\rExercises\rThe function above is serviceable but quite basic. I can think of several additional features that one might like to have for use in research simulations, but I’m feeling both cheeky and lazy at the moment, so I’ll leave them for you, dear reader. Here are some suggested exercises:\nAdd an argument to the function, Hedges_g = TRUE, which controls where the simulated effect size is Hedges’ \\(g\\) or Cohen’s \\(d\\). If it is Hedges’ g, make sure that the standard error is corrected too.\n\rAdd an argument to the function, p_val = TRUE, which allows the user to control whether or not to return \\(p\\)-values from the test of mean differences for each outcome. Note that the p-values should be for a test of the raw mean differences between groups, rather than a test of the effect size \\(\\delta_{jk} = 0\\).\n\rAdd an argument to the function, corr_mat = FALSE, which controls whether the function returns just the simulated effect sizes and SEs or both the simulated effect sizes and the full sampling variance-covariance matrix of the effect sizes. See here for the relevant formulas.\n\r\r\r\rSimulating summary statistics\rAnother approach to simulating SMDs is to sample from the distribution of the summary statistics used in calculating the effect size. This approach should simplify the code, at the cost of having to use a bit of distribution theory. Let \\(\\mathbf{\\bar{y}}_{Tk}\\) and \\(\\mathbf{\\bar{y}}_{Ck}\\) be the \\(J_k \\times 1\\) vectors of sample means for the treatment and control groups, respectively. Let \\(\\mathbf{S}_k\\) be the \\(J_k \\times J_k\\) sample covariance matrix of the outcomes, pooled across the treatment and control groups. Again assuming multi-variate normality, and following the same notation as above:\r\\[\r\\mathbf{\\bar{y}}_{Ck} \\sim N\\left(\\mathbf{0}, \\frac{2}{N_k} \\boldsymbol\\Psi_k\\right), \\qquad \\mathbf{\\bar{y}}_{Tk} \\sim N\\left(\\boldsymbol\\delta_k, \\frac{2}{N_k} \\boldsymbol\\Psi_k\\right),\r\\]\rand\r\\[\r\\left(\\mathbf{\\bar{y}}_{Tk} - \\mathbf{\\bar{y}}_{Ck}\\right) \\sim N\\left(\\boldsymbol\\delta_k, \\frac{4}{N_k} \\boldsymbol\\Psi_k\\right).\r\\]\rThis shows how we could directly simulate the numerator of the standardized mean difference.\nA further bit of distribution theory says that the pooled sample covariance matrix follows a multiple of a Wishart distribution with \\(N_k - 2\\) degrees of freedom and scale matrix \\(\\Psi_k\\):\r\\[\r(N_k - 2) \\mathbf{S}_k \\sim Wishart\\left(N_k - 2, \\Psi_k \\right).\r\\]\rThus, to simulate the denominators of the SMD estimates, we can simulate a single Wishart matrix, pull out the diagonal entries, divide by \\(N_k - 2\\), and take the square root. In all, we draw a single \\(J_k \\times 1\\) observation from a multi-variate normal distribution and a single \\(J_k \\times J_k\\) observation from a Wishart distribution. In contrast, the raw data approach requires simulating \\(N_k\\) observations from a multi-variate normal distribution, then calculating \\(4 J_k\\) summary statistics (M and SD for each group on each outcome).\nExercises\rOnce again, I’ll leave it to you, dear reader, to do the fun programming bits:\nCreate a modified version of the function r_SMDs_raw that simulates summary statistics instead of raw data (Call it r_SMDs_stats).\rUse the microbenchmark package (or your preferred benchmarking tool) to compare the computational efficiency of both versions of the function.\rCheck your work! Verify that both versions of the function generate the same distributions if the same parameters are used as input.\r\r\r\rWhich approach is better?\rLike many things in research, there’s no clearly superior method here. The advantage of the summary statistics approach is computational efficiency. It should generally be faster than the raw data approach, and if you need to generate 10,000 meta-analysis each with 80 studies in them, the computational savings might add up. On the other hand, computational efficiency isn’t everything.\nI see two potential advantages of the raw data approach. First is interpretability: simulating raw data is likely easier to understand. It feels tangible and familiar, harkening back to those bygone days we spent learning ANOVA, whereas the summary statistics approach requires a bit of distribution theory to follow (bookmark this blog post!). Second is extensibility: it is relatively straightforward to extend the approach to use other distributional models for the raw dat (perhaps you want to look at outcomes that follow a multi-variate \\(t\\) distribution?) or more complicated estimators of the SMD (difference-in-differences? covariate-adjusted? cluster-randomized trial?). To use the summary statistics approach in more complicated scenarios, you’d have to work out the sampling distributions for yourself, or locate the right reference.\nOf course, there’s also no need to choose between these two approaches. As I’m trying to hint at in Exercise 6, it’s actually useful to write both. Then, you can use the (potentially slower) raw data version to verify that the summary statistics version is correct.\n\rSimulating full meta-analyses\rSo far we’ve got a data-generating function that simulates a single study’s worth of effect size estimates. To study meta-analytic methods, we’ll need to build out the function to simulate multiple studies. To do so, I think it’s useful to use the technique of mapping, as implemented in the purrr package’s map_* functions. The idea here is to first generate a “menu” of study-specific parameters for each of \\(K\\) studies, then apply the r_SMDs function to each parameter set.\nLet’s consider how to do this for a simple random effects model, where the true effect size parameter is constant within each study (i.e., \\(\\boldsymbol\\delta_k = (\\delta_k \\cdots \\delta_k)\u0026#39;\\)), and in a model without covariates. We’ll need to generate a true effect for each study, along with a sample size, an outcome dimension, and a correlation between outcomes. For the true effects, I’ll assume that\r\\[\r\\delta_k \\sim N(\\mu, \\tau^2),\r\\]\r\\[\rJ_k \\sim 2 + Poisson(3),\r\\]\r\\[\rN_k \\sim 20 + 2 \\times Poisson(10),\r\\]\rand\r\\[\rr_k \\sim Beta\\left(\\rho \\nu, (1 - \\rho)\\nu\\right),\r\\]\rwhere \\(\\rho = \\text{E}(r_k)\\) and \\(\\nu \u0026gt; 0\\) controls the variability of \\(r_k\\) across studies, with smaller \\(\\nu\\) corresponding to more variable correlations.\rSpecifically, \\(\\text{Var}(r_k) = \\rho (1 - \\rho) / (1 + \\nu)\\).\rThese distributions are just made up, without any particular justification.\nHere’s what these distributional models look like in R code:\nK \u0026lt;- 6\rmu \u0026lt;- 0.2\rtau \u0026lt;- 0.05\rJ_mean \u0026lt;- 5\rN_mean \u0026lt;- 45\rrho \u0026lt;- 0.6\rnu \u0026lt;- 39\rstudy_data \u0026lt;- data.frame(\rdelta = rnorm(K, mean = mu, sd = tau),\rJ = 2 + rpois(K, J_mean - 2),\rN = 20 + 2 * rpois(K, (N_mean - 20) / 2),\rPsi = rbeta(K, rho * nu, (1 - rho) * nu)\r)\rstudy_data\r## delta J N Psi\r## 1 0.1749657 6 56 0.6670410\r## 2 0.1371771 4 52 0.7952095\r## 3 0.1430044 2 46 0.5551301\r## 4 0.1953675 6 46 0.5339670\r## 5 0.1653242 4 42 0.5623903\r## 6 0.1419457 7 40 0.6615825\rOnce we have the “menu” of study-level characteristics, it’s just a matter of mapping the parameters to the data-generating function. One way to do this is with pmap_df:\nlibrary(purrr)\rmeta_data \u0026lt;- pmap_df(study_data, r_SMDs_raw, .id = \u0026quot;study\u0026quot;)\rmeta_data\r## study ES SE N\r## 1 1 0.427048814 0.2704019 56\r## 2 1 0.206502285 0.2679989 56\r## 3 1 0.270244756 0.2685234 56\r## 4 1 0.423149362 0.2703451 56\r## 5 1 0.525878094 0.2720096 56\r## 6 1 0.746186579 0.2767383 56\r## 7 2 -0.005809721 0.2773507 52\r## 8 2 -0.082222645 0.2774719 52\r## 9 2 0.114670949 0.2775871 52\r## 10 2 -0.001432641 0.2773501 52\r## 11 3 -0.031231291 0.2949027 46\r## 12 3 0.302264458 0.2966391 46\r## 13 4 0.085338908 0.2950242 46\r## 14 4 -0.062511255 0.2949592 46\r## 15 4 -0.040178730 0.2949150 46\r## 16 4 -0.082519741 0.2950151 46\r## 17 4 0.207953122 0.2957160 46\r## 18 4 -0.005713721 0.2948845 46\r## 19 5 0.293666394 0.3103483 42\r## 20 5 0.258312309 0.3099551 42\r## 21 5 0.362126706 0.3112512 42\r## 22 5 0.177656049 0.3092452 42\r## 23 6 -0.115158991 0.3165035 40\r## 24 6 0.094349350 0.3164129 40\r## 25 6 -0.052996601 0.3162862 40\r## 26 6 -0.042766762 0.3162658 40\r## 27 6 -0.314584445 0.3182800 40\r## 28 6 0.078519103 0.3163560 40\r## 29 6 -0.103034241 0.3164486 40\rtable(meta_data$study)\r## ## 1 2 3 4 5 6 ## 6 4 2 6 4 7\rPutting it all together into a function, we have\nr_meta \u0026lt;- function(K, mu, tau, J_mean, N_mean, rho, nu) {\rrequire(purrr)\rstudy_data \u0026lt;- data.frame(\rdelta = rnorm(K, mean = mu, sd = tau),\rJ = 2 + rpois(K, J_mean - 2),\rN = 20 + 2 * rpois(K, (N_mean - 20) / 2),\rPsi = rbeta(K, rho * nu, (1 - rho) * nu)\r)\rpmap_df(study_data, r_SMDs_raw, .id = \u0026quot;study\u0026quot;)\r}\rExercises\rModify r_meta so that it uses r_SMDs_stats.\n\rAdd options to r_meta for Hedges_g, p_val = TRUE, and corr_mat = FALSE and ensure that these get passed along to the r_SMDs function.\n\rOne way to check that the r_meta function is working properly is to generate a very large meta-analytic dataset, then to verify that the generated distributions align with expectations. Here’s a very large meta-analytic dataset:\nmeta_data \u0026lt;- r_meta(100000, mu = 0.2, tau = 0.05, J_mean = 5, N_mean = 40, rho = 0.6, nu = 39)\rCompare the distribution of the simulated dataset against what you would expect to get based on the input parameters.\n\rModify the r_meta function so that \\(J_k\\) and \\(N_k\\) are correlated, according to\r\\[\r\\begin{align}\rJ_k \u0026amp;\\sim 2 + Poisson(\\mu_J - 2) \\\\\rN_k \u0026amp;\\sim 20 + 2 \\times Poisson\\left(\\frac{1}{2}(\\mu_N - 20) + \\alpha (J_k - \\mu_J) \\right)\r\\end{align}\r\\]\rfor user-specified values of \\(\\mu_J\\) (the average number of outcomes per study), \\(\\mu_N\\) (the average total sample size per study), and \\(\\alpha\\), which controls the degree of dependence between \\(J_k\\) and \\(N_k\\).\n\r\r\rA challenge\rThe meta-analytic model that we’re using here is quite simple—simplistic, even—and for some simulation studies, something more complex might be needed. For example, we might need to generate data from a model that includes within-study random effects, as in:\r\\[\r\\delta_{jk} = \\mu + u_k + v_{jk}, \\quad \\text{where}\\quad u_k \\sim N(0, \\tau^2), \\quad v_{jk} \\sim N(0, \\omega^2).\r\\]\rEven more complex would be to simulate from a multi-level meta-regression model\r\\[\r\\delta_{jk} = \\mathbf{x}_{jk} \\boldsymbol\\beta + u_k + v_{jk}, \\quad \\text{where}\\quad u_k \\sim N(0, \\tau^2), \\quad v_{jk} \\sim N(0, \\omega^2),\r\\]\rwhere \\(\\mathbf{x}_{jk}\\) is a \\(1 \\times p\\) row-vector of covariates describing outcome \\(j\\) in study \\(k\\) and \\(\\boldsymbol\\beta\\) is a \\(p \\times 1\\) vector of meta-regression coefficients. In past work, I’ve done this by writing a data-generating function that takes a fixed design matrix \\(\\mathbf{X} = \\left(\\mathbf{x}_{11}\u0026#39; \\cdots \\mathbf{x}_{J_K K}\u0026#39;\\right)\u0026#39;\\) as an input argument, along with \\(\\boldsymbol\\beta\\). The design matrix would also include an identifier for each unique study. There are surely better (simpler, easier to follow) ways to implement the multi-level meta-regression model. I’ll once again leave it to you to work out an approach.\n\r\r","date":1569801600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569801600,"objectID":"24bbf495a378ae52d1faef0dd0a520c3","permalink":"/simulating-correlated-smds/","publishdate":"2019-09-30T00:00:00Z","relpermalink":"/simulating-correlated-smds/","section":"post","summary":"As I’ve discussed in previous posts, meta-analyses in psychology, education, and other areas often include studies that contribute multiple, statistically dependent effect size estimates.\rI’m interested in methods for meta-analyzing and meta-regressing effect sizes from data structures like this, and studying this sort of thing often entails conducting Monte Carlo simulations.","tags":["effect size","standardized mean difference","meta-analysis","simulation","programming","distribution theory"],"title":"Simulating correlated standardized mean differences for meta-analysis","type":"post"},{"authors":["James E. Pustejovsky","Daniel M. Swan","Kyle W. English"],"categories":null,"content":"","date":1564704000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564704000,"objectID":"fb5003a59b2807e48dcf91029cb99d5b","permalink":"/publication/measurement-procedures-and-baseline-outcomes/","publishdate":"2019-08-02T00:00:00Z","relpermalink":"/publication/measurement-procedures-and-baseline-outcomes/","section":"publication","summary":"There has been growing interest in using statistical methods to analyze data and estimate effect size indices from studies that use single-case designs (SCDs), as a complement to traditional visual inspection methods. The validity of a statistical method rests on whether its assumptions are plausible representations of the process by which the data were collected, yet there is evidence that some assumptions---particularly regarding normality of error distributions---may be inappropriate for single-case data. To develop more appropriate modelling assumptions and statistical methods, researchers must attend to the features of real SCD data. In this study, we examine several features of SCDs with behavioral outcome measures in order to inform development of statistical methods. Drawing on a corpus of over 300 studies, including approximately 1800 cases, from seven systematic reviews that cover a range of interventions and outcome constructs, we report the distribution of study designs, distribution of outcome measurement procedures, and features of baseline outcome data distributions for the most common types of measurements used in single-case research. We discuss implications for the development of more realistic assumptions regarding outcome distributions in SCD studies, as well as the design of Monte Carlo simulation studies evaluating the performance of statistical analysis techniques for SCED data.","tags":["single-case design","behavioral observation","systematic review"],"title":"An examination of measurement procedures and characteristics of baseline outcome data in single-case research","type":"publication"},{"authors":[],"categories":null,"content":"","date":1563802200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563802200,"objectID":"70f95299aca9ee7b61883ad9e37d8fc0","permalink":"/talk/srsm-2019-generalized-excess-significance-test/","publishdate":"2019-07-22T12:00:00Z","relpermalink":"/talk/srsm-2019-generalized-excess-significance-test/","section":"talk","summary":"","tags":[],"title":"A generalized excess significance test for selective outcome reporting with dependent effect sizes","type":"talk"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rIn meta-analyses of psychology, education, and other social science research, it is very common that some of the included studies report more than one relevant effect size.\rFor example, in a meta-analysis of intervention effects on reading outcomes, some studies may have used multiple measures of reading outcomes (each of which meets inclusion criteria), or may have measured outcomes at multiple follow-up times; some studies might have also investigated more than one version of an intervention, and it might be of interest to include effect sizes comparing each version to the no-intervention control condition;\rand it’s even possible that some studies may have all of these features, potentially contributing lots of effect size estimates.\nThese situations create a technical challenge for conducting a meta-analysis.\rBecause effect size estimates from the same study are correlated, it’s not usually reasonable to use methods that are premised on each effect size estimate being independent (i.e., univariate methods).\rInstead, the analyst needs to apply methods that take into account the dependencies among estimates coming from the same study.\rIt used to be common to use ad hoc approaches for handling dependence, such as averaging the estimates together or selecting one estimate per study and then using univariate methods (cf. Becker, 2000).\rMore sophisticated, multivariate meta-analysis (MVMA) models that directly account for correlations among the effect size estimates had been developed (Kalaian \u0026amp; Raudenbush, 1996) but were challenging to implement and so rarely used (at least, that’s my impression).\rMore recently, techniques such as multi-level meta-analysis (MLMA; Van den Noortgate et al., 2013, 2015) and robust variance estimation (RVE; Hedges et al., 2010) have emerged, which account for dependencies while using all available effect size estimates and still being feasible to implement.\rThese new techniques of MLMA and RVE are starting to be more widely adopted in practice, and it is not implausible that they will become the standard approach in psychological and educational meta-analysis within a few years.\nGiven the extent of interest in MLMA and RVE, one might wonder: are the older ad hoc approaches ever reasonable or appropriate?\rI think that some are, under certain circumstances.\rIn this post I’ll highlight one such circumstance, where aggregating effect size estimates is not only reasonable but leads to exactly the same results as a multivariate model. This occurs when two conditions are met:\nWe are not interested in within-study heterogeneity of effects and\rAny predictors included in the model vary between studies but not within a given study (i.e., effect sizes from the same study all have the same values of the predictors).\r\rIn short, if all we care about is understanding between-study variation in effect sizes, then it is fine to aggregate them up to the study level.\nA model that’s okay to average\rTo make this argument precise, let me lay out a model where it applies.\rFor full generality, I’ll consider a meta-regression model for a collection of \\(K\\) studies, where study \\(k\\) contributes \\(J_k \\geq 1\\) effect size estimates.\rLet \\(T_{jk}\\) denote effect size estimate \\(j\\) in study \\(k\\), with sampling variance \\(S_{jk}^2\\).\rEffect size estimates from study \\(k\\) maybe be correlated at the sampling level, with correlation \\(\\rho_{ijk}\\) between effect size estimates \\(i\\) and \\(j\\) from study \\(k\\).\rI will assume that the correlations are known, although in practice one might need to just take a guess about the degree of correlation, such as by assuming \\(\\rho_{ijk} = 0.7\\) for all pairs of estimates from each included study.\rLet \\(\\mathbf{x}_k\\) be a row vector of predictor variables for study \\(k\\).\rNote that the predictors do not have a subscript \\(j\\) because I’m assuming here that they are constant within a study.\nA multivariate meta-regression model for these data might be:\r\\[\rT_{jk} = \\mathbf{x}_k \\boldsymbol\\beta + u_k + e_{jk},\r\\]\rwhere \\(u_k\\) is a between-study random effect with variance \\(\\tau^2\\) and \\(e_{jk}\\) is the sampling error for effect size \\(j\\) from study \\(k\\), assumed to have known variance \\(S_{jk}^2\\).\rErrors from the same study are correlated, so \\(\\text{Cov}(e_{ik}, e_{jk}) = \\rho_{ijk} S_{ik} S_{jk}\\).\rThis is a commonly considered model for dependent effect size estimates.\rIn the paper that introduced RVE, Hedges et al. (2010) termed it the “correlated effects” model (implemented in robumeta as model = \"CORR\", which is the default).\rNote that it also satisfies the conditions I outlined above: no within-study random effects, predictors that vary only between study.\rWe can fit it using the rma.mv() function in the metafor package, as I will demonstrate below.\nAn alternative to this multivariate model would be to first average the effects within each study, then fit a univariate random effects model.\rJust how we do the averaging will matter: we’ll need to use inverse-variance weighting.\rLet \\(\\mathbf{T}_k\\) be the \\(J_k \\times 1\\) vector of effect size estimates from study \\(k\\). Let \\(\\mathbf{S}_k\\) be the \\(J_k \\times J_k\\) sampling covariance matrix for \\(\\mathbf{T}_k\\), and let \\(\\mathbf{1}_k\\) be a \\(J_k \\times 1\\) vector of 1s. The inverse-variance weighted average of the effects from study k can then be written as\r\\[\r\\bar{T}_k = V_k \\mathbf{1}_k’ \\mathbf{S}_k^{-1} \\mathbf{T}_k, \\]\rwhere \\(V_k = 1 / (\\mathbf{1}_k’ \\mathbf{S}_k^{-1} \\mathbf{1}_k)\\). The quantity \\(V_k\\) is also the sampling variance of \\(\\bar{T}_k\\).1\nA conventional, univariate random effects model for the averaged effect sizes is\r\\[\r\\bar{T}_k = \\mathbf{x}_k \\boldsymbol\\beta + u_k + \\bar{e}_k, \\]\rwhere \\(\\text{Var}(u_k) = \\tau^2\\) and \\(\\text{Var}(\\bar{e}_k) = V_k\\).\rThis model can be fit using rma.uni from metafor.\rIn fact, doing so will yield the same estimates of model parameters as fitting the multivariate model—for all intents and purposes, they are equivalent models.\rThere are at several different ways to see that this equivalence holds.\rI’ll offer three, from most practical to most theoretical.\r(If you’d rather just take my word that this claim is true, feel free to skip down to the last section, where I comment on implications.)\n\rComputational equivalence\rOne good way to check the equivalence of the univariate and multivariate models is to apply both to a dataset. I’ll use the data from a stylized example described in Tanner-Smith \u0026amp; Tipton (2013), looking at the effects of alcohol abuse interventions on alcohol consumption among adolescents and young adults. (The data are simulated for teaching purposes, so don’t infer anything about real life from the results below!) The data are included in the robumeta package:\nlibrary(tidyverse)\rdata(corrdat, package = \u0026quot;robumeta\u0026quot;)\r# sort by study\rcorrdat \u0026lt;- arrange(corrdat, studyid, esid)\rThe data consist of 172 effect sizes from 39 studies. Some studies report effects at multiple follow-up times and/or for multiple programs compared to a common control condition, leading to dependent effect size estimates.The data also include variables encoding a variety of sample and study characteristics, such as whether the study was conducted with a college student sample and the gender composition of the sample:\nhead(corrdat)\r## esid studyid effectsize var binge followup males college\r## 1 4006 1 0.2086383 0.03246468 1 51.42857 67 0\r## 2 4016 1 0.2244635 0.03244931 1 51.42857 67 0\r## 3 4026 1 0.3151743 0.03278697 1 51.42857 67 0\r## 4 3513 2 0.2220929 0.01972874 0 17.14286 81 1\r## 5 3514 2 -0.1922628 0.02031393 0 17.14286 86 1\r## 6 3556 2 0.3273109 0.01987042 0 17.14286 81 1\rSuppose that we are interested in estimating the differences in average effects by type of sample (college versus adolescent), controlling for the proportion of males in the study. For some reason, there is within-study variation in the percentage of males, so I’ll take the study-level average for this covariate:\ncorrdat \u0026lt;-\rcorrdat %\u0026gt;%\rgroup_by(studyid) %\u0026gt;%\rmutate(males = mean(males))\rWe can then fit this model using a multi-variate meta-regression in metafor.\nIn order to estimate the model, we’ll first need to create a variance-covariance matrix for the effect size estimates in each study, which can be accomplished using impute_covariance_matrix from clubSandwich (further details here). I’ll assume a correlation of 0.6 between pairs of effect sizes within a given study:\nlibrary(clubSandwich)\rlibrary(metafor)\rV_list \u0026lt;- impute_covariance_matrix(vi = corrdat$var, cluster = corrdat$studyid, r = 0.6)\rMV_fit \u0026lt;- rma.mv(effectsize ~ college + males, V = V_list, random = ~ 1 | studyid,\rdata = corrdat, method = \u0026quot;REML\u0026quot;)\rMV_fit\r## ## Multivariate Meta-Analysis Model (k = 172; method: REML)\r## ## Variance Components:\r## ## estim sqrt nlvls fixed factor ## sigma^2 0.0590 0.2429 39 no studyid ## ## Test for Residual Heterogeneity:\r## QE(df = 169) = 815.2448, p-val \u0026lt; .0001\r## ## Test of Moderators (coefficients 2:3):\r## QM(df = 2) = 9.9016, p-val = 0.0071\r## ## Model Results:\r## ## estimate se zval pval ci.lb ci.ub ## intrcpt 0.6466 0.2693 2.4007 0.0164 0.1187 1.1744 * ## college 0.3703 0.1317 2.8123 0.0049 0.1122 0.6283 ** ## males -0.0076 0.0038 -1.9832 0.0473 -0.0152 -0.0001 * ## ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rAlternately, we could aggregate the effects up to the study level and then fit a univariate meta-regression using the same moderators. Here is a function to calculate the aggregated effect size estimates and variances:\nagg_effects \u0026lt;- function(yi, vi, r = 0.6) {\rcorr_mat \u0026lt;- r + diag(1 - r, nrow = length(vi))\rsd_mat \u0026lt;- tcrossprod(sqrt(vi))\rV_inv_mat \u0026lt;- chol2inv(chol(sd_mat * corr_mat))\rV \u0026lt;- 1 / sum(V_inv_mat)\rdata.frame(es = V * sum(yi * V_inv_mat), var = V)\r}\rHere’s the data-munging:\ncorrdat_agg \u0026lt;-\rcorrdat %\u0026gt;%\rgroup_by(studyid) %\u0026gt;%\rsummarise(\res = list(agg_effects(yi = effectsize, vi = var, r = 0.6)),\rmales = mean(males),\rcollege = mean(college)\r) %\u0026gt;%\runnest()\r## Warning: `cols` is now required.\r## Please use `cols = c(es)`\rhead(corrdat_agg)\r## # A tibble: 6 x 5\r## studyid es var males college\r## \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1 0.249 0.0239 67 0\r## 2 2 -0.0210 0.0129 81 1\r## 3 3 0.726 0.0819 76.2 0\r## 4 4 0.370 0.0431 80 1\r## 5 5 -0.0911 0.0281 79 0\r## 6 6 -0.416 0.0111 74 0\rAnd here’s the meta-regression:\nuni_fit \u0026lt;- rma.uni(es ~ college + males, vi = var, data = corrdat_agg, method = \u0026quot;REML\u0026quot;)\runi_fit\r## ## Mixed-Effects Model (k = 39; tau^2 estimator: REML)\r## ## tau^2 (estimated amount of residual heterogeneity): 0.0590 (SE = 0.0242)\r## tau (square root of estimated tau^2 value): 0.2429\r## I^2 (residual heterogeneity / unaccounted variability): 61.42%\r## H^2 (unaccounted variability / sampling variability): 2.59\r## R^2 (amount of heterogeneity accounted for): 19.12%\r## ## Test for Residual Heterogeneity:\r## QE(df = 36) = 96.7794, p-val \u0026lt; .0001\r## ## Test of Moderators (coefficients 2:3):\r## QM(df = 2) = 9.9016, p-val = 0.0071\r## ## Model Results:\r## ## estimate se zval pval ci.lb ci.ub ## intrcpt 0.6466 0.2693 2.4007 0.0164 0.1187 1.1744 * ## college 0.3703 0.1317 2.8123 0.0049 0.1122 0.6283 ** ## males -0.0076 0.0038 -1.9832 0.0473 -0.0152 -0.0001 * ## ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rThe heterogeneity estimates are nearly equal (the difference is due to using numerical optimization):\nMV_fit$sigma2\r## [1] 0.0589972\runi_fit$tau2\r## [1] 0.05899673\rAnd the meta-regression coefficient estimates are identical to six decimal places:\ncoef(MV_fit)\r## intrcpt college males ## 0.646561371 0.370274721 -0.007633517\rcoef(uni_fit)\r## intrcpt college males ## 0.646561352 0.370274307 -0.007633519\rall.equal(coef(MV_fit), coef(uni_fit))\r## [1] \u0026quot;Mean relative difference: 4.243578e-07\u0026quot;\rFor this example we arrive at the same results using either multivariate meta-analysis or univariate meta-analysis of aggregated effect size estimates.2 The main limitation of this illustration is generality—how can we be sure that these results aren’t just a quirk of this particular dataset? Would we get the same results for any dataset?\n\rFrom multivariate to univariate model\rHere’s another, somewhat more general perspective on the relationship between the models: the univariate model can be derived directly from the multivariate one. Start with the multivariate model in matrix form:\r\\[\r\\mathbf{T}_k = \\mathbf{x}_k \\boldsymbol\\beta \\mathbf{1}_k + u_k \\mathbf{1}_k + \\mathbf{e}_k,\r\\]\rwhere \\(\\mathbf{e}_k\\) is the vector of sampling errors for study \\(k\\), with \\(\\text{Var}(\\mathbf{e}_k) = \\mathbf{S}_k\\). Pre-multiply both sides by \\(V_k \\mathbf{1}_k’ \\mathbf{S}_k^{-1}\\) to get\r\\[\r\\begin{aligned}\rV_k \\mathbf{1}_k’ \\mathbf{S}_k^{-1} \\mathbf{T}_k \u0026amp;= V_k \\left(\\mathbf{1}_k’ \\mathbf{S}_k^{-1} \\mathbf{1}_k\\right) \\mathbf{x}_k \\boldsymbol\\beta + u_k V_k \\left(\\mathbf{1}_k’ \\mathbf{S}_k^{-1} \\mathbf{1}_k\\right) + V_k \\mathbf{1}_k’ \\mathbf{S}_k^{-1} \\mathbf{e}_k \\\\\r\\bar{T}_k \u0026amp;= \\mathbf{x}_k \\boldsymbol\\beta + u_k + \\bar{e}_k,\r\\end{aligned}\r\\]\rwhere \\(\\text{Var}(\\bar{e}_k) = V_k \\mathbf{1}_k’ \\mathbf{S}_k^{-1} \\mathbf{S}_k \\mathbf{S}_k^{-1} \\mathbf{1}_k V_k = V_k\\), just as in the univariate model.\nThis demonstrates that the parameters of the two models are the same quantities—that is, both models are estimating the same thing. But that would also hold if we used any weighted average of \\(\\mathbf{T}_k\\)—it needn’t be inverse-variance. The only thing that would be different is \\(\\text{Var}(\\bar{e}_k)\\). To fully establish the equivalence of the two models, I’ll examine the likelihoods of each model.\n\rEquivalence of likelihoods\rMultivariate meta-analysis models are typically estimated by full maximum likelihood (FML) or restricted maximum likelihood methods. FML and RML are also commonly used for univariate meta-analysis. With these methods, estimates are obtained as the parameter values that maximize the log likelihood of the model, given the data (or the restricted likelihood for RML). Therefore, we can establish the exact equivalence of parameter estimates by showing that the log likelihood of the univariate and multivariate models differ by a constant value (so that the location of the maxima are identical).\nFull likelihood\rFor the univariate model, the log-likelihood contribution of study \\(k\\):\r\\[\rl^{U}_k\\left(\\boldsymbol\\beta, \\tau^2\\right) = -\\frac{1}{2} \\log\\left(\\tau^2 + V_k\\right) - \\frac{1}{2} \\frac{\\left(\\bar{T}_k - \\mathbf{x}_k \\boldsymbol\\beta\\right)^2}{\\tau^2 + V_k}.\r\\]\rFor the multivariate model, the log-likelihood contribution of study \\(k\\) is:\r\\[\rl^{MV}_k\\left(\\boldsymbol\\beta, \\tau^2\\right) = -\\frac{1}{2} A -\\frac{1}{2} B\r\\]\rwhere\r\\[\rA = \\log\\left|\\tau^2\\mathbf{1}_k\\mathbf{1}_k\u0026#39; + \\mathbf{S}_k\\right| \\]\rand\r\\[\rB = \\left(\\mathbf{T}_k - \\mathbf{x}_k \\boldsymbol\\beta \\mathbf{1}_k\\right)\u0026#39; \\left(\\tau^2\\mathbf{1}_k\\mathbf{1}_k\u0026#39; + \\mathbf{S}_k\\right)^{-1} \\left(\\mathbf{T}_k - \\mathbf{x}_k \\boldsymbol\\beta \\mathbf{1}_k\\right).\r\\]\rThe term \\(A\\) can be rearranged as\r\\[\rA = \\log\\left|\\left(\\tau^2\\mathbf{1}_k\\mathbf{1}_k\u0026#39;\\mathbf{S}_k^{-1} + \\mathbf{I}_k\\right) \\mathbf{S}_k\\right|\r\\]\rwhere \\(\\mathbf{I}_k\\) is a \\(J_k \\times J_k\\) identity matrix. One of the properties of determinants is that the determinant of a product of two matrices is equal to the product of the determinants. Another is that, for two vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\), \\(\\left|\\mathbf{I} + \\mathbf{u}\\mathbf{v}\u0026#39;\\right| = 1 + \\mathbf{v}\u0026#39;\\mathbf{u}\\). Applying both of these properties, it follows that\r\\[\r\\begin{aligned}\rA \u0026amp;= \\log\\left|\\left(\\tau^2\\mathbf{1}_k\\mathbf{1}_k\u0026#39;\\mathbf{S}_k^{-1} + \\mathbf{I}_k\\right) \\mathbf{S}_k\\right| \\\\\r\u0026amp;= \\log \\left( \\left|\\mathbf{I}_k + \\tau^2\\mathbf{1}_k\\mathbf{1}_k\u0026#39;\\mathbf{S}_k^{-1}\\right| \\left|\\mathbf{S}_k\\right|\\right) \\\\\r\u0026amp;= \\log \\left(1 + \\frac{\\tau^2}{V_k}\\right) + \\log \\left|\\mathbf{S}_k\\right| \\\\\r\u0026amp;= \\log(\\tau^2 + V_k) - \\log(V_k) + \\log \\left|\\mathbf{S}_k\\right|.\r\\end{aligned}\r\\]\rThe \\(B\\) term takes a little more work.\rFrom the Sherman-Morrison identity, we have that:\r\\[\r\\left(\\tau^2\\mathbf{1}_k\\mathbf{1}_k\u0026#39; + \\mathbf{S}_k\\right)^{-1} = \\mathbf{S}_k^{-1} - \\mathbf{S}_k^{-1} \\mathbf{1}_k \\left(\\frac{1}{\\tau^2} + \\frac{1}{V_k}\\right)^{-1} \\mathbf{1}_k\u0026#39;\\mathbf{S}_k^{-1},\r\\tag{1}\r\\]\rby which it follows that\r\\[\r\\mathbf{1}_k\u0026#39;\\left(\\tau^2\\mathbf{1}_k\\mathbf{1}_k\u0026#39; + \\mathbf{S}_k\\right)^{-1}\\mathbf{1}_k = \\frac{1}{\\tau^2 + V_k}.\r\\tag{2}\r\\]\rNow, rearrange the \\(B\\) term to get\r\\[\r\\begin{aligned}\rB \u0026amp;= \\left[\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k + \\left(\\bar{T}_k - \\mathbf{x}_k \\boldsymbol\\beta\\right) \\mathbf{1}_k\\right]\u0026#39; \\left(\\tau^2\\mathbf{1}_k\\mathbf{1}_k\u0026#39; + \\mathbf{S}_k\\right)^{-1} \\left[\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k + \\left(\\bar{T}_k - \\mathbf{x}_k \\boldsymbol\\beta\\right) \\mathbf{1}_k\\right] \\\\\r\u0026amp;= B_1 + 2 B_2 + B_3\r\\end{aligned}\r\\]\rwhere\r\\[\r\\begin{aligned}\rB_1 \u0026amp;= \\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right)\u0026#39; \\left(\\tau^2\\mathbf{1}_k\\mathbf{1}_k\u0026#39; + \\mathbf{S}_k\\right)^{-1} \\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right) \\\\\rB_2 \u0026amp;= \\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right)\u0026#39; \\left(\\tau^2\\mathbf{1}_k\\mathbf{1}_k\u0026#39; + \\mathbf{S}_k\\right)^{-1} \\mathbf{1}_k \\left(\\bar{T}_k - \\mathbf{x}_k \\boldsymbol\\beta\\right) \\\\\rB_3 \u0026amp;= \\left(\\bar{T}_k - \\mathbf{x}_k \\boldsymbol\\beta\\right) \\mathbf{1}_k\u0026#39; \\left(\\tau^2\\mathbf{1}_k\\mathbf{1}_k\u0026#39; + \\mathbf{S}_k\\right)^{-1} \\mathbf{1}_k \\left(\\bar{T}_k - \\mathbf{x}_k \\boldsymbol\\beta\\right)\r\\end{aligned}\r\\]\rApplying (1) to \\(B_1\\),\r\\[\r\\begin{aligned}\rB_1 \u0026amp;= \\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right)\u0026#39; \\left[\\mathbf{S}_k^{-1} - \\mathbf{S}_k^{-1} \\mathbf{1}_k \\left(\\frac{1}{\\tau^2} + \\frac{1}{V_k}\\right)^{-1} \\mathbf{1}_k\u0026#39;\\mathbf{S}_k^{-1}\\right] \\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right) \\\\ \u0026amp;= \\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right)\u0026#39; \\mathbf{S}_k^{-1}\\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right) - \\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right)\u0026#39; \\mathbf{S}_k^{-1} \\mathbf{1}_k \\left(\\frac{1}{\\tau^2} + \\frac{1}{V_k}\\right)^{-1} \\mathbf{1}_k\u0026#39;\\mathbf{S}_k^{-1}\\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right) \\\\\r\u0026amp;= \\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right)\u0026#39; \\mathbf{S}_k^{-1}\\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right).\r\\end{aligned}\r\\]\rThe second term drops out because \\(\\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right)\u0026#39; \\mathbf{S}_k^{-1} \\mathbf{1}_k = \\bar{T}_k / V_k - \\bar{T}_k / V_k = 0\\). Along similar lines,\r\\[\r\\begin{aligned}\rB_2 \u0026amp;= \\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right)\u0026#39; \\left[\\mathbf{S}_k^{-1} - \\mathbf{S}_k^{-1} \\mathbf{1}_k \\left(\\frac{1}{\\tau^2} + \\frac{1}{V_k}\\right)^{-1} \\mathbf{1}_k\u0026#39;\\mathbf{S}_k^{-1}\\right] \\mathbf{1}_k \\left(\\bar{T}_k - \\mathbf{x}_k \\boldsymbol\\beta\\right) \\\\ \u0026amp;= \\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right)\u0026#39; \\mathbf{S}_k^{-1}\\mathbf{1}_k \\left(\\bar{T}_k - \\mathbf{x}_k \\boldsymbol\\beta\\right) - \\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right)\u0026#39; \\mathbf{S}_k^{-1} \\mathbf{1}_k \\left(\\frac{1}{\\tau^2} + \\frac{1}{V_k}\\right)^{-1} \\mathbf{1}_k\u0026#39;\\mathbf{S}_k^{-1}\\mathbf{1}_k \\left(\\bar{T}_k - \\mathbf{x}_k \\boldsymbol\\beta\\right) \\\\\r\u0026amp;= 0.\r\\end{aligned}\r\\]\rFinally, the third term simplifies using (2):\r\\[\rB_3 = \\frac{\\left(\\bar{T}_k - \\mathbf{x}_k \\boldsymbol\\beta\\right)^2}{\\tau^2 + V_k}.\r\\]\rThus, the full \\(B\\) term reduces to\r\\[\rB = \\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right)\u0026#39; \\mathbf{S}_k^{-1}\\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right) + \\frac{\\left(\\bar{T}_k - \\mathbf{x}_k \\boldsymbol\\beta\\right)^2}{\\tau^2 + V_k}\r\\]\rand the multivariate log likelihood contribution is\r\\[\r\\begin{aligned}\rl^{MV}_k\\left(\\boldsymbol\\beta, \\tau^2\\right) \u0026amp;= -\\frac{1}{2} \\log(\\tau^2 + V_k) + \\frac{1}{2} \\log(V_k) - \\frac{1}{2}\\log \\left|\\mathbf{S}_k\\right| - \\frac{1}{2} \\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right)\u0026#39; \\mathbf{S}_k^{-1}\\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right) -\\frac{1}{2} \\frac{\\left(\\bar{T}_k - \\mathbf{x}_k \\boldsymbol\\beta\\right)^2}{\\tau^2 + V_k} \\\\\r\u0026amp;= l^U_k\\left(\\boldsymbol\\beta, \\tau^2\\right) + \\frac{1}{2} \\log(V_k) - \\frac{1}{2}\\log \\left|\\mathbf{S}_k\\right| - \\frac{1}{2} \\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right)\u0026#39; \\mathbf{S}_k^{-1}\\left(\\mathbf{T}_k - \\bar{T}_k \\mathbf{1}_k\\right).\r\\end{aligned}\r\\]\rThe last three terms depend on the data (\\(\\mathbf{T}_k\\) and \\(\\mathbf{S}_k\\)) but not on the parameters \\(\\boldsymbol\\beta\\) or \\(\\tau^2\\). Therefore, the univariate and multivariate likelihoods will be maximized at the same parameter values, i.e., the FML estimators are identical.\n\rRestricted likelihood\rIn practice, it is more common to use RML estimation rather than FML.\rThe RML estimators maximize a different objective function that includes the full likelihood, plus an additional term. The RML objective function for the univariate model is\r\\[\r\\sum_{k=1}^K l^U_k(\\boldsymbol\\beta, \\tau^2) - \\frac{1}{2} R^U(\\tau^2)\r\\]\rwhere\r\\[\rR^U(\\tau^2) = \\log \\left|\\sum_{k=1}^k\\frac{\\mathbf{x}_k\u0026#39; \\mathbf{x}_k}{\\tau^2 + V_k} \\right|.\r\\]\rFor the multivariate model, the RML objective is\r\\[\r\\sum_{k=1}^K l^{MV}_k(\\boldsymbol\\beta, \\tau^2) - \\frac{1}{2} R^{MV}(\\tau^2).\r\\]\rwhere\r\\[\r\\begin{aligned}\rR^{MV}(\\tau^2) \u0026amp;= \\log \\left|\\sum_{k=1}^k \\mathbf{x}_k\u0026#39;\\mathbf{1}_k\u0026#39;\\left(\\tau^2\\mathbf{1}_k\\mathbf{1}_k\u0026#39; + \\mathbf{S}_k\\right)^{-1}\\mathbf{1}_k \\mathbf{x}_k \\right|\\\\\r\u0026amp;= \\log \\left|\\sum_{k=1}^k\\frac{\\mathbf{x}_k\u0026#39; \\mathbf{x}_k}{\\tau^2 + V_k} \\right| \\\\\r\u0026amp;= R^U(\\tau^2)\r\\end{aligned}\r\\]\rbecause of (2). Thus, the univariate and multivariate models also have the same RML estimators.\n\r\rSo what?\rBeyond being a good excuse to write a bunch of matrix algebra, why does any of this matter? I think there are two main implications. First, it is useful to recognize the equivalence of these models in order to understand when the multivariate model is necessary. If both of the conditions that I’ve described hold, then it is entirely acceptable to use aggregation rather than the more complicated multivariate model.3 Using the simpler univariate model might be desirable in practice because it makes the analysis easier to follow, because it makes it easier to run diagnostics or create illustrations of the results, or because of software limitations. Conversely, if either of the conditions does not hold, then there may be differences between the two approaches and the analyst will need to think carefully about which method better addresses their research questions.\nA second implication is computational: because it gives the same results, the univariate model could be used as a short-cut for fitting the multivariate model. Compare the differences in computational time:\nlibrary(microbenchmark)\rmicrobenchmark(\runi = rma.uni(es ~ college + males, vi = var, data = corrdat_agg, method = \u0026quot;REML\u0026quot;),\rmulti = rma.mv(effectsize ~ college + males, V = V_list, random = ~ 1 | studyid,\rdata = corrdat, method = \u0026quot;REML\u0026quot;)\r)\r## Unit: milliseconds\r## expr min lq mean median uq max neval\r## uni 8.5638 8.7961 11.13178 8.96360 9.20370 110.2299 100\r## multi 78.7393 82.2588 85.56066 83.22175 84.71775 182.2056 100\rIf the aggregation is done in advance, it is way quicker to fit the univariate model. The short-cut would be useful if we needed to estimate lots of multi-variate meta-regressions (as long as the equivalence conditions hold). For example, if we needed to bootstrap the multivariate model, we could pre-compute the aggregated effects and then just bootstrap the much simpler, much quicker univariate model.\nI suspect that the results I’ve presented here can be further generalized, but this will need a bit of further investigation. For one, there are also equivalences between variance estimators: using the CR2 cluster-robust variance estimator for the multivariate model is equivalent to using the HC2 heteroskedasticity-robust variance estimator for the univariate model with aggregated effects.4\rFor another, the same sort of equivalence relationships hold even if there are additional random effects in the model, so long as the random effects are at the study level or higher levels of aggregation (e.g., lab effects, where labs are nested within studies).\rI’ll leave these generalizations as exercises for a future rainy day.\n\rReferences\rBecker, B. J. (2000). Multivariate meta-analysis. In S. D. Brown \u0026amp; H. E. A. Tinsley (Eds.), Handbook of applied multivariate statistics and mathematical modeling (pp. 499–525). Academic Press. https://doi.org/10.1016/B978-012691360-6/50018-5\n\rBorenstein, M., Hedges, L. V., Higgins, J. P. T., \u0026amp; Rothstein, H. R. (2009). Introduction to Meta-Analysis. John Wiley \u0026amp; Sons, Ltd. https://doi.org/10.1002/9780470743386\n\rHedges, L. V., Tipton, E., \u0026amp; Johnson, M. C. (2010). Robust variance estimation in meta-regression with dependent effect size estimates. Research Synthesis Methods, 1(1), 39–65. https://doi.org/10.1002/jrsm.5\n\rKalaian, H. a., \u0026amp; Raudenbush, S. W. (1996). A multivariate mixed linear model for meta-analysis. Psychological Methods, 1(3), 227–235. https://doi.org/10.1037/1082-989X.1.3.227\n\rTanner-Smith, E. E., \u0026amp; Tipton, E. (2013). Robust variance estimation with dependent effect sizes: Practical considerations including a software tutorial in Stata and SPSS. Research Synthesis Methods, 5(1), 1–34. https://doi.org/10.1002/jrsm.1091\n\rVan den Noortgate, W., López-López, J. A., Marín-Martínez, F., \u0026amp; Sánchez-Meca, J. (2013). Three-level meta-analysis of dependent effect sizes. Behavior Research Methods, 45(2), 576–594. https://doi.org/10.3758/s13428-012-0261-6\n\rVan den Noortgate, W., López-López, J. A., Marín-Martínez, F., \u0026amp; Sánchez-Meca, J. (2015). Meta-analysis of multiple outcomes: A multilevel approach. Behavior Research Methods, 47(4), 1274–1294. https://doi.org/10.3758/s13428-014-0527-2\n\r\r\r\rA common special case is that the sampling variances for effect sizes within a given study \\(k\\) are all equal, so that \\(S_{ik} = s_{jk} = S_k\\) for \\(i,j = 1,...,J_ik\\) and \\(k = 1,...,K\\). We might further posit that there is a constant sampling correlation between every pair of effect sizes within a given study, so that \\(\\rho_{ijk} = \\rho_k\\) for \\(i,j = 1,...,J_ik\\) and \\(k = 1,...,K\\). If both of these conditions hold, then the inverse-variance weighted average effect size simplifies to the arithmetic average\r\\[\r\\bar{T}_k = \\frac{1}{J_k} \\sum_{j=1}^{J_k} T_{jk}\r\\]\rwith sampling variance\r\\[\rV_k = \\frac{(J_k - 1)\\rho_k + 1}{J} \\times S_k^2\r\\]\r(cf. Borenstein et al., 2009, Eq. (24.6), p. 230).↩︎\n\rThe same thing holds if we use FML rather than RML estimation—try it for yourself and see!↩︎\n\rAs RVE and MLMA become more wide-spread, I could imagine it happening that a meta-analyst who uses aggregation and a univariate model might get push-back from a reviewer, who uncritically recommends using a “more advanced” method to handle dependence. The results in this post provide a way for the meta-analyst to establish that doing so would be unnecessary.↩︎\n\rHere’s verification with the computational example from above:\n# multivariate CR2\rcoef_test(MV_fit, vcov = \u0026quot;CR2\u0026quot;)\r## Coef. Estimate SE t-stat d.f. p-val (Satt) Sig.\r## 1 intrcpt 0.64656 0.17647 3.66 11.5 0.00345 **\r## 2 college 0.37027 0.18648 1.99 11.9 0.07053 .\r## 3 males -0.00763 0.00287 -2.66 14.5 0.01826 *\r# univariate HC2\rcoef_test(uni_fit, vcov = \u0026quot;CR2\u0026quot;, cluster = corrdat_agg$studyid)\r## Coef. Estimate SE t-stat d.f. p-val (Satt) Sig.\r## 1 intrcpt 0.64656 0.17622 3.67 11.5 0.00342 **\r## 2 college 0.37027 0.18597 1.99 11.9 0.06985 .\r## 3 males -0.00763 0.00287 -2.66 14.5 0.01808 *\r↩︎\r\r\r","date":1562025600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562025600,"objectID":"29d9c13241b57b23cbb322ce695c0fed","permalink":"/sometimes-aggregating-effect-sizes-is-fine/","publishdate":"2019-07-02T00:00:00Z","relpermalink":"/sometimes-aggregating-effect-sizes-is-fine/","section":"post","summary":"In meta-analyses of psychology, education, and other social science research, it is very common that some of the included studies report more than one relevant effect size.\rFor example, in a meta-analysis of intervention effects on reading outcomes, some studies may have used multiple measures of reading outcomes (each of which meets inclusion criteria), or may have measured outcomes at multiple follow-up times; some studies might have also investigated more than one version of an intervention, and it might be of interest to include effect sizes comparing each version to the no-intervention control condition;\rand it’s even possible that some studies may have all of these features, potentially contributing lots of effect size estimates.","tags":["effect size","meta-analysis","dependent effect sizes"],"title":"Sometimes, aggregating effect sizes is fine","type":"post"},{"authors":["James E. Pustejovsky","Megha Joshi"],"categories":null,"content":"","date":1561852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561852800,"objectID":"35d8155e0f8782843c60348e15e8deea","permalink":"/publication/transition-to-college-mathematics-year-1/","publishdate":"2019-06-30T00:00:00Z","relpermalink":"/publication/transition-to-college-mathematics-year-1/","section":"publication","summary":"Texas House Bill 5 introduced requirements that school districts partner with institutions of higher education to provide college preparatory courses in mathematics and English for high school seniors who are not yet college ready. As districts and college partners begin to respond to these provisions, there is a need for empirical research on the effects of different approaches to implementing the college preparatory courses. In response to House Bill 5 requirements, the Charles A. Dana Center has developed a model college preparatory mathematics course, Transition to College Mathematics Course (TCMC), which has been adopted by dozens of school districts across Texas over the past several school years. We examine the effects of TCMC on students’ progress into post-secondary education by comparing students who participated in TCMC during the 2016-17 school year (the first year of implementation) to observationally similar students, either from a previous cohort that did not have access to TCMC or from the same cohort but who did not enroll in the course. We find that, although students who took TCMC graduated at slightly higher rates than comparison students, they had lower rates of enrollment in post-secondary education, driven by lower rates of enrollment in 4-year colleges or universities. Enrollment gradually became more similar over the four semesters following graduation from high school. We find that students who took TCMC were also less likely than students in the comparison group to pass college-level and developmental math courses. Longer-term cumulative outcomes showed stronger reductions in rates of math course passage. However, these results must be interpreted cautiously because we were unable to fully assess and account for students’ college-readiness status at the start of their senior year.","tags":["causal inference","evaluation","Transition to College Mathematics"],"title":"Evaluating the Transition to College Mathematics Course in Texas high schools: Findings from the first year of implementation","type":"publication"},{"authors":["Tom V. Merluzzi","James E. Pustejovsky","Errol J. Philip","Stephanie J. Sohl","Mark Berendsen","John A. Salsman"],"categories":null,"content":"","date":1559779200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559779200,"objectID":"f597dcd3637a4926f5e574440009a402","permalink":"/publication/interventions-to-enhance-self-efficacy-in-cancer-patients/","publishdate":"2019-06-06T00:00:00Z","relpermalink":"/publication/interventions-to-enhance-self-efficacy-in-cancer-patients/","section":"publication","summary":"Objective: Self-efficacy expectations are associated with improvements in problematic outcomes widely considered clinically significant (i.e., emotional distress, fatigue, pain), related to positive health behaviors, and, as a type of personal agency, inherently valuable. Self-efficacy expectancies, estimates of confidence to execute behaviors, are important in that changes in selfefficacy expectations are positively related to future behaviors that promote health and wellbeing. The current meta-analysis investigated the impact of psychological interventions on self-efficacy expectations for a variety of health behaviors among cancer patients. Methods: Ovid Medline, PsycINFO, CINAHL, EMBASE, Cochrane Library, and Web of Science were searched with specific search terms for identifying randomized controlled trials (RCTs) that focused on psychologically-based interventions. Included studies had: 1) an adult cancer sample, 2) a self-efficacy expectation measure of specific behaviors and 3) an RCT design. Standard screening and reliability procedures were used for selecting and coding studies. Coding included theoretically informed moderator variables. Results: Across 79 RCTs, 223 effect sizes, and 8678 participants, the weighted average effect of self-efficacy expectations was estimated as g=0.274 (p","tags":["psycho-social intervention","cancer","meta-analysis","self-efficacy","systematic review"],"title":"Interventions to enhance self-efficacy in cancer patients and survivors: A meta-analysis of randomized controlled trials","type":"publication"},{"authors":[],"categories":null,"content":"","date":1558890000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558890000,"objectID":"f8f67d3640e4f349ad6a5c713e3c3e2e","permalink":"/talk/abai-2019-log-response-ratios/","publishdate":"2019-05-26T17:00:00Z","relpermalink":"/talk/abai-2019-log-response-ratios/","section":"talk","summary":"","tags":[],"title":"Log response ratio effect sizes: Rationale and methods for single case designs with behavioral outcomes","type":"talk"},{"authors":["Charis L. Wahman","James E. Pustejovsky","Michaelene M. Ostrosky","Rosa Milagros Santos"],"categories":null,"content":"","date":1557878400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557878400,"objectID":"cf8079e3e16291073f4434799c0840de","permalink":"/publication/effects-of-social-stories-on-challenging-behavior-and-prosocial-skills/","publishdate":"2019-05-15T00:00:00Z","relpermalink":"/publication/effects-of-social-stories-on-challenging-behavior-and-prosocial-skills/","section":"publication","summary":"Social stories are a commonly used intervention practice in early childhood special education. Recent systematic reviews have documented the evidence-base for social stories, but findings are mixed. We examined the efficacy of social stories for young children (i.e., 3-5 years) with challenging behavior across 12 single-case studies, that included 30 participants. The What Works Clearinghouse standards for single case research design were used to evaluate the rigor of studies that included social stories as a primary intervention. For studies meeting standards, we synthesized findings on the efficacy of social stories using meta-analysis techniques and a recently developed parametric effect size measure, the log response ratio. Trends in participants’ response to treatment also were explored. Results indicate variability in rigor and efficacy for the use of social stories as an isolated intervention and in combination with other intervention approaches. Additional studies that investigate the efficacy of social stories as a primary intervention are warranted.","tags":["single-case design","meta-analysis","systematic review"],"title":"Examining the effects of social stories on challenging behavior and prosocial skills in young children: A systematic review and meta-analysis","type":"publication"},{"authors":["Crystal L. Park","James E. Pustejovsky","Kelly Trevino","Allen C. Sherman","Craig Esposito","Mark Berendsen","John A. Salsman"],"categories":null,"content":"","date":1556496000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556496000,"objectID":"cc781803b81a48bce62f4b8ae535ab73","permalink":"/publication/psychosocial-interventions-meaning-and-purpose/","publishdate":"2019-04-29T00:00:00Z","relpermalink":"/publication/psychosocial-interventions-meaning-and-purpose/","section":"publication","summary":"Meaning and purpose in life are associated with the mental and physical health of patients with cancer and survivors and also constitute highly valued outcomes in themselves. Because meaning and purpose are often threatened by a cancer diagnosis and treatment, interventions have been developed to promote meaning and purpose. The present meta-analysis of randomized controlled trials (RCTs) evaluated effects of psychosocial interventions on meaning/purpose in adults with cancer and tested potential moderators of intervention effects. Six literature databases were systematically searched to identify RCTs of psychosocial interventions in which meaning or purpose was an outcome. Using Preferred Reporting Items for Systematic Reviews and Meta‐Analyses guidelines, rater pairs extracted and evaluated data for quality. Findings were synthesized across studies with standard meta‐analytic methods, including meta‐regression with robust variance estimation and risk-of-bias sensitivity analysis. Twenty-nine RCTs were identified, and they encompassed 82 treatment effects among 2305 patients/survivors. Psychosocial interventions were associated with significant improvements in meaning/purpose (g = 0.37; 95% CI, 0.22-0.52; P ","tags":["cancer","meaning and purpose","meta-analysis","psycho-social intervention","systematic review"],"title":"Effects of psychosocial interventions on meaning and purpose in adults with cancer: A systematic review and meta-analysis","type":"publication"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\r\r2020-05-03 This post describes an implementation of code folding for an older version of the Academic Theme. It does not work with Academic 4.+. See my updated instructions to get it working with newer versions of Academic.\r\r\rRmarkdown documents now have a very nifty code folding option, which allows the reader of a compiled html document to toggle whether to view or hide code chunks. However, the feature is not supported in blogdown, the popular Rmarkdown-based website/blog creation package. I recently ran across an implementation of codefolding for blogdown, developed by Sébastien Rochette. I have been putzing around, trying to get it to work with my blog, which uses the Hugo Academic theme—alas, to no avail. To my amazement and good fortune, Sébastien swooped in with a pull request that cleaned up my blundering attempts at implementation. Now all of my posts have working code folding!\nIn this post, I’ll lay out how to make Sébastien’s code folding feature work with the Academic theme. To be totally clear, all of the hard bits of this were solved by Sébastien. I don’t know javascript to save my life, and my only contribution is to write down the instructions in what I hope is a coherent fashion, so that you too can soon be doing the happy code folding dance if you so desire.\nCode folding with the Academic theme\rYou’ll first need to pull in some javascript assets. Create a folder called js under the \\static directory of your site. Add the files transition.js, collapse.js, and dropdown.js from bootstrap.\n\rAlso add Sébastien’s codefolding javascript, codefolding.js.\n\rCreate a folder called css under the \\static directory of your site. Add the file codefolding.css. This is the css for the buttons that will appear on your posts.\n\rAdd the file article_footer_js.html to the \\layouts\\partials directory of your site.\n\rAdd the file header_maincodefolding.html to the \\layouts\\partials directory of your site.\n\rIf you do not already have a file head_custom.html in the \\layouts\\partials directory, create it.. Add the following lines of code to the file:\n{{ if not .Site.Params.disable_codefolding }}\r\u0026lt;script src=\u0026quot;{{ \u0026quot;js/collapse.js\u0026quot; | relURL }}\u0026quot;\u0026gt;\u0026lt;/script\u0026gt;\r\u0026lt;script src=\u0026quot;{{ \u0026quot;js/dropdown.js\u0026quot; | relURL }}\u0026quot;\u0026gt;\u0026lt;/script\u0026gt;\r\u0026lt;script src=\u0026quot;{{ \u0026quot;js/transition.js\u0026quot; | relURL }}\u0026quot;\u0026gt;\u0026lt;/script\u0026gt;\r{{ end }}\rIf you do not already have a file footer.html in the \\layouts\\partials directory, copy it over from \\themes\\hugo-academic\\layouts\\partials. Add the following lines of code to it, somewhere towards the bottom (see my version for example):\n\u0026lt;!-- Init code folding --\u0026gt;\r{{ partial \u0026quot;article_footer_js.html\u0026quot; . }}\rIf you do not already have the file single.html in the directory \\layouts\\_default, copy it over from \\themes\\hugo-academic\\layouts\\_default. Add the following line of code at an appropriate point so that your posts will include the “Show/hide code” button (I put it after the title, before the meta-data; see here):\n {{ partial \u0026quot;header_maincodefolding\u0026quot; . }}\rModify your config.toml file (in the base directory of your site) to include the following lines:\n# Set to true to disable code folding\rdisable_codefolding = false\r# Set to \u0026quot;hide\u0026quot; or \u0026quot;show\u0026quot; all codes by default\rcodefolding_show = \u0026quot;show\u0026quot;\r# Set to true to exclude the \u0026quot;Show/hide all\u0026quot; button\rcodefolding_nobutton = false\rAlso edit the custom_css parameter so that the codefolding.css file will get loaded:\ncustom_css = [\u0026quot;codefolding.css\u0026quot;]\r\r\rUsing the codefolding parameters\rThe config.toml file now has three parameters that control code folding:\n\rdisable_codefolding controls whether to load the code folding scripts on your site. Set it to true to disable code folding globally.\rcodefolding_show controls whether code blocks will be shown or hidden by default. If your previous posts have lots of code in them, set the default to show to minimize changes in the appearance of your site.\rcodefolding_nobutton controls whether the “Show/hide code” button will appear at the top of posts that include code blocks. Set it to true to disable the button but keep the other code folding functionality.\r\rThe above parameters are defaults for your entire site. To over-ride the defaults, you can also set the parameters in the YAML header of any post:\n\rSet disable_codefolding: true to turn off code folding for the post.\rSet codefolding_show: hide to hide the code blocks in the post (as in this post).\rSet codefolding_nobutton: true to turn off the “Show/hide code” button at the top of the post (as in the present post).\r\rI hope these instructions work for you. If not, questions, corrections, and clarifications are welcome. Thanks again to Sébastien Rochette for working out this solution and for graciously troubleshooting my attempt at implementation. Happy blogging, y’all!\n\r","date":1555200000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555200000,"objectID":"2574b2225a625ff539018e39fbdfabcf","permalink":"/code-folding-with-blogdown-academic/","publishdate":"2019-04-14T00:00:00Z","relpermalink":"/code-folding-with-blogdown-academic/","section":"post","summary":"2020-05-03 This post describes an implementation of code folding for an older version of the Academic Theme. It does not work with Academic 4.+. See my updated instructions to get it working with newer versions of Academic.","tags":["programming","Rstats"],"title":"Code folding with blogdown + Academic theme","type":"post"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\r\rAt AERA this past weekend, one of the recurring themes was how software availability (and its usability and default features) influences how people conduct meta-analyses. That got me thinking about the R packages that I’ve developed, how to understand the extent to which people are using them, how they’re being used, and so on. I’ve had badges on my github repos for a while now:\n\rclubSandwich: \rARPobservation: \rscdhlm: \rSingleCaseES: \r\rThese statistics come from the METACRAN site, which makes available data on daily downloads of all packages on CRAN (one of the main repositories for sharing R packages). The downloads are from the RStudio mirror of CRAN, which is only one of many mirrors around the world. Although the data do not represent complete tallies of all package downloads, they are nonetheless the best available source that I’m aware of.\nThe thing is, the download numbers are rather hard to interpret. Beyond knowing that somebody out there is at least trying to use the tools I’ve made, it’s pretty hard to gauge whether 300 or 3000 or 3 million downloads a month is a good usage level. In this post, I’ll attempt to put just a little bit of context around these numbers. Emphasis on little bit, as I’m not all that satisfied with what I’ll show below, but at least it’s something beyond four numbers floating in the air.\nGetting package download data\rI used the cranlogs package to get daily download counts of all currently available CRAN packages over the period 2018-04-05 18:00:00 through 2019-04-06. I then limited the sample to packages that had been downloaded at least once between 2018-04-05 18:00:00 and 2018-10-05. This had the effect of excluding about 1000 packages that were either only recently added to CRAN or that had been discontinued but were still sitting on CRAN.\nlibrary(tidyverse)\rlibrary(lubridate)\rlibrary(cranlogs)\rto_date \u0026lt;- \u0026quot;2019-04-06\u0026quot;\rfrom_date \u0026lt;- as.character(as_date(to_date) - duration(1, \u0026quot;year\u0026quot;))\rfile_name \u0026lt;- paste0(\u0026quot;CRAN package downloads \u0026quot;, to_date, \u0026quot;.rds\u0026quot;)\rpkg_downloads \u0026lt;-\ravailable.packages() %\u0026gt;%\ras_tibble() %\u0026gt;%\rselect(Package, Version) %\u0026gt;%\rmutate(grp = 1 + trunc((row_number() - 1) / 100)) %\u0026gt;%\rnest(Package, Version) %\u0026gt;%\rmutate(downloads = map(.$data, ~ cran_downloads(packages = .$Package, from = from_date, to = to_date))) %\u0026gt;%\rselect(-data) %\u0026gt;%\runnest()\rdownloaded_last_yr \u0026lt;- pkg_downloads %\u0026gt;%\rfilter(date \u0026lt;= as_date(as_date(to_date) - duration(6, \u0026quot;months\u0026quot;))) %\u0026gt;%\rgroup_by(package) %\u0026gt;%\rsummarise(\rcount = sum(count),\r.groups = \u0026quot;drop\u0026quot;\r) %\u0026gt;%\rfilter(count \u0026gt; 0) %\u0026gt;%\rselect(package)\rThis yielded 12925 packages. For each of these packages, I then calculated the average monthly download rate over the most recent six months, along with where that rate falls as a percentile of all packages in the sample.\ndownloads_past_six \u0026lt;-\rpkg_downloads %\u0026gt;%\rfilter(date \u0026gt; as_date(as_date(to_date) - duration(6, \u0026quot;months\u0026quot;))) %\u0026gt;%\rsemi_join(downloaded_last_yr, by = \u0026quot;package\u0026quot;) %\u0026gt;%\rgroup_by(package) %\u0026gt;%\rsummarise(\rcount = sum(count) / 6,\r.groups = \u0026quot;drop\u0026quot;\r) %\u0026gt;%\rmutate(\rpackage = fct_reorder(factor(package), count),\rpct_less = cume_dist(count)\r)\r\rPusto’s packages\rI have developed four packages that are currently available on CRAN:\n\rThe clubSandwich package provides cluster-robust variance estimators for a variety of different linear models (including meta-regression, hierarchical linear models, panel data models, etc.), as well as (more recently) some instrumental variables models. The package has received some attention in connection with estimating meta-analysis and meta-regression models, and it’s also relevant to applied micro-economics, field experiments, and other fields.\rThe scdhlm and SingleCaseES packages provide functions and interactive web apps for calculating various effect sizes for single-case experimental designs. The SingleCaseES package is fairly new and I haven’t yet written any articles that feature it. Both it and scdhlm are relevant in fairly specialized fields where single-case experimental designs are commonly used—and where there is a need to meta-analyze results from such designs—and so I would not expect them to be widely downloaded.\rThe ARPobservation package provides tools for simulating behavioral observation data based on an alternating renewal process model. I developed this package for my own dissertation work, and my students and I have used it in some subsequent work. I think of it mostly as a tool for my group’s work on statistical methods for single-case experimental designs, and so would not expect to be widely downloaded or used outside of this area.\r\rAs points of comparison to my contributions, it is perhaps useful to look at two popular packages for conducting meta-analysis, the metafor package and the robumeta package:\n\rThe metafor package, developed by Wolfgang Viechtbauer, has been around for 10 years and includes all sorts of incredible tools for calculating effect sizes, estimating meta-analysis and meta-regression models, investigating fitted models, and representing the results graphically. In contrast, the clubSandwich package is narrower in scope—it just calculates robust standard errors, confidence intervals, etc.—so metafor is not a perfect point of comparison.\rThe robumeta package, by Zachary Fisher and Elizabeth Tipton, is a closer match in terms of scope. It is used for estimating meta-regression models with robust variance estimation, using specific methods proposed by Hedges, Tipton, and Johnson (2010).\r\rI am having a harder time thinking of good comparables for the scdhlm, SingleCaseES, and ARPobservation packages due to their specialized focus. (Ideas? Suggestions? I’m all ears!)\nWith that background, here are the average monthly download rates (over the past six months) for each of my four packages, along with metafor and robumeta:\nlibrary(knitr)\rlibrary(kableExtra)\rPusto_pkgs \u0026lt;- c(\u0026quot;ARPobservation\u0026quot;,\u0026quot;scdhlm\u0026quot;,\u0026quot;SingleCaseES\u0026quot;,\u0026quot;clubSandwich\u0026quot;)\rmeta_pkgs \u0026lt;- c(\u0026quot;metafor\u0026quot;,\u0026quot;robumeta\u0026quot;)\rfocal_downloads \u0026lt;- downloads_past_six %\u0026gt;%\rfilter(package %in% c(Pusto_pkgs, meta_pkgs)) %\u0026gt;%\rmutate(\rcount = round(count),\rpct_less = round(100 * pct_less, 1)\r) %\u0026gt;%\rarrange(desc(count))\rfocal_downloads %\u0026gt;%\rrename(`Average monthly downloads` = count, `Percentile of CRAN packages` = pct_less) %\u0026gt;%\rkable() %\u0026gt;%\rkable_styling(bootstrap_options = c(\u0026quot;hover\u0026quot;, \u0026quot;condensed\u0026quot;), full_width = FALSE)\r\r\rpackage\r\rAverage monthly downloads\r\rPercentile of CRAN packages\r\r\r\r\r\rmetafor\r\r7348\r\r94.0\r\r\r\rclubSandwich\r\r2992\r\r90.3\r\r\r\rrobumeta\r\r2025\r\r87.9\r\r\r\rARPobservation\r\r387\r\r55.5\r\r\r\rSingleCaseES\r\r306\r\r36.4\r\r\r\rscdhlm\r\r229\r\r7.5\r\r\r\r\rThus, clubSandwich sits in between metafor and robumeta, at the 90th percentile among all active packages on CRAN. The other packages are much less widely downloaded, averaging between 200 and 400 downloads per month. The distribution of monthly download rates is highly skewed, as can be seen in the figure below. About 68% of packages are downloaded 500 times or fewer per month, while only 7% of packages get more than 5000 downloads per month.\nlibrary(colorspace)\rlibrary(ggrepel)\rdownloads_sample \u0026lt;- downloads_past_six %\u0026gt;%\rarrange(count) %\u0026gt;%\rmutate(\rfocal = package %in% c(Pusto_pkgs,meta_pkgs),\rtenth = (row_number(count) %% 10) == 1\r) %\u0026gt;%\rfilter(focal | tenth)\rfocal_pkg_dat \u0026lt;- downloads_sample %\u0026gt;%\rfilter(focal) %\u0026gt;%\rmutate(Pusto = if_else(package %in% Pusto_pkgs, \u0026quot;Pusto\u0026quot;,\u0026quot;comparison\u0026quot;))\rtitle_str \u0026lt;- paste(\u0026quot;Average monthly downloads of R packages from\u0026quot;, as_date(as_date(to_date) - duration(6, \u0026quot;months\u0026quot;)),\u0026quot;through\u0026quot;,to_date)\rqualitative_hcl(n = 2, h = c(140, -30), c = 90, l = 40, register = \u0026quot;custom-qual\u0026quot;)\rggplot(downloads_sample, aes(x = package, y = count)) +\rgeom_col() + geom_col(data = focal_pkg_dat, aes(color = Pusto, fill = Pusto), size = 1.5) + geom_label_repel(\rdata = focal_pkg_dat, aes(color = Pusto, label = package),\rsegment.size = 0.4,\rsegment.color = \u0026quot;grey50\u0026quot;,\rnudge_y = 0.5,\rpoint.padding = 0.3\r) + scale_y_log10(breaks = c(20, 50, 200, 500, 2000, 5000, 20000, 50000, 200000), labels = scales::comma) + scale_fill_discrete_qualitative(palette = \u0026quot;custom-qual\u0026quot;) + scale_color_discrete_qualitative(palette = \u0026quot;custom-qual\u0026quot;) + labs(x = \u0026quot;\u0026quot;, y = \u0026quot;Downloads (per month)\u0026quot;, title = title_str) + theme(legend.position = \u0026quot;none\u0026quot;, axis.line.x = element_blank(), axis.ticks.x = element_blank(), axis.text.x = element_blank())\r\rDownloads over time\rHere are the weekly download rates for each of my packages over the past two years. (Note that the vertical scales of the graphs differ.)\nweekly_downloads \u0026lt;- pkg_downloads %\u0026gt;%\rmutate(\ryr = year(date),\rwk = week(date)\r) %\u0026gt;%\rgroup_by(package, yr, wk) %\u0026gt;%\rmutate(\rdate = max(date)\r) %\u0026gt;%\rgroup_by(package, date) %\u0026gt;%\rsummarise(\rcount = sum(count),\rdays = n(),\r.groups = \u0026quot;drop\u0026quot;\r)\rweekly_downloads %\u0026gt;%\rfilter(\rdays == 7,\rpackage %in% Pusto_pkgs\r) %\u0026gt;%\rggplot(aes(date, count, color = package)) + geom_line() + expand_limits(y = 0) + facet_wrap(~ package, scales = \u0026quot;free\u0026quot;, ncol = 2) + theme_minimal() + labs(x = \u0026quot;\u0026quot;, y = \u0026quot;Downloads (per month)\u0026quot;) + theme(legend.position = \u0026quot;none\u0026quot;)\rThere are a couple of curious features in these plots. For one, there are big spikes in downloads of ARPobservation and SingleCaseES. The ARPobservation spike was in mid-June of 2018, when I was at the IES Single-Case Design training institute and demonstrated some of the package’s tools. The SingleCaseES spike was in early January, 2019. Perhaps someone was teaching a class in single-case research and demonstrated the package? Or something at the IES PI meeting (January 9-10, 2019)?\nAnother interesting pattern is in the download rate of scdhlm, which looks like it increased systematically starting in September, 2018. I wonder if this was the result of someone demonstrating or incorporating use of the package into a course. Lacking details about where the downloads are coming from, it’s hard to do anything but speculate.\n\rCaveats and musings\rClearly, download counts are only a very rough proxy for package usage. In marketing-speak, they might be more like leads than conversion, in that people might be downloading a package only to discover that it’s not good for anything and then never use it to accomplish anything. Downloads are also not one-time events. If they use it in their work, a single person will likely download a package many times, over a span of time as new versions are released, onto multiple machines that they might use, by accident in the process of trying to install some other package, and so on. Downloads of inter-related packages are likely to be highly correlated too, as they will be with release of new major versions of R, which probably makes it a bit tricky to do event studies.\nUltimately, I don’t know that knowing where my packages stand in terms of download rankings is all that useful. The packages that I’ve developed are all aimed at fairly academic audiences, which means that citations would probably be a better measure of contribution. The problem is, many people don’t know that they should be citing software, or how to do it. As usual, there’s an R function for that. Here’s how to get the citation for clubSandwich:\ncitation(package=\u0026quot;clubSandwich\u0026quot;) %\u0026gt;%\rprint(style = \u0026quot;textVersion\u0026quot;)\rwhich returns the following:\n\rJames Pustejovsky (2020). clubSandwich: Cluster-Robust (Sandwich) Variance Estimators with Small-Sample\rCorrections. R package version 0.5.0. https://CRAN.R-project.org/package=clubSandwich\n\r\r","date":1554768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554768000,"objectID":"be2d6aeb1397241e3f0074f90939ca68","permalink":"/package-downloads/","publishdate":"2019-04-09T00:00:00Z","relpermalink":"/package-downloads/","section":"post","summary":"At AERA this past weekend, one of the recurring themes was how software availability (and its usability and default features) influences how people conduct meta-analyses. That got me thinking about the R packages that I’ve developed, how to understand the extent to which people are using them, how they’re being used, and so on.","tags":["programming","Rstats","meta-analysis"],"title":"CRAN downloads of my packages","type":"post"},{"authors":[],"categories":null,"content":"","date":1554681600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554681600,"objectID":"bf7e9b84c2ccc548683ce91f128fd826","permalink":"/talk/aera-2019-orb-dependence/","publishdate":"2019-04-08T16:01:00Z","relpermalink":"/talk/aera-2019-orb-dependence/","section":"talk","summary":"","tags":[],"title":"Evaluating meta-analytic methods to detect outcome reporting bias in the presence of dependent effect sizes","type":"talk"},{"authors":[],"categories":null,"content":"","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"33335d23834b8aee16b7967d4e045a2f","permalink":"/talk/aera-2019-outcome-measurement-procedures/","publishdate":"2019-04-07T16:00:00Z","relpermalink":"/talk/aera-2019-outcome-measurement-procedures/","section":"talk","summary":"","tags":[],"title":"An examination of measurement procedures and baseline behavioral outcomes in single-case research","type":"talk"},{"authors":[],"categories":null,"content":"","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"9dc4f7c924e2458776b589c3b667c78b","permalink":"/talk/aera-2019-response-guided-algorithms/","publishdate":"2019-04-07T16:01:00Z","relpermalink":"/talk/aera-2019-response-guided-algorithms/","section":"talk","summary":"","tags":[],"title":"The impact of response-guided designs on count outcomes in single-case design baselines","type":"talk"},{"authors":["James E. Pustejovsky"],"categories":null,"content":"","date":1554076800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554076800,"objectID":"8e75aa43cbe2d755ffb4da906799c5cb","permalink":"/publication/procedural-sensitivities-of-scd-effect-sizes/","publishdate":"2019-04-01T00:00:00Z","relpermalink":"/publication/procedural-sensitivities-of-scd-effect-sizes/","section":"publication","summary":"A wide variety of effect size indices have been proposed for quantifying the magnitude of treatment effects in single-case designs. Commonly used measures include parametric indices such as the standardized mean difference, as well as non-overlap measures such as the percentage of non-overlapping data, improvement rate difference, and non-overlap of all pairs. Currently, little is known about the properties of these indices when applied to behavioral data collected by systematic direct observation, even though systematic direct observation is the most common method for outcome measurement in single-case research. This study uses Monte Carlo simulation to investigate the properties of several widely used single-case effect size measures when applied to systematic direct observation data. Results indicate that the magnitude of the non-overlap measures and of the standardized mean difference can be strongly influenced by procedural details of the study's design, which is a significant limitation to using these indices as effect sizes for meta-analysis of single-case designs. A less widely used parametric index, the log-response ratio, has the advantage of being insensitive to sample size and observation session length, although its magnitude is influenced by the use of partial interval recording.","tags":["alternating renewal process","effect size","response ratio","single-case design","behavioral observation","non-overlap measures"],"title":"Procedural sensitivities of effect sizes for single-case designs with behavioral outcome measures","type":"publication"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rThis year, Dr. Laura Dunne and I are serving as program co-chairs for the AERA special interest group on Systematic Reviews and Meta-Analysis, which is a great group of scholars interested in the methodology and application of research synthesis to questions in education and the broader social sciences. We had a strong batch of submissions to the SIG and (since we’re new and still a fairly small group) only a few sessions to fill with them. In assembling this year’s program, Laura and I noted a few common themes that stood out to us. In this post, I’ll highlight a few of them and hopefully whet your appetite to hear more during our sessions at this year’s convention. And if you want to skip the details for now, just take a look at our handy pdf with the full SIG program.\nSIG highlights\rFirst, two of this year’s presentations deal with network meta-analysis, an approach that goes beyond a single intervention-control comparison, to instead synthesize evidence on the comparative effects of multiple alternative interventions (not just red pill vs blue pill, but also red versus green, green versus blue, etc.). Network meta-analysis is increasingly important in clinical medicine (for example, here’s a recent synthesis examining the relative efficacy of 21 different anti-depressant drugs) but it is still relatively rare in education and other social science meta-analyses. Not in this year’s SIG program though! Both our Sunday morning paper session and Monday round table feature applications of network meta-analysis: one on distance and face-to-face learning, and one on interventions for treatment of post-traumatic stress disorder.\nSecond, publication bias and other forms of outcome reporting bias remain one of the most vexing challenges for meta-analysis. Our Sunday morning paper session includes an innovative methodological study on how to detect selective outcome reporting in multi-level meta-analyses—an important setting where publication bias techniques have yet to be explored. Even with very sophisticated statistical tools, though, the best way to address publication bias is probably to try and prevent it in the first place. To that end, our Monday round table session includes a presentation on locating unreported outcome data for use in meta-analysis.\nThird, the Systematic Reviews and Meta-Analysis SIG has always included a mix of theory and practice. In this year’s program, we’ve tried to preserve that mix within each of our sessions, so that our Sunday paper session and Monday round table each include both methodological research and substantive applications of meta-analysis. We hope that this will promote interesting and valuable dialogues within our community.\nFinally, I am very excited that our business meeting will feature an address by Dr. Rebecca Maynard, who is the University Trustee Chair Professor of Education and Social Policy at the University of Pennsylvania Graduate School of Education, and an influential voice in the use of research synthesis methods to inform education and social policy. She’ll be speaking on Expanded Roles for Meta-Analysis in Supporting Evidence-Based Policy and Practice.\nBe sure to check out the SIG program for more details and other sessions of interest. I look forward to seeing everyone in Toronto!\n\r","date":1553558400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553558400,"objectID":"c42859b0280c5b96a9a8037f3b50e476","permalink":"/aera-2019-srma-sig/","publishdate":"2019-03-26T00:00:00Z","relpermalink":"/aera-2019-srma-sig/","section":"post","summary":"This year, Dr. Laura Dunne and I are serving as program co-chairs for the AERA special interest group on Systematic Reviews and Meta-Analysis, which is a great group of scholars interested in the methodology and application of research synthesis to questions in education and the broader social sciences.","tags":["meta-analysis","AERA"],"title":"Systematic Reviews and Meta-analysis SIG at AERA 2019","type":"post"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rI’m just back from the Society for Research on Educational Effectiveness meetings, where I presented work on small-sample corrections for cluster-robust variance estimators in two-stage least squares models, which I’ve implemented in the clubSandwich R package. Here’s my presentation. So I had “clubSandwich” estimators on the brain when a colleague asked me about whether the methods were implemented in SAS.\nThe short answer is “no.”\nThe moderately longer answer is “not unless we can find funding to pay someone who knows how to program properly in SAS.” However, for the specific model that my colleague was interested in, it turns out that the small-sample corrections implemented in clubSandwich can be expressed in closed form, and they’re simple enough that they could easily be hand-calculated. I’ll sketch out the calculations in the remainder of this post.\nA multi-site trial\rConsider a multi-site trial conducted across \\(J\\) sites, which we take as a sample from a larger super-population of sites. Each site consists of \\(n_j\\) units, of which \\(p_j n_j\\) are randomized to treatment and the remainder \\((1 - p_j) n_j\\) are randomized to control. For each unit \\(i\\) in each site \\(j\\), we have an outcome \\(y_{ij}\\) and a treatment indicator \\(t_{ij}\\).\nA conventional approach to estimating the overall average impact in this setting is to use a model with a treatment indicator and fixed effects for each site:\r\\[\ry_{ij} = \\beta_j + \\delta t_{ij} + e_{ij}\r\\]\rand then to cluster the standard errors by site. Clustering by site makes sense here if (and only if) we’re interested in generalizing to the super-population of sites.\nLet \\(\\hat\\delta_j\\) denote the impact estimate from site \\(j\\), calculated as the difference in means between treated and untreated units at site \\(j\\):\r\\[\r\\hat\\delta_j = \\frac{1}{n_j p_j} \\left(\\sum_{i=1}^{n_j} t_{ij} y_{ij}\\right) - \\frac{1}{n_j (1 - p_j)} \\left(\\sum_{i=1}^{n_j} (1 - t_{ij}) y_{ij}\\right).\r\\]\rfor \\(j = 1,..,J\\). The overall impact estimate here is a precision-weighted average of the site-specific impacts:\r\\[\r\\hat\\delta = \\frac{1}{W} \\sum_{j=1}^J w_j \\hat\\delta_j,\r\\]\rwhere \\(w_j = n_j p_j (1 - p_j)\\) and \\(W = \\sum_j w_j\\).\n\rSandwich estimators\rThe conventional clustered variance estimator (or sandwich estimator) for \\(\\hat\\delta\\) is a simple function of the (weighted) sample variance of the site-specific effects. It can be calculated directly as:\r\\[\rV^{CR0} = \\frac{1}{W^2} \\sum_{j=1}^J w_j^2 \\left(\\hat\\delta_j - \\hat\\delta\\right)^2.\r\\]\rUnder a conventional random effects model for the \\(\\delta_j\\)s, this estimator has a downward bias in finite samples.\nThe clubSandwich variance estimator here uses an estimator for the sample variance of site-specific effects that is unbiased under a certain working model. It is only slightly more complicated to calculate:\r\\[\rV^{CR2} = \\frac{1}{W^2} \\sum_{j=1}^J \\frac{w_j^2 \\left(\\hat\\delta_j - \\hat\\delta\\right)^2}{1 - w_j / W}.\r\\]\nThe other difference between conventional methods and the clubSandwich approach is in the reference distribution used to calculate hypothesis tests and confidence intervals. The conventional approach uses a standard normal reference distribution (i.e., a z-test) that is asymptotically justified. The clubSandwich approach uses a \\(t\\) reference distribution, with degrees of freedom estimated using a Satterthwaite approximation. In the present context, the degrees of freedom are a little bit ugly but still not hard to calculate:\r\\[\rdf = \\left[\\sum_{j=1}^J \\frac{w_j^2}{(W - w_j)^2} - \\frac{2}{W}\\sum_{j=1}^J \\frac{w_j^3}{(W - w_j)^2} + \\frac{1}{W^2} \\left(\\sum_{j=1}^J \\frac{w_j^2}{W - w_j} \\right)^2 \\right]^{-1}.\r\\]\nIn the special case that all sites are of the same size and use a constant treatment allocation, the weights become equal. The clubSandwich variance estimator then reduces to\r\\[\rV^{CR2} = \\frac{S_\\delta^2}{J} \\qquad \\text{where} \\qquad S_\\delta^2 = \\frac{1}{J - 1}\\sum_{j=1}^J \\left(\\hat\\delta_j - \\hat\\delta\\right)^2,\r\\]\rand the degrees of freedom reduce to simply \\(df = J - 1\\).\n\rTennessee STAR\rHere is a worked example of the calculations (using R of course, because my SAS programming skills atrophied years ago). I’ll use data from the famous Tennessee STAR class size experiment, which was a multi-site trial in which students were randomized to small or regular-sized kindergarten classes within each of several dozen schools. To make the small-sample issues more pronounced, I’ll limit the sample to urban schools and look at impacts of small class-size on reading and math scores at the end of kindergarten. STAR was actually a three-arm trial—the third arm being a regular-sized class but with an additional teacher aide. For simplicity (and following convention), I’ll collapse the teacher-aide condition and the regular-sized class condition into a single arm and also limit the sample to students with complete outcome data on both tests.\nlibrary(tidyverse)\r## Warning: package \u0026#39;ggplot2\u0026#39; was built under R version 4.0.3\r## Warning: package \u0026#39;tibble\u0026#39; was built under R version 4.0.3\r## Warning: package \u0026#39;readr\u0026#39; was built under R version 4.0.3\rdata(STAR, package = \u0026quot;AER\u0026quot;)\rSTAR_urban \u0026lt;-\rSTAR %\u0026gt;%\rfilter(\r# limit to urban/inner city schools\rschoolk %in% c(\u0026quot;urban\u0026quot;,\u0026quot;inner-city\u0026quot;),\r# limit to complete outcome data\r!is.na(readk), !is.na(mathk)\r) %\u0026gt;%\rdroplevels() %\u0026gt;%\r# collapse control conditions\rmutate(stark = fct_collapse(stark, regular = c(\u0026quot;regular\u0026quot;,\u0026quot;regular+aide\u0026quot;))) %\u0026gt;%\rselect(schoolidk, stark, readk, mathk)\rSTAR_summary \u0026lt;- STAR_urban %\u0026gt;%\rcount(schoolidk)\rAfter these exclusions, the data include a total of 1810 students from 23 schools, ranging in size from 34 to 134 students.\nFor starters, let’s get the average impacts using a seeming unrelated regression specification, with both conventional and clubSandwich standard errors.\nlibrary(clubSandwich)\r## Warning: package \u0026#39;clubSandwich\u0026#39; was built under R version 4.0.3\r## Registered S3 method overwritten by \u0026#39;clubSandwich\u0026#39;:\r## method from ## bread.mlm sandwich\rSTAR_fit \u0026lt;- lm(cbind(readk, mathk) ~ 0 + schoolidk + stark, data = STAR_urban)\r# conventional SEs\rCR0 \u0026lt;- coef_test(STAR_fit, vcov = \u0026quot;CR0\u0026quot;, cluster = STAR_urban$schoolidk, test = \u0026quot;z\u0026quot;,\rcoefs = c(\u0026quot;readk:starksmall\u0026quot;,\u0026quot;mathk:starksmall\u0026quot;))\rCR0\r## Coef. Estimate SE t-stat p-val (z) Sig.\r## 1 readk:starksmall 6.16 2.73 2.25 0.0241 *\r## 2 mathk:starksmall 12.13 4.79 2.53 0.0113 *\r# clubSandwich SEs\rCR2 \u0026lt;- coef_test(STAR_fit, vcov = \u0026quot;CR2\u0026quot;, cluster = STAR_urban$schoolidk, coefs = c(\u0026quot;readk:starksmall\u0026quot;,\u0026quot;mathk:starksmall\u0026quot;))\rCR2\r## Coef. Estimate SE t-stat d.f. p-val (Satt) Sig.\r## 1 readk:starksmall 6.16 2.81 2.19 19 0.0409 *\r## 2 mathk:starksmall 12.13 4.92 2.47 19 0.0234 *\rNow I’ll do it “by hand”—or rather, with a bit of dplyr:\n# summary statistics by site\rschool_summaries \u0026lt;- STAR_urban %\u0026gt;%\rgroup_by(schoolidk, stark) %\u0026gt;%\rsummarise(\r# means by arm and site\rreadk = mean(readk),\rmathk = mean(mathk),\rn_arm = n()\r) %\u0026gt;%\rsummarise(\r# impact estimates by site\rreadk = diff(readk),\rmathk = diff(mathk),\rn = sum(n_arm),\rp = n_arm[stark==\u0026quot;small\u0026quot;] / n\r) %\u0026gt;%\rmutate(w = n * p * (1 - p))\r## `summarise()` regrouping output by \u0026#39;schoolidk\u0026#39; (override with `.groups` argument)\r## `summarise()` ungrouping output (override with `.groups` argument)\r# overall impacts\rschool_summaries %\u0026gt;%\rgather(\u0026quot;subject\u0026quot;,\u0026quot;impact_j\u0026quot;, readk, mathk) %\u0026gt;%\rgroup_by(subject) %\u0026gt;%\rsummarise(\rimpact = weighted.mean(impact_j, w = w),\rSE_CR0 = sqrt(sum(w^2 * (impact_j - impact)^2) / sum(w)^2),\rSE_CR2 = sqrt(sum(w^2 * (impact_j - impact)^2 / (1 - w / sum(w))) / sum(w)^2),\rdf_CR2 = 1 / (sum(w^2 / (sum(w) - w)^2) - 2 * sum(w^3 / (sum(w) - w)^2) / sum(w) + sum(w^2 / (sum(w) - w))^2 / sum(w)^2)\r) %\u0026gt;%\rknitr::kable(digits = 2)\r## `summarise()` ungrouping output (override with `.groups` argument)\r\r\rsubject\rimpact\rSE_CR0\rSE_CR2\rdf_CR2\r\r\r\rmathk\r12.13\r4.79\r4.92\r18.99\r\rreadk\r6.16\r2.73\r2.81\r18.99\r\r\r\rThe CR0 and CR2 standard errors match the results from coef_test, as do the Satterthwaite degrees of freedom. Note that the degrees of freedom are equal to 19 in this case, a bit less than \\(J - 1 = 22\\) due to variation in the weight assigned to each school.\n\rOther weights\rSome analysts might not like the approach of using precision-weighted average of the site-specific impacts, as I’ve examined here. Instead, one might choose to weight the site-specific effects by the site-specific sample sizes, or to use some sort of random effects weighting that allows for random heterogeneity across sites. The formulas given above for conventional and clubSandwich clustered variance estimators apply directly to other weighting schemes too. Just substitute your favorite weights in place of \\(w_j\\). When doing so, the clubSandwich estimator will be exactly unbiased under the assumption that your preferred weighting scheme corresponds to inverse-variance weighting, and the Satterthwaite degrees of freedom approximation will be derived under the same model.\n\r","date":1552089600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552089600,"objectID":"43a42db48d3098fc41c85b22c374ff8c","permalink":"/handmade-clubsandwich/","publishdate":"2019-03-09T00:00:00Z","relpermalink":"/handmade-clubsandwich/","section":"post","summary":"I’m just back from the Society for Research on Educational Effectiveness meetings, where I presented work on small-sample corrections for cluster-robust variance estimators in two-stage least squares models, which I’ve implemented in the clubSandwich R package.","tags":["sandwiches","robust variance estimation","econometrics","weighting"],"title":"A handmade clubSandwich for multi-site trials","type":"post"},{"authors":[],"categories":null,"content":"","date":1552003200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552003200,"objectID":"3d2071d19be6ffac01c10e1fef93b9c0","permalink":"/talk/sree-2019-2sls-crve/","publishdate":"2019-03-08T16:15:00Z","relpermalink":"/talk/sree-2019-2sls-crve/","section":"talk","summary":"","tags":[],"title":"Small-sample cluster-robust variance estimators for two-stage least squares models","type":"talk"},{"authors":["James E. Pustejovsky","Melissa A. Rodgers"],"categories":null,"content":"","date":1551398400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551398400,"objectID":"240a59cb8dac45cbd251eca4bac8c421","permalink":"/publication/testing-for-funnel-plot-asymmetry-of-smds/","publishdate":"2019-03-01T00:00:00Z","relpermalink":"/publication/testing-for-funnel-plot-asymmetry-of-smds/","section":"publication","summary":"Publication bias and other forms of outcome reporting bias are critical threats to the validity of findings from research syntheses. A variety of methods have been proposed for detecting selective outcome reporting in a collection of effect size estimates, including several methods based on assessment of asymmetry of funnel plots, such as Egger's regression test, the rank correlation test, and the Trim-and-Fill test. Previous research has demonstated that Egger's regression test is mis-calibrated when applied to log-odds ratio effect size estimates, due to artifactual correlation between the effect size estimate and its standard error. This study examines similar problems that occur in meta-analyses of the standardized mean difference, a ubiquitous effect size measure in educational and psychological research. In a simulation study of standardized mean difference effect sizes, we assess the Type I error rates of conventional tests of funnel plot asymmetry, as well as the likelihood ratio test from a three-parameter selection model. Results demonstrate that the conventional tests have inflated Type I error due to correlation between the effect size estimate and its standard error, while tests based on either a simple modification to the conventional standard error formula or a variance-stabilizing transformation both maintain close-to-nominal Type I error.","tags":["meta-analysis","publication bias"],"title":"Testing for funnel plot asymmetry of standardized mean differences","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic \rAcademic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click \rPDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot;\rif porridge == \u0026quot;blueberry\u0026quot;:\rprint(\u0026quot;Eating...\u0026quot;)\r  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}}\r{{% fragment %}} **Two** {{% /fragment %}}\r{{% fragment %}} Three {{% /fragment %}}\r Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}}\r- Only the speaker can read these notes\r- Press `S` key to view\r{{% /speaker_note %}}\r Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}}\r{{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}}\r{{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}\r  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1,\r.reveal section h2,\r.reveal section h3 {\rcolor: navy;\r}\r  Questions? \rAsk\n\rDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rIn settings with independent observations, sample size is one way to quickly characterize the precision of an estimate. But what if your estimate is based on weighted data, where each observation doesn’t necessarily contribute to equally to the estimate? Here, one useful way to gauge the precision of an estimate is the effective sample size or ESS. Suppose that we have \\(N\\) independent observations \\(Y_1,...,Y_N\\) drawn from a population with standard deviation \\(\\sigma\\), and that observation \\(i\\) receives weight \\(w_i\\). We take the weighted sample mean\r\\[\r\\tilde{y} = \\frac{1}{W} \\sum_{i=1}^N w_i Y_i, \\qquad \\text{where} \\qquad W = \\sum_{i=1}^N w_i.\r\\]\rwith sampling variance\r\\[\r\\text{Var}(\\tilde{y}) = \\frac{\\sigma^2}{W^2} \\sum_{i=1}^N w_i^2.\r\\]\nThe ESS is the number of observations from an equally weighted sample that would yield the same level of precision as the weighted sample mean. In an equally weighted sample of size \\(\\tilde{N}\\), the variance would be simply \\(\\sigma^2 / \\tilde{N}\\), and so ESS is the value of \\(\\tilde{N}\\) that solves\r\\[\r\\frac{\\sigma^2}{\\tilde{N}} = \\frac{\\sigma^2}{W^2} \\sum_{i=1}^N w_i^2.\r\\]\nRe-arranging, the ESS is thus defined as\r\\[\r\\tilde{N} = \\frac{W^2}{\\sum_{i=1}^N w_i^2}.\r\\]\nThe ESS is reported in several packages for propensity score weighting, including twang and optweight. In the propensity score context, ESS is a useful measure for comparing different sets of estimated propensity weights, in that weights (or propensity score models/matching methods) that have a larger ESS will yield a more precise estimate of a treatment effect. Given two sets of weights that achieve equivalent degrees of balance, the weights with larger ESS are thus preferable. Methods introduced by Zubizarreta (2015)—and implemented in the optweight package—take this logic a step further by using ESS as an objective function to be minimized, subject to specified balancing constraints.\nMulti-site effective sample size\rTwo of my recent projects have involved applying propensity score weighting methods in multi-site settings, where we are interested in estimating site-specific treatment effects as well as an overall aggregate effect. It is straight-forward to calculate an ESS for each site, but how then should we aggregate the ESS across sites to characterize the precision of the overall estimate? Several times now, I have found myself having to re-derive the aggregated ESS, and so I am going to work through it here now so as to save future-me (and perhaps you, dear reader) some time.\nSuppose that we have \\(J\\) sites, \\(n_j\\) observations from site \\(j\\) for \\(j = 1,...,J\\), and total sample size \\(N = \\sum_{j=1}^J n_j\\). Observation \\(i\\) from site \\(j\\) has outcome \\(Y_{ij}\\) and weight \\(w_{ij}\\). The site-specific weighted average at site \\(j\\) is then\r\\[\r\\tilde{y}_j = \\frac{1}{W_j} \\sum_{i=1}^{n_j} w_{ij} Y_{ij}, \\qquad \\text{where} \\qquad W_j = \\sum_{i=1}^{n_j} w_{ij}\r\\]\rand the overall average is\r\\[\r\\tilde{y} = \\frac{1}{N} \\sum_{j=1}^J n_j \\ \\tilde{y}_j = \\frac{1}{N} \\sum_{j=1}^J \\sum_{i=1}^{n_j} \\frac{n_j w_{ij}}{W_j} Y_{ij}.\r\\]\nFor calculating the overall average, observation \\(i\\) from unit \\(j\\) contributes weight \\(u_{ij} = n_j w_{ij} / W_j\\).\nUsing these unit-specific weights, the effective sample size for the overall average is\r\\[\rESS = \\frac{N^2}{\\sum_{j=1}^J \\sum_{i=1}^{n_j} u_{ij}^2}.\r\\]\rWe can also define a site-specific ESS for site \\(j\\):\r\\[\rESS_j = \\frac{W_j^2}{\\sum_{i=1}^{n_j} w_{ij}^2}.\r\\]\nUsing the decomposition of the weights as \\(u_{ij} = n_j w_{ij} / W_j\\), the overall ESS can be written as\r\\[\rESS = \\frac{N^2}{\\sum_{j=1}^J n_j^2 \\left(\\sum_{i=1}^{n_j} w_{ij}^2 / W_j^2\\right)}.\r\\]\rNoting that the term in the parentheses of the denominator is equivalent to \\(1 / ESS_j\\), the overall ESS can therefore be written in terms of the site-specific ESSs and sample sizes:\r\\[\rESS = \\frac{N^2}{\\sum_{j=1}^J n_j^2 / ESS_j}.\r\\]\nThere you go. Future me will thank me for this!\n\r","date":1548115200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548115200,"objectID":"b83424e4fa1bfe877d83379de72f089f","permalink":"/effective-sample-size-aggregation/","publishdate":"2019-01-22T00:00:00Z","relpermalink":"/effective-sample-size-aggregation/","section":"post","summary":"In settings with independent observations, sample size is one way to quickly characterize the precision of an estimate. But what if your estimate is based on weighted data, where each observation doesn’t necessarily contribute to equally to the estimate?","tags":["econometrics","causal inference","weighting"],"title":"Effective sample size aggregation","type":"post"},{"authors":["Elizabeth Tipton","James E. Pustejovsky","Hedyeh Ahmadi"],"categories":null,"content":"","date":1546819200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546819200,"objectID":"6c01093c9dee2fe255d86919f7453310","permalink":"/publication/current-practices-in-meta-regression/","publishdate":"2019-01-07T00:00:00Z","relpermalink":"/publication/current-practices-in-meta-regression/","section":"publication","summary":"Having surveyed the history and methods of meta‐regression in a previous paper,1 in this paper we review which and how meta‐regression methods are applied in recent research syntheses. To do so, we reviewed studies published in 2016 across four leading research synthesis journals: _Psychological Bulletin_, the _Journal of Applied Psychology_, _Review of Education Research_, and the _Cochrane Library_. We find that the best practices defined in the previous review are rarely carried out in practice. In light of the identified discrepancies, we consider how to move forward, first by identifying areas where further methods development is needed to address persistent problems in the field, and second by discussing how to more effectively disseminate points of methodological consensus.","tags":["meta-analysis","meta-regression","systematic review"],"title":"Current practices in meta-regression in psychology, education, and medicine","type":"publication"},{"authors":["Elizabeth Tipton","James E. Pustejovsky","Hedyeh Ahmadi"],"categories":null,"content":"","date":1545868800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1545868800,"objectID":"a2d9e4f6e07ed7240c1bc637474497e2","permalink":"/publication/history-of-meta-regression/","publishdate":"2018-12-27T00:00:00Z","relpermalink":"/publication/history-of-meta-regression/","section":"publication","summary":"At the beginning of the development of meta‐analysis, understanding the role of moderators was given the highest priority, with meta‐regression provided as a method for achieving this goal. Yet in current practice, meta‐regression is not as commonly used as anticipated. This paper seeks to understand this mismatch by reviewing the history of meta‐regression methods over the past 40 years. We divide this time span into four periods and examine three types of methodological developments within each period: technical, conceptual, and practical. Our focus is broad and includes development of methods in the fields of education, psychology, and medicine. We conclude the paper with a discussion of five consensus points, as well as open questions and areas of research for the future.","tags":["meta-analysis","meta-regression"],"title":"A history of meta-regression: Technical, conceptual, and practical developments between 1974 and 2018","type":"publication"},{"authors":["James E. Pustejovsky","Elizabeth Tipton"],"categories":null,"content":"","date":1541116800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541116800,"objectID":"7a5c2e8876e462052b59c92b3df40db7","permalink":"/publication/rve-in-fixed-effects-models/","publishdate":"2018-11-02T00:00:00Z","relpermalink":"/publication/rve-in-fixed-effects-models/","section":"publication","summary":"In panel data models and other regressions with unobserved effects, fixed effects estimation is often paired with cluster-robust variance estimation (CRVE) to account for heteroscedasticity and un-modeled dependence among the errors. Although asymptotically consistent, CRVE can be biased downward when the number of clusters is small, leading to hypothesis tests with rejection rates that are too high. More accurate tests can be constructed using bias-reduced linearization (BRL), which corrects the CRVE based on a working model, in conjunction with a Satterthwaite approximation for t-tests. We propose a generalization of BRL that can be applied in models with arbitrary sets of fixed effects, where the original BRL method is undefined, and describe how to apply the method when the regression is estimated after absorbing the fixed effects. We also propose a small-sample test for multiple-parameter hypotheses, which generalizes the Satterthwaite approximation for t-tests. In simulations covering a wide range of scenarios, we find that the conventional cluster-robust Wald test can severely over-reject while the proposed small-sample test maintains Type I error close to nominal levels. The proposed methods are implemented in an R package called clubSandwich. This article has online supplementary materials.","tags":["robust variance estimation","panel data","fixed effects"],"title":"Small sample methods for cluster-robust variance estimation and hypothesis testing in fixed effects models","type":"publication"},{"authors":[],"categories":null,"content":"","date":1538395200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538395200,"objectID":"26bc3fffe402bc178ca99edb7ea522ad","permalink":"/talk/utaustin-2018-combining-rve-with-models/","publishdate":"2018-07-18T09:00:00Z","relpermalink":"/talk/utaustin-2018-combining-rve-with-models/","section":"talk","summary":"","tags":[],"title":"Combining robust variance estimation with models for dependent effect sizes","type":"talk"},{"authors":[],"categories":null,"content":"","date":1531904400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1531904400,"objectID":"66d4e59eff9a4ea6e5c3d2eb514c2087","permalink":"/talk/srsm-2018-combining-rve-with-models/","publishdate":"2018-07-18T09:00:00Z","relpermalink":"/talk/srsm-2018-combining-rve-with-models/","section":"talk","summary":"","tags":[],"title":"Combining robust variance estimation with models for dependent effect sizes","type":"talk"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rEarlier this month, I taught at the Summer Research Training Institute on Single-Case Intervention Design and Analysis workshop, sponsored by the Institute of Education Sciences’ National Center for Special Education Research.\rWhile I was there, I shared a web-app for simulating data from a single-case design.\rThis is a tool that I put together a couple of years ago as part of my ARPobservation R package, but haven’t ever really publicized or done anything formal with.\rIt provides an easy way to simulate “mock” data from a single-case design where the dependent variable is measured using systematic direct observation of behavior.\rThe simulated data can be viewed in the form of a graph or downloaded as a csv file.\rAnd it’s quite fast—simulating 1000’s of mock single-case designs takes only a few seconds.\rThe tool also provides a visualization of the distribution of effect size estimates that you could anticipate observing in a single-case design, given a set of assumptions about how the dependent variable is measured and how it changes in response to treatment.\nDemo\rHere’s an example of the sort of data that the tool generates and the assumptions it asks you to make.\rSay that you’re interested in evaluating the effect of a Social Stories intervention on the behavior of a child with autism spectrum disorder, and that you plan to use a treatment reversal design.\rYour primary dependent variable is inappropriate play behavior, measured using frequency counts over ten minute observation sessions.\nThe initial baseline and treatment phases will be 7 sessions long.\rAt baseline, the child engages in inappropriate play at a rate of about 0.8 per minute.\rYou anticipate that the intervention could reduce inappropriate play by as much as 90% from baseline.\rEnter all of these details and assumptions into the simulator, and it will generate a graph like this:\nHit the “Simulate!” button again and you might get something like this:\nOr one of these:\nAll of the above graphs were generated from the same hypothetical model—the variation in the clarity and strength of the functional relation is due to random error alone.\rThe simulator can also produce graphs that show multiple realizations of the data-generating process. Here’s one with five replications:\nAnd here’s the same figure, but with trend lines added:\nThe trend lines represent the overall average level of the dependent variable during each session, across infinitely replications of the study.\rThe variability around the trend line provides a sense of the extent of random error in the measurements of the dependent variable.\nI think it’s a rather interesting exercise to try and draw inferences based on visual inspection of randomly generated graphs like this—particularly because it forces you to grapple with random measurement error in a way that using only real data (or only hand-drawn mock data) doesn’t allow.\rIt seems like it could really help a visual analyst to calibrate their interpretations of single-case graphs with visually apparent time trends, outliers, etc.\n\rUse cases\rSo far, this tool is really only a toy—something that I’ve puttered with off and on for a while, but never developed or applied for any substantive purpose.\rHowever, it occurs to me that it (or something similar to it) might have a number of purposes related to planning single-case studies, studying the process of visual inspection, or training single-case researchers.\nWhen I originally put the tool together, the leading case I imagined was to use the tool to help researchers make principled decisions about how to measure dependent variables in single-case designs.\rBy using the tool to simulate hypothetical single-case studies, a researcher would be able to experiment with different measurement strategies—such as using partial interval recording instead of continuous duration recording, using shorter or longer observation sessions, or using short or longer baseline phases—before collecting data on real-life behavior in the field.\rI’m not sure if this is something that well-trained single-case researchers would actually find helpful, but it seems like it might help a novice (like me!) to temper one’s expectations or to move towards a more reliable measurement system.\nThere’s been quite a bit of research examining the reliability and error rates of inferences based on visual inspection (see Chapter 4 of Kratochwill \u0026amp; Levin, 2014 for a review of some of this literature).\rSome of this work has compared the inferences drawn by novices versus experts or by un-aided visual inspection versus visual inspection supplemented with graphical guides (like trend lines).\rBut there are many other factors that could be investigated too, such as phase lengths (this could help to better justify the WWC single-case design standards around minimum phase lengths), use of different measurement systems, or use of different design elements on single-case graphs (can we get some color on these graphs, folks?!? And stop plotting 14 different dependent variables on the same graph?!?).\rThe simulator would be an easy way to generate the stimuli one would need to do this sort of work.\nA closely related use-case is to generate stimuli for training researchers to do systematic visual inspection.\rSome of the SCD Institute instructors (including Tom Kratochwill, Rob Horner, Joel Levin, along with some of their other colleagues) have developed the website www.singlecase.org with a bunch of exercises meant to help researchers develop and test their visual analysis skills.\rIt looks to me like the site uses simulated data (though I’m not entirely sure).\rThe ARPsimulator tool could be used to do something similar, but based on a data-generating process that captures many of the features of systematic direct observation data.\rThis might let researchers test their skills under more challenging and ambiguous, yet plausible, conditions, similar to what they will encounter when collecting real data in the field.\n\rFuture directions\rA number of future directions for this project have crossed my mind:\n\rCurrently, the outcome data are simulated as independent across observation sessions (given the true time trend). It wouldn’t be too hard to add a further option to generate auto-correlated data, although this would further increase the complexity of the model. Perhaps there would be a way to add this as an “advanced” option that would be concealed unless the user asks for it (i.e., “Are you Really Sure you want to go down this rabbit hole?”). So far, I have avoided adding these features because I’m not sure what reasonable defaults would be.\rJoel Levin, John Ferron, and some of the other SCD Institute instructors are big proponents of incorporating randomization procedures into the design of single-case studies, at least when circumstances allow. Currently, the ARPsimulator generates data based on a fixed, pre-specified design, such as an ABAB design with 6 sessions per phase or a multiple baseline design with 25 sessions total and intervention start-times of 8, 14, and 20. It wouldn’t be too hard to incorporate randomized phase-changes into the simulator. This might make a nice, contained project for a student who wants to learn more about randomization designs.\rAlong similar lines, John Ferron has developed and evaluated masked visual analysis procedures, which blend randomization and traditional response-guided approaches to designing single-case studies. It would take a bit more work, but it would be pretty nifty to incorporate these designs into ARPsimulator too.\rCurrently, the model behind ARPsimulator asks the user to specify a fixed baseline level of behavior, and this level of behavior is used for every simulated case—even in designs involving multiple cases. A more realistic (albeit more complicated) data-generating model would allow for between-case variation in the baseline level of behavior.\rPerhaps the most important outstanding question about the premise of this work is just how well the alternating renewal process model captures the features of real single-case data. Validating the model against empirical data from single-case studies would allow use to assess whether it is really a realistic approach to simulation, at least for certain classes of behavior. Another product of such an investigation would be to develop realistic default assumptions for the model’s parameters.\r\rAt the moment I have no plans to implement any of these unless there’s a reasonably focused need (sadly, I don’t have time to putter and putz to the same extent that I used to).\rIf you, dear reader, would be interested in helping to pursue any of these directions, or if you have other, better ideas for how to make use of this tool, I would love to hear from you.\n\r","date":1529539200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1529539200,"objectID":"a3f4227ca0afcb26a924918c86187243","permalink":"/easily-simulate-thousands-of-single-case-designs/","publishdate":"2018-06-21T00:00:00Z","relpermalink":"/easily-simulate-thousands-of-single-case-designs/","section":"post","summary":"Earlier this month, I taught at the Summer Research Training Institute on Single-Case Intervention Design and Analysis workshop, sponsored by the Institute of Education Sciences’ National Center for Special Education Research.","tags":["behavioral observation","simulation","single-case design"],"title":"Easily simulate thousands of single-case designs","type":"post"},{"authors":["Samuel L. Odom","Erin E. Barton","Brian Reichow","Hariharan Swaminathan","James E. Pustejovsky"],"categories":null,"content":"","date":1527897600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527897600,"objectID":"6a1d58adc1a74f5a1d680c7be7b6ea33","permalink":"/publication/bcsmd-examination-of-two-methods/","publishdate":"2018-06-02T00:00:00Z","relpermalink":"/publication/bcsmd-examination-of-two-methods/","section":"publication","summary":"An increasing movement in single case research is to employ statistical analyses as one form of data analysis. Researchers have proposed different statistical approaches. The purpose of this paper is to examine the utility and discriminant validity of two novel types of between-case standardized effect size analyses with two existing systematic reviews. The between-case analyses found greater effect sizes for the studies in the object play review and smaller effect sizes for studies of sensory intervention, which were consistent with the overall conclusions reached in the original systematic reviews. These findings provide evidence of discriminant validity, although concerns remain around the methods’ utility across different single case research designs. Future directions for research and development also are provided.","tags":["single-case design","design-comparable SMD","effect size"],"title":"Between-case standardized effect size analysis of single case design: Examination of the two methods","type":"publication"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rI’m very happy to share a new paper, co-authored with my student Danny Swan, “A gradual effects model for single-case designs,” which is now available online at Multivariate Behavioral Research. You can access the published version at the journal website (click here for free access while supplies last) or the pre-print on PsyArxiv (always free!). Here’s the abstract and the supplementary materials. Danny wrote R functions for fitting the model, (available as part of the SingleCaseES package) as well as a slick web interface, if you prefer to point-and-click.\nThis paper grew out of Danny’s qualifying process (QP), which is the major exam that our doctoral students have to pass before they can begin their dissertation work. For the QP, students work with a faculty advisor to develop an extensive literature review and proposal for an original research project. They produce a written research proposal, then take written and oral exams on their work. For Danny’s QP, he picked up one of the many loose ends in my dissertation, studied up on generalized linear models to understand how to express and fit the model, and developed a simulation study evaluating the model. After he successfully passed his QP, we worked together to refine the estimation methods and the simulation design, and then draft a manuscript (much of it cribbed from his QP proposal). I’m very proud and pleased that Danny continued to develop the work and saw it through to a first-authored publication.\n","date":1526256000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526256000,"objectID":"f5a5e090817aa6d00f9fd7efffa6c8c2","permalink":"/gradual-effects-model-paper/","publishdate":"2018-05-14T00:00:00Z","relpermalink":"/gradual-effects-model-paper/","section":"post","summary":"I’m very happy to share a new paper, co-authored with my student Danny Swan, “A gradual effects model for single-case designs,” which is now available online at Multivariate Behavioral Research.","tags":["single-case design","effect size","generalized linear model"],"title":"New paper: A gradual effects model for single-case designs","type":"post"},{"authors":null,"categories":null,"content":"This course introduces the contemporary statistical approach to addressing questions about the causal effects of programs, policies, or interventions, with a focus on applied data-analysis strategies and interpretation. The course begins with an introduction to the potential outcomes framework for expressing causal quantities, followed by an examination of (idealized) simple and block randomized experiments as prototypes for learning about causal effects. The remainder of the course covers theory and data-analysis strategies for drawing causal inferences from observational studies, in which treatment conditions are not randomly assigned. Analysis techniques such as matching methods, propensity-score methods, and instrumental variables are covered both in theory and in application. Further, advanced topics are covered based on student interest.\n \r2020 (Spring) Syllabus \r2017 (Fall) Syllabus \r2014 (Fall) Syllabus  ","date":1526014800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526014800,"objectID":"3e0f43902b2556f94f7a18c13248cf7b","permalink":"/teaching/causal-inference/","publishdate":"2018-05-11T00:00:00-05:00","relpermalink":"/teaching/causal-inference/","section":"teaching","summary":"This course introduces the contemporary statistical approach to addressing questions about the causal effects of programs, policies, or interventions, with a focus on applied data-analysis strategies and interpretation. The course begins with an introduction to the potential outcomes framework for expressing causal quantities, followed by an examination of (idealized) simple and block randomized experiments as prototypes for learning about causal effects.","tags":[],"title":"Causal Inference","type":"teaching"},{"authors":null,"categories":null,"content":" For a long time I have thought I was a statistician, interested in inferences from the particular to the general. But as I have watched mathematical statistics evolve, I have had cause to wonder and to doubt…. All in all, I have come to feel that my central interest is in data analysis, which I take to include, among other things: procedures for analyzing data, techniques for interpreting the results of such procedures, ways of planning the gathering of data to make its analysis easier, more precise or more accurate, and all the machinery and results of (mathematical) statistics which apply to analyzing data. [Tukey, J., 1962. The future of data analysis. The Annals of Mathematical Statistics, 33(1), 1–67.]\n This course provides training in using the open-source statistical programming environment called R to accomplish 1) real-world, reproducible data analysis and 2) design and implementation of statistical simulations, which are an important tool for evaluating the performance of statistical estimation and inference procedures. Topics covered include:\n the logic of R’s primary data structures and how to work with functions tools and best practices for accessing, cleaning, and manipulating data reproducibility as a fundamental tenet of high-quality data analysis data visualization techniques selected statistical models and methods that are useful for data-analysis, including linear regression models and generalized linear models.  Content relevant to designing and implementing Monte Carlo simulation studies is interwoven throughout the course.\n \r2019 (Spring) Syllabus \r2017 (Spring) Syllabus \r2015 (Spring) Syllabus  ","date":1526014800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526014800,"objectID":"680e871405dcb29386258631076b9300","permalink":"/teaching/daspir/","publishdate":"2018-05-11T00:00:00-05:00","relpermalink":"/teaching/daspir/","section":"teaching","summary":"For a long time I have thought I was a statistician, interested in inferences from the particular to the general. But as I have watched mathematical statistics evolve, I have had cause to wonder and to doubt….","tags":[],"title":"Data Analysis, Simulation, and Programming in R","type":"teaching"},{"authors":null,"categories":null,"content":"This course surveys essential concepts and methods used in quantitative empirical research in the fields of education and psychology, in order to prepare students both to be informed consumers of research and to conduct empirical research of their own. The course is organized around four main themes: measurement, populations and sampling, experimental causal research, and quasi-experimental causal research. On each theme, we read relevant theoretical and methodological literature, discuss empirical research in light of those concepts, and develop research proposals using the methods that we discuss. Throughout, emphasis is on building intuition and heuristics regarding research designs and methods.\n \r2019 (Fall) Syllabus \r2019 (Spring) Syllabus \r2018 (Fall) Syllabus \r2018 (Spring) Syllabus \r2017 (Spring) Syllabus \r2016 (Fall) Syllabus \r2016 (Spring) Syllabus \r2015 (Fall) Syllabus \r2015 (Spring) Syllabus \r2014 (Fall) Syllabus \r2014 (Spring) Syllabus \r2013 (Fall) Syllabus  ","date":1526014800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526014800,"objectID":"a0c77b3112fb141e0db2f5c692647ece","permalink":"/teaching/research-design/","publishdate":"2018-05-11T00:00:00-05:00","relpermalink":"/teaching/research-design/","section":"teaching","summary":"This course surveys essential concepts and methods used in quantitative empirical research in the fields of education and psychology, in order to prepare students both to be informed consumers of research and to conduct empirical research of their own.","tags":[],"title":"Research Design and Methods for Psychology and Education","type":"teaching"},{"authors":null,"categories":null,"content":"This course covers the principles and procedures involved in analyzing data from experimental designs. Approaches for analyzing simple (one-way) designs, factorial designs, and repeated measures designs are presented using the analysis of variance (ANOVA) framework. Lectures focus on developing conceptual understanding of the experimental designs and corresponding analytical models and on interpreting the results of analysis procedures. Laboratory sections focus on using statistical software for data analysis.\n \r2016 (Spring) Syllabus \r2015 (Fall) Syllabus  ","date":1526014800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526014800,"objectID":"8bc82bbc3c66814debe4eb85d0b9ad58","permalink":"/teaching/experimental-data/","publishdate":"2018-05-11T00:00:00-05:00","relpermalink":"/teaching/experimental-data/","section":"teaching","summary":"This course covers the principles and procedures involved in analyzing data from experimental designs. Approaches for analyzing simple (one-way) designs, factorial designs, and repeated measures designs are presented using the analysis of variance (ANOVA) framework.","tags":[],"title":"Statistical Analysis of Experimental Data","type":"teaching"},{"authors":["Daniel M. Swan","James E. Pustejovsky"],"categories":null,"content":"","date":1525132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525132800,"objectID":"ed0c4e60cd3d21105624f31bb4722ac9","permalink":"/publication/gradual-effects-model/","publishdate":"2018-05-01T00:00:00Z","relpermalink":"/publication/gradual-effects-model/","section":"publication","summary":"Single-case designs are a class of repeated measures experiments used to evaluate the effects of interventions for specialized populations, such as individuals with low-incidence disabilities. There has been growing interest in systematic reviews and syntheses of evidence from single-case designs, but there remains a need to further develop appropriate statistical models and effect sizes for data from the designs. We propose a novel model for single-case data that exhibit non-linear time trends created by an intervention that produces gradual effects, which build up and dissipate over time. The model expresses a structural relationship between a pattern of treatment assignment and an outcome variable, making it appropriate for both treatment reversal and multiple baseline designs. It is formulated as a generalized linear model so that it can be applied to outcomes measured as frequency counts or proportions, both of which are commonly used in single-case research, while providing readily interpretable effect size estimates such as log response ratios or log odds ratios. We demonstrate the gradual effects model by applying it to data from a single-case study and examine the performance of proposed estimation methods in a Monte Carlo simulation of frequency count data.","tags":["effect size","non-linear model","response ratio","single-case design"],"title":"A gradual effects model for single-case designs","type":"publication"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rLast night I attended a joint meetup between the Austin R User Group and R Ladies Austin, which was great fun. The evening featured several lightning talks on a range of topics, from breaking into data science to network visualization to starting your own blog. I gave a talk about sandwich standard errors and my clubSandwich R package. Here are links to some of the talks:\n\rCaitlin Hudon: Getting Plugged into Data Science\rClaire McWhite: A quick intro to networks\rNathaniel Woodward: Blogdown Demo! (link includes his slides and a demo screencast)\rme: Robust, easy standard errors with the clubSandwich package.\r\r","date":1524700800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524700800,"objectID":"a38cfce74f28a35d2e9d3e91b50bebb5","permalink":"/clubsandwich-at-rug/","publishdate":"2018-04-26T00:00:00Z","relpermalink":"/clubsandwich-at-rug/","section":"post","summary":"Last night I attended a joint meetup between the Austin R User Group and R Ladies Austin, which was great fun. The evening featured several lightning talks on a range of topics, from breaking into data science to network visualization to starting your own blog.","tags":["Rstats","robust variance estimation","sandwiches"],"title":"clubSandwich at the Austin R User Group Meetup","type":"post"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rConsider Pearson’s correlation coefficient, \\(r\\), calculated from two variables \\(X\\) and \\(Y\\) with population correlation \\(\\rho\\). If one calculates \\(r\\) from a simple random sample of \\(N\\) observations, then its sampling variance will be approximately\n\\[\r\\text{Var}(r) \\approx \\frac{1}{N}\\left(1 - \\rho^2\\right)^2.\r\\]\nBut what if the observations are drawn from a multi-stage sample? If one uses the raw correlation between the observations (ignoring the multi-level structure), then the \\(r\\) will actually be a weighted average of within-cluster and between-cluster correlations (see Snijders \u0026amp; Bosker, 2012). Intuitively, I would expect that the sampling variance of the between-cluster correlation will be a function of the number of clusters (regardless of the number of observations per cluster), so the variance of \\(r\\) from a multi-stage sample would not necessarily be the same as that from a simple random sample. What is the sampling variance of \\(r\\) in this design?\nLet me be more precise here by formalizing the sampling process. Suppose that we have a sample with \\(m\\) clusters, \\(n_j\\) observations in cluster \\(j\\), and total sample size \\(N = \\sum_{j=1}^m n_j\\). Assume that\n\\[\r\\begin{aligned}\rX_{ij} \u0026amp;= \\mu_x + v^x_j + e^x_{ij} \\\\\rY_{ij} \u0026amp;= \\mu_y + v^y_j + e^y_{ij},\r\\end{aligned}\r\\]\nfor \\(i=1,...,n_j\\) and \\(j=1,...,m\\), where\n\\[\r\\begin{aligned}\r\\left[\\begin{array}{c} v^x_j \\\\ v^y_j \\end{array}\\right] \u0026amp;\\sim N\\left(\\left[\\begin{array}{c}0 \\\\ 0 \\end{array}\\right], \\left[\\begin{array}{cc}\\omega_x^2 \u0026amp; \\phi \\omega_x \\omega_y \\\\ \\phi \\omega_x \\omega_y \u0026amp; \\omega_y^2\\end{array}\\right]\\right) \\\\ \\left[\\begin{array}{c} e^x_{ij} \\\\ e^y_{ij} \\end{array}\\right] \u0026amp;\\sim N\\left(\\left[\\begin{array}{c}0 \\\\ 0 \\end{array}\\right], \\left[\\begin{array}{cc}\\sigma_x^2 \u0026amp; \\rho \\sigma_x \\sigma_y \\\\ \\rho \\sigma_x \\sigma_y \u0026amp; \\sigma_y^2\\end{array}\\right]\\right)\r\\end{aligned}\r\\]\nand the error terms are mutually independent unless otherwise noted. The raw Pearson’s \\(r\\) is calculated using the total sums of squares and cross-products:\n\\[\rr = \\frac{SS_{xy}}{\\sqrt{SS_{xx} SS_{yy}}},\r\\]\nwhere\n\\[\r\\begin{aligned}\rSS_{xx} \u0026amp;= \\sum_{j=1}^m \\sum_{i=1}^{n_j} \\left(X_{ij} - \\bar{\\bar{x}}\\right)^2, \\qquad \\bar{\\bar{x}} = \\frac{1}{N} \\sum_{j=1}^m \\sum_{i=1}^{n_j} X_{ij} \\\\\rSS_{xy} \u0026amp;= \\sum_{j=1}^m \\sum_{i=1}^{n_j} \\left(Y_{ij} - \\bar{\\bar{y}}\\right)^2, \\qquad \\bar{\\bar{y}} = \\frac{1}{N} \\sum_{j=1}^m \\sum_{i=1}^{n_j} Y_{ij} \\\\\rSS_{xy} \u0026amp;= \\sum_{j=1}^m \\sum_{i=1}^{n_j} \\left(X_{ij} - \\bar{\\bar{x}}\\right) \\left(Y_{ij} - \\bar{\\bar{y}}\\right).\r\\end{aligned}\r\\]\nCommon correlation and ICC\rThe distribution of the total correlation seems to be pretty complicated. So far, I’ve been able to obtain the variance of \\(r\\) for a special case that makes some further, fairly restrictive assumptions. Specifically, assume that the correlation is constant across the two levels, so that \\(\\phi = \\rho\\), and that the intra-class correlation of \\(X\\) is the same as that of \\(Y\\). Let \\(k = \\omega_x^2 / \\sigma_x^2 = \\omega_y^2 / \\sigma_y^2\\) and \\(\\psi = k / (k + 1) = \\omega_x^2 / (\\omega_x^2 + \\sigma_x^2)\\). Then\n\\[\r\\text{Var}(r) \\approx \\frac{(1 - \\rho^2)^2}{\\tilde{N}},\r\\]\nwhere\n\\[\r\\tilde{N} = \\frac{N[g_1 k + 1]^2}{g_2 k^2 + 2 g_1 k + 1} \\approx \\frac{N}{1 + (g_2 - g_1^2)\\psi^2},\r\\]\nwith \\(\\displaystyle{g_1 = 1 - \\frac{1}{N^2}\\sum_{j=1}^m n_j^2}\\), and \\(\\displaystyle{g_2 = \\frac{1}{N}\\sum_{j=1}^m n_j^2 - \\frac{2}{N^2}\\sum_{j=1}^m n_j^3 + \\frac{1}{N^3} \\left(\\sum_{j=1}^m n_j^2 \\right)^2}\\).\nIf the clusters are all of equal size \\(n\\), then\n\\[\r\\tilde{N} = \\frac{nm[k(m - 1) / m + 1]^2}{k^2 n (m - 1)/m + 2 k (m - 1) / m + 1} \\approx \\frac{N}{1 + (n - 1) \\psi^2},\r\\]\nThe right-hand expression is a further approximation that will be very close to right so long as \\(m\\) is not too too small.\n\rZ-transformation\rUnder the (restrictive) assumptions of common correlation and equal ICCs, Fisher’s z transformation is variance-stabilizing (as it is under simple random sampling), so it seems reasonable to use\n\\[\r\\text{Var}\\left(z(r)\\right) \\approx \\frac{1}{\\tilde{N} - 3}.\r\\]\n\rDesign effect\rThe design effect (\\(DEF\\)) is the ratio of the actual sampling variance of \\(r\\) to the sampling variance in a simple random sample of the same size. For the special case that I’ve described,\n\\[\rDEF = \\frac{N}{\\tilde{N}} = 1 + (g_2 - g_1^2) \\psi^2,\r\\]\nor with equal cluster-sizes, \\(DEF = 1 + (n - 1)\\psi^2\\). These expressions make it clear that the design effect for the correlation is not equivalent to the well-known design effect for means or mean differences in cluster-randomized designs, which is \\(1 + (n - 1)\\psi\\). We need to take the square of the ICC here, which will make the design effect for \\(r\\) smaller than the design effect for a mean (or difference in means) based on the same sample.\n\rOther special cases\rThere are some further special cases that are not to hard to work out and could be useful as rough approximations at least. One is if the within-cluster correlation is zero \\((\\rho = 0)\\) and we’re interested in the between-cluster correlation \\(\\phi\\). Then the total correlation can be corrected for what is essentially measurement error using formulas from Hunter and Schmidt (2004). A further specialization is if \\(X\\) is a cluster-level measure, so that \\(\\sigma_x^2 = 0\\). I’ll consider these in a later post, perhaps.\n\r","date":1524096000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524096000,"objectID":"43d262b0818be936e379949af0138dae","permalink":"/variance-of-r-in-two-level-design/","publishdate":"2018-04-19T00:00:00Z","relpermalink":"/variance-of-r-in-two-level-design/","section":"post","summary":"Consider Pearson’s correlation coefficient, \\(r\\), calculated from two variables \\(X\\) and \\(Y\\) with population correlation \\(\\rho\\). If one calculates \\(r\\) from a simple random sample of \\(N\\) observations, then its sampling variance will be approximately","tags":["effect size","correlation","meta-analysis","delta method","distribution theory"],"title":"Sampling variance of Pearson r in a two-level design","type":"post"},{"authors":[],"categories":null,"content":"","date":1523780100,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1523780100,"objectID":"34b2ec29fe3f44dc60e7d15a786a3176","permalink":"/talk/aera-2018-dependent-effects/","publishdate":"2018-04-15T08:15:00Z","relpermalink":"/talk/aera-2018-dependent-effects/","section":"talk","summary":"","tags":[],"title":"Meta-analysis of dependent effects: A review and consolidation of methods","type":"talk"},{"authors":[],"categories":null,"content":"","date":1523780100,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1523780100,"objectID":"6b7de4b9f2da7b65bc11330364f8c809","permalink":"/talk/aera-2018-meta-analysis-of-single-case-research/","publishdate":"2018-04-15T06:30:00Z","relpermalink":"/talk/aera-2018-meta-analysis-of-single-case-research/","section":"talk","summary":"","tags":[],"title":"Meta-analysis of single-case research: A brief and breezy tour","type":"talk"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rThe delta method is surely one of the most useful techniques in classical statistical theory. It’s perhaps a bit odd to put it this way, but I would say that the delta method is something like the precursor to the bootstrap, in terms of its utility and broad range of applications—both are “first-line” tools for solving statistical problems. There are many good references on the delta-method, ranging from the Wikipedia page to a short introduction in The American Statistician (Oehlert, 1992). Many statistical theory textbooks also include a longer or shorter discussion of the method (e.g., Stuart \u0026amp; Ord, 1996; Casella \u0026amp; Berger, 2002).\nI use the delta method all the time in my work, especially to derive approximations to the sampling variance of some estimator (or covariance between two estimators). Here I’ll give one formulation of the multivariate delta method that I find particularly useful for this purpose. (This is nothing at all original. I’m only posting it on the off chance that others might find my crib notes helpful—and by “others” I mostly mean myself in six months…)\nMulti-variate delta method covariances\rSuppose that we have a \\(p\\)-dimensional vector of statistics \\(\\mathbf{T} = \\left(T_1,...,T_p \\right)\\) that converge in distribution to the parameter vector \\(\\boldsymbol\\theta = \\left(\\theta_1,...,\\theta_p\\right)\\) and have asymptotic covariance matrix \\(\\boldsymbol\\Sigma / n\\), i.e.,\n\\[\r\\sqrt{n} \\left(\\mathbf{T} - \\boldsymbol\\theta\\right) \\stackrel{D}{\\rightarrow} N\\left( \\mathbf{0}, \\boldsymbol\\Sigma \\right).\r\\]\nNow consider two functions \\(f\\) and \\(g\\), both of which take vectors as inputs, return scalar quantities, and don’t have funky (discontinuous) derivatives. The asymptotic covariance between \\(f(\\mathbf{T})\\) and \\(g(\\mathbf{T})\\) is then approximately\n\\[\r\\text{Cov} \\left(f(\\mathbf{T}), g(\\mathbf{T}) \\right) \\approx \\frac{1}{n} \\sum_{j=1}^p \\sum_{k=1}^p \\frac{\\partial f}{ \\partial \\theta_j}\\frac{\\partial g}{ \\partial \\theta_k}\\sigma_{jk}, \\]\nwhere \\(\\sigma_{jk}\\) is the entry in row \\(j\\) and column \\(k\\) of the matrix \\(\\boldsymbol\\Sigma\\). If the entries of \\(\\mathbf{T}\\) are asymptotically uncorrelated , then this simplifies to\n\\[\r\\text{Cov} \\left(f(\\mathbf{T}), g(\\mathbf{T}) \\right) \\approx \\frac{1}{n} \\sum_{j=1}^p \\frac{\\partial f}{ \\partial \\theta_j}\\frac{\\partial g}{ \\partial \\theta_j} \\sigma_{jj}. \\]\nIf we are interested in the variance of a single statistic, then the above formulas simplify further to\n\\[\r\\text{Var} \\left(f(\\mathbf{T})\\right) \\approx \\frac{1}{n} \\sum_{j=1}^p \\sum_{k=1}^p \\frac{\\partial f}{ \\partial \\theta_j}\\frac{\\partial f}{ \\partial \\theta_k}\\sigma_{jk} \\]\nor\n\\[\r\\text{Var} \\left(f(\\mathbf{T}) \\right) \\approx \\frac{1}{n}\\sum_{j=1}^p \\left(\\frac{\\partial f}{ \\partial \\theta_j}\\right)^2 \\sigma_{jj}\r\\]\nin the case of uncorrelated \\(\\mathbf{T}\\).\nFinally, if we are dealing with a univariate transformation \\(f(\\theta)\\), then of course the above simplifies even further to\n\\[\r\\text{Var}\\left(f(T)\\right) = \\left(\\frac{\\partial f}{\\partial \\theta}\\right)^2 \\text{Var}(T)\r\\]\n\rPearson’s \\(r\\)\rThese formulas are useful for all sorts of things. For example, they can be used to derive the sampling variance of Pearson’s correlation coefficient. Suppose we have a simple random sample of \\(n\\) observations from a multivariate normal distribution with mean 0 and variance-covariance matrix \\(\\boldsymbol\\Phi = \\left[\\begin{array}{cc}\\phi_{xx} \u0026amp; \\phi_{xy} \\\\ \\phi_{xy} \u0026amp; \\phi_{yy} \\end{array}\\right]\\). Pearson’s correlation is calculated as\n\\[\rr = \\frac{s_{xy}}{\\sqrt{s_{xx} s_{yy}}},\r\\]\nwhere \\(s_{xx}\\) and \\(s_{yy}\\) are sample variances and \\(s_{xy}\\) is the sample covariance. These sample variances and covariances are unbiased estimates of \\(\\phi_{xx}\\), \\(\\phi_{yy}\\), and \\(\\phi_{xy}\\), respectively. So in terms of the above notation, we have \\(\\mathbf{T} = \\left(s_{xx}, s_{yy}, s_{xy}\\right)\\), \\(\\boldsymbol\\theta = \\left(\\phi_{xx}, \\phi_{yy}, \\phi_{xy}\\right)\\), and \\(\\rho = \\phi_{xy} / \\sqrt{\\phi_{xx} \\phi_{yy}}\\).\nFrom a previous post, we can work out the variance-covariance matrix of \\(\\mathbf{T}\\):\n\\[\r\\text{Var}\\left(\\sqrt{n - 1} \\left[\\begin{array}{c} s_{xx} \\\\ s_{yy} \\\\ s_{xy}\\end{array}\\right]\\right) = \\boldsymbol\\Sigma = \\left[\\begin{array}{ccc} 2 \\phi_{xx}^2 \u0026amp; \u0026amp; \\\\ 2 \\phi_{xy}^2 \u0026amp; 2 \\phi_{yy}^2 \u0026amp; \\\\ 2 \\phi_{xy} \\phi_{xx} \u0026amp; 2 \\phi_{xy} \\phi_{yy} \u0026amp; \\phi_{xy}^2 + \\phi_{xx} \\phi_{yy}\\end{array}\\right].\r\\]\nThe last piece is to find the derivatives of \\(r\\) with respect to \\(\\mathbf{T}\\):\n\\[\r\\begin{aligned}\r\\frac{\\partial r}{\\partial \\phi_{xy}} \u0026amp;= \\phi_{xx}^{-1/2} \\phi_{yy}^{-1/2} \\\\\r\\frac{\\partial r}{\\partial \\phi_{xx}} \u0026amp;= -\\frac{1}{2} \\phi_{xy} \\phi_{xx}^{-3/2} \\phi_{yy}^{-1/2} \\\\\r\\frac{\\partial r}{\\partial \\phi_{yy}} \u0026amp;= -\\frac{1}{2} \\phi_{xy} \\phi_{xx}^{-1/2} \\phi_{yy}^{-3/2}\r\\end{aligned}\r\\]\nPutting the pieces together, we have\n\\[\r\\begin{aligned}\r(n - 1) \\text{Var}(r) \u0026amp;\\approx \\sigma_{11} \\left(\\frac{\\partial r}{\\partial \\phi_{xy}}\\right)^2 + \\sigma_{22} \\left(\\frac{\\partial r}{ \\partial \\phi_{xx}}\\right)^2 + \\sigma_{33} \\left(\\frac{\\partial r}{ \\partial \\phi_{yy}}\\right)^2 \\\\\r\u0026amp; \\qquad \\qquad + 2 \\sigma_{12} \\frac{\\partial r}{\\partial \\phi_{xy}}\\frac{\\partial r}{\\partial \\phi_{xx}} + 2 \\sigma_{13} \\frac{\\partial r}{\\partial \\phi_{xy}}\\frac{\\partial r}{\\partial \\phi_{yy}}+ 2 \\sigma_{23} \\frac{\\partial r}{\\partial \\phi_{xx}}\\frac{\\partial r}{\\partial \\phi_{yy}} \\\\\r\u0026amp;= \\frac{\\phi_{xy}^2 + \\phi_{xx} \\phi_{yy}}{\\phi_{xx} \\phi_{yy}} + \\frac{\\phi_{xy}^2\\phi_{xx}^2}{2 \\phi_{xx}^3 \\phi_{yy}} + \\frac{\\phi_{xy}^2\\phi_{yy}^2}{2 \\phi_{xx} \\phi_{yy}^3} \\\\\r\u0026amp; \\qquad \\qquad - \\frac{2\\phi_{xy} \\phi_{xx}}{\\phi_{xx}^2 \\phi_{yy}} - \\frac{2\\phi_{xy} \\phi_{yy}}{\\phi_{xx} \\phi_{yy}^2} + \\frac{\\phi_{xy}^4}{\\phi_{xx}^2 \\phi_{yy}^2} \\\\\r\u0026amp;= 1 - 2\\frac{\\phi_{xy}^2}{\\phi_{xx} \\phi_{yy}} + \\frac{\\phi_{xy}^4}{\\phi_{xx}^2 \\phi_{yy}^2} \\\\\r\u0026amp;= \\left(1 - \\rho^2\\right)^2.\r\\end{aligned}\r\\]\n\rFisher’s \\(z\\)-transformation\rMeta-analysts will be very familiar with Fisher’s \\(z\\)-transformation of \\(r\\), given by \\(z(\\rho) = \\frac{1}{2} \\log\\left(\\frac{1 + \\rho}{1 - \\rho}\\right)\\).\rFisher’s \\(z\\) is the variance-stabilizing (and also normalizing) transformation of \\(r\\), meaning that \\(\\text{Var}\\left(z(r)\\right)\\) is approximately a constant function of sample size, not depending on the degree of correlation \\(\\rho\\). We can see this using another application of the delta method:\n\\[\r\\frac{\\partial z}{\\partial \\rho} = \\frac{1}{1 - \\rho^2}.\r\\]\nThus,\n\\[\r\\text{Var}\\left(z(r)\\right) \\approx \\frac{1}{(1 - \\rho^2)^2} \\times \\text{Var}(r) = \\frac{1}{n - 1}.\r\\]\nThe variance of \\(z\\) is usually given as \\(1 / (n - 3)\\), which is even closer to exact. Here we’ve obtained the variance of \\(z\\) using two applications of the delta-method. Because of the chain rule, we’d have ended up with the same result if we’d gone straight from the sample variances and covariances, using the multivariate delta method and the derivatives of \\(z\\) with respect to \\(\\boldsymbol\\theta\\).\n\rCovariances between correlations\rThese same techniques can be used to work out expressions for the covariances between correlations estimated on the same sample. For instance, suppose you’ve measured four variables, \\(W\\), \\(X\\), \\(Y\\), and \\(Z\\), on a simple random sample of \\(n\\) observations. What is \\(\\text{Cov}(r_{xy}, r_{xz})\\)? What is \\(\\text{Cov}(r_{wx}, r_{yz})\\)? I’ll leave the derivations for you to work out. See Steiger (1980) for solutions.\n\r","date":1523404800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1523404800,"objectID":"7b505c6f8f37404b324a4589326e127a","permalink":"/multivariate-delta-method/","publishdate":"2018-04-11T00:00:00Z","relpermalink":"/multivariate-delta-method/","section":"post","summary":"The delta method is surely one of the most useful techniques in classical statistical theory. It’s perhaps a bit odd to put it this way, but I would say that the delta method is something like the precursor to the bootstrap, in terms of its utility and broad range of applications—both are “first-line” tools for solving statistical problems.","tags":["delta method","distribution theory"],"title":"The multivariate delta method","type":"post"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rI’m pleased to announce that my article “Using response ratios for meta-analyzing SCDs with behavioral outcomes” has been accepted at Journal of School Psychology. There’s a multitude of ways that you can access this work:\n\rFor the next 6 weeks or so, the published version of the article will be available at the journal website.\rThe pre-print will always remain available at PsyArXiv.\rSome supporting materials and replication code are available on the Open Science Framework.\r\rHere’s the abstract of the paper:\n\rMethods for meta-analyzing single-case designs (SCDs) are needed to inform evidence-based practice in clinical and school settingsand to draw broader and more defensible generalizations in areas where SCDs comprise a large part of the research base. The most widely used outcomesin single-case research are measures of behavior collected using systematic direct observation, which typically take the form of rates or proportions. For studies that use such measures, one simple and intuitive way to quantify effect sizes is in terms of proportionate change from baseline, using an effect size known as the log response ratio. This paper describes methods for estimating log response ratios and combining the estimates using meta-analysis. The methods are based on a simple model for comparing two phases, where the level of the outcome is stable within each phase and the repeated outcome measurements are independent. Although auto-correlation will lead to biased estimates of the sampling variance of the effect size, meta-analysis of response ratios can be conducted with robust variance estimation procedures that remain valid even when sampling variance estimates are biased. The methods are demonstrated using data from a recent meta-analysis on group contingency interventions for student problem behavior.\n\r","date":1521158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1521158400,"objectID":"6f4584e2d14257678a63ee12dea6aa68","permalink":"/using-response-ratios-paper/","publishdate":"2018-03-16T00:00:00Z","relpermalink":"/using-response-ratios-paper/","section":"post","summary":"I’m pleased to announce that my article “Using response ratios for meta-analyzing SCDs with behavioral outcomes” has been accepted at Journal of School Psychology. There’s a multitude of ways that you can access this work:","tags":["single-case design","effect size","response ratio","meta-analysis"],"title":"New paper: Using response ratios for meta-analyzing SCDs with behavioral outcomes","type":"post"},{"authors":["Kathleen N. Zimmerman","James E. Pustejovsky","Jennifer R. Ledford","Erin E. Barton","Katherine E. Severini","Blair P. Lloyd"],"categories":null,"content":"","date":1520380800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1520380800,"objectID":"e2c9de270486930d8c7b16c2fa52cbdf","permalink":"/publication/scd-synthesis-tools-ii/","publishdate":"2018-03-07T00:00:00Z","relpermalink":"/publication/scd-synthesis-tools-ii/","section":"publication","summary":"Varying methods for evaluating the outcomes of single case research designs (SCD) are currently used in reviews and meta-analyses of interventions. Quantitative effect size measures are often presented alongside visual analysis conclusions. Six measures across two classes—overlap measures (percentage non-overlapping data, improvement rate difference, and Tau) and parametric within-case effect sizes (standardized mean difference and log response ratio [increasing and decreasing])—were compared to determine if choice of synthesis method within and across classes impacts conclusions regarding effectiveness. The effectiveness of sensory-based interventions (SBI), a commonly used class of treatments for young children, was evaluated. Separately from evaluations of rigor and quality, authors evaluated behavior change between baseline and SBI conditions. SBI were unlikely to result in positive behavior change across all measures except IRD. However, subgroup analyses resulted in variable conclusions, indicating that the choice of measures for SCD meta-analyses can impact conclusions. Suggestions for using the log response ratio in SCD meta-analyses and considerations for understanding variability in SCD meta-analysis conclusions are discussed.","tags":["non-overlap measures","response ratio","single-case design","systematic review"],"title":"Single-case synthesis tools II: Comparing overlap measures and parametric effect sizes for synthesizing antecedent sensory-based interventions","type":"publication"},{"authors":["Kathleen N. Zimmerman","Jennifer R. Ledford","Katherine E. Severini","James E. Pustejovsky","Erin E. Barton","Blair P. Lloyd"],"categories":null,"content":"","date":1520035200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1520035200,"objectID":"eed63ce27260183937e234caa54d8bb6","permalink":"/publication/scd-synthesis-tools-i/","publishdate":"2018-03-03T00:00:00Z","relpermalink":"/publication/scd-synthesis-tools-i/","section":"publication","summary":"Tools for evaluating the quality and rigor of single case research designs (SCD) are often used when conducting SCD syntheses. Preferred components include evaluations of design features related to the internal validity of SCD to obtain quality and/or rigor ratings. Three tools for evaluating the quality and rigor of SCD (Council for Exceptional Children, What Works Clearinghouse, and Single-Case Analysis and Design Framework) were compared to determine if conclusions regarding the effectiveness of antecedent sensory-based interventions for young children changed based on choice of quality evaluation tool. Evaluation of SCD quality differed across tools, suggesting selection of quality evaluation tools impacts evaluation findings. Suggestions for selecting an appropriate quality and rigor assessment tool are provided and across-tool conclusions are drawn regarding the quality and rigor of studies. Finally, authors provide guidance for using quality evaluations in conjunction with outcome analyses when conducting syntheses of interventions evaluated in the context of SCD.","tags":["single-case design","systematic review"],"title":"Single-case synthesis tools I: Evaluating the quality and rigor of research on antecedent sensory-based interventions","type":"publication"},{"authors":["James E. Pustejovsky"],"categories":null,"content":"","date":1517443200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517443200,"objectID":"db1804ff22c2850cc7c9f922acca03ba","permalink":"/publication/using-response-ratios/","publishdate":"2018-02-01T00:00:00Z","relpermalink":"/publication/using-response-ratios/","section":"publication","summary":"Methods for meta-analyzing single-case designs (SCDs) are needed to inform evidence-based practice in clinical and school settings and to draw broader and more defensible generalizations in areas where SCDs comprise a large part of the research base. The most widely used outcomes in single-case research are measures of behavior collected using systematic direct observation, which typically take the form of rates or proportions. For studies that use such measures, one simple and intuitive way to quantify effect sizes is in terms of proportionate change from baseline, using an effect size known as the log response ratio. This paper describes methods for estimating log response ratios and combining the estimates using meta-analysis. The methods are based on a simple model for comparing two phases, where the level of the outcome is stable within each phase and the repeated outcome measurements are independent. Although auto-correlation will lead to biased estimates of the sampling variance of the effect size, meta-analysis of response ratios can be conducted with robust variance estimation procedures that remain valid even when sampling variance estimates are biased. The methods are demonstrated using data from a recent meta-analysis on group contingency interventions for student problem behavior.","tags":["effect size","meta-analysis","response ratio","robust variance estimation","single-case design"],"title":"Using response ratios for meta-analyzing single-case designs with behavioral outcomes","type":"publication"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rI’m very happy to share that my article “Procedural sensitivities of effect sizes for single-case designs with directly observed behavioral outcome measures” has been accepted at Psychological Methods. There’s no need to delay in reading it, since you can check out the pre-print and supporting materials. Here’s the abstract:\n\rA wide variety of effect size indices have been proposed for quantifying the magnitude of treatment effects in single-case designs. Commonly used measures include parametric indices such as the standardized mean difference, as well as non-overlap measures such as the percentage of non-overlapping data, improvement rate difference, and non-overlap of all pairs. Currently, little is known about the properties of these indices when applied to behavioral data collected by systematic direct observation, even though systematic direct observation is the most common method for outcome measurement in single-case research. This study uses Monte Carlo simulation to investigate the properties of several widely used single-case effect size measures when applied to systematic direct observation data. Results indicate that the magnitude of the non-overlap measures and of the standardized mean difference can be strongly influenced by procedural details of the study’s design, which is a significant limitation to using these indices as effect sizes for meta-analysis of single-case designs. A less widely used parametric index, the log-response ratio, has the advantage of being insensitive to sample size and observation session length, although its magnitude is influenced by the use of partial interval recording.\n\rThis paper was a long time coming. The core idea came out of a grant proposal I wrote during the summer of 2014, which I fleshed out for a poster presented at AERA in April of 2015. After finishing a draft of the paper, I tried to publish it in a special education journal, reasoning that the main audience for the paper is researchers interested in meta-analyzing single case research studies that are commonly used in some parts of special education. That turned out to be a non-starter. Four rejection letters later, I re-worked the paper a bit to give more technical details, then submitted it to a more methods-ish journal. This yielded an R\u0026amp;R, I revised the paper extensively, resubmitted it, and it was declined. Buying in fully to the sunk costs fallacy, I sent the paper to Psychological Methods. This time, I received very extensive and helpful feedback from several anonymous reviewers and an associate editor (thank you, anonymous peers!), which helped me to revise the paper yet again, and this time it was accepted. Sixth time is the charm, as they say.\nHere’s the complete time-line of submissions:\n\rAugust 5, 2015: submitted to journal #1 (special education)\rAugust 28, 2015: desk reject decision from journal #1\rSeptember 3, 2015: submitted to journal #2 (special education)\rNovember 6, 2015: reject decision (after peer review) from journal #2\rNovember 18, 2015: submitted to journal #3 (special education)\rNovember 22, 2015: desk reject decision from journal #3 as not appropriate for their audience. I was grateful to get a quick decision.\rNovember 23, 2015: submitted to journal #4 (special education)\rFebruary 17, 2016: reject decision (after peer review) from journal #4\rApril 19, 2016: submitted to journal #5 (methods)\rAugust 16, 2016: revise-and-resubmit decision from journal #5\rOctober 14, 2016: re-submitted to journal #5\rFebruary 2, 2017: reject decision from journal #5\rMay 10, 2017: submitted to Psychological Methods\rSeptember 1, 2017: revise-and-resubmit decision from Psychological Methods\rSeptember 26, 2017: re-submitted to Psychological Methods\rNovember 22, 2017: conditional acceptance\rDecember 6, 2017: re-submitted with minor revisions\rJanuary 10, 2018: accepted at Psychological Methods\r\r","date":1515628800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515628800,"objectID":"bc4af10d33bc79d02962c0a95197f9f3","permalink":"/procedural-sensitivities-paper/","publishdate":"2018-01-11T00:00:00Z","relpermalink":"/procedural-sensitivities-paper/","section":"post","summary":"I’m very happy to share that my article “Procedural sensitivities of effect sizes for single-case designs with directly observed behavioral outcome measures” has been accepted at Psychological Methods. There’s no need to delay in reading it, since you can check out the pre-print and supporting materials.","tags":["single-case design","effect size","response ratio","non-overlap measures","simulation"],"title":"New paper: procedural sensitivities of effect size measures for SCDs","type":"post"},{"authors":[],"categories":null,"content":"","date":1515572100,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515572100,"objectID":"b2c1c9d798390dcc5138cb20f731ca57","permalink":"/talk/ies-2018-gradual-effects-model/","publishdate":"2018-04-15T08:15:00Z","relpermalink":"/talk/ies-2018-gradual-effects-model/","section":"talk","summary":"","tags":[],"title":"A gradual effects model for single case designs","type":"talk"},{"authors":[],"categories":null,"content":"","date":1515572100,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515572100,"objectID":"48efcf1ae2938e90741fb3d0c9a904be","permalink":"/talk/ies-2018-randomization-inference/","publishdate":"2018-01-10T08:15:00Z","relpermalink":"/talk/ies-2018-randomization-inference/","section":"talk","summary":"","tags":[],"title":"Randomization inference for single-case experimental designs","type":"talk"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rI’m just back from the Institute of Education Sciences’ Principle Investigators conference in Washington D.C. It was an envigorating trip for me, and not only because of the opportunity to catch up with colleagues and friends from across the country. A running theme across several of the keynote addresses was the importance of increasing the transparency and replicability of education research, and it was exciting to hear about promising reforms underway and to talk about how to change the norms of our discipline(s).\nI contributed to the conference in two ways. First, I gave a presentation on incorporating randomization and randomization inference into single-case designs, as part of a session on innovations in single-case research methods organized by Dr. Wendy Machalicek. You can a static version of my slides here; unfortunately the animations don’t work in pdf.\nSecond, I brought a poster presenting some work from my IES-funded methods grant. Thanks very much to the folks who stopped by to talk during the poster session! Y’all gave me some very helpful feedback about technical aspects of the work and about how to better contextualize it for single case researchers.\nIf you didn’t make it: this project was joint work with Danny Swan, a doctoral student in our Quantitative Methods program. It involved developing a model for estimating effect sizes from single-case designs where the effects of the intervention take time to reach full potency. Rather than assuming that the intervention produces immediate shifts in the level of the outcome, we model the effects using an impulse response function (cribbed from an old paper by Box and Tiao) that leads to non-linear trends in response to the introduction of the intervention. Using an impulse response function also makes it possible to model more complex design patterns, like treatment reversal designs with returns-to-baseline and treatment re-introduction phases, in a very parsimonious way. Check out the full paper, the accompanying R package, and Danny’s interactive web-app.\n","date":1515542400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515542400,"objectID":"0bff53ac37005867f82638709f046213","permalink":"/back-from-ies-pi-meeting/","publishdate":"2018-01-10T00:00:00Z","relpermalink":"/back-from-ies-pi-meeting/","section":"post","summary":"I’m just back from the Institute of Education Sciences’ Principle Investigators conference in Washington D.C. It was an envigorating trip for me, and not only because of the opportunity to catch up with colleagues and friends from across the country.","tags":["single-case design","hypothesis testing"],"title":"Back from the IES PI meeting","type":"post"},{"authors":["Daniel M. Maggin","Kathleen L. Lane","James E. Pustejovsky"],"categories":null,"content":"","date":1509494400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1509494400,"objectID":"74e21499a9ad56bfa254adc9c3922aa2","permalink":"/publication/rase-special-issue-introduction/","publishdate":"2017-11-01T00:00:00Z","relpermalink":"/publication/rase-special-issue-introduction/","section":"publication","summary":"This special issue provides an update on recent conceptual and methodological developments for conducting systematic reviews and meta-analyses of single-case research. In this introductory article, we (a) describe the important role of systematic reviews and meta-analyses within special education; (b) discuss several methodological issues authors must consider when planning and conducting a rigorous single-case review; and (c) summarize current approaches for addressing each of these issues. Following this overview, we describe each article in the special issue, paying particular attention to the methodological areas highlighted. We conclude with recommendations for continued research and development in several areas.","tags":["effect size","meta-analysis","single-case design"],"title":"Introduction to the special issue on single-case systematic reviews and meta-analysis","type":"publication"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rI just covered instrumental variables in my course on causal inference, and so I have two-stage least squares (2SLS) estimation on the brain. In this post I’ll share something I realized in the course of prepping for class: that standard errors from 2SLS estimation are equivalent to delta method standard errors based on the Wald IV estimator. (I’m no econometrician, so this had never occurred to me before. Perhaps it will be interesting to other non-econometrician readers. And perhaps the econometricians can point me to the relevant page in Wooldridge or Angrist and Pischke or whomever that explains this better than I have.)\nLet’s consider a system with an outcome \\(y_i\\), a focal treatment \\(t_i\\) identified by a single instrument \\(z_i\\), along with a row-vector of exogenous covariates \\(\\mathbf{x}_i\\), all for \\(i = 1,...,n\\). The usual estimating equations are:\n\\[\r\\begin{aligned}\ry_i \u0026amp;= \\mathbf{x}_i \\delta_0 + t_i \\delta_1 + e_i \\\\\rt_i \u0026amp;= \\mathbf{x}_i \\alpha_0 + z_i \\alpha_1 + u_i.\r\\end{aligned}\r\\]\nWith a single-instrument, the 2SLS estimator of \\(\\delta_1\\) is exactly equivalent to the Wald estimator\n\\[\r\\hat\\delta_1 = \\frac{\\hat\\beta_1}{\\hat\\alpha_1},\r\\]\nwhere \\(\\hat\\alpha_1\\) is the OLS estimator from the first-stage regression of \\(t_i\\) on \\(\\mathbf{x}_i\\) and \\(z_i\\) and \\(\\hat\\beta_1\\) is the OLS estimator from the regression\n\\[\ry_i = \\mathbf{x}_i \\beta_0 + z_i \\beta_1 + v_i.\r\\]\nThe delta-method approximation for \\(\\text{Var}(\\hat\\delta_1)\\) is\n\\[\r\\text{Var}\\left(\\hat\\delta_1\\right) \\approx \\frac{1}{\\alpha_1^2}\\left[ \\text{Var}\\left(\\hat\\beta_1\\right) + \\delta_1^2 \\text{Var}\\left(\\hat\\alpha_1\\right) - 2 \\delta_1 \\text{Cov}\\left(\\hat\\beta_1, \\hat\\alpha_1\\right) \\right]. \\]\nSubstituting the estimators in place of parameters, and using heteroskedasticity-consistent (HC0, to be precise) estimators for \\(\\text{Var}\\left(\\hat\\beta_1\\right)\\), \\(\\text{Var}\\left(\\hat\\alpha_1\\right)\\), and \\(\\text{Cov}\\left(\\hat\\beta_1, \\hat\\alpha_1\\right)\\), it turns out the feasible delta-method variance estimator is exactly equivalent to the HC0 variance estimator from 2SLS.\nConnecting delta-method and 2SLS\rTo demonstrate this claim, let’s first partial out the covariates, taking \\(\\mathbf{\\ddot{y}} = \\left[\\mathbf{I} - \\mathbf{X}\\left(\\mathbf{X}\u0026#39;\\mathbf{X}\\right)^{-1}\\mathbf{X}\u0026#39;\\right]\\mathbf{y}\\), \\(\\mathbf{\\ddot{t}} = \\left[\\mathbf{I} - \\mathbf{X}\\left(\\mathbf{X}\u0026#39;\\mathbf{X}\\right)^{-1}\\mathbf{X}\u0026#39;\\right]\\mathbf{t}\\), and \\(\\mathbf{\\ddot{z}} = \\left[\\mathbf{I} - \\mathbf{X}\\left(\\mathbf{X}\u0026#39;\\mathbf{X}\\right)^{-1}\\mathbf{X}\u0026#39;\\right]\\mathbf{z}\\). The OLS estimators of \\(\\beta_1\\) and \\(\\alpha_1\\) are then\n\\[\r\\hat\\beta_1 = \\left(\\mathbf{\\ddot{z}}\u0026#39;\\mathbf{\\ddot{z}}\\right)^{-1}\\mathbf{\\ddot{z}}\u0026#39;\\mathbf{\\ddot{y}}, \\qquad \\text{and} \\qquad \\hat\\alpha_1 = \\left(\\mathbf{\\ddot{z}}\u0026#39;\\mathbf{\\ddot{z}}\\right)^{-1}\\mathbf{\\ddot{z}}\u0026#39;\\mathbf{\\ddot{t}}.\r\\]\nThe HC0 variance and covariance estimators for these coefficients have the usual sandwich form:\n\\[\r\\begin{aligned}\rV^{\\beta_1} \u0026amp;= \\left(\\mathbf{\\ddot{z}}\u0026#39;\\mathbf{\\ddot{z}}\\right)^{-1}\\left(\\sum_{i=1}^n \\ddot{z}_i^2 \\ddot{v}_i^2\\right) \\left(\\mathbf{\\ddot{z}}\u0026#39;\\mathbf{\\ddot{z}}\\right)^{-1} \\\\\rV^{\\alpha_1} \u0026amp;= \\left(\\mathbf{\\ddot{z}}\u0026#39;\\mathbf{\\ddot{z}}\\right)^{-1}\\left(\\sum_{i=1}^n \\ddot{z}_i^2 \\ddot{u}_i^2\\right) \\left(\\mathbf{\\ddot{z}}\u0026#39;\\mathbf{\\ddot{z}}\\right)^{-1} \\\\\rV^{\\alpha_1\\beta_1} \u0026amp;= \\left(\\mathbf{\\ddot{z}}\u0026#39;\\mathbf{\\ddot{z}}\\right)^{-1}\\left(\\sum_{i=1}^n \\ddot{z}_i^2 \\ddot{u}_i \\ddot{v}_i\\right) \\left(\\mathbf{\\ddot{z}}\u0026#39;\\mathbf{\\ddot{z}}\\right)^{-1},\r\\end{aligned}\r\\]\nwhere \\(\\ddot{v}_i\\) and \\(\\ddot{u}_i\\) are the residuals from the regressions of \\(\\mathbf{\\ddot{y}}\\) on \\(\\mathbf{\\ddot{z}}\\) and \\(\\mathbf{\\ddot{t}}\\) on \\(\\mathbf{\\ddot{z}}\\), respectively. Combining all these terms, the delta-method variance estimator is then\n\\[\rV^{\\delta_1} = \\frac{1}{\\hat\\alpha_1^2}\\left(\\mathbf{\\ddot{z}}\u0026#39;\\mathbf{\\ddot{z}}\\right)^{-1}\\left[\\sum_{i=1}^n \\ddot{z}_i^2\\left(\\ddot{v}_i^2 + \\hat\\delta_1^2 \\ddot{u}_i^2 - 2 \\hat\\delta_1\\ddot{u}_i \\ddot{v}_i\\right)\\right] \\left(\\mathbf{\\ddot{z}}\u0026#39;\\mathbf{\\ddot{z}}\\right)^{-1}.\r\\]\nRemember this formula because we’ll return to it shortly.\nNow consider the 2SLS estimator. To calculate this, we begin by taking the fitted values from the regression of \\(\\mathbf{\\ddot{t}}\\) on \\(\\mathbf{\\ddot{z}}\\):\n\\[\r\\mathbf{\\tilde{t}} = \\mathbf{\\ddot{z}}\\left(\\mathbf{\\ddot{z}}\u0026#39;\\mathbf{\\ddot{z}}\\right)^{-1}\\mathbf{\\ddot{z}}\u0026#39;\\mathbf{\\ddot{t}} = \\mathbf{\\ddot{z}} \\hat\\alpha_1.\r\\]\nWe then regress \\(\\mathbf{\\ddot{y}}\\) on \\(\\mathbf{\\tilde{t}}\\):\n\\[\r\\hat\\delta_1 = \\left(\\mathbf{\\tilde{t}}\u0026#39;\\mathbf{\\tilde{t}}\\right)^{-1} \\mathbf{\\tilde{t}}\u0026#39; \\mathbf{\\ddot{y}}.\r\\]\nThe HC0 variance estimator corresponding to the 2SLS estimator is\n\\[\rV^{2SLS} = \\left(\\mathbf{\\tilde{t}}\u0026#39;\\mathbf{\\tilde{t}}\\right)^{-1} \\left(\\sum_{i=1}^n \\tilde{t}_i^2 \\tilde{e}_i^2 \\right) \\left(\\mathbf{\\tilde{t}}\u0026#39;\\mathbf{\\tilde{t}}\\right)^{-1},\r\\]\nwhere \\(\\tilde{e}_i = \\ddot{y}_i - \\ddot{t}_i \\hat\\delta_1\\). Note that these residuals are calculated based on \\(\\ddot{t}_i\\), the full treatment variable, not the fitted values \\(\\tilde{t}_i\\). The full treatment variable can be expressed as \\(\\ddot{t}_i = \\tilde{t}_i + \\ddot{u}_i\\), by which it follows that\n\\[\r\\tilde{e}_i = \\ddot{y}_i - \\tilde{t}_i \\hat\\delta_1 - \\ddot{u}_i \\hat\\delta_1.\r\\]\nBut \\(\\tilde{t}_i \\hat\\delta_1 = \\ddot{z}_i \\hat\\alpha_1 \\hat\\delta_1 = \\ddot{z}_i \\hat\\beta_1\\), and so\n\\[\r\\tilde{e}_i = \\ddot{y}_i - \\ddot{z}_i \\hat\\beta_1 - \\ddot{u}_i \\hat\\delta_1 = \\ddot{v}_i - \\ddot{u}_i \\hat\\delta_1.\r\\]\nThe 2SLS variance estimator is therefore\n\\[\r\\begin{aligned}\rV^{2SLS} \u0026amp;= \\left(\\mathbf{\\tilde{t}}\u0026#39;\\mathbf{\\tilde{t}}\\right)^{-1} \\left(\\sum_{i=1}^n \\tilde{t}_i^2 \\tilde{e}_i^2 \\right) \\left(\\mathbf{\\tilde{t}}\u0026#39;\\mathbf{\\tilde{t}}\\right)^{-1} \\\\\r\u0026amp;= \\left(\\hat\\alpha_1^2 \\mathbf{\\ddot{z}}\u0026#39;\\mathbf{\\ddot{z}}\\right)^{-1} \\left(\\sum_{i=1}^n \\hat\\alpha_1^2 \\ddot{z}_i^2 \\tilde{e}_i^2 \\right) \\left(\\hat\\alpha_1^2 \\mathbf{\\ddot{z}}\u0026#39;\\mathbf{\\ddot{z}}\\right)^{-1} \\\\\r\u0026amp;= \\frac{1}{\\hat\\alpha_1^2}\\left(\\mathbf{\\ddot{z}}\u0026#39;\\mathbf{\\ddot{z}}\\right)^{-1} \\left(\\sum_{i=1}^n \\ddot{z}_i^2 \\tilde{e}_i^2 \\right) \\left(\\mathbf{\\ddot{z}}\u0026#39;\\mathbf{\\ddot{z}}\\right)^{-1} \\\\\r\u0026amp;= \\frac{1}{\\hat\\alpha_1^2}\\left(\\mathbf{\\ddot{z}}\u0026#39;\\mathbf{\\ddot{z}}\\right)^{-1} \\left[\\sum_{i=1}^n \\ddot{z}_i^2 \\left(\\ddot{v}_i - \\ddot{u}_i \\hat\\delta_1\\right)^2 \\right] \\left(\\mathbf{\\ddot{z}}\u0026#39;\\mathbf{\\ddot{z}}\\right)^{-1},\r\\end{aligned}\r\\]\nwhich agrees with \\(V^{\\delta_1}\\) as given above.\n\rSo what?\rIf you’ve continued reading this far…I’m slightly amazed…but if you have, you may be wondering why it’s worth knowing about this relationship. The equivalence between the 2SLS variance estimator and the delta method interests me for a couple of reasons.\n\rFirst is that I had always taken the 2SLS variance estimator as being conditional on \\(\\mathbf{t}\\)–that is, not accounting for random variation in the treatment assignment. The delta-method form of the variance makes it crystal clear that this isn’t the case—the variance does include terms for \\(\\text{Var}(\\hat\\alpha_1)\\) and \\(\\text{Cov}(\\hat\\beta_1, \\hat\\alpha_1)\\).\rOn the other hand, there’s perhaps a sense that equivalence with the 2SLS variance estimator (the more familiar form) validates the delta method variance estimator—that is, we wouldn’t be doing something fundamentally different by using the delta method variance with a Wald estimator. For instance, we might want to estimate \\(\\alpha_1\\) and/or \\(\\beta_1\\) by some other means (e.g., by estimating \\(\\alpha_1\\) as a marginal effect from a logistic regression or estimating \\(\\beta_1\\) with a multi-level model). It would make good sense in this instance to use the Wald estimator \\(\\hat\\beta_1 / \\hat\\alpha_1\\) and to estimate its variance using the delta method form.\rOne last reason I’m interested in this is that writing out the variance estimators will likely help in understanding how to approach small-sample corrections to \\(V^{2SLS}\\). But I’ll save that for another day.\r\r\r","date":1507334400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1507334400,"objectID":"755093666e854e907b1886145c5d0f7c","permalink":"/delta-method-and-2sls-ses/","publishdate":"2017-10-07T00:00:00Z","relpermalink":"/delta-method-and-2sls-ses/","section":"post","summary":"I just covered instrumental variables in my course on causal inference, and so I have two-stage least squares (2SLS) estimation on the brain. In this post I’ll share something I realized in the course of prepping for class: that standard errors from 2SLS estimation are equivalent to delta method standard errors based on the Wald IV estimator.","tags":["instrumental variables","causal inference","delta method","distribution theory"],"title":"2SLS standard errors and the delta-method","type":"post"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rA colleague recently asked me about how to apply cluster-robust hypothesis tests and confidence intervals, as calculated with the clubSandwich package, when dealing with multiply imputed datasets.\rStandard methods (i.e., Rubin’s rules) for pooling estimates from multiple imputed datasets are developed under the assumption that the full-data estimates are approximately normally distributed. However, this might not be reasonable when working with test statistics based on cluster-robust variance estimators, which can be imprecise when the number of clusters is small or the design matrix of predictors is unbalanced in certain ways. Barnard and Rubin (1999) proposed a small-sample correction for tests and confidence intervals based on multiple imputed datasets. In this post, I’ll show how to implement their technique using the output of clubSandwich, with multiple imputations generated using the mice package.\nSetup\rTo begin, let me create missingness in a dataset containing multiple clusters of observations:\nlibrary(mlmRev)\rlibrary(mice)\rlibrary(dplyr)\rdata(bdf)\rbdf \u0026lt;- bdf %\u0026gt;%\rselect(schoolNR, IQ.verb, IQ.perf, sex, ses, langPRET, aritPRET, aritPOST) %\u0026gt;%\rmutate(\rschoolNR = factor(schoolNR),\rsex = as.numeric(sex)\r) %\u0026gt;%\rfilter(as.numeric(schoolNR) \u0026lt;= 30) %\u0026gt;%\rdroplevels()\rbdf_missing \u0026lt;- bdf %\u0026gt;% select(-schoolNR) %\u0026gt;%\rampute(run = TRUE)\rbdf_missing \u0026lt;- bdf_missing$amp %\u0026gt;%\rmutate(schoolNR = bdf$schoolNR)\rsummary(bdf_missing)\r## IQ.verb IQ.perf sex ses ## Min. : 4.00 Min. : 5.333 Min. :1.000 Min. :10.00 ## 1st Qu.:10.50 1st Qu.: 9.333 1st Qu.:1.000 1st Qu.:20.00 ## Median :11.50 Median :10.667 Median :1.000 Median :27.00 ## Mean :11.72 Mean :10.733 Mean :1.462 Mean :28.58 ## 3rd Qu.:13.00 3rd Qu.:12.333 3rd Qu.:2.000 3rd Qu.:38.00 ## Max. :18.00 Max. :16.667 Max. :2.000 Max. :50.00 ## NA\u0026#39;s :37 NA\u0026#39;s :39 NA\u0026#39;s :40 NA\u0026#39;s :37 ## langPRET aritPRET aritPOST schoolNR ## Min. :15.00 Min. : 1.00 Min. : 2.00 40 : 35 ## 1st Qu.:30.00 1st Qu.: 9.00 1st Qu.:12.00 54 : 31 ## Median :34.00 Median :11.00 Median :18.00 55 : 30 ## Mean :33.87 Mean :11.64 Mean :17.57 38 : 28 ## 3rd Qu.:39.00 3rd Qu.:14.00 3rd Qu.:23.00 1 : 25 ## Max. :48.00 Max. :20.00 Max. :30.00 18 : 24 ## NA\u0026#39;s :32 NA\u0026#39;s :31 NA\u0026#39;s :36 (Other):354\rNow I’ll use mice to create 10 multiply imputed datasets:\nImpute_bdf \u0026lt;- mice(bdf_missing, m=10, meth=\u0026quot;norm.nob\u0026quot;, seed=24)\rAm I imputing while ignoring the hierarchical structure of the data? Yes, yes I am. Is this is a good way to do imputation? Probably not. But this is a quick and dirty example, so we’re going to have to live with it.\n\rModel\rSuppose that the goal of our analysis is to estimate the coefficients of the following regression model:\n\\[\r\\text{aritPOST}_{ij} = \\beta_0 + \\beta_1 \\text{aritPRET}_{ij} + \\beta_2 \\text{langPRET}_{ij} + \\beta_3 \\text{sex}_{ij} + \\beta_4 \\text{SES}_{ij} + e_{ij},\r\\]\nwhere \\(i\\) indexes students and \\(j\\) indexes schools, and where we allow for the possibility that errors from the same cluster are correlated in an unspecified way. With complete data, we could estimate the model by ordinary least squares and then use clubSandwich to get standard errors that are robust to within-cluster dependence and heteroskedasticity. The code for this is as follows:\nlibrary(clubSandwich)\r## Registered S3 method overwritten by \u0026#39;clubSandwich\u0026#39;:\r## method from ## bread.mlm sandwich\rlm_full \u0026lt;- lm(aritPOST ~ aritPRET + langPRET + sex + ses, data = bdf)\rcoef_test(lm_full, cluster = bdf$schoolNR, vcov = \u0026quot;CR2\u0026quot;)\r## Coef. Estimate SE t-stat d.f. p-val (Satt) Sig.\r## 1 (Intercept) -2.1921 1.3484 -1.626 22.9 0.1177 ## 2 aritPRET 1.0053 0.0833 12.069 23.4 \u0026lt;0.001 ***\r## 3 langPRET 0.2758 0.0294 9.371 24.1 \u0026lt;0.001 ***\r## 4 sex -1.2040 0.4706 -2.559 23.8 0.0173 *\r## 5 ses 0.0233 0.0266 0.876 20.5 0.3909\rIf cluster dependence were no concern, we could simply use the model-based standard errors and test statistics. The mice package provides functions that will fit the model to each imputed dataset and then combine them by Rubin’s rules. The code is simply:\nwith(data = Impute_bdf, lm(aritPOST ~ aritPRET + langPRET + sex + ses)\r) %\u0026gt;%\rpool() %\u0026gt;%\rsummary()\r## term estimate std.error statistic df p.value\r## 1 (Intercept) -2.28650029 1.11111424 -2.057844 417.9634 4.022469e-02\r## 2 aritPRET 0.97135842 0.07152843 13.580033 250.9260 0.000000e+00\r## 3 langPRET 0.27866679 0.03722404 7.486205 308.6377 7.474021e-13\r## 4 sex -1.06494919 0.41317983 -2.577447 272.5258 1.047928e-02\r## 5 ses 0.03220417 0.02142008 1.503457 124.5671 1.352524e-01\rHowever, this approach ignores the possibility of correlation in the errors of units in the same cluster, which is clearly a concern in this dataset:\n# ratio of CRVE to conventional variance estimates\rdiag(vcovCR(lm_full, cluster = bdf$schoolNR, type = \u0026quot;CR2\u0026quot;)) / diag(vcov(lm_full))\r## (Intercept) aritPRET langPRET sex ses ## 1.5296837 1.5493134 0.6938735 1.4567650 2.0053186\rSo we need a way to pool results based on the cluster-robust variance estimators, while also accounting for the relatively small number of clusters in this dataset.\n\rBarnard \u0026amp; Rubin (1999)\rBarnard and Rubin (1999) proposed a small-sample correction for tests and confidence intervals based on multiple imputed datasets that seems to work in this context. Rather than using large-sample normal approximations for inference, they derive an approximate degrees-of-freedom that combines uncertainty in the standard errors calculated from each imputed dataset with between-imputation uncertainty. The method is as follows.\nSuppose that we have \\(m\\) imputed datasets. Let \\(\\hat\\beta_{(j)}\\) be the estimated regression coefficient from imputed dataset \\(j\\), with (in this case cluster-robust) sampling variance estimate \\(V_{(j)}\\). Further, let \\(\\eta_{(j)}\\) be the degrees of freedom corresponding to \\(V_{(j)}\\). To combine these estimates, calculate the averages across multiply imputed datasets:\n\\[\r\\bar\\beta = \\frac{1}{m}\\sum_{j=1}^m \\hat\\beta_{(j)}, \\qquad \\bar{V} = \\frac{1}{m}\\sum_{j=1}^m V_{(j)}, \\qquad \\bar\\eta = \\frac{1}{m}\\sum_{j=1}^m \\eta_{(j)}.\r\\]\nAlso calculate the between-imputation variance\n\\[\rB = \\frac{1}{m - 1} \\sum_{j=1}^m \\left(\\hat\\beta_{(j)} - \\bar\\beta\\right)^2\r\\]\nAnd then combine the between- and within- variance estimates using Rubin’s rules:\n\\[\rV_{total} = \\bar{V} + \\frac{m + 1}{m} B.\r\\]\nThe degrees of freedom associated with \\(V_{total}\\) modify the estimated complete-data degrees of freedom \\(\\bar\\eta\\) using quantities that depend on the fraction of missing information in a coefficient. The fraction of missing information is given by\n\\[\r\\hat\\gamma_m = \\frac{(m+1)B}{m V_{total}}\r\\]\nThe degrees of freedom are then given by\n\\[\r\\nu_{total} = \\left(\\frac{1}{\\nu_m} + \\frac{1}{\\nu_{obs}}\\right)^{-1},\r\\]\nwhere\n\\[\r\\nu_m = \\frac{(m - 1)}{\\hat\\gamma_m^2}, \\quad \\text{and} \\quad \\nu_{obs} = \\frac{\\bar\\eta (\\bar\\eta + 1) (1 - \\hat\\gamma)}{\\bar\\eta + 3}.\r\\]\nHypothesis tests and confidence intervals are based on the approximation\n\\[\r\\frac{\\bar\\beta - \\beta_0}{\\sqrt{V_{total}}} \\ \\stackrel{\\cdot}{\\sim} \\ t(\\nu_{total})\r\\]\n\rImplementation\rHere is how to carry out these calculations using the results of clubSandwich::coef_test and a bit of dplyr:\n# fit results with clubSandwich standard errors\rmodels_robust \u0026lt;- with(data = Impute_bdf, lm(aritPOST ~ aritPRET + langPRET + sex + ses) %\u0026gt;% coef_test(cluster=bdf$schoolNR, vcov=\u0026quot;CR2\u0026quot;)\r) # pool results with clubSandwich standard errors\rrobust_pooled \u0026lt;- models_robust$analyses %\u0026gt;%\r# add coefficient names as a column\rlapply(function(x) {\rx$coef \u0026lt;- row.names(x)\rx\r}) %\u0026gt;%\rbind_rows() %\u0026gt;%\ras.data.frame() %\u0026gt;%\r# summarize by coefficient\rgroup_by(coef) %\u0026gt;%\rsummarise(\rm = n(),\rB = var(beta),\rbeta_bar = mean(beta),\rV_bar = mean(SE^2),\reta_bar = mean(df)\r) %\u0026gt;%\rmutate(\r# calculate intermediate quantities to get df\rV_total = V_bar + B * (m + 1) / m,\rgamma = ((m + 1) / m) * B / V_total,\rdf_m = (m - 1) / gamma^2,\rdf_obs = eta_bar * (eta_bar + 1) * (1 - gamma) / (eta_bar + 3),\rdf = 1 / (1 / df_m + 1 / df_obs),\r# calculate summary quantities for output\rse = sqrt(V_total),\rt = beta_bar / se,\rp_val = 2 * pt(abs(t), df = df, lower.tail = FALSE),\rcrit = qt(0.975, df = df),\rlo95 = beta_bar - se * crit,\rhi95 = beta_bar + se * crit\r)\rrobust_pooled %\u0026gt;%\rselect(coef, est = beta_bar, se, t, df, p_val, lo95, hi95, gamma) %\u0026gt;%\rmutate_at(vars(est:gamma), round, 3)\r## # A tibble: 5 x 9\r## coef est se t df p_val lo95 hi95 gamma\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 (Intercept) -2.29 1.34 -1.70 20.4 0.104 -5.08 0.51 0.039\r## 2 aritPRET 0.971 0.092 10.5 19.0 0 0.778 1.16 0.076\r## 3 langPRET 0.279 0.036 7.71 19.5 0 0.203 0.354 0.106\r## 4 ses 0.032 0.03 1.09 16.3 0.292 -0.03 0.095 0.117\r## 5 sex -1.06 0.472 -2.26 19.6 0.036 -2.05 -0.08 0.089\rIt is instructive to compare the calculated df to eta_bar and df_m:\nrobust_pooled %\u0026gt;%\rselect(coef, df, df_m, eta_bar) %\u0026gt;%\rmutate_at(vars(df, df_m, eta_bar), round, 1)\r## # A tibble: 5 x 4\r## coef df df_m eta_bar\r## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 (Intercept) 20.4 6006. 23 ## 2 aritPRET 19 1550. 22.5\r## 3 langPRET 19.5 806. 24.1\r## 4 ses 16.3 657. 20.7\r## 5 sex 19.6 1138. 23.7\rHere, eta_bar is the average of the complete data degrees of freedom, and it can be seen that the total degrees of freedom are somewhat less than the average complete-data degrees of freedom. This is by construction. Further df_m is the conventional degrees of freedom used in multiple-imputation, which assume that the complete-data estimates are normally distributed, and in this example they are way far off.\n\rFurther thoughts\rHow well does this method perform in practice? I’m not entirely sure—I’m just trusting that Barnard and Rubin’s approximation is sound and would work in this setting (I mean, they’re smart people!). Are there other, better approaches? Totally possible. I have done zero literature review beyond the Barnard and Rubin paper. In any case, exploring the performance of this method (and any other alternatives) seems like it would make for a very nice student project.\nThere’s also the issue of how to do tests of multi-dimensional constraints (i.e., F-tests). The clubSandwich package implements Wald-type tests for multi-dimensional constraints, using a small-sample correction that we developed (Tipton \u0026amp; Pustejovsky, 2015; Pustejovsky \u0026amp; Tipton, 2016). But it would take some further thought to figure out how to handle multiply imputed data with this type of test…\n\r","date":1506470400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1506470400,"objectID":"691aa9b8e4df62968fe58f20a00d3c15","permalink":"/mi-with-clubsandwich/","publishdate":"2017-09-27T00:00:00Z","relpermalink":"/mi-with-clubsandwich/","section":"post","summary":"A colleague recently asked me about how to apply cluster-robust hypothesis tests and confidence intervals, as calculated with the clubSandwich package, when dealing with multiply imputed datasets.\rStandard methods (i.","tags":["missing data","sandwiches","small-sample","Rstats"],"title":"Pooling clubSandwich results across multiple imputations","type":"post"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rIn many systematic reviews, it is common for eligible studies to contribute effect size estimates from not just one, but multiple relevant outcome measures, for a common sample of participants. If those outcomes are correlated, then so too will be the effect size estimates. To estimate the degree of correlation, you would need the sample correlation among the outcomes—information that is woefully uncommon for primary studies to report (and best of luck to you if you try to follow up with author queries). Thus, the meta-analyst is often left in a situation where the sampling variances of the effect size estimates can be reasonably well approximated, but the sampling covariances are unknown for some or all studies.\nSeveral solutions to this conundrum have been proposed in the meta-analysis methodology literature. One possible strategy is to just impute a correlation based on subject-matter knowledge (or at least feigned expertise), and assume that this correlation is constant across studies. This analysis could be supplemented with sensitivity analyses to examine the extent to which the parameter estimates and inferences are sensitive to alternative assumptions about the inter-correlation of effects within studies. A related strategy, described by Wei and Higgins (2013), is to meta-analyze any available correlation estimates and then use the results to impute correlations for any studies with missing correlations.\nBoth of these approaches require the meta-analyst to calculate block-diagonal sampling covariance matrices for the effect size estimates, which can be a bit unwieldy. I often use the impute-the-correlation strategy in my meta-analysis work and have written a helper function to compute covariance matrices, given known sampling variances and imputed correlations for each study. In the interest of not repeating myself, I’ve added the function to the latest version of my clubSandwich package. In this post, I’ll explain the function and demonstrate how to use it for conducting meta-analysis of correlated effect size estimates.\nAn R function for block-diagonal covariance matrices\rHere is the function:\nlibrary(clubSandwich)\r## Registered S3 method overwritten by \u0026#39;clubSandwich\u0026#39;:\r## method from ## bread.mlm sandwich\rimpute_covariance_matrix\r## function (vi, cluster, r, return_list = identical(as.factor(cluster), ## sort(as.factor(cluster)))) ## {\r## cluster \u0026lt;- droplevels(as.factor(cluster))\r## vi_list \u0026lt;- split(vi, cluster)\r## r_list \u0026lt;- rep_len(r, length(vi_list))\r## vcov_list \u0026lt;- Map(function(V, rho) (rho + diag(1 - rho, nrow = length(V))) * ## tcrossprod(sqrt(V)), V = vi_list, rho = r_list)\r## if (return_list) {\r## return(vcov_list)\r## }\r## else {\r## vcov_mat \u0026lt;- metafor::bldiag(vcov_list)\r## cluster_index \u0026lt;- order(order(cluster))\r## return(vcov_mat[cluster_index, cluster_index])\r## }\r## }\r## \u0026lt;bytecode: 0x0000000018309e80\u0026gt;\r## \u0026lt;environment: namespace:clubSandwich\u0026gt;\rThe function takes three required arguments:\n\rvi is a vector of sampling variances.\rcluster is a vector identifying the study from which effect size estimates are drawn. Effects with the same value of cluster will be treated as correlated.\rr is the assumed value(s) of the correlation between effect size estimates from each study. Note that r can also be a vector with separate values for each study.\r\rHere is a simple example to demonstrate how the function works. Say that there are just three studies, contributing 2, 3, and 4 effects, respectively. I’ll just make up some values for the effect sizes and variances:\ndat \u0026lt;- data.frame(study = rep(LETTERS[1:3], 2:4), yi = rnorm(9), vi = 4:12)\rdat\r## study yi vi\r## 1 A -1.33148823 4\r## 2 A -0.02725897 5\r## 3 B -0.70125406 6\r## 4 B -1.71119746 7\r## 5 B -0.70957554 8\r## 6 C -0.40639264 9\r## 7 C -0.13290344 10\r## 8 C -1.10272160 11\r## 9 C -0.38033372 12\rI’ll assume that effect size estimates from a given study are correlated at 0.7:\nV_list \u0026lt;- impute_covariance_matrix(vi = dat$vi, cluster = dat$study, r = 0.7)\rV_list\r## $A\r## [,1] [,2]\r## [1,] 4.000000 3.130495\r## [2,] 3.130495 5.000000\r## ## $B\r## [,1] [,2] [,3]\r## [1,] 6.000000 4.536518 4.849742\r## [2,] 4.536518 7.000000 5.238320\r## [3,] 4.849742 5.238320 8.000000\r## ## $C\r## [,1] [,2] [,3] [,4]\r## [1,] 9.000000 6.640783 6.964912 7.274613\r## [2,] 6.640783 10.000000 7.341662 7.668116\r## [3,] 6.964912 7.341662 11.000000 8.042388\r## [4,] 7.274613 7.668116 8.042388 12.000000\rThe result is a list of matrices, where each entry corresponds to the variance-covariance matrix of effects from a given study. To see that the results are correct, let’s examine the correlation matrix implied by these correlation matrices:\ncov2cor(V_list$A)\r## [,1] [,2]\r## [1,] 1.0 0.7\r## [2,] 0.7 1.0\rcov2cor(V_list$B)\r## [,1] [,2] [,3]\r## [1,] 1.0 0.7 0.7\r## [2,] 0.7 1.0 0.7\r## [3,] 0.7 0.7 1.0\rcov2cor(V_list$C)\r## [,1] [,2] [,3] [,4]\r## [1,] 1.0 0.7 0.7 0.7\r## [2,] 0.7 1.0 0.7 0.7\r## [3,] 0.7 0.7 1.0 0.7\r## [4,] 0.7 0.7 0.7 1.0\rAs requested, effects are assumed to be equi-correlated with r = 0.7.\nIf the data are sorted in order of the cluster IDs, then the list of matrices returned by impute_covariance_matrix() can be fed directly into the rma.mv function in metafor (as I demonstrate below). However, if the data are not sorted by cluster, then feeding in the list of matrices will not work correctly. Instead, the full \\(N \\times N\\) variance-covariance matrix (where \\(N\\) is the total number of effect size estimates) will need to be calculated so that the rows and columns appear in the correct order. To address this possibility, the function includes an optional argument, return_list, which determines whether to output a list of matrices (one matrix per study/cluster) or a single matrix corresponding to the full variance-covariance matrix across all studies. By default, return_list tests for whether the cluster argument is sorted and returns the appropriate form. The argument can also be set directly by the user.\nHere’s what happens if we feed in the data in a different order:\ndat_scramble \u0026lt;- dat[sample(nrow(dat)),]\rdat_scramble\r## study yi vi\r## 9 C -0.38033372 12\r## 3 B -0.70125406 6\r## 8 C -1.10272160 11\r## 5 B -0.70957554 8\r## 6 C -0.40639264 9\r## 2 A -0.02725897 5\r## 1 A -1.33148823 4\r## 4 B -1.71119746 7\r## 7 C -0.13290344 10\rV_mat \u0026lt;- round(impute_covariance_matrix(vi = dat_scramble$vi, cluster = dat_scramble$study, r = 0.7), 3)\rV_mat\r## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]\r## [1,] 12.000 0.000 8.042 0.000 7.275 0.00 0.00 0.000 7.668\r## [2,] 0.000 6.000 0.000 4.850 0.000 0.00 0.00 4.537 0.000\r## [3,] 8.042 0.000 11.000 0.000 6.965 0.00 0.00 0.000 7.342\r## [4,] 0.000 4.850 0.000 8.000 0.000 0.00 0.00 5.238 0.000\r## [5,] 7.275 0.000 6.965 0.000 9.000 0.00 0.00 0.000 6.641\r## [6,] 0.000 0.000 0.000 0.000 0.000 5.00 3.13 0.000 0.000\r## [7,] 0.000 0.000 0.000 0.000 0.000 3.13 4.00 0.000 0.000\r## [8,] 0.000 4.537 0.000 5.238 0.000 0.00 0.00 7.000 0.000\r## [9,] 7.668 0.000 7.342 0.000 6.641 0.00 0.00 0.000 10.000\rTo see that this is correct, check that the diagonal entries of V_mat are the same as vi:\nall.equal(dat_scramble$vi, diag(V_mat))\r## [1] TRUE\r\rAn example with real data\rKalaian and Raudenbush (1996) introduced a multi-variate random effects model, which can be used to perform a joint meta-analysis of studies that contribute effect sizes on distinct, related outcome constructs. They demonstrate the model using data from a synthesis on the effects of SAT coaching, where many studies reported effects on both the math and verbal portions of the SAT. The data are available in the clubSandwich package:\nlibrary(dplyr, warn.conflicts=FALSE)\rdata(SATcoaching)\r# calculate the mean of log of coaching hours\rmean_hrs_ln \u0026lt;- SATcoaching %\u0026gt;% group_by(study) %\u0026gt;%\rsummarise(hrs_ln = mean(log(hrs))) %\u0026gt;%\rsummarise(hrs_ln = mean(hrs_ln, na.rm = TRUE))\r# clean variables, sort by study ID\rSATcoaching \u0026lt;- SATcoaching %\u0026gt;%\rmutate(\rstudy = as.factor(study),\rhrs_ln = log(hrs) - mean_hrs_ln$hrs_ln\r) %\u0026gt;%\rarrange(study, test)\rSATcoaching %\u0026gt;%\rselect(study, year, test, d, V, hrs_ln) %\u0026gt;%\rhead(n = 20)\r## study year test d V hrs_ln\r## 1 Alderman \u0026amp; Powers (A) 1980 Verbal 0.22 0.0817 -0.54918009\r## 2 Alderman \u0026amp; Powers (B) 1980 Verbal 0.09 0.0507 -0.19250515\r## 3 Alderman \u0026amp; Powers (C) 1980 Verbal 0.14 0.1045 -0.14371499\r## 4 Alderman \u0026amp; Powers (D) 1980 Verbal 0.14 0.0442 -0.19250515\r## 5 Alderman \u0026amp; Powers (E) 1980 Verbal -0.01 0.0535 -0.70333077\r## 6 Alderman \u0026amp; Powers (F) 1980 Verbal 0.14 0.0557 -0.88565233\r## 7 Alderman \u0026amp; Powers (G) 1980 Verbal 0.18 0.0561 -0.09719497\r## 8 Alderman \u0026amp; Powers (H) 1980 Verbal 0.01 0.1151 1.31157225\r## 9 Burke (A) 1986 Verbal 0.50 0.0825 1.41693276\r## 10 Burke (B) 1986 Verbal 0.74 0.0855 1.41693276\r## 11 Coffin 1987 Math 0.33 0.2534 0.39528152\r## 12 Coffin 1987 Verbal -0.23 0.2517 0.39528152\r## 13 Curran (A) 1988 Math -0.08 0.1065 -0.70333077\r## 14 Curran (A) 1988 Verbal -0.10 0.1066 -0.70333077\r## 15 Curran (B) 1988 Math -0.29 0.1015 -0.70333077\r## 16 Curran (B) 1988 Verbal -0.14 0.1007 -0.70333077\r## 17 Curran (C) 1988 Math -0.34 0.1104 -0.70333077\r## 18 Curran (C) 1988 Verbal -0.16 0.1092 -0.70333077\r## 19 Curran (D) 1988 Math -0.06 0.1089 -0.70333077\r## 20 Curran (D) 1988 Verbal -0.07 0.1089 -0.70333077\rThe correlation betwen math and verbal test scores are not available, but it seems reasonable to use a correlation of r = 0.66, as reported in the SAT technical information. To synthesize these effects, I’ll first compute the required variance-covariances:\nV_list \u0026lt;- impute_covariance_matrix(vi = SATcoaching$V, cluster = SATcoaching$study, r = 0.66)\rThis can then be fed into metafor to estimate a fixed effect or random effects meta-analysis or meta-regression models:\nlibrary(metafor, quietly = TRUE)\r## Loading \u0026#39;metafor\u0026#39; package (version 2.1-0). For an overview ## and introduction to the package please type: help(metafor).\r# bivariate fixed effect meta-analysis\rMVFE_null \u0026lt;- rma.mv(d ~ 0 + test, V = V_list, data = SATcoaching)\rMVFE_null\r## ## Multivariate Meta-Analysis Model (k = 67; method: REML)\r## ## Variance Components: none\r## ## Test for Residual Heterogeneity:\r## QE(df = 65) = 72.1630, p-val = 0.2532\r## ## Test of Moderators (coefficients 1:2):\r## QM(df = 2) = 19.8687, p-val \u0026lt; .0001\r## ## Model Results:\r## ## estimate se zval pval ci.lb ci.ub ## testMath 0.1316 0.0331 3.9783 \u0026lt;.0001 0.0668 0.1965 *** ## testVerbal 0.1215 0.0313 3.8783 0.0001 0.0601 0.1829 *** ## ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r# bivariate fixed effect meta-regression\rMVFE_hrs \u0026lt;- rma.mv(d ~ 0 + test + test:hrs_ln, V = V_list, data = SATcoaching)\r## Warning in rma.mv(d ~ 0 + test + test:hrs_ln, V = V_list, data = SATcoaching):\r## Rows with NAs omitted from model fitting.\rMVFE_hrs\r## ## Multivariate Meta-Analysis Model (k = 65; method: REML)\r## ## Variance Components: none\r## ## Test for Residual Heterogeneity:\r## QE(df = 61) = 67.9575, p-val = 0.2523\r## ## Test of Moderators (coefficients 1:4):\r## QM(df = 4) = 23.7181, p-val \u0026lt; .0001\r## ## Model Results:\r## ## estimate se zval pval ci.lb ci.ub ## testMath 0.0946 0.0402 2.3547 0.0185 0.0159 0.1734 * ## testVerbal 0.1119 0.0341 3.2762 0.0011 0.0449 0.1788 ** ## testMath:hrs_ln 0.1034 0.0546 1.8946 0.0581 -0.0036 0.2103 . ## testVerbal:hrs_ln 0.0601 0.0442 1.3592 0.1741 -0.0266 0.1467 ## ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r# bivariate random effects meta-analysis\rMVRE_null \u0026lt;- rma.mv(d ~ 0 + test, V = V_list, data = SATcoaching, random = ~ test | study, struct = \u0026quot;UN\u0026quot;)\rMVRE_null\r## ## Multivariate Meta-Analysis Model (k = 67; method: REML)\r## ## Variance Components:\r## ## outer factor: study (nlvls = 47)\r## inner factor: test (nlvls = 2)\r## ## estim sqrt k.lvl fixed level ## tau^2.1 0.0122 0.1102 29 no Math ## tau^2.2 0.0026 0.0507 38 no Verbal ## ## rho.Math rho.Vrbl Math Vrbl ## Math 1 -1.0000 - no ## Verbal -1.0000 1 20 - ## ## Test for Residual Heterogeneity:\r## QE(df = 65) = 72.1630, p-val = 0.2532\r## ## Test of Moderators (coefficients 1:2):\r## QM(df = 2) = 18.1285, p-val = 0.0001\r## ## Model Results:\r## ## estimate se zval pval ci.lb ci.ub ## testMath 0.1379 0.0434 3.1783 0.0015 0.0528 0.2229 ** ## testVerbal 0.1168 0.0337 3.4603 0.0005 0.0506 0.1829 *** ## ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r# bivariate random effects meta-regression\rMVRE_hrs \u0026lt;- rma.mv(d ~ 0 + test + test:hrs_ln, V = V_list, data = SATcoaching,\rrandom = ~ test | study, struct = \u0026quot;UN\u0026quot;)\r## Warning in rma.mv(d ~ 0 + test + test:hrs_ln, V = V_list, data = SATcoaching, :\r## Rows with NAs omitted from model fitting.\rMVRE_hrs\r## ## Multivariate Meta-Analysis Model (k = 65; method: REML)\r## ## Variance Components:\r## ## outer factor: study (nlvls = 46)\r## inner factor: test (nlvls = 2)\r## ## estim sqrt k.lvl fixed level ## tau^2.1 0.0152 0.1234 28 no Math ## tau^2.2 0.0014 0.0373 37 no Verbal ## ## rho.Math rho.Vrbl Math Vrbl ## Math 1 -1.0000 - no ## Verbal -1.0000 1 19 - ## ## Test for Residual Heterogeneity:\r## QE(df = 61) = 67.9575, p-val = 0.2523\r## ## Test of Moderators (coefficients 1:4):\r## QM(df = 4) = 23.6459, p-val \u0026lt; .0001\r## ## Model Results:\r## ## estimate se zval pval ci.lb ci.ub ## testMath 0.0893 0.0507 1.7631 0.0779 -0.0100 0.1887 . ## testVerbal 0.1062 0.0357 2.9738 0.0029 0.0362 0.1762 ** ## testMath:hrs_ln 0.1694 0.0725 2.3354 0.0195 0.0272 0.3116 * ## testVerbal:hrs_ln 0.0490 0.0459 1.0681 0.2855 -0.0409 0.1389 ## ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rThe results of fitting this model using restricted maximum likelihood with metafor are actually a bit different from the estimates reported in the original paper, potentially because Kalaian and Raudenbush use a Cholesky decomposition of the sampling covariances, which alters the interpretation of the random effects variance components. The metafor fit is also a bit goofy because the correlation between the random effects for math and verbal scores is very close to -1, although evidently it is not uncommon to obtain such degenerate estimates of the random effects structure.\n\rRobust variance estimation.\rExperienced meta-analysts will no doubt point out that a further, alternative analytic strategy to the one described above would be to use robust variance estimation methods (RVE; Hedges, Tipton, \u0026amp; Johnson). However, RVE is not so much an alternative strategy as it is a complementary technique, which can be used in combination with any of the models estimated above. Robust standard errors and hypothesis tests can readily be obtained with the clubSandwich package. Here’s how to do it for the random effects meta-regression model:\nlibrary(clubSandwich)\rcoef_test(MVRE_hrs, vcov = \u0026quot;CR2\u0026quot;)\r## Coef. Estimate SE t-stat d.f. p-val (Satt) Sig.\r## 1 testMath 0.0893 0.0360 2.48 20.75 0.0218 *\r## 2 testVerbal 0.1062 0.0215 4.94 16.45 \u0026lt;0.001 ***\r## 3 testMath:hrs_ln 0.1694 0.1010 1.68 7.90 0.1325 ## 4 testVerbal:hrs_ln 0.0490 0.0414 1.18 7.57 0.2725\rRVE is also available in the robumeta R package, but there are several differences between the implementation there and the method I’ve demonstrated here. From the user’s perspective, an advantage of robumeta is that it does all of the covariance imputation calculations “under the hood,” whereas with metafor the calculations need to be done prior to fitting the model. Beyond this, differences include:\n\rrobumeta uses a specific random effects structure that can’t be controlled by the user, whereas metafor can be used to estimate a variety of different random effects structures;\rrobumeta uses a moment estimator for the between-study variance, whereas metafor provides FML or REML estimation;\rrobumeta uses semi-efficient, diagonal weights when fitting the meta-regression, whereas metafor uses weights that are fully efficient (exactly inverse-variance) under the working model.\r\rThe advantages and disadvantages of these two approaches involve some subtleties that I’ll get into in a future post.\n\r","date":1502323200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1502323200,"objectID":"4cdfd1577edeb60ac947b65ea5533ea4","permalink":"/imputing-covariance-matrices-for-multi-variate-meta-analysis/","publishdate":"2017-08-10T00:00:00Z","relpermalink":"/imputing-covariance-matrices-for-multi-variate-meta-analysis/","section":"post","summary":"In many systematic reviews, it is common for eligible studies to contribute effect size estimates from not just one, but multiple relevant outcome measures, for a common sample of participants.","tags":["meta-analysis","sandwiches","robust variance estimation","Rstats"],"title":"Imputing covariance matrices for meta-analysis of correlated effects","type":"post"},{"authors":["Daniel M. Maggin","James E. Pustejovsky","Austin H. Johnson"],"categories":null,"content":"","date":1498867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498867200,"objectID":"483ae99fd663c201e5a525c38a4c89fa","permalink":"/publication/school-based-group-contingencies-meta-analysis/","publishdate":"2017-07-01T00:00:00Z","relpermalink":"/publication/school-based-group-contingencies-meta-analysis/","section":"publication","summary":"Group contingencies are recognized as a potent intervention for addressing challenging student behavior in the classroom, with research reviews supporting the use of this intervention platform going back more than four decades. Over this time period, the field of education has increasingly emphasized the role of research evidence for informing practice, as reflected in the increased use of systematic reviews and meta-analyses. In the current article, we continue this trend by applying recently developed between-case effect size measures and transparent visual analysis procedures to synthesize an up-to-date set of group contingency studies that used single-case designs. Results corroborated recent systematic reviews by indicating that group contingencies are generally effective—particularly for addressing challenging behavior in general education classrooms. However, our review highlights the need for more research on students with disabilities and the need to collect and report information about participants’ functional level.","tags":["design-comparable SMD","meta-analysis","single-case design","systematic review"],"title":"A meta-analysis of school-based group contingency interventions for students with challenging behavior: An update","type":"publication"},{"authors":["Erin E. Barton","James E. Pustejovsky","Daniel M. Maggin","Brian Reichow"],"categories":null,"content":"","date":1498867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498867200,"objectID":"054a5153dea92a28ccbdd8c02f106c7f","permalink":"/publication/taii-meta-analysis/","publishdate":"2017-07-01T00:00:00Z","relpermalink":"/publication/taii-meta-analysis/","section":"publication","summary":"The adoption of methods and strategies validated through rigorous, experimentally oriented research is a core professional value of special education. We conducted a systematic review and meta-analysis examining the experimental literature on Technology-Aided Instruction and Intervention (TAII) using research identified as part of the National Autism Professional Development Project. We applied novel between-case effect size methods to the TAII single-case research base. In addition, we used meta-analytic methodologies to examine the methodological quality of the research, calculate average effect sizes to quantify the level of evidence for TAII, and compare effect sizes across single-case and group-based experimental research. Results identified one category of TAII—computer-assisted instruction—as an evidence-based practice across both single-case and group studies. The remaining two categories of TAII—augmentative and alternative communication and virtual reality—were not identified as evidence-based using What Works Clearinghouse summary ratings.","tags":["design-comparable SMD","meta-analysis","single-case design","systematic review"],"title":"A meta-analysis of technology-aided instruction and intervention for students with ASD","type":"publication"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rHow is statistical analysis like the Odyssey? Here’s an analogy that I used in my research methods course last semester to explain the purpose of study pre-registration. If you’ve ever read the Odyssey, you’ll recall the story of the Sirens, the enchanting lady-monsters whose singing lures to certain death any sailor who hears them. (See Wikipedia for crib notes.) On the advise of his witch-friend Circe, Odysseus pulls a stunt so that he can hear the song of the Sirens while still making it safely past. He instructs his crew to plug their ears with beeswax and then lash him to the mast of his boat. As they sail past the Sirens, Odysseus hears the beautiful voices come-hithering and begs his men to free him, but they tie him up tighter until they are all safely out of ear-shot.\nI think this is a good analogy for the benefits of pre-registering your experiments. Statistical significance testing is an alluring thing. It provides us, as data-analysts, with a way of drawing a definitive conclusion about whatever phenomenon we’re studying—“This effect is non-zero!”—and thus to write compelling articles and get published and get tenure and so on. But we also now know that statistical significance testing is easily abused. Using flexible data analysis procedures, it is easy to obtain statistically significant results from totally meaningless data. And statistical significance is so alluring, why should any scholar believe that you haven’t hacked your way to get there? Is there any way to conduct hypothesis tests and actually believe (and convince others) that you’ve ruled out a null at the end of it?\nThat’s where study pre-registration comes in. Pre-registration involves creating a public record of the exact plans you intend to follow when collecting and analyzing data, in advance of conducting the study. It is like tying your arms and legs to the mast of your ship as you sail through the straights of data collection and analysis. No matter how tempting it is to control for a couple of other covariates…no matter how much cleaner that log-transformation looks…your pre-registered protocol keeps you tied down, preventing you from throwing yourself overboard into the sea of questionable research practices. And as you come out the other side, you can actually put stock in your findings, having heard the siren song of statistical significance and lived to tell the tale.\n","date":1497830400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1497830400,"objectID":"7f8854e4d5d1c80cf2e6ccfe04121f9d","permalink":"/siren-song-of-significance/","publishdate":"2017-06-19T00:00:00Z","relpermalink":"/siren-song-of-significance/","section":"post","summary":"How is statistical analysis like the Odyssey? Here’s an analogy that I used in my research methods course last semester to explain the purpose of study pre-registration. If you’ve ever read the Odyssey, you’ll recall the story of the Sirens, the enchanting lady-monsters whose singing lures to certain death any sailor who hears them.","tags":["pre-registration","hypothesis testing"],"title":"The siren song of significance","type":"post"},{"authors":[],"categories":null,"content":"","date":1493510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493510400,"objectID":"5bad602de2a14bcac0d8abd93e65697b","permalink":"/talk/aera-2017-hc-t-tests/","publishdate":"2018-04-15T08:15:00Z","relpermalink":"/talk/aera-2017-hc-t-tests/","section":"talk","summary":"","tags":[],"title":"Heteroskedasticity-robust tests in linear regression: A review and evaluation of small-sample corrections","type":"talk"},{"authors":[],"categories":null,"content":"","date":1493337600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493337600,"objectID":"fd5651ebbf7e80da18bd6fe8b4c677a3","permalink":"/talk/aera-2017-intervention-analysis/","publishdate":"2018-04-15T08:15:00Z","relpermalink":"/talk/aera-2017-intervention-analysis/","section":"talk","summary":"","tags":[],"title":"A nonlinear intervention analysis model for treatment reversal single-case designs","type":"talk"},{"authors":[],"categories":null,"content":"","date":1493337600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493337600,"objectID":"3b3583e44bb21c45e492757d769b480f","permalink":"/talk/aera-2017-response-ratios/","publishdate":"2018-04-15T08:15:00Z","relpermalink":"/talk/aera-2017-response-ratios/","section":"talk","summary":"","tags":[],"title":"Using response ratios for meta-analyzing single-case designs with behavioral outcomes","type":"talk"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rPublication bias—or more generally, outcome reporting bias or dissemination bias—is recognized as a critical threat to the validity of findings from research syntheses. In the areas with which I am most familiar (education and psychology), it has become more or less a requirement for research synthesis projects to conduct analyses to detect the presence of systematic outcome reporting biases. Some analyses go further by trying correct for its distorting effects on average effect size estimates. Widely known analytic techniques for doing so include Begg and Mazumdar’s rank-correlation test, the Trim-and-Fill technique proposed by Duval and Tweedie, and Egger regression (in its many variants). Another class of methods involves selection models (or weight function models), as proposed by Hedges and Vevea, Vevea and Woods, and others. As far as I can tell, selection models are well known among methodologists but very seldom applied due to their complexity and lack of ready-to-use software (though an R package has recently become available). More recent proposals include the PET-PEESE technique introduced by Stanley and Doucouliagos; Simonsohn, Nelson, and Simmon’s p-curve technique; Van Assen, Van Aert, and Wichert’s p-uniform, and others. The list of techniques grows by the day.\nAmong these methods, Egger regression, PET, and PEESE are superficially quite appealing due to their simplicity. These methods each involve estimating a fairly simple meta-regression model, using as the covariate the sampling variance of the effect size or some transformation thereof. PET uses the standard error of the effect size as the regressor; PEESE uses the sampling variance (i.e., the squared standard error); PET-PEESE involves first testing whether the PET estimate is statistically significant, using PEESE if it is or PET otherwise. The intercept from one of these regressions is the average effect size estimate from a study with zero sampling variance; the estimated intercept is used as a “bias-corrected” estimator of the population average effect. These methods are also appealing due to their extensibility. Because they are just meta-regressions, it is comparatively easy to extend them to meta-regression models that control for further covariates, to use robust variance estimation to account for dependencies among effect size estimates, etc.\nIn a recent blog post, Uri Simonsohn reports some simulation evidence indicating that the PET-PEESE estimator can have large biases under certain conditions, even in the absence of publication bias. The simulations are based on standardized mean differences from two-group experiments and involve simulating collections of studies that include many with small sample sizes, as might be found in certain areas of psychology. On the basis of these performance assessments, he argues that this purported cure is actually worse than the disease—that PET-PEESE should not be used in meta-analyses of psychological research because it performs too poorly to be trusted. In a response to Uri’s post, Joe Hilgard suggests that some simple modifications to the method can improve its performance. Specifically, Joe suggests using a function of sample size as the covariate (in place of the standard error or sampling variance of \\(d\\)), and also using PET or PEESE as stand-alone estimators, rather than using them contingent on a significance test.\nIn this post, I follow up Joe’s suggestions while replicating and expanding upon Uri’s simulations, to try and provide a fuller picture of the relative performance of these estimators. In brief, the simulations show that:\n\rTests for small-sample bias that use PET or PEESE can have wildly incorrect type-I error rates in the absence of publication bias. Don’t use them.\rThe sample-size variants of PET and PEESE do maintain the correct type-I error rates in the absence of publication bias.\rThe sample-size variants of PET and PEESE are exactly unbiased in the absence of publication bias.\rHowever, these adjusted estimators still have a cost, being less precise than the conventional fixed-effect estimator.\rIn the presence of varying degrees of publication bias, none of the estimators consistently out-perform the others. If you really really need to use a regression-based correction, the sample-size variant of PEESE seems like it might be a reasonable default method, but it’s still really pretty rough.\r\rWhy use sample size?\rTo see why it makes sense to use a function of sample size as the covariate for PET-PEESE analyses, rather than using the standard error of the effect size estimate, let’s look at the formulas. Say that we have a standardized mean difference estimate from a two-group design (without covariates) with sample sizes \\(n_0\\) and \\(n_1\\):\n\\[\rd = \\frac{\\bar{y}_1 - \\bar{y}_0}{s_p},\r\\]\nwhere \\(\\bar{y}_0\\) and \\(\\bar{y}_1\\) are the sample means within each group and \\(s_p^2\\) is the pooled sample variance. Following convention, we’ll assume that the outcomes are normally distributed within each group, and the groups have common variance. The exact sampling variance of \\(d\\) is a rather complicated formula, but one which can be approximated reasonably well as\n\\[\r\\text{Var}(d) \\approx \\frac{n_0 + n_1}{n_0 n_1} + \\frac{\\delta^2}{2(n_0 + n_1)},\r\\]\nwhere \\(\\delta\\) is the true standardized mean difference parameter. This formula is a delta-method approximation. The first term captures the variance of the numerator of \\(d\\), so it gets at how precisely the unstandardized difference in means is estimated. The second term captures the variance of the denominator of \\(d\\), so it gets at how precisely the scale of the outcome is estimated. The second term also involves the unknown parameter \\(\\delta\\), which must be estimated in practice. The conventional formula for the estimated sampling variance of \\(d\\) substitutes \\(d\\) in place of \\(\\delta\\):\n\\[\rV = \\frac{n_0 + n_1}{n_0 n_1} + \\frac{d^2}{2(n_0 + n_1)}.\r\\]\nIn PET-PEESE analysis, \\(V\\) or its square root is used as a covariate in a regression of the effect sizes, as a means of adjusting for publication bias. There are two odd things about this. First, publication bias is about the statistical significance of the group differences, but statistical significance does not depend on the scale of the outcome. The test of the null hypothesis of no differences between groups is not based on \\(d / \\sqrt{V}\\). Instead, it is a function of the \\(t\\) statistic:\n\\[\rt = d / \\sqrt{\\frac{n_0 + n_1}{n_0 n_1}}.\r\\]\nConsequently, it makes sense to use only the first term of \\(V\\) as a covariate for purposes of detecting publication biases.\nThe second odd thing is that \\(V\\) is generally going to be correlated with \\(d\\) because we have to use \\(d\\) to calculate \\(V\\). As Joe explained in his response to Uri, this means that there will be a non-zero correlation between \\(d\\) and \\(V\\) (or between \\(d\\) and \\(\\sqrt{V}\\)) except in some very specific cases, even in the absence of any publication bias. Pretty funky.\nThis second problem with regression tests for publication bias has been recognized for a while in the literature (e.g., Macaskill, Walter, \u0026amp; Irwig, 2001; Peters et al., 2006; Moreno et al., 2009), but most of the work here has focused on other effect size measures, like odds ratios, that are relevant in clinical medicine. The behavior of these estimators might well differ for \\(d\\)’s because the dependence between the effect measure and its variance has a different structure.\nBelow I’ll investigate how this stuff works with standardized mean differences, which haven’t been studied as extensively as odds ratios. Actually, I know of only two simulation studies that examined the performance of PET-PEESE methods with standardized mean difference estimates: Inzlicht, Gervais, and Berkman (2015) and Stanley (2017). (Know of others? Leave a comment!) Neither considered using sample-size variants of PET-PEESE. The only source I know of that did consider this is this blog post from Will Gervais, which starts out optimistic about the sample-size variants but ends on a discouraged note. The simulations below build upon Will’s work, as well as Uri’s, by 1) considering a more extensive set of data-generating processes and 2) examining accuracy in addition to bias.\n\rSimulation model\rThe simulations are based on the following data-generating model, which closely follows the structure that Uri used:\n\rPer-cell sample size is generated as \\(n = 12 + B (n_{max} - 12)\\), where \\(B \\sim Beta(\\alpha, \\beta)\\) and \\(n_{max}\\) is the maximum observed sample size. I take \\(n_{max} = 50\\) or \\(120\\) and look at three sample size distributions (note that these distributions are pre-selection, so the observed sample size distributions will deviate from these if there is selective publication):\n\r\\(\\alpha = \\beta = 1\\) corresponds to a uniform distribution on \\([12,n_{max}]\\);\r\\(\\alpha = 1, \\beta = 3\\) is a distribution with more small studies; and\r\\(\\alpha = 3, \\beta = 1\\) is a distribution with more large studies.\r\rTrue effects are simulated as \\(\\delta \\sim N(\\mu, \\sigma^2)\\), for \\(\\mu = 0, 0.1, 0.2, ..., 1.0\\) and \\(\\sigma = 0.0, 0.1, 0.2, 0.4\\). Note that the values of \\(\\sigma\\) are standard deviations of the true effects, with \\(\\sigma = 0.0\\) corresponding to the constant effect model and \\(\\sigma = 0.4\\) corresponding to rather substantial effect heterogeneity.\n\rStandardized mean difference effect size estimates are generated as in a two-group between-subjects experiment with equal per-cell sample sizes. I do this by taking \\(t = D / \\sqrt{S / [2(n - 1)]}\\), where \\(D \\sim N(\\delta \\sqrt{n / 2}, 1)\\) and \\(S \\sim \\chi^2_{2(n - 1)}\\), then calculating\n\\[\rd = \\left(1 - \\frac{3}{8 n - 9}\\right) \\times \\sqrt{\\frac{2}{n}} \\times t.\r\\]\n(That first term is Hedges’ \\(g\\) correction, cuz that’s how I roll.)\n\rObserved effects are filtered based on statistical significance. Let \\(p\\) be the p-value corresponding to the observed \\(t\\) and the one-tailed hypothesis test of \\(\\delta \\leq 0\\). If \\(p \u0026lt; .025\\), \\(d\\) is observed with probability 1. If \\(p \\geq .025\\), then \\(d\\) is observed with probability \\(\\pi\\). Noted that this mechanism corresponds to filtering based on two-sided hypothesis tests, where effects are filtered if they are statistically non-significant effects or statistically significant but in the wrong direction. I look at three scenarios:\n\r\\(\\pi = 1.0\\) corresponds to no selective publication (all simulated effects are observed);\r\\(\\pi = 0.2\\) corresponds to an intermediate degree of selective publication (some but not non-significant effects are observed); and\r\\(\\pi = 0.0\\) corresponds to very strong selective publication (only statistically significant effects are observed).\r\rEach meta-analysis includes a total of \\(k = 100\\) observed studies. Note that in scenarios with publication bias, more (sometimes many more) than 100 studies are generated in order to get 100 observed effects.\n\r\rFor each simulated meta-sample, I calculated the following:\n\rthe usual fixed-effect meta-analytic average (I skipped random effects for simplicity);\rthe PET estimator (including intercept and slope);\rthe PEESE estimator (including intercept and slope);\rPET-PEESE, which is equal to the PEESE intercept if \\(H_0: \\beta_0 \\leq 0\\) is rejected at the 10% level, and is otherwise equal to the PET intercept (this definition follows Stanley, 2017);\rthe modified PET estimator, which I’ll call “SPET” for “sample-size PET” (suggestions for better names welcome);\rthe modified PEESE estimator, which I’ll call “SPEESE”; and\rSPET-SPEESE, which follows the same conditional logic as PET-PEESE.\r\rSimulation results are summarized across 4000 replications. The R code for all this lives here. Complete numerical results live here. Code for creating the graphs below lives here.\n\rResults\rFalse-positive rates for publication bias detection\rFirst, let’s consider the performance of PET and PEESE as tests for detecting publication bias. Here, a statistically significant estimate for the coefficient on the SE (for PET) or on \\(V\\) (for PEESE) is taken as evidence of small-sample bias. For that logic to hold, the tests should maintain the nominal error rates in the absence of publication bias.\nThe figure below depicts the Type-I error rates of the PET and PEESE tests when \\(\\pi = 1\\) (so no publication bias at all), for a one-sided test of \\(H_0: \\beta_1 \\leq 0\\) at the nominal level of \\(\\alpha = .05\\). Rejection rates are plotted for varying true mean effects, levels of heterogeneity, and sample size distributions. Separate colors are used for maximum sample sizes of 50 or 120.\nBoth tests are horribly mis-calibrated, tending to reject the null hypothesis far more often than they should. This happens because there is a non-zero correlation between \\(d\\) and \\(V\\), even in the absence of publication bias. Thus, it does not follow that rejecting \\(H_0: \\beta_1 \\leq 0\\) implies rejection of the hypothesis that there is no publication bias. (Sorry, that’s at least a triple negative!)\nHere’s the same graph, but using the SPET and SPEESE estimators:\nYes, this may be the World’s Most Boring Figure, but it does make clear that both the SPET and SPEESE tests maintain the correct Type-I error rate. (Any variation in rejection rates is just Monte Carlo error.) Thus, it seems pretty clear that if we want to test for small-sample bias, SPET or SPEESE should be used rather than PET or PEESE.\n\rBias of bias-corrected estimators\rNow let’s consider the performance of these methods as estimators of the population mean effect. Uri’s analysis focused on the bias of the estimators, meaning the difference between the average value of the estimator (across repeated samples) and the true parameter. The plot below depicts the expected level of PET, PEESE, and PET-PEESE as a function of the true mean effect, using the uniform distribution of studies and a maximum sample size of \\(n = 50\\):\nAll three of these estimators are pretty bad in terms of bias. In the absence of publication bias, they consistently under-estimate the true mean effect. With intermediate or strong publication bias, PET and PET-PEESE have a consistent downward bias. As an unconditional estimator, PEESE tends to have a positive bias when the true effect is small, but this decreases and becomes negative as the true effect increases. For all three estimators, bias increases as the degree of heterogeneity increases.\nHere is how these estimators compare to the modified SPET, SPEESE, and SPET-SPEESE estimators, as well as to the usual fixed-effect average with no correction for publication bias:\nIn the left column, we see that SPET and SPEESE are exactly unbiased (and SPET-SPEESE is nearly so) in the absence of selective publication. So is regular old fixed effect meta-analysis, of course. In the middle and right columns, studies are selected based partially or fully on statistical significance, and things get messy. Overall, there’s no consistent winner between PEESE versus SPEESE. At small or moderate levels of between-study heterogeneity, and when the true mean effect is small, PEESE, SPEESE, and SPET-SPEESE have fairly similar biases, but PEESE appears to have a slight edge. This seems to me to be nothing but a fortuitous accident, in that the bias induced by the correlation between \\(d\\) and \\(V\\) just happens to work in the right direction. Then, as the true mean effect increases, SPEESE and SPET-SPEESE start to edge out PEESE. This makes sense because the bias induced by the correlation between \\(d\\) and \\(V\\) will be larger when the true effect sizes are larger.\nThese trends seem mostly to hold for the other sample size distributions I examined too, although the biases of PEESE and PET-PEESE aren’t as severe when the maximum sample size is larger. You can see for yourself here:\n\rUniform distribution of studies, maximum sample size of 120\rMore small studies, maximum sample size of 50\rMore small studies, maximum sample size of 120\rMore large studies, maximum sample size of 50\rMore large studies, maximum sample size of 120\r\r\rAccuracy of bias-corrected estimators\rBias isn’t everything, of course. Now let’s look at the overall accuracy of these estimators, as measured by root mean squared error (RMSE). RMSE is a function of both bias and sampling variance, and so is one way to weigh an estimator that is biased but fairly precise against an estimator that is perfectly unbiased but noisy. The following chart plots the RMSE of all of the estimators (following the same layout as above, just with a different vertical axis):\nStarting in the left column where there’s no selective publication, we can see that the normal fixed-effect average has the smallest RMSE (and so is most accurate). The next most accurate is SPEESE, which uniformly beats out PEESE, PET-PEESE, SPET, and SPET-SPEESE. It’s worth noting, though, that there is a fairly large penalty for using SPEESE when it is unnecessary: even with a quite large sample of 100 studies, SPEESE still has twice the RMSE of the FE estimator.\nThe middle column shows these estimators’ RMSE when there is an intermediate degree of selective publication. Because of the “fortuitous accident” of how the correlation between \\(d\\) and \\(V\\) affects the PEESE estimator, it is more accurate than SPEESE for small values of the true mean effect. Its advantage is larger when heterogeneity is larger, and heterogeneity also affects the point (i.e., what true mean effect) at which SPEESE catches up with PEESE. Then at larger true mean effects, the accuracy of SPEESE continues to improve while the accuracy of PEESE degrades. It is also interesting to note that at this intermediate degree of selective publication, none of the other bias-correction estimators (PET-PEESE, SPET, SPET-SPEESE) compete with PEESE and SPEESE.\nFinally, the right column plots RMSE when there’s strong selective publication, so only statistically significant effects appear. Just as in the middle column, PEESE edges out SPEESE for smaller values of the true mean effect. For very small true effects, both of these estimators are edged out by PET-PEESE and SPET-SPEESE. This only holds over a very small range for the true mean effect though, and for true effects above that range these conditional estimators perform poorly—consistently worse than just using PEESE or SPEESE.\nHere are charts for the other sample size distributions:\n\rUniform distribution of studies, maximum sample size of 120\rMore small studies, maximum sample size of 50\rMore small studies, maximum sample size of 120\rMore large studies, maximum sample size of 50\rMore large studies, maximum sample size of 120\r\rThe trends that I’ve noted mostly seem to hold for the other sample size distributions (but correct me if you disagree! I’m getting kind of bleary-eyed at the moment…). One difference worth noting is that when the sample size distribution skews towards having more large studies, the accuracy of the regular fixed-effect estimator improves a bit. At intermediate degrees of selective publication, the fixed-effect estimator is consistently more accurate than SPEESE, and mostly more accurate than PEESE too. With strong selective publication, though, the FE estimator blows up just as before.\n\r\rConclusions, caveats, further thoughts\rWhere does this leave us? The one thing that seems pretty clear is that if the meta-analyst’s goal is to test for potential small-sample bias, then SPET or SPEESE should be used rather than PET or PEESE. Beyond that, we’re in a bit of a morass. None of the estimators consistently out-performs the others across the conditions of the simulation. It’s only under certain conditions that any of the bias-correction methods are more accurate than using the regular FE estimator, and those conditions aren’t easy to identify in a real data analysis because they depend on the degree of publication bias.\nCaveats\rThese findings are also pretty tentative because of the limitations of the simulation conditions examined here. The distribution of sample sizes seems to affect the relative accuracy of the estimators to a certain degree, but I’ve only looked at a limited set of possibilities, and also limited consideration to rather large meta-samples of 100 studies.\nAnother caveat is that the simulations are based on \\(d\\) estimates from a two-group, between-subjects design with no covariates. In many applications, there is considerably more diversity in study designs. A given meta-analysis might include two-group, post-test only designs as well as between-subjects designs with a pre-test covariate or with repeated measures, as well as two-group designs with multiple (or multi-dimensional) outcomes. All of this introduces further layers of complexity into the relationship between sample size, effect magnitude, and selective publication.\nA further, quite important caveat is that selective publication is not the only possible explanation for a correlation between effect size and sample sizes. In another recent post, Uri sketches a scenario where investigators choose sample size to achieve adequate power (so following best practice!) for predicted effect sizes. If 1) true effects are heterogeneous and 2) investigators’ predictions are correlated with true effect sizes, then a meta-analysis will have effect size estimates that are correlated with sample size even in the absence of publication bias. A blog post by Richard Morey illustrates another possibility that leads to effect-sample size correlation, in which resource constraints induce negative correlation between sample size and the reliability of the outcome measure.\n\rHold me hostage\rIt seems to me that one lesson we can draw from this is that these regression-based corrections are pretty meager as analytic methods. We need to understand the mechanism of selective publication in order to be able to correct for its consequences, but the regression-based corrections don’t provide direct information here (even though their performance depends on it!). I think this speaks to the need for methods that directly model the mechanism, which means turning to selection models and studying the distribution of p-values. Also, without bringing in other pieces of information (like p-values), it seems more or less impossible to tease apart selective publication from other possible explanations for effect-sample size correlation.\nIf I had to pick one of the regression-based bias-correction method to use in an application—as in, if you handcuffed me to my laptop and threatened to not let me go until I analyzed your effect sizes—then on the basis of these simulation exercises, I think I would probably go with SPEESE as a default, and perhaps also report PEESE, but I wouldn’t bother with any of the others. Even though SPEESE is less accurate than PEESE and some other estimators under certain conditions, on a practical level it seems kind of silly to use different estimators when testing for publication bias versus trying to correct for it. And whatever advantage that regular PEESE has over SPEESE strikes me as kind of like cheating—it relies on an induced correlation between \\(d\\) and \\(V\\) to gain an accuracy advantage under certain conditions, but that correlation causes big problems under other conditions.\nEven if you chained me to the laptop, I would also definitely include a caution that these estimators should be interpreted more as sensitivity analyses than as bias-corrected estimates of the overall mean effect. This is roughly in line with the conclusions of Inzlicht, Gervais, and Berkman (2015). From their abstract:\n\rOur simulations revealed that not one of the bias-correction techniques revealed itself superior in all conditions, with corrections performing adequately in some situations but inadequately in others. Such a result implies that meta-analysts ought to present a range of possible effect sizes and to consider them all as being possible.\n\rTheir conclusion was in reference to PET, PEESE, and PET-PEESE. Unfortunately, the tweaks of SPET and SPEESE don’t clarify the situation.\n\rOutstanding questions\rThese exercises have left me wondering about a couple of things, which I’ll just mention briefly:\n\rI haven’t calculated confidence interval coverage levels for these simulations. I should probably add that but need to move on at the moment.\rThe ever-popular Trim-and-Fill procedure is based on the assumption that a funnel plot will be symmetric in the absence of publication bias. This assumption won’t hold if there’s correlation between \\(d\\) and \\(V\\), and so it would be interesting to see if using a function of sample size (i.e., just the first term of \\(V\\)) could improve the performance of Trim-and-Fill.\rUnder the model examined here, the bias in PET, PEESE, SPET, and SPEESE comes from the fact that the relevant regression relationships aren’t actually linear under selective publication. I do wonder whether using some more flexible sort of regression model (perhaps including a non-linear term) could reduce bias. The trick would be to find something that’s still constrained enough so that bias improvements aren’t swamped by increased variance.\rMany of the applications that I am familiar with involve syntheses where some studies contribute multiple effect size estimates, which might also be inter-correlated. Very little work has examined how regression corrections like PET-PEESE perform in such settings (the only study I know of is Reed, 2015, which involves a specialized and I think rather unusual data-generating model). For that matter, I don’t know of any work that looks at other publication bias correction methods either. Or what selective publication even means in this setting. Somebody should really work on that.\r\r\r\r","date":1493251200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493251200,"objectID":"a40b013a8b7b582c8cb5219219472990","permalink":"/pet-peese-performance/","publishdate":"2017-04-27T00:00:00Z","relpermalink":"/pet-peese-performance/","section":"post","summary":"Publication bias—or more generally, outcome reporting bias or dissemination bias—is recognized as a critical threat to the validity of findings from research syntheses. In the areas with which I am most familiar (education and psychology), it has become more or less a requirement for research synthesis projects to conduct analyses to detect the presence of systematic outcome reporting biases.","tags":["meta-analysis","publication bias","standardized mean difference","simulation"],"title":"You wanna PEESE of d's?","type":"post"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rOne of the papers that came out of my dissertation work (Pustejovsky, 2015) introduced an effect size metric called the log response ratio (or LRR) for use in meta-analysis of single-case research—particularly for single-case studies that measure behavioral outcomes through systematic direct observation. The original paper was pretty technical since it focused mostly on a formal measurement model for behavioral observation data. I’ve just completed a tutorial paper that demonstrates how to use the LRR for meta-analyzing single-case studies with behavioral outcomes. In this paper, I’ve tried to present the methods in as accessible a manner as I could muster, to provide a sort of “user’s guide” for researchers wanting to apply the LRR for their own work. You can find the working paper and supplementary materials (including data and replication code) on the Open Science Framework. I would welcome your feedback and questions about this work!\n","date":1493164800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493164800,"objectID":"1a73fda0037036693c066a8fd5376e49","permalink":"/using-log-response-ratios/","publishdate":"2017-04-26T00:00:00Z","relpermalink":"/using-log-response-ratios/","section":"post","summary":"One of the papers that came out of my dissertation work (Pustejovsky, 2015) introduced an effect size metric called the log response ratio (or LRR) for use in meta-analysis of single-case research—particularly for single-case studies that measure behavioral outcomes through systematic direct observation.","tags":["single-case design","effect size","response ratio","meta-analysis"],"title":"New working paper: Using log response ratios for meta-analyzing SCDs with behavioral outcomes","type":"post"},{"authors":[],"categories":null,"content":"","date":1488412800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488412800,"objectID":"10ae8bd9b47c18442ae579b376e1378e","permalink":"/talk/sree-2017-small-sample-corrections/","publishdate":"2018-04-15T08:15:00Z","relpermalink":"/talk/sree-2017-small-sample-corrections/","section":"talk","summary":"","tags":[],"title":"Small sample corrections for use of cluster-robust standard errors in the analysis of school-based experiments","type":"talk"},{"authors":["Eric A. Common","Kathleen L. Lane","James E. Pustejovsky","Austin H. Johnson","Liane E. Johl"],"categories":null,"content":"","date":1488326400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1488326400,"objectID":"40bac85ab57b57b4407fb844fc81cbd7","permalink":"/publication/fabi-meta-analysis/","publishdate":"2017-03-01T00:00:00Z","relpermalink":"/publication/fabi-meta-analysis/","section":"publication","summary":"This systematic review investigated one systematic approach to designing, implementing, and evaluating functional assessment–based interventions (FABI) for use in supporting school-age students with or at-risk for high-incidence disabilities. We field tested several recently developed methods for single-case design syntheses. First, we appraised the quality of individual studies and the overall body of work using Council for Exceptional Children’s standards. Next, we calculated and meta-analyzed within-case and between-case effect sizes. Results indicated that studies were of high methodological quality, with nine studies identified as being methodologically sound and demonstrating positive outcomes across 14 participants. However, insufficient evidence was available to classify the evidence base for FABIs due to small number of participants within (fewer than recommended three) and across (fewer than recommended 20) studies. Nonetheless, average within-case effect sizes were equivalent to increases of 118% between baseline and intervention phases. Finally, potential moderating variables were examined. Limitations and future directions are discussed.","tags":["design-comparable SMD","effect size","meta-analysis","response ratio","single-case design","systematic review"],"title":"Functional assessment-based interventions for students with or at-risk for high incidence disabilities: Field-testing single-case synthesis methods","type":"publication"},{"authors":["James E. Pustejovsky","John Ferron"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"826ea2f4fe473968695077cc1d1abae7","permalink":"/publication/meta-analysis-of-scd/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/meta-analysis-of-scd/","section":"publication","summary":"","tags":["design-comparable SMD","effect size","meta-analysis","response ratio","single-case design"],"title":"Research synthesis and meta-analysis of single-case designs","type":"publication"},{"authors":["Jeffrey C. Valentine","Emily E. Tanner-Smith","James E. Pustejovsky","Timothy S. Lau"],"categories":null,"content":"","date":1482105600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1482105600,"objectID":"b65cf94403316f41e29205d9928a99b2","permalink":"/publication/bc-smd-primer-and-tutorial/","publishdate":"2016-12-19T00:00:00Z","relpermalink":"/publication/bc-smd-primer-and-tutorial/","section":"publication","summary":"We describe a standardised mean difference statistic (d) for single-case designs that is equivalent to the usual d in between-groups experiments. We show how it can be used to summarise treatment effects over cases within a study, to do power analyses in planning new studies and grant proposals, and to meta-analyse effects across studies of the same question. We discuss limitations of this d-statistic, and possible remedies to them. Even so, this d-statistic is better founded statistically than other effect size measures for single-case design, and unlike many general linear model approaches such as multilevel modelling or generalised additive models, it produces a standardised effect size that can be integrated over studies with different outcome measures. SPSS macros for both effect size computation and power analysis are available.","tags":["single-case design","design-comparable SMD","effect size"],"title":"Between-case standardized mean difference effect sizes for single-case designs: A primer and tutorial using the scdhlm web application","type":"publication"},{"authors":[],"categories":null,"content":"","date":1482105600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1482105600,"objectID":"8533a34552c4295251acf50d77c1ecb2","permalink":"/talk/ies-2016-single-case-effect-sizes/","publishdate":"2018-04-15T08:15:00Z","relpermalink":"/talk/ies-2016-single-case-effect-sizes/","section":"talk","summary":"","tags":[],"title":"Effect sizes for single-case research","type":"talk"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rI’m pleased to announce that the Campbell Collaboration has just published a new discussion paper that I wrote with my colleagues Jeff Valentine and Emily Tanner-Smith about between-case standardized mean difference effect sizes for single-case designs.\rThe paper provides a relatively non-technical introduction to BC-SMD effect sizes and a tutorial on how to use the scdhlm web-app for calculating estimates of the BC-SMD for user-provided data.\rIf you have any questions or feedback about the app, please feel free to contact me!\n","date":1482105600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1482105600,"objectID":"c8d1fd5388627dda66ef6a38940c3c1e","permalink":"/scdhlm-tutorial/","publishdate":"2016-12-19T00:00:00Z","relpermalink":"/scdhlm-tutorial/","section":"post","summary":"I’m pleased to announce that the Campbell Collaboration has just published a new discussion paper that I wrote with my colleagues Jeff Valentine and Emily Tanner-Smith about between-case standardized mean difference effect sizes for single-case designs.","tags":["single-case design","effect size","design-comparable SMD"],"title":"New tutorial paper on BC-SMD effect sizes","type":"post"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rI am just back from the Institute of Education Sciences 2016 Principal Investigators meeting. Rob Horner had organized a session titled “Single-case methods: Current status and needed directions” as a tribute to our colleague Will Shadish, who passed away this past year. Rob invited me to give some brief remarks about Will as a mentor, and then to present some of my work with Will and Larry Hedges on effect sizes for single-case research. Here are the slides from my part of the presentation.\n","date":1482105600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1482105600,"objectID":"61daf5f8ea6e31de0fe106a63b802de7","permalink":"/ies-2016-pi-meeting/","publishdate":"2016-12-19T00:00:00Z","relpermalink":"/ies-2016-pi-meeting/","section":"post","summary":"I am just back from the Institute of Education Sciences 2016 Principal Investigators meeting. Rob Horner had organized a session titled “Single-case methods: Current status and needed directions” as a tribute to our colleague Will Shadish, who passed away this past year.","tags":["single-case design","effect size"],"title":"Presentation at IES 2016 PI meeting","type":"post"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rAbout one year ago, the nlme package introduced a feature that allowed the user to specify a fixed value for the residual variance in linear mixed effect models fitted with lme(). This feature is interesting to me because, when used with the varFixed() specification for the residual weights, it allows for estimation of a wide variety of meta-analysis models, including basic random effects models, bivariate models for estimating effects by trial arm, and other sorts of multivariate/multi-level random effects models. However, in kicking the tires on this feature, I noticed that the results that it produces are not quite consistent with the results produced by metafor, which is the main package I use for fitting meta-analytic models.\nIn this post, I document several examples of discrepant estimates between lme() and rma.mv(), using standard datasets included in the metafor package. The main take-aways are:\nThe discrepancies arise only with REML estimation (not with ML estimation).\rThe discrepancies are present whether or not the varFixed specification is used.\rThe discrepancies are mostly small (with minimal impact on the standard errors of the fixed effect estimates), but are larger than I would expect from computational/convergence differences alone.\r\rAnother example, based on a different dataset, is documented in this bug report. Wolfgang Viechtbauer, author of the metafor package, identified this problem with lme a few months ago already (see his responses in this thread on the R mixed models mailing list) and noted that the issue was localized to REML estimation. My thanks to Wolfgang for providing feedback on this post.\nBasic random effects model\rThis example fits a basic random effects model to the BCG vaccine data, available within metafor:\nlibrary(metafor)\rlibrary(nlme)\rbcg_example \u0026lt;- function(method = \u0026quot;REML\u0026quot;, constant_var = FALSE) {\rdata(dat.bcg)\rdat \u0026lt;- escalc(measure=\u0026quot;OR\u0026quot;, ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)\rv_bar \u0026lt;- mean(dat$vi)\rif (constant_var) dat$vi \u0026lt;- v_bar\r# random-effects model using rma.uni()\rLOR_uni_fit \u0026lt;- rma(yi, vi, data=dat, method = method)\rLOR_uni \u0026lt;- with(LOR_uni_fit, data.frame(f = \u0026quot;rma.uni\u0026quot;, logLik = logLik(LOR_uni_fit),\rest = as.numeric(b), se = se, tau = sqrt(tau2)))\r# random-effects model using rma.mv()\rLOR_mv_fit \u0026lt;- rma.mv(yi, vi, random = ~ 1 | trial, data=dat, method = method)\rLOR_mv \u0026lt;- with(LOR_mv_fit, data.frame(f = \u0026quot;rma.mv\u0026quot;, logLik = logLik(LOR_mv_fit),\rest = as.numeric(b), se = se, tau = sqrt(sigma2)))\r# random-effects model using lme()\rif (constant_var) {\rLOR_lme_fit \u0026lt;- lme(yi ~ 1, data = dat, method = method, random = ~ 1 | trial,\rcontrol = lmeControl(sigma = sqrt(v_bar)))\rtau \u0026lt;- sqrt(as.numeric(coef(LOR_lme_fit$modelStruct$reStruct, unconstrained = FALSE)) * v_bar) } else {\rLOR_lme_fit \u0026lt;- lme(yi ~ 1, data = dat, method = method, random = ~ 1 | trial,\rweights = varFixed(~ vi),\rcontrol = lmeControl(sigma = 1))\rtau \u0026lt;- sqrt(as.numeric(coef(LOR_lme_fit$modelStruct$reStruct, unconstrained = FALSE)))\r}\rLOR_lme \u0026lt;- data.frame(f = \u0026quot;lme\u0026quot;, logLik = logLik(LOR_lme_fit),\rest = as.numeric(fixef(LOR_lme_fit)), se = as.numeric(sqrt(vcov(LOR_lme_fit))), tau = tau)\rrbind(LOR_uni, LOR_mv, LOR_lme)\r}\rbcg_example(\u0026quot;REML\u0026quot;, constant_var = FALSE)\r## f logLik est se tau\r## 1 rma.uni -12.57566 -0.7451778 0.1860279 0.5811816\r## 2 rma.mv -12.57566 -0.7451778 0.1860280 0.5811818\r## 3 lme -13.34043 -0.7471979 0.1916902 0.6030524\rbcg_example(\u0026quot;REML\u0026quot;, constant_var = TRUE)\r## f logLik est se tau\r## 1 rma.uni -12.96495 -0.7716272 0.1977007 0.5911451\r## 2 rma.mv -12.96495 -0.7716272 0.1977007 0.5911452\r## 3 lme -15.62846 -0.7716272 0.1899448 0.5571060\rbcg_example(\u0026quot;ML\u0026quot;, constant_var = FALSE)\r## f logLik est se tau\r## 1 rma.uni -13.07276 -0.7419668 0.1779534 0.5499605\r## 2 rma.mv -13.07276 -0.7419669 0.1779534 0.5499608\r## 3 lme -13.07276 -0.7419668 0.1779534 0.5499605\rbcg_example(\u0026quot;ML\u0026quot;, constant_var = TRUE)\r## f logLik est se tau\r## 1 rma.uni -13.525084 -0.7716272 0.1899447 0.5571059\r## 2 rma.mv -13.525084 -0.7716272 0.1899447 0.5571059\r## 3 lme -2.479133 -0.7716272 0.1899447 0.5571060\r\rBi-variate random effects model\rThis example fits a bi-variate random effects model, also to the BCG vaccine data:\nbcg_bivariate \u0026lt;- function(method = \u0026quot;REML\u0026quot;, constant_var = FALSE) {\rdata(dat.bcg)\rdat_long \u0026lt;- to.long(measure=\u0026quot;OR\u0026quot;, ai=tpos, bi=tneg, ci=cpos, di=cneg, data=dat.bcg)\rlevels(dat_long$group) \u0026lt;- c(\u0026quot;exp\u0026quot;, \u0026quot;con\u0026quot;)\rdat_long$group \u0026lt;- relevel(dat_long$group, ref=\u0026quot;con\u0026quot;)\rdat_long \u0026lt;- escalc(measure=\u0026quot;PLO\u0026quot;, xi=out1, mi=out2, data=dat_long)\rv_bar \u0026lt;- mean(dat_long$vi)\rif (constant_var) dat_long$vi \u0026lt;- v_bar\r# bivariate random-effects model using rma.mv()\rbv_rma_fit \u0026lt;- rma.mv(yi, vi, mods = ~ group, random = ~ group | study, struct = \u0026quot;UN\u0026quot;, method = method,\rdata=dat_long)\rbv_rma \u0026lt;- with(bv_rma_fit, data.frame(f = \u0026quot;rma.mv\u0026quot;,\rlogLik = logLik(bv_rma_fit),\rtau1 = sqrt(tau2[1]),\rtau2 = sqrt(tau2[2])))\r# bivariate random-effects model using lme()\rif (constant_var) {\rbv_lme_fit \u0026lt;- lme(yi ~ group, data = dat_long, method = method, random = ~ group | study,\rcontrol = lmeControl(sigma = sqrt(v_bar)))\rtau_sq \u0026lt;- colSums(coef(bv_lme_fit$modelStruct$reStruct, unconstrained = FALSE) * matrix(c(1,0,0, 1,2,1), 3, 2)) * v_bar\r} else {\rbv_lme_fit \u0026lt;- lme(yi ~ group, data = dat_long, method = method, random = ~ group | study,\rweights = varFixed(~ vi),\rcontrol = lmeControl(sigma = 1))\rtau_sq \u0026lt;- colSums(coef(bv_lme_fit$modelStruct$reStruct, unconstrained = FALSE) * matrix(c(1,0,0, 1,2,1), 3, 2))\r}\rbv_lme \u0026lt;- data.frame(f = \u0026quot;lme\u0026quot;,\rlogLik = logLik(bv_lme_fit),\rtau1 = sqrt(tau_sq[1]),\rtau2 = sqrt(tau_sq[2]))\rrbind(bv_rma, bv_lme)\r}\rbcg_bivariate(\u0026quot;REML\u0026quot;, constant_var = FALSE)\r## f logLik tau1 tau2\r## 1 rma.mv -31.50167 1.617807 1.244429\r## 2 lme -32.32612 1.631619 1.254437\rbcg_bivariate(\u0026quot;REML\u0026quot;, constant_var = TRUE)\r## f logLik tau1 tau2\r## 1 rma.mv -31.09623 1.644897 1.191679\r## 2 lme -37.06035 1.578435 1.142260\rbcg_bivariate(\u0026quot;ML\u0026quot;, constant_var = FALSE)\r## f logLik tau1 tau2\r## 1 rma.mv -33.08793 1.551558 1.196399\r## 2 lme -33.08793 1.551558 1.196399\rbcg_bivariate(\u0026quot;ML\u0026quot;, constant_var = TRUE)\r## f logLik tau1 tau2\r## 1 rma.mv -32.647023 1.578434 1.14226\r## 2 lme -2.237355 1.578434 1.14226\r\rThree-level random-effects model\rThis example fits a three-level random-effects model to the data from Konstantopoulos (2011):\nKonstantopoulos \u0026lt;- function(method = \u0026quot;REML\u0026quot;, constant_var = FALSE) {\rdat \u0026lt;- get(data(dat.konstantopoulos2011))\rv_bar \u0026lt;- mean(dat$vi)\rif (constant_var) dat$vi \u0026lt;- v_bar\r# multilevel random-effects model using rma.mv()\rml_rma_fit \u0026lt;- rma.mv(yi, vi, random = ~ 1 | district/school, data=dat, method = method)\rml_rma \u0026lt;- with(ml_rma_fit, data.frame(f = \u0026quot;rma.mv\u0026quot;, logLik = logLik(ml_rma_fit),\rest = as.numeric(b), se = se, tau1 = sqrt(sigma2[1]), tau2 = sqrt(sigma2[2])))\r# multilevel random-effects model using lme()\rif (constant_var) {\rml_lme_fit \u0026lt;- lme(yi ~ 1, data = dat, method = method, random = ~ 1 | district / school,\rcontrol = lmeControl(sigma = sqrt(v_bar)))\rtau \u0026lt;- sqrt(as.numeric(coef(ml_lme_fit$modelStruct$reStruct, unconstrained = FALSE)) * v_bar)\r} else {\rml_lme_fit \u0026lt;- lme(yi ~ 1, data = dat, method = method, random = ~ 1 | district / school,\rweights = varFixed(~ vi),\rcontrol = lmeControl(sigma = 1))\rtau \u0026lt;- sqrt(as.numeric(coef(ml_lme_fit$modelStruct$reStruct, unconstrained = FALSE)))\r} ml_lme \u0026lt;- data.frame(f = \u0026quot;lme\u0026quot;,\rlogLik = logLik(ml_lme_fit),\rest = as.numeric(fixef(ml_lme_fit)),\rse = as.numeric(sqrt(diag(vcov(ml_lme_fit)))),\rtau1 = tau[2],\rtau2 = tau[1])\rrbind(ml_rma, ml_lme)\r}\rKonstantopoulos(\u0026quot;REML\u0026quot;, constant_var = FALSE)\r## f logLik est se tau1 tau2\r## 1 rma.mv -7.958724 0.1847132 0.08455592 0.2550724 0.1809324\r## 2 lme -10.716781 0.1841827 0.08641374 0.2605790 0.1884588\rKonstantopoulos(\u0026quot;REML\u0026quot;, constant_var = TRUE)\r## f logLik est se tau1 tau2\r## 1 rma.mv -9.724839 0.1724309 0.08052701 0.2401816 0.1878155\r## 2 lme -16.119274 0.1724309 0.07980479 0.2380275 0.1848778\rKonstantopoulos(\u0026quot;ML\u0026quot;, constant_var = FALSE)\r## f logLik est se tau1 tau2\r## 1 rma.mv -8.394936 0.1844554 0.08048168 0.2402881 0.1812865\r## 2 lme -8.394936 0.1844554 0.08048168 0.2402881 0.1812865\rKonstantopoulos(\u0026quot;ML\u0026quot;, constant_var = TRUE)\r## f logLik est se tau1 tau2\r## 1 rma.mv -10.11095 0.1712365 0.07645094 0.2250687 0.1881229\r## 2 lme 90.21692 0.1712365 0.07645093 0.2250687 0.1881228\r\r","date":1478476800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1478476800,"objectID":"1c729a817b06a6eee5f9666ddf7b8514","permalink":"/bug-in-nlme-with-fixed-sigma/","publishdate":"2016-11-07T00:00:00Z","relpermalink":"/bug-in-nlme-with-fixed-sigma/","section":"post","summary":"About one year ago, the nlme package introduced a feature that allowed the user to specify a fixed value for the residual variance in linear mixed effect models fitted with lme().","tags":["Rstats","programming","hierarchical models","nlme"],"title":"Bug in nlme::lme with fixed sigma and REML estimation","type":"post"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rParker, Vannest, Davis, and Sauber (2011) proposed the Tau-U index—actually several indices, rather—as effect size measures for single-case designs. The original paper describes several different indices that involve corrections for trend during the baseline phase, treatment phase, both phases, or neither phase. Without correcting for trends in either phase, the index is equal to the Mann-Whitney \\(U\\) statistic calculated by comparing every pair of observations containing one point from each phase, scaled by the total number of such pairs. This version, which I’ll call just “Tau”, is simply a linear re-scaling of the NAP statistic to the range [-1,1].\nTo correct for baseline trend, the original paper proposes to calculate Kendall’s rank correlation (\\(\\tau_A\\)) between the phase A outcome data and the session numbers and use the result to make an adjustment to Tau. The other analyses presented in the original paper (incorporating adjustments for time trends during the treatment phase) are not presented in subsequent review papers, nor are they implemented in the web-calculator created by the authors, and so I won’t discuss them further here. Instead, in this post I will examine the calculation of the version of Tau-U that incorporates a baseline trend correction. This version seems to be the most widely applied in practice (likely due to the availability of the web-calculator) and is presented in several review papers by the same authors. It turns out though, that the definition of the index has shifted from the original paper to subsequent presentations.\nTo make this concrete, let me first define a couple of things. Suppose that we have data from the baseline and treatment phases for a single case, where the baseline phase has \\(m\\) observations and treatment phase has \\(n\\) observations. Let \\(y^A_1,...,y^A_m\\) denote the baseline phase data and \\(y^B_1,...,y^B_n\\) denote the treatment phase data. Let \\(S_P\\) denote Kendall’s S statistic calculated for the comparison between phases and \\(S_A\\) denote Kendall’s S statistic calculated on the baseline trend. More precisely,\n\\[\r\\begin{aligned}\rS_P \u0026amp;= \\sum_{i=1}^m \\sum_{j=1}^n \\left[I\\left(y^B_j \u0026gt; y^A_i\\right) - I\\left(y^B_j \u0026lt; y^A_i\\right)\\right] \\\\\rS_A \u0026amp;= \\sum_{i=1}^{m - 1} \\sum_{j = i + 1}^m \\left[I\\left(y^A_j \u0026gt; y^A_i\\right) - I\\left(y^A_j \u0026lt; y^A_i\\right)\\right].\r\\end{aligned}\r\\]\nNote that \\(S_P\\) is calculated from \\(m \\times n\\) pairs of observations, and Tau (without trend correction) is equal to \\(\\text{Tau} = S_P / (m n)\\). Furthermore, \\(S_A\\) is calculated from \\(m (m - 1) / 2\\) pairs of observations and Kendall’s rank correlation coefficient for the baseline phase observations is \\(t_A = S_A / [m (m - 1) / 2]\\).\nThe original version\rIn the original paper, the authors explain that values of Tau-U can be calculated by adding or substracting values of \\(\\tau\\), weighted by the corresponding number of pairs. Thus, Tau-U would be calculated as\n\\[\r\\text{Tau-U} = \\frac{S_P - S_A}{mn + m(m - 1) / 2} = \\frac{2n}{2n + m - 1} \\text{Tau} - \\frac{m - 1}{2n + m - 1} t_A.\r\\]\nBoth \\(\\text{Tau}\\) and \\(t_A\\) have range [-1,1], and so Tau-U has the same range. This version of Tau-U can be calculated using this web app by Rumen Manolov, which is based on this R code by Kevin Tarlow. (The app and the R script also provide the other variants of Tau-U described in Parker, Vannest, Davis, and Sauber (2011).)\n\rThe revised (?) version\rParker, Vannest, and Davis (2011) reviewed nine different non-overlap indices for use with data from single-case designs, including Tau-U. Rather than describing all four variations from the original paper, the authors define the index as follows:\n\rTau-U (Parker et al., in press) extends [Tau] to control for undesirable positive baseline trend (monotonic trend). Monotonic trend is the upward progression of data points in any configuration, whether linear, curvilinear, or even in a mixed pattern of “fits and starts” (p. 11).\n\rIn this and subsequent review articles, Tau-U seems to refer exclusively to the variant involving comparison between phases A and B, with an adjustment for phase A trend. That seems a sensible enough choice, which could have been due to space limitations, guidance from the journal editor, or further refinement of the methods (i.e., recognizing which of the variants would be most useful in application). However, the presentation of Tau-U in this article involved more than a change in emphasis—the definition of the index also changed. Following the notation above, Tau-U was now defined as\n\\[\r\\text{Tau-U} = \\frac{S_P - S_A}{mn} = \\text{Tau} - \\frac{m - 1}{2n} t_A.\r\\]\nThe logical range of this version of the index is from \\(-(2n + m - 1) / (2n)\\) to \\((2n + m - 1) / (2n)\\).\nThis is the version of Tau-U implemented in the singlecaseresearch.org web calculator. It is also the version described in a later chapter by the same authors (Parker, Vannest, \u0026amp; Davis, 2014) and a review article by Rakap (2015). My previous post about Tau-U also presented this version of the index and noted that its magnitude is sensitive to the lengths of the baseline and treatment phases, which makes it rather difficult to interpret the Tau-U index as a measure of treatment effect magnitude.\n\rComparison\rHere is an R function for calculating the original or revised versions of Tau-U:\nTau_U \u0026lt;- function(A_data, B_data, version = \u0026quot;revised\u0026quot;) {\rm \u0026lt;- length(A_data)\rn \u0026lt;- length(B_data)\rQ_A \u0026lt;- sapply(A_data, function(j) (j \u0026gt; A_data) - (j \u0026lt; A_data))\rQ_P \u0026lt;- sapply(B_data, function(j) (j \u0026gt; A_data) - (j \u0026lt; A_data))\rif (version==\u0026quot;original\u0026quot;) {\r(sum(Q_P) - sum(Q_A[upper.tri(Q_A)])) / (m * n + m * (m - 1) / 2)\r} else {\r(sum(Q_P) - sum(Q_A[upper.tri(Q_A)])) / (m * n)\r}\r}\rThe papers I’ve mentioned above all provide examples of the calculation of Tau-U. The following table reports the data from each of these examples (Parker, Vannest, Davis, and Sauber, 2011a; Parker, Vannest, and Davis, 2011b; Parker, Vannest, \u0026amp; Davis, 2014), along with the value of Tau-U based on the original and revised formulas. The differences in magnitude are non-trivial.\n\r\rSource\rPhase A data\rPhase B data\roriginal\rrevised\r\r\r\r2011a, Figure 2\r2, 3, 5, 3\r4, 5, 5, 7, 6\r0.5000000\r0.6500000\r\r2011b, Figure 1\r20, 20, 26, 25, 22, 23\r28, 25, 24, 27, 30, 30, 29\r0.5438596\r0.7380952\r\r2011b, Table 1\r3, 3, 4, 5\r4, 5, 6, 7, 7\r0.4230769\r0.5500000\r\r2014, Figure 4.1\r22, 21, 23, 23, 23, 22\r24, 22, 23, 23, 24, 26, 25\r0.4385965\r0.5952381\r\r\r\r\rImplications\rRather than one effect size index called “Tau-U”, there are instead two different definitions, which can lead to quite different values of the index. Given this, researchers who apply Tau-U should endeavor to be clear and unambiguous about which version of the index they use. This can be done by stating exactly which software routine, web-app, or formula was used in making the calculations. If the calculations are done using a computer script, then the script should be made available (e.g., through the Open Science Framework) so that other researchers can replicate the calculations.\nFurthermore, researchers need to be careful about applying interpretive guidelines for Tau-U, since those guidelines will not apply uniformly across the different versions of the index.\nFinally, I would recommend that any researchers who conduct a meta-analysis of single-case research make available the raw data used for effect size calculations, so that other researchers can scrutinize, replicate, and extend their analyses. The whole enterprise of research synthesis rests on the availability of data from primary studies (at least in summary form). It seems to me that meta-analysts thus have a duty to make the data that they assemble and organize readily accessible for others to use. Particularly in the context of meta-analysis of single-case research—where new methods are developing rapidly and there is not currently consensus around best practices—it seems especially appropriate and prudent to make one’s data available for future re-analysis.\n\r","date":1478131200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1478131200,"objectID":"35439de18a170c1c2e5c6d51c2fffb10","permalink":"/what-is-tau-u/","publishdate":"2016-11-03T00:00:00Z","relpermalink":"/what-is-tau-u/","section":"post","summary":"Parker, Vannest, Davis, and Sauber (2011) proposed the Tau-U index—actually several indices, rather—as effect size measures for single-case designs. The original paper describes several different indices that involve corrections for trend during the baseline phase, treatment phase, both phases, or neither phase.","tags":["effect size","single-case design","non-overlap measures"],"title":"What is Tau-U?","type":"post"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rI’ve just posted a new version of my working paper, Procedural sensitivities of effect sizes for single-case designs with behavioral outcome measures. The abstract is below. This version is a major update of an earlier paper that focused only on the non-overlap measures. The new version also includes analysis of two other effect sizes (the within-case standardized mean difference and the log response ratio) as well as additional results and more succinct summaries of the main findings.\nThe paper itself is available on the Open Science Framework (here), as are the supplementary materials and Source code. I also created interaction versions of the graphics from the main paper and the supplementary materials, which can be viewed in this shiny app. I would welcome any comments, questions, or feedback that readers may have.\nAbstract\rA wide variety of effect size indices have been proposed for quantifying the magnitude of treatment effects in single-case designs. Commonly used measures include parametric indices such as the standardized mean difference, as well as non-overlap measures, such as the percentage of non-overlapping data, improvement rate difference, and non-overlap of all pairs. Currently, little is known about the properties of these indices when applied to behavioral data collected by systematic direct observation, even though systematic direct observation is the most common approach to outcome measurement in single-case research. This study uses computer simulation to investigate the properties of several single-case effect size measures when applied to systematic direct observation data. Results indicate that the magnitude of the non-overlap measures and of the standardized mean difference can be strongly influenced by procedural details of the study’s design, which is a significant limitation to using these indices as effect sizes for meta-analysis of single-case designs. A less widely used parametric index, the log-response ratio, has the advantage of being insensitive to sample size and observation session length, although its magnitude is influenced by the use of partial interval recording.\n\r","date":1476662400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1476662400,"objectID":"369cc7229d5fb32210b3da7688356d9f","permalink":"/scd-effect-size-sensitivities/","publishdate":"2016-10-17T00:00:00Z","relpermalink":"/scd-effect-size-sensitivities/","section":"post","summary":"I’ve just posted a new version of my working paper, Procedural sensitivities of effect sizes for single-case designs with behavioral outcome measures. The abstract is below. This version is a major update of an earlier paper that focused only on the non-overlap measures.","tags":["effect size","single-case design","response ratio","non-overlap measures","simulation"],"title":"New working paper: Procedural sensitivities of SCD effect sizes","type":"post"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rIn today’s Quant Methods colloquium, I gave an introduction to the logic and purposes of Monte Carlo simulation studies, with examples written in R.\n\rHere are the slides from my presentation.\rYou can find the code that generates the slides here.\rHere is my presentation on the same topic from a couple of years ago.\rDavid Robinson’s blog has a much more in-depth discussion of beta-binomial regression.\rThe data I used is from Lahman’s baseball database.\r\r","date":1475020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1475020800,"objectID":"c872757b795456eeb9379cc437b31cde","permalink":"/simulation-studies-in-r-2016/","publishdate":"2016-09-28T00:00:00Z","relpermalink":"/simulation-studies-in-r-2016/","section":"post","summary":"In today’s Quant Methods colloquium, I gave an introduction to the logic and purposes of Monte Carlo simulation studies, with examples written in R.\n\rHere are the slides from my presentation.","tags":["Rstats","simulation","programming"],"title":"Simulation studies in R (Fall, 2016 version)","type":"post"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rI have recently been working to ensure that my clubSandwich package works correctly on fitted lme and gls models from the nlme package, which is one of the main R packages for fitting hierarchical linear models. In the course of digging around in the guts of nlme, I noticed a bug in the getVarCov function. The purpose of the function is to extract the estimated variance-covariance matrix of the errors from a fitted lme or gls model.\nIt seems that this function is sensitive to the order in which the input data are sorted. This bug report noted the problem, but unfortunately their proposed fix doesn’t seem to solve the problem. In this post I’ll demonstrate the bug and a solution. (I’m posting this here because the R project’s bug reporting system is currently closed to people who were not registered as of early July, evidently due to some sort of spamming problem.)\nThe issue\rHere’s a simple demonstration of the problem. I’ll first fit a gls model with a heteroskedastic variance function and an AR(1) auto-correlation structure (no need to worry about the substance of the specification—we’re just worried about computation here) and then extract the variances for each of the units.\n# Demonstrate the problem with gls model\rlibrary(nlme)\rdata(Ovary)\rgls_raw \u0026lt;- gls(follicles ~ sin(2*pi*Time) + cos(2*pi*Time), data = Ovary,\rcorrelation = corAR1(form = ~ 1 | Mare),\rweights = varPower())\rMares \u0026lt;- levels(gls_raw$groups)\rV_raw \u0026lt;- lapply(Mares, function(g) getVarCov(gls_raw, individual = g))\rNow I’ll repeat the process using the same data, but sorted in a different order\nOvary_sorted \u0026lt;- Ovary[with(Ovary, order(Mare, Time)),]\rgls_sorted \u0026lt;- update(gls_raw, data = Ovary_sorted)\rV_sorted \u0026lt;- lapply(Mares, function(g) getVarCov(gls_sorted, individual = g))\rThe variance component estimates are essentially equal:\nall.equal(gls_raw$modelStruct, gls_sorted$modelStruct)\r## [1] TRUE\rHowever, the extracted variance-covariance matrices are not:\nall.equal(V_raw, V_sorted)\r## [1] \u0026quot;Component 1: Mean relative difference: 0.03256\u0026quot; ## [2] \u0026quot;Component 3: Mean relative difference: 0.05830791\u0026quot;\r## [3] \u0026quot;Component 4: Mean relative difference: 0.1142209\u0026quot; ## [4] \u0026quot;Component 5: Mean relative difference: 0.03619692\u0026quot;\r## [5] \u0026quot;Component 6: Mean relative difference: 0.09260648\u0026quot;\r## [6] \u0026quot;Component 8: Mean relative difference: 0.08650327\u0026quot;\r## [7] \u0026quot;Component 9: Mean relative difference: 0.07627162\u0026quot;\r## [8] \u0026quot;Component 10: Mean relative difference: 0.018103\u0026quot; ## [9] \u0026quot;Component 11: Mean relative difference: 0.1020658\u0026quot;\rHere’s the code of the relevant function:\nnlme:::getVarCov.gls\r## function (obj, individual = 1, ...) ## {\r## S \u0026lt;- corMatrix(obj$modelStruct$corStruct)[[individual]]\r## if (!is.null(obj$modelStruct$varStruct)) {\r## ind \u0026lt;- obj$groups == individual\r## vw \u0026lt;- 1/varWeights(obj$modelStruct$varStruct)[ind]\r## }\r## else vw \u0026lt;- rep(1, nrow(S))\r## vars \u0026lt;- (obj$sigma * vw)^2\r## result \u0026lt;- t(S * sqrt(vars)) * sqrt(vars)\r## class(result) \u0026lt;- c(\u0026quot;marginal\u0026quot;, \u0026quot;VarCov\u0026quot;)\r## attr(result, \u0026quot;group.levels\u0026quot;) \u0026lt;- names(obj$groups)\r## result\r## }\r## \u0026lt;bytecode: 0x000000001bc39d00\u0026gt;\r## \u0026lt;environment: namespace:nlme\u0026gt;\rThe issue is in the 4th line of the body. getVarCov.gls assumes that varWeights(obj$modelStruct$varStruct) is sorted in the same order as obj$groups, which is not necessarily true. Instead, varWeights seem to return the weights sorted according to the grouping variable. For this example, that means that the varWeights will not depend on the order in which the groups are sorted.\nidentical(gls_raw$groups, gls_sorted$groups)\r## [1] FALSE\ridentical(varWeights(gls_raw$modelStruct$varStruct), varWeights(gls_sorted$modelStruct$varStruct))\r## [1] TRUE\r\rFix for nlme:::getVarCov.gls\rI think this can be solved by either\n\rputting the varWeights back into the same order as the raw data or\rsorting obj$groups before identifying the rows corresponding to the specified individual.\r\rHere’s a revised function that takes the second approach:\n# proposed patch for getVarCov.gls\rgetVarCov_revised_gls \u0026lt;- function (obj, individual = 1, ...) {\rS \u0026lt;- corMatrix(obj$modelStruct$corStruct)[[individual]]\rif (!is.null(obj$modelStruct$varStruct)) {\rind \u0026lt;- sort(obj$groups) == individual\rvw \u0026lt;- 1 / varWeights(obj$modelStruct$varStruct)[ind]\r}\relse vw \u0026lt;- rep(1, nrow(S))\rvars \u0026lt;- (obj$sigma * vw)^2\rresult \u0026lt;- t(S * sqrt(vars)) * sqrt(vars)\rclass(result) \u0026lt;- c(\u0026quot;marginal\u0026quot;, \u0026quot;VarCov\u0026quot;)\rattr(result, \u0026quot;group.levels\u0026quot;) \u0026lt;- names(obj$groups)\rresult\r}\rTesting that it works correctly:\nV_raw \u0026lt;- lapply(Mares, function(g) getVarCov_revised_gls(gls_raw, individual = g))\rV_sorted \u0026lt;- lapply(Mares, function(g) getVarCov_revised_gls(gls_sorted, individual = g))\rall.equal(V_raw, V_sorted)\r## [1] TRUE\r\rFix for nlme:::getVarCov.lme\rThe same issue comes up in getVarCov.lme. Here’s the fix and verification:\n# proposed patch for getVarCov.lme\rgetVarCov_revised_lme \u0026lt;- function (obj, individuals, type = c(\u0026quot;random.effects\u0026quot;, \u0026quot;conditional\u0026quot;, \u0026quot;marginal\u0026quot;), ...) {\rtype \u0026lt;- match.arg(type)\rif (any(\u0026quot;nlme\u0026quot; == class(obj))) stop(\u0026quot;not implemented for \\\u0026quot;nlme\\\u0026quot; objects\u0026quot;)\rif (length(obj$group) \u0026gt; 1) stop(\u0026quot;not implemented for multiple levels of nesting\u0026quot;)\rsigma \u0026lt;- obj$sigma\rD \u0026lt;- as.matrix(obj$modelStruct$reStruct[[1]]) * sigma^2\rif (type == \u0026quot;random.effects\u0026quot;) {\rresult \u0026lt;- D\r}\relse {\rresult \u0026lt;- list()\rgroups \u0026lt;- sort(obj$groups[[1]])\rugroups \u0026lt;- unique(groups)\rif (missing(individuals)) individuals \u0026lt;- as.matrix(ugroups)[1, ]\rif (is.numeric(individuals)) individuals \u0026lt;- ugroups[individuals]\rfor (individ in individuals) {\rindx \u0026lt;- which(individ == ugroups)\rif (!length(indx)) stop(gettextf(\u0026quot;individual %s was not used in the fit\u0026quot;, sQuote(individ)), domain = NA)\rif (is.na(indx)) stop(gettextf(\u0026quot;individual %s was not used in the fit\u0026quot;, sQuote(individ)), domain = NA)\rind \u0026lt;- groups == individ\rif (!is.null(obj$modelStruct$corStruct)) {\rV \u0026lt;- corMatrix(obj$modelStruct$corStruct)[[as.character(individ)]]\r}\relse V \u0026lt;- diag(sum(ind))\rif (!is.null(obj$modelStruct$varStruct)) sds \u0026lt;- 1/varWeights(obj$modelStruct$varStruct)[ind]\relse sds \u0026lt;- rep(1, sum(ind))\rsds \u0026lt;- obj$sigma * sds\rcond.var \u0026lt;- t(V * sds) * sds\rdimnames(cond.var) \u0026lt;- list(1:nrow(cond.var), 1:ncol(cond.var))\rif (type == \u0026quot;conditional\u0026quot;) result[[as.character(individ)]] \u0026lt;- cond.var\relse {\rZ \u0026lt;- model.matrix(obj$modelStruct$reStruc, getData(obj))[ind, , drop = FALSE]\rresult[[as.character(individ)]] \u0026lt;- cond.var + Z %*% D %*% t(Z)\r}\r}\r}\rclass(result) \u0026lt;- c(type, \u0026quot;VarCov\u0026quot;)\rattr(result, \u0026quot;group.levels\u0026quot;) \u0026lt;- names(obj$groups)\rresult\r}\rlme_raw \u0026lt;- lme(follicles ~ sin(2*pi*Time) + cos(2*pi*Time), random = ~ 1 | Mare,\rcorrelation = corExp(form = ~ Time),\rweights = varPower(),\rdata=Ovary)\rlme_sorted \u0026lt;- update(lme_raw, data = Ovary_sorted)\rall.equal(lme_raw$modelStruct, lme_sorted$modelStruct)\r## [1] TRUE\r# current getVarCov\rV_raw \u0026lt;- lapply(Mares, function(g) getVarCov(lme_raw, individual = g, type = \u0026quot;marginal\u0026quot;))\rV_sorted \u0026lt;- lapply(Mares, function(g) getVarCov(lme_sorted, individual = g, type = \u0026quot;marginal\u0026quot;))\rall.equal(V_raw, V_sorted)\r## [1] \u0026quot;Component 1: Component 1: Mean relative difference: 0.003989954\u0026quot; ## [2] \u0026quot;Component 3: Component 1: Mean relative difference: 0.003784181\u0026quot; ## [3] \u0026quot;Component 4: Component 1: Mean relative difference: 0.003028662\u0026quot; ## [4] \u0026quot;Component 5: Component 1: Mean relative difference: 0.0005997944\u0026quot;\r## [5] \u0026quot;Component 6: Component 1: Mean relative difference: 0.002350456\u0026quot; ## [6] \u0026quot;Component 7: Component 1: Mean relative difference: 0.007103733\u0026quot; ## [7] \u0026quot;Component 8: Component 1: Mean relative difference: 0.001887638\u0026quot; ## [8] \u0026quot;Component 9: Component 1: Mean relative difference: 0.0009601843\u0026quot;\r## [9] \u0026quot;Component 10: Component 1: Mean relative difference: 0.004748783\u0026quot;\r## [10] \u0026quot;Component 11: Component 1: Mean relative difference: 0.001521097\u0026quot;\r# revised getVarCov V_raw \u0026lt;- lapply(Mares, function(g) getVarCov_revised_lme(lme_raw, individual = g, type = \u0026quot;marginal\u0026quot;))\rV_sorted \u0026lt;- lapply(Mares, function(g) getVarCov_revised_lme(lme_sorted, individual = g, type = \u0026quot;marginal\u0026quot;))\rall.equal(V_raw, V_sorted)\r## [1] TRUE\r\rSession info\rsessionInfo()\r## R version 3.6.3 (2020-02-29)\r## Platform: x86_64-w64-mingw32/x64 (64-bit)\r## Running under: Windows 10 x64 (build 17763)\r## ## Matrix products: default\r## ## locale:\r## [1] LC_COLLATE=English_United States.1252 ## [2] LC_CTYPE=English_United States.1252 ## [3] LC_MONETARY=English_United States.1252\r## [4] LC_NUMERIC=C ## [5] LC_TIME=English_United States.1252 ## ## attached base packages:\r## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages:\r## [1] nlme_3.1-144\r## ## loaded via a namespace (and not attached):\r## [1] Rcpp_1.0.4.6 bookdown_0.14 lattice_0.20-38 digest_0.6.25 ## [5] grid_3.6.3 magrittr_1.5 evaluate_0.14 blogdown_0.18 ## [9] rlang_0.4.5 stringi_1.4.3 rmarkdown_2.1 tools_3.6.3 ## [13] stringr_1.4.0 xfun_0.12 yaml_2.2.0 compiler_3.6.3 ## [17] htmltools_0.4.0 knitr_1.28\r\r","date":1470787200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1470787200,"objectID":"ba010e322771a5d0702ab7cfc81a366b","permalink":"/bug-in-nlme-getvarcov/","publishdate":"2016-08-10T00:00:00Z","relpermalink":"/bug-in-nlme-getvarcov/","section":"post","summary":"I have recently been working to ensure that my clubSandwich package works correctly on fitted lme and gls models from the nlme package, which is one of the main R packages for fitting hierarchical linear models.","tags":["Rstats","programming","hierarchical models","nlme"],"title":"Bug in nlme::getVarCov","type":"post"},{"authors":[],"categories":null,"content":"","date":1469923200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1469923200,"objectID":"6bc6180dd36c7830895f1fb4a4a168dd","permalink":"/talk/jsm-2016-small-sample-crve/","publishdate":"2018-04-15T08:15:00Z","relpermalink":"/talk/jsm-2016-small-sample-crve/","section":"talk","summary":"","tags":[],"title":"Small-sample methods for cluster-robust variance estimation and hypothesis testing in fixed effects models","type":"talk"},{"authors":[],"categories":null,"content":"","date":1468800000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1468800000,"objectID":"424c3f2ae85b2931a0044339d0d013bd","permalink":"/talk/air-2016-when-large-samples-act-small/","publishdate":"2018-04-15T08:15:00Z","relpermalink":"/talk/air-2016-when-large-samples-act-small/","section":"talk","summary":"","tags":[],"title":"When large samples act small: The importance of small-sample adjustments for cluster-robust inference in impact evaluations","type":"talk"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rThe standardized mean difference (SMD) is surely one of the best known and most widely used effect size metrics used in meta-analysis. In generic terms, the SMD parameter is defined as the difference in population means between two groups (often this difference represents the effect of some intervention), scaled by the population standard deviation of the outcome metric. Estimates of the SMD can be obtained from a wide variety of experimental designs, ranging from simple, completely randomized designs, to repeated measures designs, to cluster-randomized trials.\nThere’s some nuance involved in figuring out how to calculate estimates of the SMD from each design, mostly to do with exactly what sort of standard deviation to use in the denominator of the effect size. I’ll leave that discussion for another day. Here, I’d like to look at the question of how to estimate the sampling variance of the SMD. An estimate of the sampling variance is needed in order to meta-analyze a collection of effect sizes, and so getting the variance calculations right is an important (and sometimes time consuming) part of any meta-analysis project. However, the standard textbook treatments of effect size calculations cover this question only for a limited number of simple cases. I’d like to suggest a different, more general way of thinking about it, which provides a way to estimate the SMD and its variance in some non-standard cases (and also leads to slight differences from conventional formulas for the standard ones). All of this will be old hat for seasoned synthesists, but I hope it might be useful for students and researchers just getting started with meta-analysis.\nTo start, let me review (regurgitate?) the standard presentation.\nSMD from a simple, independent groups design\rTextbook presentations of the SMD estimator almost always start by introducing the estimator in the context of a simple, independent groups design. Call the groups T and C, the sample sizes \\(n_T\\) and \\(n_C\\), the sample means \\(\\bar{y}_T\\) and \\(\\bar{y}_C\\), and the sample variances \\(s_T^2\\) and \\(s_C^2\\). A basic moment estimator of the SMD is then\n\\[\rd = \\frac{\\bar{y}_T - \\bar{y}_C}{s_p}\r\\]\nwhere \\(s_p^2 = \\frac{\\left(n_T - 1\\right)s_T^2 + \\left(n_C - 1\\right) s_C^2}{n_T + n_C - 2}\\) is a pooled estimator of the population variance. The standard estimator for the sampling variance of \\(d\\) is\n\\[\rV_d = \\frac{n_T + n_C}{n_T n_C} + \\frac{d^2}{2\\left(n_T + n_C - 2\\right)},\r\\]\nor some slight variant thereof. This estimator is based on a delta-method approximation for the asymptotic variance of \\(d\\).\nIt is well known that \\(d\\) has a small sample bias that depends on sample sizes. Letting\n\\[\rJ(x) = 1 - \\frac{3}{4x - 1},\r\\]\nthe bias-corrected estimator is\n\\[\rg = J\\left(n_T + n_C - 2\\right) \\times d,\r\\]\nand is often referred to as Hedges’ \\(g\\) because it was proposed in Hedges (1981). Some meta-analysts use \\(V_d\\), but with \\(d^2\\) replaced by \\(g^2\\), as an estimator of the large-sample variance of \\(g\\); others use\n\\[\rV_g = J^2\\left(n_T + n_C - 2\\right) \\left(\\frac{n_T + n_C}{n_T n_C} + \\frac{g^2}{2\\left(n_T + n_C - 2\\right)}\\right).\r\\]\nViechtbauer (2007) provides further details on variance estimation and confidence intervals for the SMD in this case.\n\rA general formula for \\(g\\) and its sampling variance\rThe above formulas are certainly useful, but in practice meta-analyses often include studies that use other, more complex designs.\rGood textbook presentations also cover computation of \\(g\\) and its variance for some other cases (e.g., Borenstein, 2009, also covers one-group pre/post designs and analysis of covariance). Less careful presentations only cover the simple, independent groups design and thus may inadvertently leave the impression that the variance estimator \\(V_d\\) given above applies in general. With other types of studies, \\(V_d\\) can be a wildly biased estimator of the actual sampling variance of \\(d\\), because it is derived under the assumption that the numerator of \\(d\\) is estimated as the difference in means of two simple random samples. In some designs (e.g., ANCOVA designs, randomized block designs, repeated measures designs), the treatment effect estimate will be much more precise than this; in other designs (e.g., cluster-randomized trials), it will be less precise.\nHere’s what I think is a more useful way to think about the sampling variance of \\(d\\). Let’s suppose that we have an unbiased estimator for the difference in means that goes into the numerator of the SMD. Call this estimator \\(b\\), its sampling variance \\(\\text{Var}(b)\\), and its standard error \\(se_{b}\\). Also suppose that we have an unbiased (or reasonably close-to-unbiased) estimator of the population variance of the outcome, the square root of which goes into the denominator of the SMD. Call this estimator \\(S^2\\), with expectation \\(\\text{E}\\left(S^2\\right) = \\sigma^2\\) and sampling variance \\(\\text{Var}(S^2)\\). Finally, suppose that \\(b\\) and \\(S^2\\) are independent (which will often be a pretty reasonable assumption). A delta-method approximation for the sampling variance of \\(d = b / S\\) is then\n\\[\r\\text{Var}\\left(d\\right) \\approx \\frac{\\text{Var}(b)}{\\sigma^2} + \\frac{\\delta^2}{2 \\nu},\r\\]\nwhere \\(\\nu = 2 \\left[\\text{E}\\left(S^2\\right)\\right]^2 / \\text{Var}\\left(S^2\\right)\\). Plugging in sample estimates of the relevant parameters provides a reasonable estimator for the sampling variance of \\(d\\):\n\\[\rV_d = \\left(\\frac{se_b}{S}\\right)^2 + \\frac{d^2}{2 \\nu}.\r\\]\nThis estimator has two parts. The first part involves \\(se_b / S\\), which is just the standard error of \\(b\\), but re-scaled into standard deviation units; this part captures the variability in \\(d\\) from its numerator. This scaled standard error can be calculated directly if an article reports \\(se_b\\).\nThe second part of \\(V_d\\) is \\(d^2 / (2 \\nu)\\), which captures the variability in \\(d\\) due to its denominator. More precise estimates of \\(\\sigma\\) will have larger degrees of freedom, so that the second part will be smaller. For some designs, the degrees of freedom \\(\\nu\\) depend only on sample sizes, and thus can be calculated exactly. For some other designs, \\(\\nu\\) must be estimated.\nThe same degrees of freedom can also be used in the small-sample correction for the bias of \\(d\\), as given by\n\\[\rg = J(\\nu) \\times d.\r\\]\nThis small-sample correction is based on a Satterthwaite-type approximation to the distribution of \\(d\\).\nHere’s another way to express the variance estimator for \\(d\\):\n\\[\rV_d = d^2 \\left(\\frac{1}{t^2} + \\frac{1}{2 \\nu}\\right),\r\\]\nwhere \\(t\\) is the test statistic corresponding to the hypothesis test for no difference between groups. I’ve never seen that formula in print before, but it could be convenient if an article reports the \\(t\\) statistic (or \\(F = t^2\\) statistic).\n\rNon-standard estimators of \\(d\\)\rThe advantage of this formulation of \\(d\\), \\(g\\), and \\(V_d\\) is that it can be applied in quite a wide variety of circumstances, including cases that aren’t usually covered in textbook treatments. Rather than having to use separate formulas for every combination of design and analytic approach under the sun, the same formulas apply throughout. What changes are the components of the formulas: the scaled standard error \\(se_b / S\\) and the degrees of freedom \\(\\nu\\). The general formulation also makes it easier to swap in different estimates of \\(b\\) or \\(S\\)—i.e., if you estimate the numerator a different way but keep the denominator the same, you’ll need a new scaled standard error but can still use the same degrees of freedom. A bunch of examples:\nIndependent groups with different variances\rSuppose that we’re looking at two independent groups but do not want to assume that their variances are the same. In this case, it would make sense to standardize the difference in means by the control group standard deviation (without pooling), so that \\(d = \\left(\\bar{y}_T - \\bar{y}_C\\right) / s_C\\). Since \\(s_C^2\\) has \\(\\nu = n_C - 1\\) degrees of freedom, the small-sample bias correction will then need to be \\(J(n_C - 1)\\). The scaled standard error will be\n\\[\r\\frac{se_b}{s_C} = \\sqrt{\\frac{s_T^2}{s_C^2 n_T} + \\frac{1}{n_C}}.\r\\]\nThis is then everything that we need to calculate \\(V_d\\), \\(g\\), \\(V_g\\), etc.\n\rMultiple independent groups\rSuppose that the study involves \\(K - 1\\) treatment groups, 1 control group, and \\(N\\) total participants. If the meta-analysis will include SMDs comparing each treatment group to the control group, it would make sense to pool the sample variance across all \\(K\\) groups rather than just the pair of groups, so that a common estimate of scale is used across all the effect sizes. The pooled standard deviation is then calculated as\n\\[\rs_p^2 = \\frac{1}{N - K} \\sum_{k=0}^K (n_k - 1) s_k^2.\r\\]\nFor a comparison between treatment group \\(k\\) and the control group, we would then use\n\\[\rd = \\frac{\\bar{y}_k - \\bar{y}_C}{s_p}, \\qquad \\nu = N - K, \\qquad \\frac{se_b}{s_p} = \\sqrt{\\frac{1}{n_C} + \\frac{1}{n_k}},\r\\]\nwhere \\(n_k\\) is the sample size for treatment group \\(k\\) (cf. Gleser \u0026amp; Olkin, 2009).\n\rSingle group, pre-test post-test design\rSuppose that a study involves taking pre-test and post-test measurements on a single group of \\(n\\) participants. Borenstein (2009) recommends calculating the standardized mean difference for this study as the difference in means between the post-test and pre-test, scaled by the pooled (across pre- and post-test measurements) standard deviation. With obvious notation:\n\\[\rd = \\frac{\\bar{y}_{post} - \\bar{y}_{pre}}{s_p}, \\qquad \\text{where} \\qquad s_p^2 = \\frac{1}{2}\\left(s_{pre}^2 + s_{post}^2\\right).\r\\]\nIn this design,\n\\[\r\\frac{se_b}{s_p} = \\frac{2(1 - r)}{n},\r\\]\nwhere \\(r\\) is the sample correlation between the pre- and post-tests. The remaining question is what to use for \\(\\nu\\). Borenstein (2009) uses \\(\\nu = n - 1\\). My previous post on the sampling covariance of sample variances gave the result that \\(\\text{Var}(s_p^2) = \\sigma^4 (1 + \\rho^2) / (n - 1)\\), which would instead suggest using\n\\[\r\\nu = \\frac{2 (n - 1)}{1 + r^2}. \\]\nThis formula will tend to give slightly larger degrees of freedom, but probably won’t be that discrepant from Borenstein’s approach except in quite small samples. It would be interesting to investigate which approach is better in small samples (i.e., leading to less biased estimates of the SMD and more accurate estimates of sampling variance, and by how much), although its possible than neither is all that good because the variance estimator itself is based on a large-sample approximation.\n\rTwo group, pre-test post-test design: ANCOVA estimation\rSuppose that a study involves taking pre-test and post-test measurements on two groups of participants, with sample sizes \\(n_T\\) and \\(n_C\\) respectively. One way to analyze this design is via ANCOVA using the pre-test measure as the covariate, so that the treatment effect estimate is the difference in adjusted post-test means. In this design, the scaled standard error will be approximately\n\\[\r\\frac{se_b}{S} = \\frac{(n_C + n_T)(1 - r^2)}{n_C n_T},\r\\]\nwhere \\(r\\) is the pooled, within-group sample correlation between the pre-test and the post-test measures (this approximation assumes that the pre-test SMD between groups is relatively small). Alternately, if \\(se_b\\) is provided then the scaled standard error could be calculated directly.\nBorenstein (2009) suggests calculating \\(d\\) as the difference in adjusted means, scaled by the pooled sample variances on the post-test measures. The post-test pooled sample variance will have the same degrees of freedom as in the two-sample t-test case: \\(\\nu = n_C + n_T - 2\\). (Borenstein instead uses \\(\\nu = n_C + n_T - 2 - q\\), where \\(q\\) is the number of covariates in the analysis, but this won’t usually make much difference unless the total sample size is quite small.)\nScaling by the pooled post-test sample variance isn’t the only reasonable way to estimate the SMD though. If the covariate is a true pre-test, then why not scale by the pooled pre-test sample variance instead? To do so, you would need to calculate \\(se_b / S\\) directly and use \\(\\nu = n_C + n_T - 2\\). If it is reasonable to assume that the pre- and post-test population variances are equal, then another alternative would be to pool across the pre-test and post-test sample variances in each group. Using this approach, you would again need to calculate \\(se_b / S\\) directly and then use \\(\\nu = 2(n_C + n_T - 2) / (1 + r^2)\\).\n\rTwo group, pre-test post-test design: repeated measures estimation\rAnother way to analyze the data from the same type of study design is to use repeated measures ANOVA. I’ve recently encountered a number of studies that use this approach (here’s a recent example from a highly publicized study in PLOS ONE—see Table 2). The studies I’ve seen typically report the sample means and variances in each group and at each time point, from which the difference in change scores can be calculated. Let \\(\\bar{y}_{gt}\\) and \\(s_{gt}^2\\) denote the sample mean and sample variance in group \\(g = T, C\\) at time \\(t = 0, 1\\). The numerator of \\(d\\) would then be calculated as\n\\[\rb = \\left(\\bar{y}_{T1} - \\bar{y}_{T0}\\right) - \\left(\\bar{y}_{C1} - \\bar{y}_{C0}\\right),\r\\]\nwhich has sampling variance \\(\\text{Var}(b) = 2(1 - \\rho)\\sigma^2\\left(n_C + n_T \\right) / (n_C n_T)\\), where \\(\\rho\\) is the correlation between the pre-test and the post-test measures. Thus, the scaled standard error is\n\\[\r\\frac{se_b}{S} = \\frac{2(1 - r)(n_C + n_T)}{n_C n_T}.\r\\]\nAs with ANCOVA, there are several potential options for calculating the denominator of \\(d\\):\n\rUsing the pooled sample variances on the post-test measures, with \\(\\nu = n_C + n_T - 2\\);\n\rUsing the pooled sample variances on the pre-test measures, with \\(\\nu = n_C + n_T - 2\\); or\n\rUsing the pooled sample variances at both time points and in both groups, i.e.,\n\\[\rS^2 = \\frac{(n_C - 1)(s_{C0}^2 + s_{C1}^2) + (n_T - 1)(s_{T0}^2 + s_{T1}^2)}{2(n_C + n_T - 2)},\r\\]\nwith \\(\\nu = 2(n_C + n_T - 2) / (1 + r^2)\\).\n\r\rThe range of approaches to scaling is the same as for ANCOVA. This makes sense because both analyses are based on data from the same study design, so the parameter of interest should be the same (i.e., the target parameter should not change based on the analytic method). Note that all of these approaches are a bit different than the effect size estimator proposed by Morris and DeShon (2002) for the two-group, pre-post design; their approach does not fit into my framework because it involves taking a difference between standardized effect sizes (and therefore involves two separate estimates of scale, rather than just one).\n\rRandomized trial with longitudinal follow-up\rMany independent-groups designs—especially randomized trials in field settings—involve repeated, longitudinal follow-up assessments. An increasingly common approach to analysis of such data is through hierarchical linear models, which can be used to account for the dependence structure among measurements taken on the same individual. In this setting, Feingold (2009) proposes that the SMD be calculated as the model-based estimate of the treatment effect at the final follow-up time, scaled by the within-groups variance of the outcome at that time point. Let \\(\\hat\\beta_1\\) denote the estimated difference in slopes (change per unit time) between groups in a linear growth model, \\(F\\) denote the duration of the study, and \\(s_{pF}^2\\) denote the pooled sample variance of the outcome at the final time point. For this model, Feingold (2009) proposes to calculate the standardized mean difference as\n\\[\rd = \\frac{F \\hat\\beta_1}{s_{pF}}.\r\\]\nIn a later paper, Feingold (2015) proposes that the sampling variance of \\(d\\) be estimated as \\(F \\times se_{\\hat\\beta_1} / s_{pF}\\), where \\(se_{\\hat\\beta_1}\\) is the standard error of the estimated slope. My framework suggests that a better estimate of the sampling variance, which accounts for the uncertainty of the scale estimate, would be to use\n\\[\rV_d = \\left(\\frac{F \\times se_{\\hat\\beta_1}}{s_{pF}}\\right)^2 + \\frac{d^2}{2 \\nu},\r\\]\nwith \\(\\nu = n_T + n_C - 2\\). The same \\(\\nu\\) could be used to bias-correct the effect size estimate.\nIf estimates of the variance components of the HLM are reported, one could use them to construct a model-based estimate of the scale parameter in the denominator of \\(d\\). I explored this approach in a paper that uses HLM to model single-case designs, which are a certain type of longitudinal experiment that typically involve a very small number of participants (Pustejovsky, Hedges, \u0026amp; Shadish, 2014). Estimates of the scale parameter can usually be written as\n\\[\rS_{model}^2 = \\mathbf{r}\u0026#39;\\boldsymbol\\omega,\r\\]\nwhere \\(\\boldsymbol\\omega\\) is a vector of all the variance components in the model and \\(\\mathbf{r}\\) is a vector of weights that depend on the model specification and length of follow-up. This estimate of scale will usually be more precise than \\(s_{pF}^2\\) because it makes use of all of the data (and modeling assumptions). However, it can be challenging to determine appropriate degrees of freedom for \\(S_{model}^2\\). For single-case designs, I used estimates of \\(\\text{Var}(\\boldsymbol\\omega)\\) based on the inverse of the expected information matrix—call the estimate \\(\\mathbf{V}_{\\boldsymbol\\omega}\\)—in which case\n\\[\r\\nu = \\frac{2 S_{model}^4}{\\mathbf{r}\u0026#39; \\mathbf{V}_{\\boldsymbol\\omega} \\mathbf{r}}.\r\\]\nHowever, most published articles will not provide estimates of the sampling variances of the variance components—in fact, a lot of software for estimating HLMs does not even provide these. It would be useful to work out some reasonable approximations for the degrees of freedom in these models—approximations that can be calculated based on the information that’s typically available—and to investigate the extent to which there’s any practical benefit to using \\(S_{model}^2\\) over \\(s_{pF}^2\\).\n\rCluster-randomized trials\rHedges (2007) addresses estimation of standardized mean differences for cluster-randomized trials, in which the units of measurement are nested within higher-level clusters that comprise the units of randomization. Such designs involve two variance components (within- and between-cluster variance), and thus there are three potential approaches to scaling the treatment effect: standardize by the total variance (i.e., the sum of the within- and between-cluster components), standardize by the within-cluster variance, or standardize by the between-cluster variance. Furthermore, some of the effect sizes can be estimated in several different ways, each with a different sampling variance. Hedges (2007) gives sampling variance estimates for each estimator of each effect size, but they all follow the same general formula as given above. (The appendix of the article actually gives the same formula as above, but using a more abstract formulation.)\nFor example, suppose the target SMD parameter uses the total variance and that we have data from a two-level, two-arm cluster randomized trial with \\(M\\) clusters, \\(n\\) observations per cluster, and total sample sizes in each arm of \\(N_T\\) and \\(N_C\\), respectively. Let \\(\\tau^2\\) be the between-cluster variance, \\(\\sigma^2\\) be the within-cluster variance, and \\(\\rho = \\tau^2 / (\\tau^2 + \\sigma^2)\\). The target parameter is \\(\\delta = \\left(\\mu_T - \\mu_C\\right) / \\left(\\tau^2 + \\sigma^2\\right)\\). The article assumes that the treatment effect will be estimated by the difference in grand means, \\(\\bar{\\bar{y}}_T - \\bar{\\bar{y}}_C\\). Letting \\(S_B^2\\) be the pooled sample variance of the cluster means within each arm and \\(S_W^2\\) be the pooled within-cluster sample variance, the total variance is estimated as\n\\[\rS_{total}^2 = S_B^2 + \\frac{n - 1}{n} S_W^2. \\]\nAn estimate of the SMD is then\n\\[\rd = \\left(\\bar{\\bar{y}}_T - \\bar{\\bar{y}}_C \\right) / \\sqrt{S_{total}^2}. \\]\nThe scaled standard error of \\(\\bar{\\bar{y}}_T - \\bar{\\bar{y}}_C\\) is\n\\[\rse_b = \\left(\\frac{N_C + N_T}{N_C N_T}\\right)\\left[1 + (n - 1)\\rho\\right].\r\\]\nThe appendix of the article demonstrates that \\(\\text{E}\\left(S_{total}^2\\right) = \\tau^2 + \\sigma^2\\) and\n\\[\r\\text{Var}\\left( S_{total}^2 \\right) = \\frac{2}{n^2}\\left(\\frac{(n \\tau^2 + \\sigma^2)^2}{M - 2} + \\frac{(n - 1)^2 \\sigma^4}{N_C + N_T - M}\\right),\r\\]\nby which it follows that\n\\[\r\\nu = \\frac{n^2 M (M - 2)}{M[(n - 1)\\rho + 1]^2 + (M - 2)(n - 1)(1 - \\rho)^2}.\r\\]\nSubstituting \\(se_b / S_{total}\\) and \\(\\nu\\) into the formula for \\(V_d\\) gives the same as Expression (14) in the article.\nA limitation of Hedges (2007) is that it only covers the case where the treatment effect is estimated by the difference in grand means (although it does cover the case of unequal cluster sizes, which gets quite messy). In practice, every cluster-randomized trial I’ve ever seen uses baseline covariates to adjust the mean difference (often based on a hierarchical linear model) and improve the precision of the treatment effect estimate. The SMD estimate should also be based on this covariate-adjustment estimate, scaled by the total variance without adjusting for the covariate. An advantage of the general formulation given above is that its clear how to estimate the sampling variance of \\(d\\). I would guess that it will often be possible to calculate the scaled standard error directly, given the standard error of the covariate-adjusted treatment effect estimate. And since \\(S_{total}\\) would be estimated just as before, its degrees of freedom remain the same.\nHedges (2011) discusses estimation of SMDs in three-level cluster-randomized trials—an even more complicated case. However, the general approach is the same; all that’s needed are the scaled standard error and the degrees of freedom \\(\\nu\\) of whatever combination of variance components go into the denominator of the effect size. In both the two-level and three-level cases, the degrees of freedom get quite complicated in unbalanced samples and are probably not calculable from the information that is usually provided in an article. Hedges (2007, 2011) comments on a couple of cases where more tractable approximations can be used, although it seems like there might be room for further investigation here.\n\r\rClosing thoughts\rI think this framework is useful in that it unifies a large number of cases that have been treated separately, and can also be applied (more-or-less immediately) to \\(d\\) estimators that haven’t been widely considered before, such as the \\(d\\) that involves scaling by the pooled pre-and-post, treatment-and-control sample variance. I hope it also illustrates that, while the point estimator \\(d\\) can be applied across a large number of study designs, the sampling variance of \\(d\\) depends on the details of the design and estimation methods. The same is true for other families of effect sizes as well. For example, in other work I’ve demonstrated that the sampling variance of the correlation coefficient depends on the design from which the correlations are estimated (Pustejovsky, 2014).\nIf you have read this far, I’d love to get your feedback about whether you think this is a useful way to organize the calculations of \\(d\\) estimators. Is this helpful? Or nothing you didn’t already know? Or still more complicated than it should be? Leave a comment!\n\rReferences\rBorenstein, M. (2009). Effect sizes for continuous data. In H. M. Cooper, L. V Hedges, \u0026amp; J. C. Valentine (Eds.), The Handbook of Research Synthesis and Meta-Analysis (pp. 221–236). New York, NY: Russell Sage Foundation.\nFeingold, A. (2009). Effect sizes for growth-modeling analysis for controlled clinical trials in the same metric as for classical analysis. Psychological Methods, 14(1), 43–53. doi:10.1037/a0014699\nFeingold, A. (2015). Confidence interval estimation for standardized effect sizes in multilevel and latent growth modeling. Journal of Consulting and Clinical Psychology, 83(1), 157–168. doi:10.1037/a0037721\nGleser, L. J., \u0026amp; Olkin, I. (2009). Stochastically dependent effect sizes. In H. Cooper, L. V. Hedges, \u0026amp; J. C. Valentine (Eds.), The Handbook of Research Synthesis and Meta-Analysis (2nd ed., pp. 357–376). New York, NY: Russell Sage Foundation.\nHedges, L. V. (2007). Effect sizes in cluster-randomized designs. Journal of Educational and Behavioral Statistics, 32(4), 341–370. doi:10.3102/1076998606298043\nHedges, L. V. (2011). Effect sizes in three-level cluster-randomized experiments. Journal of Educational and Behavioral Statistics, 36(3), 346–380. doi:10.3102/1076998610376617\nMorris, S. B., \u0026amp; DeShon, R. P. (2002). Combining effect size estimates in meta-analysis with repeated measures and independent-groups designs. Psychological Methods, 7(1), 105–125. doi:10.1037//1082-989X.7.1.105\nPustejovsky, J. E. (2014). Converting from d to r to z when the design uses extreme groups, dichotomization, or experimental control. Psychological Methods, 19(1), 92–112. doi:10.1037/a0033788\nPustejovsky, J. E., Hedges, L. V, \u0026amp; Shadish, W. R. (2014). Design-comparable effect sizes in multiple baseline designs: A general modeling framework. Journal of Educational and Behavioral Statistics, 39(5), 368–393. doi:10.3102/1076998614547577\nViechtbauer, W. (2007). Approximate confidence intervals for standardized effect sizes in the two-independent and two-dependent samples design. Journal of Educational and Behavioral Statistics, 32(1), 39–60. doi:10.3102/1076998606298034\n\r","date":1464912000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1464912000,"objectID":"bc5842d9f4798a30a23d170d6247c1c4","permalink":"/alternative-formulas-for-the-smd/","publishdate":"2016-06-03T00:00:00Z","relpermalink":"/alternative-formulas-for-the-smd/","section":"post","summary":"The standardized mean difference (SMD) is surely one of the best known and most widely used effect size metrics used in meta-analysis. In generic terms, the SMD parameter is defined as the difference in population means between two groups (often this difference represents the effect of some intervention), scaled by the population standard deviation of the outcome metric.","tags":["meta-analysis","effect size","distribution theory","standardized mean difference"],"title":"Alternative formulas for the standardized mean difference","type":"post"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rHadley Wickham’s dplyr and tidyr packages completely changed the way I do data manipulation/munging in R. These packages make it possible to write shorter, faster, more legible, easier-to-intepret code to accomplish the sorts of manipulations that you have to do with practically any real-world data analysis. The legibility and interpretability benefits come from\n\rusing functions that are simple verbs that do exactly what they say (e.g., filter, summarize, group_by) and\rchaining multiple operations together, through the pipe operator %\u0026gt;% from the magrittr package.\r\rChaining is particularly nice because it makes the code read like a story. For example, here’s the code to calculate sample means for the baseline covariates in a little experimental dataset I’ve been working with recently:\nlibrary(dplyr)\rdat \u0026lt;- read.csv(\u0026quot;http://jepusto.com/data/Mineo_2009_data.csv\u0026quot;)\rdat %\u0026gt;%\rgroup_by(Condition) %\u0026gt;%\rselect(Age, starts_with(\u0026quot;Baseline\u0026quot;)) %\u0026gt;%\rsummarise_each(funs(mean)) -\u0026gt;\rbaseline_means\r## Warning: funs() is soft deprecated as of dplyr 0.8.0\r## Please use a list of either functions or lambdas: ## ## # Simple named list: ## list(mean = mean, median = median)\r## ## # Auto named with `tibble::lst()`: ## tibble::lst(mean, median)\r## ## # Using lambdas\r## list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\r## This warning is displayed once per session.\rEach line of the code is a different action: first group the data by Condition, then select the relevant variables, then summarise each of the variables with its sample mean in each group. The results are stored in a dataset called baseline_means.\nAs I’ve gotten familiar with dplyr, I’ve adopted the style of using the backwards assignment operator (-\u0026gt;) to store the results of a chain of manipulations. This is perhaps a little bit odd—in all the rest of my code I stick with the forward assignment operator (\u0026lt;-) with the object name on the left—but the alternative is to break the “flow” of the story, effectively putting the punchline before the end of the joke. Consider:\nbaseline_means \u0026lt;- dat %\u0026gt;%\rgroup_by(Condition) %\u0026gt;%\rselect(Age, starts_with(\u0026quot;Baseline\u0026quot;)) %\u0026gt;%\rsummarise_each(funs(mean))\r## Adding missing grouping variables: `Condition`\rThat’s just confusing to me. So backward assignment operator it is.\nAssigning as a verb\rMy only problem with this convention is that, with complicated chains of manipulations, I often find that I need to tweak the order of the verbs in the chain. For example, I might want to summarize all of the variables, and only then select which ones to store:\ndat %\u0026gt;%\rgroup_by(Condition) %\u0026gt;%\rsummarise_each(funs(mean)) %\u0026gt;%\rselect(Age, starts_with(\u0026quot;Baseline\u0026quot;)) -\u0026gt;\rbaseline_means\r## Warning in mean.default(Expressive.Language): argument is not numeric or\r## logical: returning NA\r## Warning in mean.default(Expressive.Language): argument is not numeric or\r## logical: returning NA\r## Warning in mean.default(Expressive.Language): argument is not numeric or\r## logical: returning NA\rIn revising the code, it’s necessary to change the symbols at the end of the second and third steps, which is a minor hassle. It’s possible to do it by very carefully cutting-and-pasting the end of the second step through everything but the -\u0026gt; after the third step, but that’s a delicate operation, prone to error if you’re programming after hours or after beer. Wouldn’t it be nice if every step in the chain ended with %\u0026gt;% so that you could move around whole lines of code without worrying about the bit at the end?\nHere’s one crude way to end each link in the chain with a pipe:\ndat %\u0026gt;%\rgroup_by(Condition) %\u0026gt;%\rselect(Age, starts_with(\u0026quot;Baseline\u0026quot;)) %\u0026gt;%\rsummarise_each(funs(mean)) %\u0026gt;%\ridentity() -\u0026gt; baseline_means\r## Adding missing grouping variables: `Condition`\rBut this is still pretty ugly—it’s got an extra function call that’s not a verb, and the name of the resulting object is tucked away in the middle of a line. What I need is a verb to take the results of a chain of operations and assign to an object. Base R has a suitable candidate here: the assign function. How about the following?\ndat %\u0026gt;%\rgroup_by(Condition) %\u0026gt;%\rselect(Age, starts_with(\u0026quot;Baseline\u0026quot;)) %\u0026gt;%\rsummarise_each(funs(mean)) %\u0026gt;%\rassign(\u0026quot;baseline_means_new\u0026quot;, .)\r## Adding missing grouping variables: `Condition`\rexists(\u0026quot;baseline_means_new\u0026quot;)\r## [1] FALSE\rThis doesn’t work because of some subtlety with the environment into which baseline_means_new is assigned. A brute-force fix would be to specify that the assign should be into the global environment. This will probably work 90%+ of the time, but it’s still not terribly elegant.\nHere’s a function that searches the call stack to find the most recent invocation of itself that does not involve non-standard evaluation, then assigns to its parent environment:\nput \u0026lt;- function(x, name, where = NULL) {\rif (is.null(where)) {\rsys_calls \u0026lt;- sys.calls()\rput_calls \u0026lt;- grepl(\u0026quot;\\\\\u0026lt;put\\\\(\u0026quot;, sys_calls) \u0026amp; !grepl(\u0026quot;\\\\\u0026lt;put\\\\(\\\\.\u0026quot;,sys_calls)\rwhere \u0026lt;- sys.frame(max(which(put_calls)) - 1)\r}\rassign(name, value = x, pos = where)\r}\rHere are my quick tests that this function is assigning to the right environment:\nput(dat, \u0026quot;dat1\u0026quot;)\rdat %\u0026gt;% put(\u0026quot;dat2\u0026quot;)\rf \u0026lt;- function(dat, name) {\rput(dat, \u0026quot;dat3\u0026quot;)\rdat %\u0026gt;% put(\u0026quot;dat4\u0026quot;)\rput(dat, name)\rc(exists(\u0026quot;dat3\u0026quot;), exists(\u0026quot;dat4\u0026quot;), exists(name))\r}\rf(dat,\u0026quot;dat5\u0026quot;)\r## [1] TRUE TRUE TRUE\rgrep(\u0026quot;dat\u0026quot;,ls(), value = TRUE)\r## [1] \u0026quot;dat\u0026quot; \u0026quot;dat1\u0026quot; \u0026quot;dat2\u0026quot;\rThis appears to work even if you’ve got multiple nested calls to put:\nput(f(dat, \u0026quot;dat6\u0026quot;), \u0026quot;dat7\u0026quot;)\rgrep(\u0026quot;dat\u0026quot;,ls(), value = TRUE)\r## [1] \u0026quot;dat\u0026quot; \u0026quot;dat1\u0026quot; \u0026quot;dat2\u0026quot; \u0026quot;dat7\u0026quot;\rdat7\r## [1] TRUE TRUE TRUE\rf(dat, \u0026quot;dat8\u0026quot;) %\u0026gt;% put(\u0026quot;dat9\u0026quot;)\rgrep(\u0026quot;dat\u0026quot;,ls(), value = TRUE)\r## [1] \u0026quot;dat\u0026quot; \u0026quot;dat1\u0026quot; \u0026quot;dat2\u0026quot; \u0026quot;dat7\u0026quot; \u0026quot;dat9\u0026quot;\rdat9\r## [1] TRUE TRUE TRUE\r\rIt works! (I think…)\rTo be consistent with the style of dplyr, let me also tweak the function to allow name to be the unquoted object name:\nput \u0026lt;- function(x, name, where = NULL) {\rname_string \u0026lt;- deparse(substitute(name))\rif (is.null(where)) {\rsys_calls \u0026lt;- sys.calls()\rput_calls \u0026lt;- grepl(\u0026quot;\\\\\u0026lt;put\\\\(\u0026quot;, sys_calls) \u0026amp; !grepl(\u0026quot;\\\\\u0026lt;put\\\\(\\\\.\u0026quot;,sys_calls)\rwhere \u0026lt;- sys.frame(max(which(put_calls)) - 1)\r}\rassign(name_string, value = x, pos = where)\r}\rReturning to my original chain of manipulations, here’s how it looks with the new function:\ndat %\u0026gt;%\rgroup_by(Condition) %\u0026gt;%\rselect(Age, starts_with(\u0026quot;Baseline\u0026quot;)) %\u0026gt;%\rsummarise_each(funs(mean)) %\u0026gt;%\rput(baseline_means_new)\r## Adding missing grouping variables: `Condition`\rprint(baseline_means_new)\r## # A tibble: 3 x 4\r## Condition Age Baseline.Gaze Baseline.Vocalizations\r## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 OtherVR 122. 91.9 2.86\r## 2 SelfVid 121. 102. 1.86\r## 3 SelfVR 139. 95.5 1.43\rIf you’ve been following along, let me know what you think of this. Is it a good idea, or is it dangerous? Are there cases where this will break? Can you think of a better name?\n\r","date":1463097600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1463097600,"objectID":"7ae6e1a6cef30553286c55e36ef89451","permalink":"/assigning-after-dplyr/","publishdate":"2016-05-13T00:00:00Z","relpermalink":"/assigning-after-dplyr/","section":"post","summary":"Hadley Wickham’s dplyr and tidyr packages completely changed the way I do data manipulation/munging in R. These packages make it possible to write shorter, faster, more legible, easier-to-intepret code to accomplish the sorts of manipulations that you have to do with practically any real-world data analysis.","tags":["Rstats","programming"],"title":"Assigning after dplyr","type":"post"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rA colleague asked me the other day:\n\rI wonder if you have any suggestions for what to do if random assignment results in big group differences on the pre-test scores of the main outcome measure? My default is just to shrug, use the pretest scores as a covariate and interpret with caution, but if there’s a suggestion you have I’d be most grateful for being pointed in the right direction. These are paid participants (otherwise I’d ask the student to collect more data), 25 and 28 in each group, randomization done by Qualtrics survey program, pretest differences are pronounced (p = .002) and NOT attributable to outliers.\n\rI would guess that many statisticians probably get a question along these lines on a pretty regular basis. So as not to repeat myself in the future, I’m posting my response here.\nThese sorts of things happen just by dumb luck sometimes, and the possibility of unlucky randomizations like this are one of the primary reasons to collect pre-test data and use it in the analysis. My main advice would therefore be to do just as you’ve described: control for the covariate just as you would otherwise. There are certainly other analyses you could run (such as using propensity scores to re-balance the data), but whatever advantages they offer might well be offset by the cost of 1) deviating from your initial protocol and 2) having to explain a less familiar and more complicated analysis.\nIf I were analyzing these data, I would do the following:\nCheck that the randomization software was actually working correctly, and that the unbalanced data wasn’t the result of a glitch in Qualtrics or something like that.\rLook at histograms of the pretest scores for each group to get a sense of how big the difference in the distributions is.\rIf there are other baseline variables, check to see whether there are big group differences on any of those as well.\rEnsure that the write-up characterizes the magnitude of observed differences on the pre-test and any other baseline variables (i.e., report an effect size like the standardized mean difference, in addition to the p-value).\rLarger baseline differences tend to make the results more sensitive to how the data are analyzed. As a result, I would be extra thorough in checking the required assumptions for an analysis of covariance—especially linearity and homogeneity of slopes—and would examine whether the treatment effect estimates are sensitive to including a pretest-by-treatment interaction.\rFor future studies, investigate whether it would be possible to block-randomize (e.g., block by low/middle/high scores on the pretest) in order to insure against the possibility of getting big baseline differences.\r\r","date":1462924800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1462924800,"objectID":"07dff959233ca471dd453a9c71921347","permalink":"/unlucky-randomization/","publishdate":"2016-05-11T00:00:00Z","relpermalink":"/unlucky-randomization/","section":"post","summary":"A colleague asked me the other day:\n\rI wonder if you have any suggestions for what to do if random assignment results in big group differences on the pre-test scores of the main outcome measure?","tags":["experimental design"],"title":"Unlucky randomization","type":"post"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461733200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461733200,"objectID":"80be3c9fcc86014efab0cec0f14957f6","permalink":"/project/deep-learning/","publishdate":"2016-04-27T00:00:00-05:00","relpermalink":"/project/deep-learning/","section":"project","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit.","tags":null,"title":"Deep Learning","type":"project"},{"authors":null,"categories":null,"content":"","date":1461733200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461733200,"objectID":"553a94c5dfd3b8b099d8a12b2d248093","permalink":"/project/example-external-project/","publishdate":"2016-04-27T00:00:00-05:00","relpermalink":"/project/example-external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":null,"title":"External Project","type":"project"},{"authors":["James E. Pustejovsky"],"categories":null,"content":"An R package for simulating different methods of recording data based on direct observation of behavior, where behavior is modeled by an alternating renewal process.\n \rAvailable on the Comprehensive R Archive Network \rInstallation instructions \rSource code on Github \rARPsimulator: An interactive web application for simulating systematic direct observation data based on the alternating renewal process model.  ","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"29789d3ac21b78a4822fb44b82dc2b64","permalink":"/software/arpobservation/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/software/arpobservation/","section":"software","summary":"Simulate systematic direct observation data","tags":["behavioral observation","alternating renewal process","simulation","R"],"title":"ARPobservation","type":"software"},{"authors":["James E. Pustejovsky"],"categories":null,"content":"R and Stata packages for calculating cluster-robust variance estimators (i.e., sandwich estimators) with small-sample corrections, including the bias-reduced linearization estimator of Bell and McCaffrey (2002) and extensions proposed in Tipton (2015), Tipton and Pustejovsky (2015), and Pustejovsky and Tipton (2016).\n R package available on the Comprehensive R Archive Network \rR source code on Github Stata package available on the SSC Archive \rStata source code on Github  ","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"64adaa81beacd4ff8dd05d2454fee784","permalink":"/software/clubsandwich/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/software/clubsandwich/","section":"software","summary":"Cluster-robust variance estimation","tags":["sandwiches","robust variance estimation","Rstats","Stata"],"title":"clubSandwich","type":"software"},{"authors":["James E. Pustejovsky"],"categories":null,"content":"An R package implementing several methods of estimating a design-comparable standardized mean difference effect size based on data from a single-case design. Methods include those from Hedges, Pustejovsky, \u0026amp; Shadish (\r2012, 2013) and Pustejovsky, Hedges, \u0026amp; Shadish (2014).\n \rAvailable on the Comprehensive R Archive Network \rInstallation instructions \rSource code on Github \rscdhlm: An interactive web application for calculating design-comparable standardized mean difference effect sizes.  ","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"9e24674d6842b82c3de1619a104d333f","permalink":"/software/scdhlm/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/software/scdhlm/","section":"software","summary":"Between-case SMD for single-case designs","tags":["design-comparable SMD","effect size","hierarchical models","single-case design","Rstats"],"title":"scdhlm","type":"software"},{"authors":["James E. Pustejovsky","Daniel M. Swan"],"categories":null,"content":"An R package for calculating basic effect size indices for single-case designs, including several non-overlap measures and parametric effect size measures, and for estimating the gradual effects model developed by Swan and Pustejovsky (2017).\n \rAvailable on the Comprehensive R Archive Network \rSource code and installation instructions on Github \rSingle case effect size calculator: An interactive web application for calculating basic effect size indices. \rGradual Effect Model calculator: An interactive web application for estimating effect sizes using the gradual effects model.  ","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"92bdaef62e1b9651396742e706aa23b8","permalink":"/software/singlecasees/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/software/singlecasees/","section":"software","summary":"Single-case design effect size calculator","tags":["effect size","response ratio","non-overlap measures","single-case design","Rstats"],"title":"SingleCaseES","type":"software"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rA colleague and her students asked me the other day whether I knew of a citation that gives the covariance between the sample variances of two outcomes from a common sample. This sort of question comes up in meta-analysis problems occasionally. I didn’t know of a convenient reference that directly answers the question, but I was able to suggest some references that would help (listed below). While the students work on deriving it, I’ll provide the answer here so that they can check their work.\nSuppose that we have a sample of \\(n\\) observations \\(\\mathbf{y}_1,...,\\mathbf{y}_n\\) from a \\(p\\)-dimensional multivariate normal distribution with mean \\(\\boldsymbol\\mu\\) and covariance \\(\\boldsymbol\\Sigma = \\left[\\sigma_{jk}\\right]_{j,k=1,...,p}\\). Let \\(\\mathbf{\\bar{y}}\\) denote the (multivariate) sample mean, with entries \\(\\bar{y}_1,...,\\bar{y}_p\\). Let \\(\\mathbf{S}\\) denote the sample covariance matrix, with entries \\(\\left[s_{jk}\\right]_{j,k=1,...,p}\\) where\n\\[\rs_{jk} = \\frac{1}{n - 1}\\sum_{i=1}^n (y_{ij} - \\bar{y}_j)(y_{ik} - \\bar{y}_k).\r\\]\nThen \\((n - 1)\\mathbf{S}\\) follows a Wishart distribution with \\(n - 1\\) degrees of freedom and scale matrix \\(\\boldsymbol\\Sigma\\) (Searle, 2006, p. 352; Muirhead, 1982, p. 86; or any textbook on multivariate analysis).\nThe sampling covariance between two sample covariances, say \\(s_{jk}\\) and \\(s_{lm}\\), can then be derived from the properties of the Wishart distribution. Expressions for this are available in Searle (2006) or Muirhead (1982). The former is a bit hard to parse because it uses the \\(\\text{vec}\\) and Kronecker product operators; Muirhead (1982, p. 90) gives the following simple expression:\n\\[\r\\text{Cov}\\left(s_{jk}, s_{lm}\\right) = \\frac{\\sigma_{jl}\\sigma_{km} + \\sigma_{jm}\\sigma_{kl}}{n - 1}.\r\\]\nFor sample variances, this reduces to\n\\[\r\\text{Cov}\\left(s_j^2, s_l^2\\right) = \\frac{2\\sigma_{jl}^2}{n - 1}.\r\\]\nThe formula also reduces to the well-known result that the sampling variance of the sample variance is\n\\[\r\\text{Var}\\left(s_j^2\\right) = \\frac{2 \\sigma_{jj}^2}{n - 1}.\r\\]\nOne application of this bit of distribution theory is to find the sampling variance of an average of sample variances. Suppose that we have a bivariate normal distribution where both measures have the same variance \\(\\sigma_{11} = \\sigma_{22} = \\sigma^2\\) and correlation \\(\\rho\\). One estimate of this common variance is to take the simple average of the sample variances, \\(s_{\\bullet}^2 = \\left(s_1^2 + s_2^2\\right) / 2\\). Then using the above:\n\\[\\begin{aligned}\r\\text{Var}\\left(s_{\\bullet}^2\\right) \u0026amp;= \\frac{1}{4}\\left[\\text{Var}\\left(s_1^2\\right) + \\text{Var}\\left(s_2^2\\right) + 2\\text{Cov}\\left(s_1^2, s_2^2\\right) \\right] \\\\\r\u0026amp;= \\frac{\\sigma^4 \\left(1 + \\rho^2\\right)}{n - 1}.\r\\end{aligned}\\]\nTo see that this is correct, consider the extreme cases. If the two measures are perfectly correlated, then averaging the sample variances has no benefit because \\(\\text{Var}\\left(s_{\\bullet}^2\\right) = \\text{Var}\\left(s_1^2\\right) = \\text{Var}\\left(s_2^2\\right)\\). If they are exactly uncorrelated, then averaging the sample variances is equivalent to pooling the sample variance from two independent samples.\nReferences\rMuirhead, R. J. (1982). Aspects of Multivariate Statistical Theory. New York, NY: John Wiley \u0026amp; Sons.\nSearle, S. R. (2006). Matrix Algebra Useful for Statistics. Hoboken, NJ: John Wiley \u0026amp; Sons.\n\r","date":1461542400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461542400,"objectID":"d6b637cc211d50be250d1a17ea52a02a","permalink":"/distribution-of-sample-variances/","publishdate":"2016-04-25T00:00:00Z","relpermalink":"/distribution-of-sample-variances/","section":"post","summary":"A colleague and her students asked me the other day whether I knew of a citation that gives the covariance between the sample variances of two outcomes from a common sample.","tags":["distribution theory"],"title":"The sampling distribution of sample variances","type":"post"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rParker, Vannest, Davis, and Sauber (2011) proposed Tau-U as an effect size measure for use in single-case designs that exhibit baseline trend. In their original paper, they actually conceptualize Tau-U as a family of four distinct indices, distinguished by a) whether the index includes an adjustment for the presence of baseline trend and b) whether the index incorporates information about trend during the intervention phase. However, in subsequent presentations the authors seem to have focused exclusively on the index that adjusts for baseline trend but not for intervention phase trend, and so I’ll do the same here. (This version is also the one available in the web-tool at singlecaseresearch.org.)\nTau-U is an elaboration on their previously proposed effect sizes NAP and Tau, which do not account for baseline trends. The index is calculated as follows. Suppose that we have data from A and B phases from a single case, where the baseline phase has \\(m\\) observations and treatment phase has \\(n\\) observations. Let \\(y^A_1,...,y^A_m\\) denote the baseline phase data and \\(y^B_1,...,y^B_n\\) denote the treatment phase data. Tau-U is then calculated as\n\\[\r\\text{Tau-U} = \\frac{S_P - S_B}{mn}\r\\]\nwhere \\(S_P\\) is Kendall’s S statistic calculated for the comparison between phases and \\(S_B\\) is Kendall’s S statistic calculated on the baseline trend. More precisely,\n\\[\r\\begin{aligned}\rS_P \u0026amp;= \\sum_{i=1}^m \\sum_{j=1}^n \\left[I\\left(y^B_j \u0026gt; y^A_i\\right) - I\\left(y^B_j \u0026lt; y^A_i\\right)\\right] \\\\\rS_B \u0026amp;= \\sum_{i=1}^{m - 1} \\sum_{j = i + 1}^m \\left[I\\left(y^A_j \u0026gt; y^A_i\\right) - I\\left(y^A_j \u0026lt; y^A_i\\right)\\right].\r\\end{aligned}\r\\]\nNote that the first term in Tau-U is equivalent to \\(\\text{Tau} = S_P / (m n)\\), which in turn is a re-scaling of NAP. The second term is related to the rank-correlation between the measurement occasions and outcomes in the baseline phase. Subtracting the second from the first thus adjusts for baseline trend, in the sense that more pronounced baseline trends will lead to smaller values of Tau-U. But looking at the measure a bit more deeply, it has some very odd features. In this post, I’ll show that the distribution of Tau-U is sensitive to the number of observations in each phase.\nSample size sensitivity\rConsider first the logical range of Tau-U. The minimum and maximum possible values of \\(S_P\\) are \\(-m n\\) and \\(m n\\); the minimum and maximum of \\(S_B\\) are \\(-m (m-1) / 2\\) and \\(m (m - 1) / 2\\). Consequently, the logical range of Tau-U is from \\(-(2n + m - 1) / (2n)\\) to \\((2n + m - 1) / (2n)\\). If the treatment phase is quite long compared to the baseline phase, then this range will be close to [-1, 1]. On the other hand, in a study with a baseline that is twice as long as the treatment phase, the range of Tau-U will be closer to [-2, 2]. That’s a very odd property.\nThe average magnitude of Tau-U is similarly influenced by the lengths of each phase. To see this, it’s helpful to think first about its target parameter–the quantity that is estimated when calculating Tau-U based on a sample of data. Since Tau-U is not defined in parametric terms, I will assume that the Tau-U statistic is an unbiased estimator of its target parameter \\(\\tau_U = \\text{E}\\left(\\text{Tau-U}\\right)\\). It follows that\n\\[\r\\tau_U = \\tau_P - \\frac{m - 1}{2n} \\tau_B,\r\\]\nwhere \\(\\tau_P\\) is Kendall’s rank correlation between the outcomes and an indicator for the treatment phase and \\(\\tau_B\\) is Kendall’s rank correlation between the measurement occasions and outcomes during baseline:\n\\[\r\\begin{aligned}\r\\tau_P \u0026amp;= \\frac{1}{mn}\\sum_{i=1}^m \\sum_{j=1}^n \\left[\\text{Pr}\\left(Y^B_j \u0026gt; Y^A_i\\right) - \\text{Pr}\\left(Y^B_j \u0026lt; Y^A_i\\right)\\right] \\\\\r\\tau_B \u0026amp;= \\frac{2}{m(m-1)} \\sum_{i=1}^{m - 1} \\sum_{j = i + 1}^m \\left[\\text{Pr}\\left(Y^A_j \u0026gt; Y^A_i\\right) - \\text{Pr}\\left(Y^A_j \u0026lt; Y^A_i\\right)\\right].\r\\end{aligned}\r\\]\nNow consider a positive a baseline trend, so that \\(\\tau_B \u0026gt; 0\\), and assume that \\(\\tau_P\\) is constant. A longer baseline phase will then lead to smaller values of Tau-U (on average), while a longer treatment phase will lead to larger values of Tau-U (on average). Again, that’s really weird. This is not a good feature for an effect size measure because it means that Tau-U values from different cases are only on the same scale if the cases have identical baseline and treatment phase lengths. In a multiple baseline study, each case is necessarily observed for a different number of occasions in baseline (otherwise it wouldn’t be a multiple baseline). Thus, it seems inadvisable to use Tau-U to quantify the magnitude of treatment effects in a multiple baseline study.\n\rSensitivity under a parametric model\rThings may be different if we allow for the magnitude of \\(\\tau_P\\) to change along with the sample size. Such would be the case under a model where the intervention phase also exhibits a trend. For example, let’s suppose that the outcome follows a linear model with a non-zero trend and the intervention leads to an immediate shift in the outcome, as in the model:\n\\[\ry_t = \\beta_0 + \\beta_1 t + \\beta_2 I(t \u0026gt; m) + \\epsilon_t.\r\\]\nFor simplicity, I’ll assume that the errors in this model are normally distributed with unit variance. Under this model,\n\\[\r\\begin{aligned}\r\\tau_B \u0026amp;= \\frac{4}{m (m - 1)} \\left[\\sum_{i=1}^{m-1} \\sum_{j=i+1}^m \\Phi\\left[\\beta_1\\left(j - i\\right) / \\sqrt{2}\\right]\\right] - 1, \\\\\r\\tau_P \u0026amp;= \\frac{2}{m n} \\left[\\sum_{i=1}^m \\sum_{j=1}^n \\Phi\\left[\\left(\\beta_1 (m + j - i) + \\beta_2\\right) / \\sqrt{2}\\right]\\right] - 1,\r\\end{aligned}\r\\]\nwhere \\(\\Phi()\\) is the standard normal cumulative distribution function. I can use the above formulas to calculate the average value of Tau-U for various degrees of baseline trend \\((\\beta_1)\\), level shift \\((\\beta_2)\\), and phase lengths \\((m,n)\\).\nE_TauU \u0026lt;- function(b1, b2, m, n) {\rtau_B \u0026lt;- sum(sapply(1:(m - 1), function(i) sum(pnorm(b1 * ((i+1):m - i) / sqrt(2))))) * 4 / (m * (m - 1)) - 1\rtau_P \u0026lt;- sum(sapply(1:m, function(i) sum(pnorm((b1 * (m + 1:n - i) + b2) / sqrt(2))))) * 2 / (m * n) - 1\rtau_P - tau_B * (m - 1) / (2 * n)\r}\rlibrary(dplyr)\rlibrary(tidyr)\rb1 \u0026lt;- c(-0.2, -0.1, 0, 0.1, 0.2)\rb2 \u0026lt;- c(0, 0.5, 1.0, 2.0)\rm \u0026lt;- c(5, 10, 15, 20)\rn \u0026lt;- 5:20\rexpand.grid(b1 = b1, b2 = b2, m = m, n = n) %\u0026gt;%\rgroup_by(b1, b2, m, n) %\u0026gt;% mutate(TauU = E_TauU(b1, b2, m, n)) -\u0026gt;\rTauU_values\rex \u0026lt;- filter(TauU_values, b1 == -0.2 \u0026amp; b2 == 0)\rlibrary(ggplot2)\rggplot(TauU_values, aes(n, TauU, color = factor(m))) + facet_grid(b1 ~ b2, labeller = \u0026quot;label_both\u0026quot;) + geom_line() + labs(y = \u0026quot;Expected magnitude of Tau-U\u0026quot;, color = \u0026quot;m\u0026quot;) + theme_bw() + theme(legend.position = \u0026quot;bottom\u0026quot;)\rIn the figure above, each plot corresponds to a different value of the baseline slope (\\(\\beta_1\\), ranging from -0.2 in the top row to 0.2 in the bottom row) and treatment shift (\\(\\beta_2\\), ranging from 0 in the first column to 2 in the last column). Within each plot, the x axis corresponds to treatment phase length and the different lines correspond to different baseline phase lengths. The thing to note is that, when the baseline slope is non-zero, the expected value of Tau-U ranges quite widely within each plot, depending on the values of \\(m\\) and \\(n\\). For example, when \\(\\beta_2 = 0\\) (in the first column), the data follow a simple linear trend with no shift. If the slope of the trend is equal to -0.2 (the first row), then the expected magnitude of Tau-U ranges from -0.8 to 0.3 depending on the phase lengths, which is quite a wide range.\nGenerally, the degree of sample size sensitivity depends on the absolute magnitude of the baseline slope, with steeper slopes leading to increased sensitivity. For steeper values of slope, it appears that the degree to which the measure is affected by sample size even swamps the degree to which the measure is sensitive to the magnitude of the treatment effect. Very peculiar.\n\rA final thought\rOf course, these results are contingent on the particular model under which I derived the expected magnitude of Tau-U. If the data followed some other model, such as a log-linear model with Poisson-distributed outcomes, then the behavior described above might change. Still, I think all of this raises the reasonable question: under what model (i.e., what sort of patterns of baseline trend, what sort of patterns of response to the intervention) does Tau-U provide a meaningful effect size measure that clearly quantifies the magnitude of treatment effects without being strongly affected by phase lengths? Unless and until such a model can be identified, I would be wary of interpreting Tau-U as a measure of treatment effect magnitude.\n\r","date":1458691200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1458691200,"objectID":"c8020a43febbe99bac590c6c6d22f8f6","permalink":"/tau-u/","publishdate":"2016-03-23T00:00:00Z","relpermalink":"/tau-u/","section":"post","summary":"Parker, Vannest, Davis, and Sauber (2011) proposed Tau-U as an effect size measure for use in single-case designs that exhibit baseline trend. In their original paper, they actually conceptualize Tau-U as a family of four distinct indices, distinguished by a) whether the index includes an adjustment for the presence of baseline trend and b) whether the index incorporates information about trend during the intervention phase.","tags":["effect size","single-case design","non-overlap measures"],"title":"Tau-U","type":"post"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rParker and Vannest (2009) proposed non-overlap of all pairs (NAP) as an effect size index for use in single-case research. NAP is defined in terms of all pair-wise comparisons between the data points in two different phases for a given case (i.e., a treatment phase versus a baseline phase). For an outcome that is desirable to increase, NAP is the proportion of all such pair-wise comparisons where the treatment phase observation exceeds the baseline phase observation, with pairs that are exactly tied getting a weight of 1/2. NAP belongs to the family of non-overlap measures, which also includes the percentage of non-overlapping data, the improvement rate difference, and several other indices. It is exactly equivalent to Vargha and Delaney’s (2000) modified Common Language Effect Size and has been proposed as an effect size index in other contexts too (e.g., Acion, Peterson, Temple, \u0026amp; Arndt, 2006).\nThe developers of NAP have created a web-based tool for calculating it (as well as several other non-overlap indices), and I have the impression that the tool is fairly widely used. For example, Roth, Gillis, and DiGennaro Reed (2014) and Whalon, Conroy, Martinez, and Welch (2015) both used NAP in their meta-analyses of single-case research, and both noted that they used singlecaseresearch.org for calculating the effect size measure. Given that the web tool is being used, it is worth scrutinizing the methods behind the calculations it reports. As of this writing, the standard error and confidence intervals reported along with the NAP statistic are incorrect, and should not be used. After introducing a bit of notation, I’ll explain why the existing methods are deficient. I’ll also suggest some methods for calculating standard errors and confidence intervals that are potentially more accurate.\nPreliminaries\rSuppose that we have data from the baseline phase and treatment phase for a single case. Let \\(m\\) denote the number of baseline observations and \\(n\\) denote the number of treatment phase observations. Let \\(y^A_1,...,y^A_m\\) denote the baseline phase data and \\(y^B_1,...,y^B_n\\) denote the treatment phase data. Then NAP is calculated as\n\\[\r\\text{NAP} = \\frac{1}{m n} \\sum_{i=1}^m \\sum_{j=1}^n \\left[I\\left(y^B_j \u0026gt; y^A_i\\right) + 0.5 I\\left(y^B_j = y^A_i\\right)\\right]\r\\]\nWhat is NAP an estimate of? The parameter of interest is the probability that a randomly selected treatment phase observation will exceed a randomly selected baseline phase observation (again, with an adjustment for ties):\n\\[\r\\theta = \\text{Pr}(Y^B \u0026gt; Y^A) + 0.5 \\text{Pr}(Y^B = Y^A).\r\\]\nVargha and Delaney call \\(\\theta\\) the measure of stochastic superiority.\nNAP is very closely related to another non-overlap index called Tau (Parker, Vannest, Davis, \u0026amp; Sauber, 2011). Tau is nothing more than a linear re-scaling of NAP to the range of [-1, 1]:\n\\[\r\\text{Tau} = \\frac{S}{m n} = 2 \\times \\text{NAP} - 1,\r\\]\nwhere\n\\[\rS = \\sum_{i=1}^m \\sum_{j=1}^n \\left[I\\left(y^B_j \u0026gt; y^A_i\\right) - I\\left(y^B_j \u0026lt; y^A_i\\right)\\right].\r\\]\nThe \\(S\\) is Kendall’s S statistic, which is closely related to the Mann-Whitney \\(U\\) test.\nHere is an R function for calculating NAP:\nNAP \u0026lt;- function(yA, yB) {\rm \u0026lt;- length(yA)\rn \u0026lt;- length(yB)\rU \u0026lt;- sum(sapply(yA, function(i) sapply(yB, function(j) (j \u0026gt; i) + 0.5 * (j == i))))\rU / (m * n)\r}\rUsing the data from the worked example in Parker and Vannest (2009), the function result agrees with their reported NAP of 0.96:\nyA \u0026lt;- c(4, 3, 4, 3, 4, 7, 5, 2, 3, 2)\ryB \u0026lt;- c(5, 9, 7, 9, 7, 5, 9, 11, 11, 10, 9)\rNAP(yA, yB)\r## [1] 0.9636364\r\rStandard errors\rThe webtool at singlecaseresearch.org reports a standard error for NAP (it is labelled as “SDnap”), which from what I can tell is based on the formula\n\\[\r\\text{SE}_{\\text{Tau}} = \\sqrt{\\frac{m + n + 1}{3 m n}}.\r\\]\nThis formula appears to actually be the standard error for Tau, rather than for NAP. Since \\(\\text{NAP} = \\left(\\text{Tau} + 1\\right) / 2\\), the standard error for NAP should be half as large:\n\\[\r\\text{SE}_{null} = \\sqrt{\\frac{m + n + 1}{12 m n}}\r\\]\n(cf. Grissom \u0026amp; Kim, 2001, p. 141). However, even the latter formula is not always correct. It is valid only when the observations are all mutually independent and when the treatment phase data are drawn from the same distribution as the baseline phase data—that is, when the treatment has no effect on the outcome. I’ve therefore denoted it as \\(\\text{SE}_{null}\\).\nOther standard error estimators\rBecause an equivalent effect size measure is used in other contexts like clinical medicine, there has actually been a fair bit of research into better approaches for assessing the uncertainty in NAP. Hanley and McNeil (1982) proposed an estimator for the sampling variance of NAP that is designed for continuous outcome measures, where exact ties are impossible. Modifying it slightly (and in entirely ad hoc fashion) to account for ties, let\n\\[\r\\begin{aligned}\rQ_1 \u0026amp;= \\frac{1}{m n^2}\\sum_{i=1}^m \\left[\\sum_{j=1}^n I\\left(y^B_j \u0026gt; y^A_i\\right) + 0.5 I\\left(y^B_j = y^A_i\\right)\\right]^2 \\\\\rQ_2 \u0026amp;= \\frac{1}{m^2 n}\\sum_{j=1}^n \\left[\\sum_{i=1}^m I\\left(y^B_j \u0026gt; y^A_i\\right) + 0.5 I\\left(y^B_j = y^A_i\\right)\\right]^2.\r\\end{aligned}\r\\]\nThen the Hanley-McNeil variance estimator is\n\\[\rV_{HM} = \\frac{1}{mn} \\left[\\text{NAP}\\left(1 - \\text{NAP}\\right) + (n - 1)\\left(Q_1 - \\text{NAP}^2\\right) + (m - 1)\\left(Q_2 - \\text{NAP}^2\\right)\\right],\r\\]\nwith \\(\\text{SE}_{HM} = \\sqrt{V_{HM}}\\).\nThe same authors also propose a different estimator, which is based on the assumption that the outcome data are exponentially distributed. Even though this is a strong and often inappropriate assumption, there is evidence that this estimator works even for other, non-exponential distributions. Newcombe (2006) suggested a further modification of their estimator, and I’ll describe his version. Let \\(h = (m + n) / 2 - 1\\). Then\n\\[\rV_{New} = \\frac{h}{mn} \\text{NAP}\\left(1 - \\text{NAP}\\right)\\left[\\frac{1}{h} + \\frac{1 - \\text{NAP}}{2 - \\text{NAP}} + \\frac{\\text{NAP}}{1 + \\text{NAP}}\\right],\r\\]\nwith \\(\\text{SE}_{New} = \\sqrt{V_{New}}\\).\nHere are R functions to calculate each of these variance estimators.\nV_HM \u0026lt;- function(yA, yB) {\rm \u0026lt;- length(yA)\rn \u0026lt;- length(yB)\rU \u0026lt;- sapply(yB, function(j) (j \u0026gt; yA) + 0.5 * (j == yA))\rt \u0026lt;- sum(U) / (m * n)\rQ1 \u0026lt;- sum(rowSums(U)^2) / (m * n^2)\rQ2 \u0026lt;- sum(colSums(U)^2) / (m^2 * n)\r(t * (1 - t) + (n - 1) * (Q1 - t^2) + (m - 1) * (Q2 - t^2)) / (m * n)\r}\rV_New \u0026lt;- function(yA, yB) {\rm \u0026lt;- length(yA)\rn \u0026lt;- length(yB)\rt \u0026lt;- NAP(yA, yB)\rh \u0026lt;- (m + n) / 2 - 1\rt * (1 - t) * (1 + h * (1 - t) / (2 - t) + h * t / (1 + t)) / (m * n)\r}\rsqrt(V_HM(yA, yB))\r## [1] 0.03483351\rsqrt(V_New(yA, yB))\r## [1] 0.04370206\rFor the worked example dataset from Parker and Vannest, the Newcombe estimator yields a standard error that is about 25% larger than the Hanley-McNeil estimator. Both of these are substantially smaller than the null standard error, which in this example is \\(\\text{SE}_{null} = 0.129\\).\n\rA small simulation\rSimulation methods can be used to examine how well these various standard error formulas estimate the actual sampling variation of NAP. For simplicity, I’ll simulate normally distributed data where\n\\[\rY^A \\sim N(0, 1) \\qquad \\text{and} \\qquad Y^B \\sim N\\left(\\sqrt{2}\\Phi^{-1}(\\theta), 1\\right)\r\\]\nfor varying values of the effect size estimand (\\(\\theta\\)) and a couple of different sample sizes.\nsample_NAP \u0026lt;- function(delta, m, n, iterations) {\rNAPs \u0026lt;- replicate(iterations, {\ryA \u0026lt;- rnorm(m)\ryB \u0026lt;- rnorm(n, mean = delta)\rc(NAP = NAP(yA, yB), V_HM = V_HM(yA, yB), V_New = V_New(yA, yB))\r})\rdata.frame(sd = sd(NAPs[\u0026quot;NAP\u0026quot;,]), SE_HM = sqrt(mean(NAPs[\u0026quot;V_HM\u0026quot;,])), SE_New = sqrt(mean(NAPs[\u0026quot;V_New\u0026quot;,])))\r}\rlibrary(dplyr)\rlibrary(tidyr)\rtheta \u0026lt;- seq(0.5, 0.95, 0.05)\rm \u0026lt;- c(5, 10, 15, 20, 30)\rn \u0026lt;- c(5, 10, 15, 20, 30)\rexpand.grid(theta = theta, m = m, n = n) %\u0026gt;%\rgroup_by(theta, m, n) %\u0026gt;% mutate(delta = sqrt(2) * qnorm(theta)) -\u0026gt;\rparams params %\u0026gt;%\rdo(sample_NAP(.$delta, .$m, .$n, iterations = 2000)) %\u0026gt;%\rmutate(se_null = sqrt((m + n + 1) / (12 * m * n))) %\u0026gt;%\rgather(\u0026quot;sd\u0026quot;,\u0026quot;val\u0026quot;, sd, SE_HM, SE_New, se_null) -\u0026gt;\rNAP_sim\rlibrary(ggplot2)\rggplot(NAP_sim, aes(theta, val, color = sd)) + facet_grid(n ~ m, labeller = \u0026quot;label_both\u0026quot;) + geom_line() + theme_bw() + theme(legend.position = \u0026quot;bottom\u0026quot;)\rIn the above figure, the actual sampling standard deviation of NAP (in red) and the value of \\(\\text{SE}_{null}\\) (in purple) are plotted against the true value of \\(\\theta\\), with separate plots for various combinations of \\(m\\) and \\(n\\). The expected value of the standard errors \\(\\text{SE}_{HM}\\) and \\(\\text{SE}_{New}\\) (actually the square root of the expectation of the variance estimators) are depicted in green and blue, respectively. The value of \\(\\text{SE}_{null}\\) agrees with the actual standard error when \\(\\delta = 0\\), but the two diverge when there is a positive treatment effect. It appears that \\(\\text{SE}_{HM}\\) and \\(\\text{SE}_{New}\\) both under-estimate the actual standard error when \\(m\\) or \\(n\\) is equal to 5, and over-estimate for the largest values of \\(\\theta\\). However, both of these estimators offer a marked improvement over \\(\\text{SE}_{null}\\).\n\r\rConfidence intervals\rThe webtool at singlecaseresearch.org also reports 85% and 90% confidence intervals for NAP. These confidence intervals appear to have the same two problems as the standard errors. First, they are constructed as CIs for Tau rather than for NAP. For the \\(100\\% \\times (1 - \\alpha)\\) CI, let \\(z_{\\alpha / 2}\\) be the appropriate critical value from a standard normal distribution. The CIs reported by the webtool are given by\n\\[\r\\text{Tau} \\pm \\text{SE}_{\\text{Tau}} \\times z_{\\alpha / 2}. \\]\nThis is probably just an oversight in the programming, which could be corrected by instead using\n\\[\r\\text{NAP} \\pm \\text{SE}_{null} \\times z_{\\alpha / 2}.\r\\]\nIn parallel with the standard error formulas, I’ll call this formula the null confidence interval. Funnily enough, the upper bound of the null CI is the same as the upper bound of the Tau CI. However, the lower bound is going to be quite a bit larger than the lower bound for the Tau CI, so that the null CI will be much narrower.\nThe second problem is that even the null CI has poor coverage properties because it is based on \\(\\text{SE}_{null}\\), which can drastically over-estimate the standard error of NAP for non-null values.\nOther confidence intervals\rAs I noted above, there has been a fair amount of previous research into how to construct CIs for \\(\\theta\\), the parameter estimated by NAP. As is often the case with these sorts of problems, there are many different methods available, scattered across the literature. Fortunately, there are two (at least) fairly comprehensive simulation studies that compare the performance of various methods under a wide range of conditions. Newcombe (2006) examined a range of methods based on inverting Wald-type test statistics (which give CIs of the form \\(\\text{estimate} \\pm \\text{SE} \\times z_{\\alpha / 2}\\), where \\(\\text{SE}\\) is some standard error estimate) and score-based methods (in which the standard error is estimated using the candidate parameter value). Based on an extensive simulation, he suggested a score-based method in which the end-points of the CI are defined the values of \\(\\theta\\) that satisfy:\n\\[\r(\\text{NAP} - \\theta)^2 = \\frac{z^2_{\\alpha / 2} h \\theta (1 - \\theta)}{mn}\\left[\\frac{1}{h} + \\frac{1 - \\theta}{2 - \\theta} + \\frac{\\theta}{1 + \\theta}\\right],\r\\]\nwhere \\(h = (m + n) / 2 - 1\\). This equation is a fourth-degree polynomial in \\(\\theta\\), easily solved using a numerical root-finding algorithm.\nIn a different simulation study, Ruscio and Mullen (2012) examined the performance of a selection of different confidence intervals for \\(\\theta\\), including several methods not considered by Newcombe. Among the methods that they examined, they find that the bias-corrected, accelerated (BCa) bootstrap CI performs particularly well (and seems to outperform the score-based CI recommended by Newcombe).\nNeither Newcombe (2006) nor Ruscio and Mullen (2012) considered constructing a confidence interval by directly pivoting the Mann-Whitney U test (the same technique used to construct confidence intervals for the Hodges-Lehmann estimator of location shift), although it seems to me that this would be possible and potentially an attractive approach in the context of SCDs. The main caveat is that such a CI would require stronger distributional assumptions than those studied in the simulations, such as that the distributions of \\(Y^A\\) and \\(Y^B\\) differ by an additive (or multiplicative) constant. In any case, it seems like it would be worth exploring this approach too.\n\rAnother small simulation\rHere is an R function for calculating several different CIs for \\(\\theta\\), including the null CI, Wald-type CIs based on \\(V_{HM}\\) and \\(V_{New}\\), and the score-type CI recommended by Newcombe (2006). I haven’t programmed the BCa bootstrap because it would take a bit more thought to figure out how to simulate it efficiently.\nThe following code simulates the coverage rates of nominal 90% CIs based on each of these methods, following the same simulation set-up as above.\nNAP_CIs \u0026lt;- function(yA, yB, alpha = .05) {\rm \u0026lt;- length(yA)\rn \u0026lt;- length(yB)\rU \u0026lt;- sapply(yB, function(j) (j \u0026gt; yA) + 0.5 * (j == yA))\rt \u0026lt;- sum(U) / (m * n)\r# variance estimators\rV_null \u0026lt;- (m + n + 1) / (12 * m * n)\rQ1 \u0026lt;- sum(rowSums(U)^2) / (m * n^2)\rQ2 \u0026lt;- sum(colSums(U)^2) / (m^2 * n)\rV_HM \u0026lt;- (t * (1 - t) + (n - 1) * (Q1 - t^2) + (m - 1) * (Q2 - t^2)) / (m * n)\rh \u0026lt;- (m + n) / 2 - 1\rV_New \u0026lt;- t * (1 - t) * (1 + h * (1 - t) / (2 - t) + h * t / (1 + t)) / (m * n)\r# Wald-type confidence intervals\rz \u0026lt;- qnorm(1 - alpha / 2)\rSEs \u0026lt;- sqrt(c(null = V_null, HM = V_HM, Newcombe = V_New))\rWald_lower \u0026lt;- t - z * SEs\rWald_upper \u0026lt;- t + z * SEs\r# score-type confidence interval\rf \u0026lt;- function(x) m * n * (t - x)^2 * (2 - x) * (1 + x) - z^2 * x * (1 - x) * (2 + h + (1 + 2 * h) * x * (1 - x))\rscore_lower \u0026lt;- if (t \u0026gt; 0) uniroot(f, c(0, t))$root else 0\rscore_upper \u0026lt;- if (t \u0026lt; 1) uniroot(f, c(t, 1))$root else 1\rlist(NAP = t, CI = data.frame(lower = c(Wald_lower, score = score_lower), upper = c(Wald_upper, score = score_upper)))\r}\rNAP_CIs(yA, yB)\r## $NAP\r## [1] 0.9636364\r## ## $CI\r## lower upper\r## null 0.7106061 1.2166666\r## HM 0.8953639 1.0319088\r## Newcombe 0.8779819 1.0492908\r## score 0.7499741 0.9950729\rsample_CIs \u0026lt;- function(delta, m, n, alpha = .05, iterations) {\rNAPs \u0026lt;- replicate(iterations, {\ryA \u0026lt;- rnorm(m)\ryB \u0026lt;- rnorm(n, mean = delta)\rNAP_CIs(yA, yB, alpha = alpha)\r}, simplify = FALSE)\rtheta \u0026lt;- mean(sapply(NAPs, function(x) x$NAP))\rcoverage \u0026lt;- rowMeans(sapply(NAPs, function(x) (x$CI$lower \u0026lt; theta) \u0026amp; (theta \u0026lt; x$CI$upper)))\rdata.frame(CI = rownames(NAPs[[1]]$CI), coverage = coverage)\r}\rparams %\u0026gt;% do(sample_CIs(delta = .$delta, m = .$m, n = .$n, alpha = .10, iterations = 5000)) -\u0026gt;\rNAP_CI_sim\rggplot(NAP_CI_sim, aes(theta, coverage, color = CI)) + facet_grid(n ~ m, labeller = \u0026quot;label_both\u0026quot;, scales = \u0026quot;free_y\u0026quot;) + geom_line() + labs(y = \u0026quot;SE\u0026quot;) + geom_hline(yintercept=.90, linetype=\u0026quot;dashed\u0026quot;) +\rtheme_bw() + theme(legend.position = \u0026quot;bottom\u0026quot;)\rThe figure above plots the coverage rates of several different confidence intervals for \\(\\theta\\): the naive CI (in blue), the HM Wald CI (red), the Newcombe Wald CI (green), and the Newcombe score CI (purple). The dashed horizontal line is the nominal coverage rate of 90%. It can be seen that the null CI has the correct coverage only when \\(\\theta \\leq .6\\); for larger values of \\(\\theta\\), its coverage becomes too conservative (tending towards 100%). The Wald-type CIs have below-nominal coverage rates, which improve as the sample size in each phase increases but remain too liberal even at the largest sample size considered. Finally, Newcombe’s score CI maintains close-to-nominal coverage over a wider range of \\(\\theta\\) values. Although these CIs have below-nominal coverage for the smallest sample sizes, they generally have good coverage for \\(\\theta \u0026lt; .9\\) and when the sample size in each phase is 10 or more. It is also notable that their coverage rates appear to become more accurate as the sample size in a given group increases, even if the sample size in the other group is fairly small and remains constant.\n\r\rCaveats\rMy aim in this post was to highlight the problems with how singlecaseresearch.org calculates standard errors and CIs for the NAP statistic. Some of these issues could easily be resolved by correcting the relevant formulas so that they are appropriate for NAP rather than Tau. However, even with these corrections, better approaches exist for calculating standard errors and CIs. I’ve highlighted some promising ones above, which seem worthy of further investigation. But I should also emphasize that these methods do come with some important caveats too.\nFirst, all of the methods I’ve discussed are premised on having mutually independent observations. In the presence of serial correlation, I would anticipate that any of these standard errors will be too small and any of the confidence intervals will be too narrow. (This could readily be verified through simulation, although I have not done so here.)\nSecond, my small simulations are based on the assumption of normally distributed, homoskedastic observations in each phase, which is not a particularly good model for the types of outcome measures commonly used in single case research. In some of my other work, I’ve developed statistical models for data collected by systematic direct observation of behavior, which is the most prevalent type of outcome data in single-case research. Before recommending any particular method, the performance of the standard error formulas (e.g., the Hanley-McNeil and Newcombe estimators) and CI methods (such as Newcombe’s score CI) should be examined under more realistic models for behavioral observation data.\n\r","date":1456617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1456617600,"objectID":"b49c57014825b902eeb8569e70296fdd","permalink":"/nap-ses-and-cis/","publishdate":"2016-02-28T00:00:00Z","relpermalink":"/nap-ses-and-cis/","section":"post","summary":"Parker and Vannest (2009) proposed non-overlap of all pairs (NAP) as an effect size index for use in single-case research. NAP is defined in terms of all pair-wise comparisons between the data points in two different phases for a given case (i.","tags":["effect size","single-case design","non-overlap measures","distribution theory"],"title":"Standard errors and confidence intervals for NAP","type":"post"},{"authors":[],"categories":null,"content":"","date":1455840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1455840000,"objectID":"1ad6f38a6fc462f8deb9565bc96523c1","permalink":"/talk/prc-2016-when-large-samples-act-small/","publishdate":"2018-04-15T08:15:00Z","relpermalink":"/talk/prc-2016-when-large-samples-act-small/","section":"talk","summary":"","tags":[],"title":"When large samples act small: Cluster-robust variance estimation and hypothesis testing with few clusters","type":"talk"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rRegression discontinuity designs (RDDs) are now a widely used tool for program evaluation in economics and many other fields. RDDs occur in situations where some treatment/program of interest is assigned on the basis of a numerical score (called the running variable), all units scoring above a certain threshold receiving treatment and all units scoring at or below the threshold having treatment withheld (or vice versa, with treatment assigned to units scoring below the threshold). This mechanism provides a way to identify the marginal average treatment effect (MATE): the average effect of treatment assignment for units on the cusp of the threshold.\nRDDs are appealing for a couple of reasons. First and foremost, RDD-like mechanism occurs all over the place, since providing treatment on the basis of a numerical measure of need/eligibility is a natural way to allocate resources. Furthermore, analysis of the designs is straight-forward, as it involves nothing more complicated than a linear regression model, estimated using (weighted or un-weighted) least squares, and which can be represented graphically using a simple scatterplot. Things get a little bit more complicated if you are trying to account for imperfect compliance with treatment assignment—as in the “fuzzy” RDD—but for the moment let me focus on “sharp” RDDs.\nThe simplest approach to estimating the MATE is to use a local linear regression in the neighborhood of the threshold, with the outcome regressed on the running variable, treatment indicator, and their interaction. However, in practice it is quite common to also include additional covariates in the local linear regression. If the covariates are also interacted with the treatment indicator, there is no longer a single regression coefficient corresponding to the treatment effect. In my last post, I suggested a “centering trick” for estimating the MATE based on a model that included covariate-by-treatment interactions. In this post, I’ll explain the reasoning behind this proposal.\nG’day, MATE\rI think it’s helpful to start by thinking about the definition of the MATE in non-parametric terms. Let \\(R\\) be the running variable, assumed to be centered at the threshold; \\(T\\) be an indicator for treatment assignment, with \\(T = I(R \u0026gt; 0)\\); and \\(X\\) be a covariate, which may be vector-valued. Denote the potential outcomes as \\(Y^0\\) (a unit’s outcome if not assigned to treatment) and \\(Y^1\\) (a unit’s outcome if assigned to treatment), so that the observed outcome is \\(Y = Y^0 (1 - T) + Y^1 T\\). Now consider the potential response surfaces\n\\[\\begin{aligned}\\mu_0(x, r) \u0026amp;= \\text{E}\\left(\\left.Y^0 \\right|X = x, R = r\\right) \\\\ \\mu_1(x, r) \u0026amp;= \\text{E}\\left(\\left.Y^1 \\right|X = x, R = r\\right).\\end{aligned}\\]\nIn an RDD, the average treatment effect at a given point \\((x, r)\\) on the response surface is not generally identified by conditioning because one of the potential outcomes will never be observed: if \\(r \u0026lt; 0\\) then \\(\\text{Pr}( T = 0 \\vert X = x, R = r) = 1\\) and \\(\\text{Pr}( T = 1 \\vert X = x, R = r) = 0\\) (and vice versa for \\(r \u0026gt; 0\\)). However, the treatment effect for the subpopulation where \\(R = 0\\) can be identified under the assumption that the potential response surfaces are continuous in a neighborhood of the threshold. Thus the MATE, which can be written as\n\\[\\begin{aligned}\r\\delta_M \u0026amp;= \\text{E}\\left(\\left. Y^1 - Y^0 \\right| R = 0\\right) \\\\\r\u0026amp;= \\text{E}\\left[\\mu_1(X, 0) - \\mu_0(X,0)\\right].\r\\end{aligned}\\]\n\rRegression estimation\rNow assume that we have a simple random sample \\(\\left(y_i,r_i,t_i, x_i\\right)_{i=1}^n\\) of units and that each unit has a weight \\(w_i\\) defined based on some measure of distance from the threshold. We can use these data to estimate the response surfaces (somehow…more on that in a minute) on each side of the cut-off, with \\(\\hat\\mu_0(x, r)\\) for \\(r \u0026lt; 0\\) and \\(\\hat\\mu_1(x, r)\\) for \\(r \u0026gt; 0\\). If we then use the sample distribution of \\(X\\) in the neighborhood of \\(R = 0\\) in place of the conditional density \\(d\\left(X = x \\vert R = 0\\right)\\), we can estimate the MATE as\n\\[\\hat\\delta_M = \\frac{1}{W} \\sum_{i=1}^n w_i \\left[\\hat\\mu_1(x_i, 0) - \\hat\\mu_0(x_i, 0)\\right],\\]\nwhere \\(W = \\sum_{i=1}^n w_i\\). This is a regression estimator for \\(\\delta_M\\). It could be non-, semi-, or fully parametric depending on the technique used to estimate the response surfaces. Note that this estimator is a little bit different than the regression estimator that would be used in the context of an observational study (see, e.g., Shafer \u0026amp; Kang, 2008). In that context, one would use \\(\\hat\\mu_j(x_i, r_i)\\) rather than \\(\\hat\\mu_j(x_i, 0)\\), but in an RDD doing so would involve extrapolating beyond the cutpoint (i.e., using \\(\\hat\\mu_1(x_i, r_i)\\) for \\(r_i \u0026lt; 0\\)).\nNow suppose that we again use a linear regression in some neighborhood of the cut-point to estimate the response surfaces. For the (weighted) sample in the neighborhood of the cut-point, we assume that\n\\[\\mu_{t_i}(x_i, r_i) = \\beta_0 + \\beta_1 r_i + \\beta_2 t_i + \\beta_3 r_i t_i + \\beta_4 x_i + \\beta_5 x_i t_i.\\]\nSubstituting this into the formula for \\(\\hat\\delta_M\\) leads to\n\\[\\begin{aligned}\\hat\\delta_M \u0026amp;= \\frac{1}{W} \\sum_{i=1}^n w_i \\left[\\hat\\beta_2 + \\hat\\beta_5 x_i \\right] \\\\\r\u0026amp;= \\hat\\beta_2 + \\hat\\beta_5 \\sum_{i=1}^n \\frac{w_i x_i}{W}.\\end{aligned}\\]\nNow, the centering trick involves nothing more than re-centering the covariate so that \\(\\sum_{i=1}^n w_i x_i = 0\\) and \\(\\hat\\delta_M = \\hat\\beta_2\\). Of course, one could just use the non-parametric form of the regression estimator, but the centering trick is useful because it comes along with an easy-to-calculate standard error (since it is just a regression coefficient estimate).\n\rMultiple covariates\rAll of this works out in the exact same way if you have interactions between the treatment and multiple covariates. However, there are a few tricky cases that are worth noting. If you include interactions between the treatment indicator and a polynomial function of the treatment, each term of the polynomial has to be centered. For example, if you want to control for \\(x\\), \\(x^2\\), and their interactions with treatment, you will need to calculate\n\\[\\tilde{x}_{1i} = x_i - \\frac{1}{W} \\sum_{i=1}^n w_i x_i, \\qquad \\tilde{x}_{2i} = x_i^2 - \\frac{1}{W} \\sum_{i=1}^n w_i x_i^2\\]\nand then use these re-centered covariates in the regression\n\\[\\mu_{t_i}(x_i, r_i) = \\beta_0 + \\beta_1 r_i + \\beta_2 t_i + \\beta_3 r_i t_i + \\beta_4 \\tilde{x}_{1i} + \\beta_5 \\tilde{x}_{2i} + \\beta_6 \\tilde{x}_{1i} t_i + \\beta_7 \\tilde{x}_{2i} t_i.\\]\nThe same principle will also hold if you want to include higher-order interactions between covariates and the treatment: calculate the interaction term first, then re-center it. There’s one exception though. If you want to include an interaction between a covariate \\(x\\), the running variable, and the treatment indicator (who knows…you might aspire to do this some day…), then all you need to do is center \\(x\\). In particular, you should not calculate the interaction \\(x_i r_i\\) and then re-center it (doing so could pull the average away from the threshold of \\(R = 0\\)).\n\rR, MATEs!\rHere’s some R code that implements the centering trick for the simulated example from my last post:\nlibrary(sandwich)\rlibrary(lmtest)\rlibrary(rdd)\r# simulate an RDD\rset.seed(20160124)\rsimulate_RDD \u0026lt;- function(n = 2000, R = rnorm(n, mean = qnorm(.2))) {\rn \u0026lt;- length(R)\rT \u0026lt;- as.integer(R \u0026gt; 0)\rX1 \u0026lt;- 10 + 0.6 * (R - qnorm(.2)) + rnorm(n, sd = sqrt(1 - 0.6^2))\rX2 \u0026lt;- sample(LETTERS[1:4], n, replace = TRUE, prob = c(0.2, 0.3, 0.35, 0.15))\rY0 \u0026lt;- 0.4 * R + 0.1 * (X1 - 10) + c(A = 0, B = 0.30, C = 0.40, D = 0.55)[X2] + rnorm(n, sd = 0.9)\rY1 \u0026lt;- 0.35 + 0.3 * R + 0.18 * (X1 - 10) + c(A = -0.50, B = 0.30, C = 0.20, D = 0.60)[X2] + rnorm(n, sd = 0.9)\rY \u0026lt;- (1 - T) * Y0 + T * Y1\rdata.frame(R, T, X1, X2, Y0, Y1, Y)\r}\rRD_data \u0026lt;- simulate_RDD(n = 2000)\r# calculate kernel weights\rbw \u0026lt;- with(RD_data, IKbandwidth(R, Y, cutpoint = 0))\rRD_data$w \u0026lt;- kernelwts(RD_data$R, center = 0, bw = bw)\r# center the covariates\rX_mat \u0026lt;- model.matrix(~ 0 + X2 + X1, data = RD_data)\rX_cent \u0026lt;- as.data.frame(apply(X_mat, 2, function(x) x - weighted.mean(x, w = RD_data$w)))\rRD_data_aug \u0026lt;- cbind(X_cent, subset(RD_data, select = c(-X1, -X2)))\rcov_names \u0026lt;- paste(names(X_cent)[-1], collapse = \u0026quot; + \u0026quot;)\r# calculate the MATE using RDestimate\rRD_form \u0026lt;- paste(\u0026quot;Y ~ R |\u0026quot;, cov_names)\rsummary(RDestimate(as.formula(RD_form), data = RD_data_aug))\r## ## Call:\r## RDestimate(formula = as.formula(RD_form), data = RD_data_aug)\r## ## Type:\r## sharp ## ## Estimates:\r## Bandwidth Observations Estimate Std. Error z value Pr(\u0026gt;|z|) ## LATE 1.0894 1177 0.2981 0.10659 2.797 0.0051559\r## Half-BW 0.5447 611 0.2117 0.14846 1.426 0.1539482\r## Double-BW 2.1787 1832 0.2734 0.08305 3.292 0.0009949\r## ## LATE ** ## Half-BW ## Double-BW ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## F-statistics:\r## F Num. DoF Denom. DoF p\r## LATE 23.30 11 1165 0\r## Half-BW 10.97 11 599 0\r## Double-BW 47.41 11 1820 0\r# or using lm\rlm_form \u0026lt;- paste(\u0026quot;Y ~ R + T + R:T + T*(\u0026quot;, cov_names,\u0026quot;)\u0026quot;)\rlm_fit \u0026lt;- lm(as.formula(lm_form), weights = w, data = subset(RD_data_aug, w \u0026gt; 0))\rcoeftest(lm_fit, vcov. = vcovHC(lm_fit, type = \u0026quot;HC1\u0026quot;))[\u0026quot;T\u0026quot;,]\r## Estimate Std. Error t value Pr(\u0026gt;|t|) ## 0.298142798 0.106588790 2.797130893 0.005240719\r\rComments\rI’ve shown that the “centering trick” is just a way to express a certain regression estimator for the marginal average treatment effect in an RDD. Having suggested that this is a good idea, I should also note a few points that might bear further investigation.\nMy regression estimator uses the sample distribution of \\(X\\) in the neighborhood of the threshold as an estimate of \\(d(X = x \\vert R = 0)\\). This seems reasonable, but I wonder whether there might be a better approach to estimating this conditional density.\rAs far as I understand, the current best practice for defining the “neighborhood” of the threshold is to use weights based on a triangular kernel and an “optimal” bandwidth proposed by Imbens and Kalyanaraman (2012). The optimal bandwidth is derived for the simple RDD model with no covariates, though the authors comment that inclusion of additional covariates should not greatly affect the result unless the covariates are strongly correlated with the outcome, conditional on the running variable. However, what if interest centers on the covariate-by-treatment interaction itself, rather than just the MATE? It is not clear that the bandwidth is optimal for estimation/inference on the interaction term.\rSo far I’ve considered the MATE identified by a sharp RDD, in which we examine the effects of treatment assignment, regardless of whether units assigned to treatment actually received/participated in it. In fuzzy RDDs, the target parameter is the average effect of treatment receipt for those on the threshold of eligibility and who comply with the assignment rule. The effect is estimated using two-stage least squares, taking treatment assignment as an instrument for treatment receipt. I’m not entirely sure how the regression estimator approach would work in this instrumental variables setting.\r\r\r","date":1453852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1453852800,"objectID":"f48dc7f7f8ef2cbdab77238ab69032ca","permalink":"/rdd-interactions-again/","publishdate":"2016-01-27T00:00:00Z","relpermalink":"/rdd-interactions-again/","section":"post","summary":"Regression discontinuity designs (RDDs) are now a widely used tool for program evaluation in economics and many other fields. RDDs occur in situations where some treatment/program of interest is assigned on the basis of a numerical score (called the running variable), all units scoring above a certain threshold receiving treatment and all units scoring at or below the threshold having treatment withheld (or vice versa, with treatment assigned to units scoring below the threshold).","tags":["econometrics","Rstats","causal inference","regression discontinuity"],"title":"Estimating average effects in regression discontinuities with covariate interactions","type":"post"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rNOTE (2019-09-24): This post pertains to version 0.56 of the rdd package. The problems described in this post have been corrected in version 0.57 of the package, which was posted to CRAN on 2016-03-14.\nThe rdd package in R provides a set of methods for analysis of regression discontinuity designs (RDDs), including methods to estimate marginal average treatment effects by local linear regression. I was working with the package recently and obtained some rather counter-intuitive treatment effect estimates in a sharp RDD model. After digging around a bit, I found that my perplexing results were the result of a subtle issue of model specification. Namely, in models with additional covariates (beyond just the running variable, treatment indicator, and interaction), the main estimation function in rdd uses a specification in which covariates are always interacted with the treatment indicator. In this post, I’ll demonstrate the issue and comment on potential work-arounds.\nA simulated example\rTo make things more concrete, here’s a hypothetical RDD. I’ll use \\(R\\) to denote the running variable, with the threshold set at zero; \\(T\\) for the treatment indicator; and \\(Y\\) for the outcome. \\(X_1\\) is a continuous covariate that is correlated with \\(R\\). \\(X_2\\) is a categorical covariate with four levels that is independent of \\(X_1\\) and \\(R\\). In order to illustrate the issue with covariate-by-treatment interactions, I use a model in which the effect of the treatment varies with \\(R\\), \\(X_1\\), and \\(X_2\\):\nset.seed(20160124)\rsimulate_RDD \u0026lt;- function(n = 2000, R = rnorm(n, mean = qnorm(.2))) {\rn \u0026lt;- length(R)\rT \u0026lt;- as.integer(R \u0026gt; 0)\rX1 \u0026lt;- 10 + 0.6 * (R - qnorm(.2)) + rnorm(n, sd = sqrt(1 - 0.6^2))\rX2 \u0026lt;- sample(LETTERS[1:4], n, replace = TRUE, prob = c(0.2, 0.3, 0.35, 0.15))\rY0 \u0026lt;- 0.4 * R + 0.1 * (X1 - 10) + c(A = 0, B = 0.30, C = 0.40, D = 0.55)[X2] + rnorm(n, sd = 0.9)\rY1 \u0026lt;- 0.35 + 0.3 * R + 0.18 * (X1 - 10) + c(A = -0.50, B = 0.30, C = 0.20, D = 0.60)[X2] + rnorm(n, sd = 0.9)\rY \u0026lt;- (1 - T) * Y0 + T * Y1\rdata.frame(R, T, X1, X2, Y0, Y1, Y)\r}\rRD_data \u0026lt;- simulate_RDD(n = 2000)\r\rSimple RDD analysis\rThe main estimand in a sharp RDD is the marginal average treatment effect (MATE)—that is, the average effect of treatment assignment for units right at/near the threshold of eligibility. Even though I simulated a treatment response surface that depends on the covariates \\(X_1,X_2\\), it is not necessary to control for them in order to identify the MATE. Rather, it is sufficient to use a local linear regression of the outcome on the running variable, treatment indicator, and their interaction:\n\\[Y_i = \\beta_0 + \\beta_1 R_i + \\beta_2 T_i + \\beta_3 R_i T_i + \\epsilon_i\\]\nTypically, this regression is estimated using the observations within a certain bandwidth of the threshold, and using weights defined on the basis of some kernel. The default in the rdd package is to use a triangular edge kernel, with bandwidth chosen using a formula proposed by Imbens and Kalyanaraman. The following code uses rdd to estimate the MATE without controlling for covariates:\nlibrary(rdd)\rbw \u0026lt;- with(RD_data, IKbandwidth(R, Y, cutpoint = 0))\rrdd_simple \u0026lt;- RDestimate(Y ~ R, data = RD_data, cutpoint = 0, bw = bw)\rsummary(rdd_simple)\r## ## Call:\r## RDestimate(formula = Y ~ R, data = RD_data, cutpoint = 0, bw = bw)\r## ## Type:\r## sharp ## ## Estimates:\r## Bandwidth Observations Estimate Std. Error z value Pr(\u0026gt;|z|) ## LATE 1.0894 1177 0.3035 0.11323 2.680 0.007355 **\r## Half-BW 0.5447 611 0.2308 0.15471 1.492 0.135722 ## Double-BW 2.1787 1832 0.2699 0.08968 3.010 0.002613 **\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## F-statistics:\r## F Num. DoF Denom. DoF p ## LATE 37.73 3 1173 0.000e+00\r## Half-BW 12.64 3 607 1.006e-07\r## Double-BW 104.74 3 1828 0.000e+00\rUsing a bandwidth of 1.09, the estimated marginal average treatment effect is 0.303. The figure below illustrates the discontinuity:\n## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r\rRDD with covariates\rIn practice, it is quite common for analysts to include additional covariates in the model specification. Doing so is not necessary for treatment effect identification, but can be useful for purposes of improving precision. For example, Cortes, Goodman, and Nomi (2015) use an RDD to estimate the effects of assigning low-performing 9th graders to double-dose algebra. Their main specifications include controls for student gender, race/ethnicity, free/reduced-price lunch status, etc. In the analysis that I’m working on, the data come from students nested within multiple schools, and so it seems sensible to include fixed effects for each school. There’s a direct analogy here to simple randomized experiments: the basic difference in means provides a randomization-unbiased estimate of the sample average treatment effect, but in practice it can be awfully useful to use an estimate from a model with additional covariates.\nReturning to my simulated example, the following table reports the estimates generated by RDestimate when controlling for neither, one, or both covariates.\nRD_est \u0026lt;- function(mod, covariates) {\rRD_fit \u0026lt;- RDestimate(as.formula(paste(mod, covariates)), data = RD_data, cutpoint = 0)\rwith(RD_fit, c(est = est[[1]], se = se[1], p = p[1]))\r}\rcovariates \u0026lt;- list(\u0026quot;No covariates\u0026quot; = \u0026quot;\u0026quot;,\r\u0026quot;X1 only\u0026quot; = \u0026quot;| X1\u0026quot;,\r\u0026quot;X2 only\u0026quot; = \u0026quot;| X2\u0026quot;,\r\u0026quot;X1 + X2\u0026quot; = \u0026quot;| X1 + X2\u0026quot;)\rlibrary(plyr)\rldply(covariates, RD_est, mod = \u0026quot;Y ~ R\u0026quot;, .id = \u0026quot;Specification\u0026quot;)\r## Specification est se p\r## 1 No covariates 0.3034839 0.1132266 0.007355079\r## 2 X1 only -0.6861864 0.8077039 0.395574210\r## 3 X2 only -0.2269958 0.1626996 0.162960539\r## 4 X1 + X2 -1.2529313 0.7315106 0.086749345\rDespite using identical bandwidths, the estimates are drastically different from each other, with standard errors that are much larger than for the simple estimate without covariates.\n\rWhat’s going on?\rIt is known that introducing covariates into an RDD analysis should have little effect on the MATE estimate (see, e.g., Lee and Lemieux, 2010). It is therefore quite perplexing that the estimates in my example (and in the real study I was analyzing) were so sensitive. It turns out that this puzzling behavior arises because, for sharp RDDs only, RDestimate always interacts the covariate(s) with the treatment indicator. Here is the relevant section of the function:\nbody(RDestimate)[[39]][[4]][[7]][[3]][[3]]\r## if (!is.null(covs)) {\r## data \u0026lt;- data.frame(Y, Tr, Xl, Xr, covs, w)\r## form \u0026lt;- as.formula(paste(\u0026quot;Y~Tr+Xl+Xr+\u0026quot;, paste(\u0026quot;Tr*\u0026quot;, names(covs), ## collapse = \u0026quot;+\u0026quot;, sep = \u0026quot;\u0026quot;), sep = \u0026quot;\u0026quot;))\r## } else {\r## data \u0026lt;- data.frame(Y, Tr, Xl, Xr, w)\r## form \u0026lt;- as.formula(Y ~ Tr + Xl + Xr)\r## }\rFor a generic covariate \\(X\\), the function uses the specification:\n\\[Y_i = \\beta_0 + \\beta_1 R_i + \\beta_2 T_i + \\beta_3 R_i T_i + \\beta_4 X_i + \\beta_5 X_i T_i + \\epsilon_i, \\]\nwhile still taking \\(\\beta_2\\) to represent the MATE. This is problematic because, as soon as the \\(X_i T_i\\) term is introduced into the model, \\(\\beta_2\\) represents the difference between treated and untreated units at the threshold (where \\(R_i = 0\\)) and where \\(X_i = 0\\). Thus, including the \\(X_1\\) interaction in the model means that \\(\\beta_2\\) is a difference extrapolated way outside the support of the data, as in the following scatterplot of the outcome versus the covariate \\(X_1\\):\n## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\rRDestimate returns as the MATE estimate the difference between the regression lines when \\(X_1 = 0\\), which in this example is -0.69. Similarly, including the \\(X_2\\) interaction in the model means that \\(\\beta_2\\) will represent the marginal average treatment effect for only one of the categories of \\(X_2\\), rather than as some sort of average across all four categories.\n\rWhat to do about this\rIf you’ve been using the rdd package to analyze your data, I can think of a couple of ways to handle this issue, depending on whether you want to use a model that interacts the covariates with the treatment indicator. Here are some options:\nFirst, suppose that you want to estimate a model that does NOT include covariate-by-treatment interactions. The most transparent (and thus probably safest) approach is to do the estimation “by hand,” so to speak. Specifically, Use the rdd package to get kernel weights, but then estimate the outcome model using plain-old lm. Here’s an example:\nlibrary(sandwich)\rlibrary(lmtest)\rRD_data$wt \u0026lt;- kernelwts(RD_data$R, center = 0, bw = bw)\rMATE_model \u0026lt;- lm(Y ~ R + T + R * T + X1 + X2, weights = wt, data = subset(RD_data, wt \u0026gt; 0))\rcoeftest(MATE_model, vcov. = vcovHC(MATE_model, type = \u0026quot;HC1\u0026quot;))\r## ## t test of coefficients:\r## ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -1.586191 0.374247 -4.2384 2.429e-05 ***\r## R 0.183542 0.136025 1.3493 0.1774938 ## T 0.292284 0.107689 2.7142 0.0067422 ** ## X1 0.130973 0.034704 3.7739 0.0001688 ***\r## X2B 0.474403 0.091835 5.1658 2.813e-07 ***\r## X2C 0.549125 0.084991 6.4610 1.523e-10 ***\r## X2D 0.713331 0.096855 7.3649 3.338e-13 ***\r## R:T 0.283663 0.222801 1.2732 0.2032105 ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rBy default, RDestimate uses the HC1 variant of heteroskedasticity-robust standard errors. To exactly replicate its behavior, I used coeftest from the lmtest package, combined with vcovHC from the sandwich package. Note that it is also necessary to estimate the model based on the subset of observations with positive weight (otherwise the sandwich standard errors will misbehave).\nAn alternative to the first approach is to “trick” RDestimate into using the desired model specification by using 2SLS estimation with \\(T\\) instrumenting itself. Because the function does not use covariate-by-treatment interactions for “fuzzy” RDDs, you get the correct model specification:\nsummary(RDestimate(Y ~ R + T| X1 + X2, data = RD_data, cutpoint = 0))\r## ## Call:\r## RDestimate(formula = Y ~ R + T | X1 + X2, data = RD_data, cutpoint = 0)\r## ## Type:\r## fuzzy ## ## Estimates:\r## Bandwidth Observations Estimate Std. Error z value Pr(\u0026gt;|z|) ## LATE 1.0894 1177 0.2923 0.10769 2.714 0.006644 **\r## Half-BW 0.5447 611 0.2041 0.14911 1.369 0.171103 ## Double-BW 2.1787 1832 0.2703 0.08447 3.200 0.001374 **\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\r## ## F-statistics:\r## F Num. DoF Denom. DoF p ## LATE 31.24 7 1169 7.490e-40\r## Half-BW 13.84 7 603 1.110e-16\r## Double-BW 68.36 7 1824 7.919e-88\rThe results based on the first bandwidth agree with the results from lm.\nNow, suppose that you DO want to retain the covariate-by-treatment interactions in the model, while also estimating the MATE. To do this, you can use what I call “the centering trick,” which entails centering each covariate at the sample average (in this case, the locally-weighted sample average). For a generic covariate \\(X\\), let\n\\[\\bar{x} = \\frac{\\sum_{i=1}^n w_i X_i}{\\sum_{i=1}^n w_i},\\]\nwhere \\(w_i\\) is the kernel weight for unit \\(i\\). Then estimate the model\n\\[Y_i = \\beta_0 + \\beta_1 R_i + \\beta_2 T_i + \\beta_3 R_i T_i + \\beta_4 \\left(X_i - \\bar{x}\\right) + \\beta_5 \\left(X_i - \\bar{x}\\right) T_i + \\epsilon_i, \\]\nThe coefficient on \\(T\\) now corresponds to the MATE. Here’s R code that implements this approach:\ncovariate_mat \u0026lt;- model.matrix(~ X1 + X2, data = RD_data)[,-1]\rcovariate_cent \u0026lt;- apply(covariate_mat, 2, function(x) x - weighted.mean(x, w = RD_data$wt))\rRD_data \u0026lt;- data.frame(subset(RD_data, select = c(R, Y, T)), covariate_cent)\rcovariates_cent \u0026lt;- list(\u0026quot;No covariates\u0026quot; = \u0026quot;\u0026quot;,\r\u0026quot;X1 only\u0026quot; = \u0026quot;| X1\u0026quot;,\r\u0026quot;X2 only\u0026quot; = \u0026quot;| X2B + X2C + X2D\u0026quot;,\r\u0026quot;X1 + X2\u0026quot; = \u0026quot;| X1 + X2B + X2C + X2D\u0026quot;)\rldply(covariates_cent, RD_est, mod = \u0026quot;Y ~ R\u0026quot;, .id = \u0026quot;Specification\u0026quot;)\r## Specification est se p\r## 1 No covariates 0.3034839 0.1132266 0.007355079\r## 2 X1 only 0.2913246 0.1125398 0.009635680\r## 3 X2 only 0.3107688 0.1071302 0.003721488\r## 4 X1 + X2 0.2981428 0.1065888 0.005155864\rThe estimates are now insensitive to the inclusion of the (properly centered) covariates, just as in the no-interactions model. In this example, the standard errors from the model that includes covariate-by-treatment interactions are just ever so slightly smaller than those from the model without interactions.\nWhy does this third approach work? I’ll explain more in a later post…\n\r","date":1453680000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1453680000,"objectID":"dfb1fef1a46fd0861d5f7b04b68422e9","permalink":"/rdd-interactions/","publishdate":"2016-01-25T00:00:00Z","relpermalink":"/rdd-interactions/","section":"post","summary":"NOTE (2019-09-24): This post pertains to version 0.56 of the rdd package. The problems described in this post have been corrected in version 0.57 of the package, which was posted to CRAN on 2016-03-14.","tags":["econometrics","Rstats","causal inference","regression discontinuity"],"title":"Regression discontinuities with covariate interactions in the rdd package","type":"post"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rI’ve recently been working with my colleague Beth Tipton on methods for cluster-robust variance estimation in the context of some common econometric models, focusing in particular on fixed effects models for panel data—or what statisticians would call “longitudinal data” or “repeated measures.” We have a new working paper, which you can find here.\nThe importance of using CRVE (i.e., “clustered standard errors”) in panel models is now widely recognized. Less widely recognized, perhaps, is the fact that standard methods for constructing hypothesis tests and confidence intervals based on CRVE can perform quite poorly in when you have only a limited number of independent clusters. What’s worse, it can be hard to determine what counts as a large-enough sample to trust standard CRVE methods, because the finite-sample behavior of the variance estimators and test statistics depends on the configuration of the covariates, not just the total sample size. For example, suppose you have state-level panel data from 50 states across 15 years and are trying to estimate the effect of some policy using difference-in-differences. If only 5 or 6 states have variation in the policy variable over time, then you’re almost certainly in small-sample territory. And the sample size issues can be subtler than this, too, as I’ll show below.\nOne solution to this problem is to use bias-reduced linearization (BRL), which was proposed by Bell and McCaffrey (2002) and has recently begun to receive attention from econometricians (e.g., Cameron \u0026amp; Miller, 2015; Imbens \u0026amp; Kolesar, 2015). The idea of BRL is to correct the bias of standard CRVE based on a working model, and then to use a degrees-of-freedom correction for Wald tests based on the bias-reduced CRVE. That may seem silly (after all, the whole point of CRVE is to avoid making distributional assumptions about the errors in your model), but it turns out that the correction can help quite a bit, even when the working model is wrong. The degrees-of-freedom correction is based on a standard Satterthwaite-type approximation, and also relies on the working model. There’s now quite a bit of evidence (which we review in the working paper) that BRL performs well even in samples with a small number of clusters.\nIn the working paper, we make two contributions to all this:\nOne problem with Bell and McCaffrey’s original formulation of BRL is that it does not work in some very common models for panel data, such as state-by-year panels that include fixed effects for each state and each year (Angrist and Pischke, 2009, point out this issue in their chapter on “non-standard standard error issues”). We propose a generalization of BRL that works even in models with arbitrary sets of fixed effects. We also address how to calculate the correction when the regression is fit using the “within” estimator, after absorbing the fixed effects.\rWe propose a method for testing hypotheses that involve multiple parameter constraints (which, in classical linear regression, you would test with an F statistic). The method involves approximating the distribution of the cluster-robust Wald statistic using Hotelling’s T-squared distribution (a multiple of an F distribution), where the denominator degrees of freedom are estimated based on the working model. For one-parameter constraints, the test reduces to a t-test with Satterthwaite degrees of freedom, and so it is a natural extension of the existing BRL methods.\r\rThe paper explains all this in greater detail, and also reports a fairly extensive simulation study that we designed to emuluate the types of covariates and study designs encountered in micro-economic applications. We’ve also got an R package that implements our methods (plus some other variants of CRVE, which I’ll explain some other time) in a fairly streamlined way. Here’s an example of how to use the package to do inference for a fixed effects panel data model.\nEffects of changing the minimum legal drinking age\rCarpenter and Dobkin (2011) analyzed the effects of changes in the minimum legal drinking age on rates of motor vehicle fatalies among 18-20 year olds, using state-level panel data from the National Highway Traffic Administration’s Fatal Accident Reporting System. In their new textbook, Angrist and Pischke (2014) developed a stylized example based on Carpenter and Dobkin’s work. I’ll use Angrist and Pischke’s data and follow their analysis, just because their data are easily available.\nThe outcome is the incidence of deaths in motor vehicle crashes among 18-20 year-olds (per 100,000 residents), for each state plus the District of Columbia, over the period 1970 to 1983. Tthere were several changes in the minimum legal drinking age during this time period, with variability in the timing of changes across states. Angrist and Pischke (following Carpenter and Dobkin) use a difference-in-differences strategy to estimate the effects of lowering the minimum legal drinking age from 21 to 18. A basic specification is\n\\[y_{it} = \\alpha_i + \\beta_t + \\gamma r_{it} + \\epsilon_{it},\\]\nfor \\(i\\) = 1,…,51 and \\(t\\) = 1970,…,1983. In this model, \\(\\alpha_i\\) is a state-specific fixed effect, \\(\\beta_t\\) is a year-specific fixed effect, \\(r_{it}\\) is the proportion of 18-20 year-olds in state \\(i\\) in year \\(t\\) who are legally allowed to drink, and \\(\\gamma\\) captures the effect of shifting the minimum legal drinking age from 21 to 18. Following Angrist and Pischke’s analysis, I’ll estimate this model both by (unweighted) OLs and by weighted least squares with weights corresponding to population size in a given state and year.\nUnweighted OLS\rThe following code does some simple data-munging and the estimates the model by OLS:\n# get data from Angrist \u0026amp; Pischke\u0026#39;s website\rlibrary(foreign)\rdeaths \u0026lt;- read.dta(\u0026quot;http://masteringmetrics.com/wp-content/uploads/2015/01/deaths.dta\u0026quot;, convert.factors=FALSE)\r# subset for 18-20 year-olds, deaths in motor vehicle accidents\rMVA_deaths \u0026lt;- subset(deaths, agegr==2 \u0026amp; dtype==2 \u0026amp; year \u0026lt;= 1983, select = c(-dtype, -agegr))\r# fit by OLS\rlm_unweighted \u0026lt;- lm(mrate ~ 0 + legal + factor(state) + factor(year), data = MVA_deaths)\rThe coef_test function from clubSandwich can then be used to test the hypothesis that changing the minimum legal drinking age has no effect on motor vehicle deaths in this cohort (i.e., \\(H_0: \\gamma = 0\\)). The usual way to test this is to cluster the standard errors by state, calculate the robust Wald statistic, and compare that to a standard normal reference distribution. The code and results are as follows:\n# devtools::install_github(\u0026quot;jepusto/clubSandwich\u0026quot;) # install the clubSandwich package\rlibrary(clubSandwich)\r## Registered S3 method overwritten by \u0026#39;clubSandwich\u0026#39;:\r## method from ## bread.mlm sandwich\rcoef_test(lm_unweighted, vcov = \u0026quot;CR1\u0026quot;, cluster = MVA_deaths$state, test = \u0026quot;z\u0026quot;)[\u0026quot;legal\u0026quot;,]\r## Coef. Estimate SE t-stat p-val (z) Sig.\r## 1 legal 7.59 2.38 3.19 0.00143 **\rOur work argues shows that a better approach would be to use the bias-reduced linearization CRVE, together with Satterthwaite degrees of freedom. In the package, the BRL adjustment is called “CR2” because it is directly analogous to the HC2 correction used in heteroskedasticity-robust variance estimation. When applied to an OLS model estimated by lm, the default working model is an identity matrix, which amounts to the “working” assumption that the errors are all uncorrelated and homoskedastic. Here’s how to apply this approach in the example:\ncoef_test(lm_unweighted, vcov = \u0026quot;CR2\u0026quot;, cluster = MVA_deaths$state, test = \u0026quot;Satterthwaite\u0026quot;)[\u0026quot;legal\u0026quot;,]\r## Coef. Estimate SE t-stat d.f. p-val (Satt) Sig.\r## 1 legal 7.59 2.43 3.12 25.7 0.00442 **\rThe Satterthwaite degrees of freedom will be different for each coefficient in the model, and so the coef_test function reports them right alongside the standard error. In this case, the degrees of freedom are about half of what you might expect, given that there are 51 clusters. The p-value for the CR2+Satterthwaite test is about twice as large as the p-value based on the standard Wald test. But of course, the coefficient is still statistically significant at conventional levels, and so the inference doesn’t change.\n\rUnweighted “within” estimation\rThe plm package in R provides another way to estimate the same model. It is convenient because it absorbs the state and year fixed effects before estimating the effect of legal. The clubSandwich package works with fitted plm models too:\nlibrary(plm)\rplm_unweighted \u0026lt;- plm(mrate ~ legal, data = MVA_deaths, effect = \u0026quot;twoways\u0026quot;, index = c(\u0026quot;state\u0026quot;,\u0026quot;year\u0026quot;))\rcoef_test(plm_unweighted, vcov = \u0026quot;CR1S\u0026quot;, cluster = \u0026quot;individual\u0026quot;, test = \u0026quot;z\u0026quot;)\r## Coef. Estimate SE t-stat p-val (z) Sig.\r## 1 legal 7.59 2.38 3.19 0.00143 **\rcoef_test(plm_unweighted, vcov = \u0026quot;CR2\u0026quot;, cluster = \u0026quot;individual\u0026quot;, test = \u0026quot;Satterthwaite\u0026quot;)\r## Coef. Estimate SE t-stat d.f. p-val (Satt) Sig.\r## 1 legal 7.59 2.43 3.12 25.7 0.00442 **\rFor the standard approach, I’ve used the variant of the correction factor implemented in Stata (called CR1S in the clubSandwich package), but this makes very little difference in the standard error or the p-value. For the test based on CR2, the degrees of freedom are slightly different than the results based on the fitted lm model, but the p-values agree to four decimals. The differences in degrees of freedom are due to numerical imprecision in the calculations.\n\rPopulation-weighted estimation\rThe difference between the standard method and the new method are not terribly exciting in the above example. However, things change quite a bit if the model is estimated using population weights. As far as I know, plm does not handle weighted least squares, and so I go back to fitting in lm with dummies for all the fixed effects.\nlm_weighted \u0026lt;- lm(mrate ~ 0 + legal + factor(state) + factor(year), weights = pop, data = MVA_deaths)\rcoef_test(lm_weighted, vcov = \u0026quot;CR1\u0026quot;, cluster = MVA_deaths$state, test = \u0026quot;z\u0026quot;)[\u0026quot;legal\u0026quot;,]\r## Coef. Estimate SE t-stat p-val (z) Sig.\r## 1 legal 7.5 2.16 3.47 \u0026lt;0.001 ***\rcoef_test(lm_weighted, vcov = \u0026quot;CR2\u0026quot;, cluster = MVA_deaths$state, test = \u0026quot;Satterthwaite\u0026quot;)[\u0026quot;legal\u0026quot;,]\r## Coef. Estimate SE t-stat d.f. p-val (Satt) Sig.\r## 1 legal 7.5 2.3 3.27 8.65 0.0103 *\rUsing population weights slightly reduces the point estimate of the effect, while also slightly increasing its precision. If you were following the standard approach, you would probably be happy with the weighted estimates and wouldn’t think about it any further. However, our recommended approach—using the CR2 variance estimator and Satterthwaite correction—produces a p-value that is an order of magnitude larger (though still significant at the conventional 5% level). The degrees of freedom are just 8.6—drastically smaller than would be expected based on the number of clusters.\nEven with weights, the coef_test function uses an “independent, homoskedastic” working model as a default for lm objects. In the present example, the outcome is a standardized rate and so a better assumption might be that the error variances are inversely proportional to population size. The following code uses this alternate working model:\ncoef_test(lm_weighted, vcov = \u0026quot;CR2\u0026quot;, cluster = MVA_deaths$state, target = 1 / MVA_deaths$pop, test = \u0026quot;Satterthwaite\u0026quot;)[\u0026quot;legal\u0026quot;,]\r## Coef. Estimate SE t-stat d.f. p-val (Satt) Sig.\r## 1 legal 7.5 2.2 3.41 13 0.00467 **\rThe new working model leads to slightly smaller standard errors and a couple of additional degrees of freedom, though we remain in small-sample territory.\n\rRobust Hausman test\rCRVE is also used in specification tests, as in the Hausman-type test for endogeneity of unobserved effects. Suppose that the model includes an additional control for the beer taxation rate in state \\(i\\) at time \\(t\\), denoted \\(s_{it}\\). The (unweighted) fixed effects model is then\n\\[y_{it} = \\alpha_i + \\beta_t + \\gamma_1 r_{it} + \\gamma_2 s_{it} + \\epsilon_{it},\\]\nand the estimated effects are as follows:\nlm_FE \u0026lt;- lm(mrate ~ 0 + legal + beertaxa + factor(state) + factor(year), data = MVA_deaths)\rcoef_test(lm_FE, vcov = \u0026quot;CR2\u0026quot;, cluster = MVA_deaths$state, test = \u0026quot;Satterthwaite\u0026quot;)[c(\u0026quot;legal\u0026quot;,\u0026quot;beertaxa\u0026quot;),]\r## Coef. Estimate SE t-stat d.f. p-val (Satt) Sig.\r## 1 legal 7.59 2.51 3.019 24.58 0.00583 **\r## 2 beertaxa 3.82 5.27 0.725 5.77 0.49663\rIf the unobserved effects \\(\\alpha_1,...,\\alpha_{51}\\) are uncorrelated with the regressors, then a more efficient way to estimate \\(\\gamma_1,\\gamma_2\\) is by weighted least squares, with weights based on a random effects model. However, if the unobserved effects covary with \\(\\mathbf{r}_i, \\mathbf{s}_i\\), then the random-effects estimates will be biased.\nWe can test for whether endogeneity is a problem by including group-centered covariates as additional regressors. Let \\(\\tilde{r}_{it} = r_{it} - \\frac{1}{T}\\sum_t r_{it}\\), with \\(\\tilde{s}_{it}\\) defined analogously. Now estimate the regression\n\\[y_{it} = \\beta_t + \\gamma_1 r_{it} + \\gamma_2 s_{it} + \\delta_1 \\tilde{r}_{it} + \\delta_2 \\tilde{s}_{it} + \\epsilon_{it},\\]\nwhich does not include state fixed effects. The parameters \\(\\delta_1,\\delta_2\\) represent the differences between the random effects and fixed effects estimands of \\(\\gamma_1, \\gamma_2\\). If these are both zero, then the random effects estimator is unbiased. Thus, the joint test for \\(H_0: \\delta_1 = \\delta_2 = 0\\) amounts to a test for non-endogeneity of the unobserved effects.\nFor efficiency, we should estimate this using weighted least squares, but OLS will work too:\nMVA_deaths \u0026lt;- within(MVA_deaths, {\rlegal_cent \u0026lt;- legal - tapply(legal, state, mean)[factor(state)]\rbeer_cent \u0026lt;- beertaxa - tapply(beertaxa, state, mean)[factor(state)]\r})\rlm_Hausman \u0026lt;- lm(mrate ~ 0 + legal + beertaxa + legal_cent + beer_cent + factor(year), data = MVA_deaths)\rcoef_test(lm_Hausman, vcov = \u0026quot;CR2\u0026quot;, cluster = MVA_deaths$state, test = \u0026quot;Satterthwaite\u0026quot;)[1:4,]\r## Coef. Estimate SE t-stat d.f. p-val (Satt) Sig.\r## 1 legal -9.180 7.62 -1.2042 24.94 0.2398 ## 2 beertaxa 3.395 9.40 0.3613 6.44 0.7295 ## 3 legal_cent 16.768 8.53 1.9665 33.98 0.0575 .\r## 4 beer_cent 0.424 9.25 0.0458 5.86 0.9650\rTo conduct a joint test on the centered covariates, we can use the Wald_test function. The usual way to test this hypothesis would be to use the CR1 variance estimator to calculate the robust Wald statistic, then use a \\(\\chi^2_2\\) reference distribution (or equivalently, compare a re-scaled Wald statistic to an \\(F(2,\\infty)\\) distribution). The Wald_test function reports the latter version:\nWald_test(lm_Hausman, constraints = c(\u0026quot;legal_cent\u0026quot;,\u0026quot;beer_cent\u0026quot;), vcov = \u0026quot;CR1\u0026quot;, cluster = MVA_deaths$state, test = \u0026quot;chi-sq\u0026quot;)\r## Test F d.f. p.val\r## chi-sq 2.93 Inf 0.0534\rThe test is just shy of significance at the 5% level. If we instead use the CR2 variance estimator and our newly proposed approximate F-test (which is the default in Wald_test), then we get:\nWald_test(lm_Hausman, constraints = c(\u0026quot;legal_cent\u0026quot;,\u0026quot;beer_cent\u0026quot;), vcov = \u0026quot;CR2\u0026quot;, cluster = MVA_deaths$state)\r## Test F d.f. p.val\r## HTZ 2.57 12.4 0.117\rThe low degrees of freedom of the test indicate that we’re definitely in small-sample territory and should not trust the asymptotic \\(\\chi^2\\) approximation.\n\r\rReferences\r\rAngrist, J. D., \u0026amp; Pischke, J.-S. (2009). Mostly harmless econometrics: An empiricist’s companion. Princeton, NJ: Princeton University Press.\rAngrist, J. D. and Pischke, J.-S. (2014). Mastering ’metrics: The Path from Cause to Effect. Princeton, NJ: Princeton University Press.\rBell, R. M., \u0026amp; McCaffrey, D. F. (2002). Bias reduction in standard errors for linear regression with multi-stage samples. Survey Methodology, 28(2), 169-181.\rCameron, A. C., \u0026amp; Miller, D. L. (2015). A practitioner’s guide to cluster-robust inference. URL: http://cameron.econ.ucdavis.edu/research/Cameron_Miller_JHR_2015_February.pdf\rCarpenter, C., \u0026amp; Dobkin, C. (2011). The minimum legal drinking age and public health. Journal of Economic Perspectives, 25(2), 133-156. doi:10.1257/jep.25.2.133\rImbens, G. W., \u0026amp; Kolesar, M. (2015). Robust standard errors in small samples: Some practical advice. URL: https://www.princeton.edu/~mkolesar/papers/small-robust.pdf\r\r\r","date":1452384000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1452384000,"objectID":"5303acb8d72bd103abada5ff398ed20a","permalink":"/clubsandwich-for-crve-fe/","publishdate":"2016-01-10T00:00:00Z","relpermalink":"/clubsandwich-for-crve-fe/","section":"post","summary":"I’ve recently been working with my colleague Beth Tipton on methods for cluster-robust variance estimation in the context of some common econometric models, focusing in particular on fixed effects models for panel data—or what statisticians would call “longitudinal data” or “repeated measures.","tags":["econometrics","fixed effects","sandwiches","Rstats"],"title":"Clustered standard errors and hypothesis tests in fixed effects models","type":"post"},{"authors":["Elizabeth Tipton","James E. Pustejovsky"],"categories":null,"content":"","date":1450137600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1450137600,"objectID":"f5c35df0a31ad1feb1ef7d83a260c1d8","permalink":"/publication/rve-for-meta-regression/","publishdate":"2015-12-15T00:00:00Z","relpermalink":"/publication/rve-for-meta-regression/","section":"publication","summary":"Meta-analyses often include studies that report multiple effect sizes based on a common pool of subjects or that report effect sizes from several samples that were treated with very similar research protocols. The inclusion of such studies introduces dependence among the effect size estimates. When the number of studies is large, robust variance estimation (RVE) provides a method for pooling dependent effects, even when information on the exact dependence structure is not available. When the number of studies is small or moderate, however, test statistics and confidence intervals based on RVE can have inflated Type I error. This article describes and investigates several small-sample adjustments to F-statistics based on RVE. Simulation results demonstrate that one such test, which approximates the test statistic using Hotelling’s T-squared distribution, is level-α and uniformly more powerful than the others. An empirical application demonstrates how results based on this test compare to the large-sample F-test.","tags":["meta-analysis","meta-regression","robust variance estimation","small-sample"],"title":"Small-sample adjustments for tests of moderators and model fit using robust variance estimation in meta-regression","type":"publication"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rYesterday evening I again had the pleasure of visiting Dr. Barnes’ pro seminar for first year students in Special Education, where I shared some of my work on research synthesis and meta-analysis of single-case research. Here are the slides from my presentation.\n","date":1448323200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1448323200,"objectID":"4b652bea0437e958616e68f5980003d3","permalink":"/sped-pro-sem-again/","publishdate":"2015-11-24T00:00:00Z","relpermalink":"/sped-pro-sem-again/","section":"post","summary":"Yesterday evening I again had the pleasure of visiting Dr. Barnes’ pro seminar for first year students in Special Education, where I shared some of my work on research synthesis and meta-analysis of single-case research.","tags":["meta-analysis","single-case design","effect size"],"title":"Special Education Pro-Sem","type":"post"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rSeveral students and colleagues have asked me recently about an issue that comes up in multivariate meta-analysis when some of the studies include multiple treatment groups and multiple outcome measures. In this situation, one might want to include effect size estimates for each treatment group and each outcome measure. In order to do so in fully multivariate meta-analysis, estimates of the covariances among all of these efffect sizes are needed. The covariance among effect sizes arises for several reasons:\nFor a single outcome measure, effect sizes based on different treatment groups compared to a common control group will be correlated because the same control group data is used to calculate both effect sizes;\rEffect sizes based on a single treatment group and a single control group, but for different outcome measures, will be correlated because the outcomes are measured on the same set of units (in both the treatment group and the control group).\rEffect sizes based on different treatment groups and for different outcome measures will be correlated because the outcomes are measured on the same set of units in the control group (though not in the treatment group).\r\rFor standardized mean difference (SMD) measures of effect size, formulas for the covariance are readily available for the first two cases (see e.g., Gleser \u0026amp; Olkin, 2009), but not for the third case. Below I review the formulas for the covariance between SMDs in the first two cases and provide a formula for the third case.\nNotation and Model\rSuppose that the experiment has a control group that includes \\(n_0\\) units and \\(T\\) treatment groups that include \\(n_1,...,n_T\\) units, respectively. Also suppose that \\(J\\) outcome measures are made on each unit in each group. The formulas below assume that the data follow a one-way MANOVA model. Let \\(y_{ijt}\\) denote the score for unit \\(i\\) on outcome \\(j\\) in group \\(t\\). Then I assume that\n\\[y_{ijt} = \\mu_{jt} + \\epsilon_{ijt},\\]\nwhere the errors are multi-variate normally distributed with mean zero, variance that can differ across outcome but not across treatment group, and correlation that is constant across treatment groups, i.e. \\(\\text{Var}\\left(\\epsilon_{ijt}\\right) = \\sigma^2_j\\), \\(\\text{Cov}\\left(\\epsilon_{ijt}, \\epsilon_{ikt} \\right) = \\rho_{jk}\\).\nDenote the mean score on outcome \\(j\\) in group \\(t\\) as \\(\\bar{y}_{jt}\\) and the standard deviation of the scores on outcome \\(j\\) in group \\(t\\) as \\(s_{jt}\\), both for \\(j = 1,...,J\\) and \\(t = 0,...,T\\) (with \\(t = 0\\) corresponding to the control group). Also required are estimates of the correlations among outcome measures 1 through \\(J\\), after partialling out differences between treatment groups. Let \\(r_{jk}\\) denote the partial correlation between measure \\(j\\) and measure \\(k\\), for \\(j = 1,...,J - 1\\) and \\(k = j + 1,...,J\\).\nWith multiple treatment groups, one might wonder how best to compute the standard deviation for purposes of scaling the treatment effect estimates. In their discussion of SMDs from multiple treatment studies, Gleser and Olkin (2009) assume (though they don’t actually state outright) that the standard deviation will be pooled across all \\(T + 1\\) groups. The pooled standard deviation for outcome \\(m\\) is calculated as the square root of the pooled variance,\n\\[s_{jP}^2 = \\frac{1}{N - T - 1} \\sum_{t=0}^T (n_t - 1)s_{jt}^2,\\]\nwhere \\(N = \\sum_{t=0}^T n_t\\). The standardized mean difference for treatment \\(t\\) on outcome \\(j\\) is then estimated as\n\\[d_{jt} = \\frac{\\bar{y}_{jt} - \\bar{y}_{j0}}{s_{jP}}\\]\nfor \\(j = 1,...,J\\) and \\(t = 1,...,T\\). The conventional estimate of the large-sample variance of \\(d_{jt}\\) is\n\\[\\text{Var}(d_{jt}) \\approx \\frac{1}{n_0} + \\frac{1}{n_t} + \\frac{d_{jt}^2}{2 (N - T - 1)}.\\]\n\rCovariances\rFor SMDs based on a common outcome measure and a common control group, but different treatment groups, the large-sample covariance between the effect size estimates can be estimated as\n\\[\\text{Cov}(d_{jt},d_{ju}) \\approx \\frac{1}{n_0} + \\frac{d_{jt} d_{ju}}{2 (N - T - 1)}.\\]\nThe above differs slightly from Gleser and Olkin (2009, Formula 19.19) because it uses the degrees of freedom \\(N - T - 1\\) in the denominator of the second term, rather than the total sample size. If the total sample size is larger relative to the number of treatment groups, the discrepancy should be minor.\nSMDs based on a single treatment group but for different outcome measures follow a structure that is essentially equivalent to what Gleser and Olkin (2009) call a “multiple-endpoint” study. The large-sample covariance between the effect size estimates can be estimated as\n\\[\\text{Cov}(d_{jt},d_{kt}) \\approx r_{jk} \\left(\\frac{1}{n_0} + \\frac{1}{n_t}\\right) + \\frac{r_{jk}^2 d_{jt} d_{kt}}{2 (N - T - 1)}\\]\n(cf. Gleser \u0026amp; Olkin, 2009, Formula 19.19). Note that if the degrees of freedom are large relative to \\(d_{jt}\\) and \\(d_{kt}\\), then the correlation between the effect sizes will be approximately equal to \\(\\text{Cor}(d_{jt},d_{kt}) \\approx r_{jk}\\).\nFinally, the large-sample covariance between SMDs based on different treatment groups and different outcome measures can be estimated as\n\\[\\text{Cov}(d_{jt},d_{ku}) \\approx \\frac{r_{jk}}{n_0} + \\frac{r_{jk}^2 d_{jt} d_{ku}}{2 (N - T - 1)}.\\]\nThis is similar to the previous formula, but does not include the term corresponding to the covariance between different outcome measures in a common treatment group.\nIf \\(r_{jj} = 1\\) is used for the correlation of an outcome measure with itself, all of the above formulas (including the variance of \\(d_{jt}\\)) can be expressed compactly as\n\\[\\text{Cov}(d_{jt},d_{ku}) \\approx r_{jk} \\left(\\frac{1}{n_0} + \\frac{I(t = u)}{n_t}\\right) + \\frac{r_{jk}^2 d_{jt} d_{ku}}{2 (N - T - 1)},\\]\nwhere \\(I(A)\\) is equal to one if \\(A\\) is true and equal to zero otherwise.\n\rReferences\r\rGleser, L. J., \u0026amp; Olkin, I. (2009). Stochastically dependent effect sizes. In H. Cooper, L. V. Hedges, \u0026amp; J. C. Valentine (Eds.), The Handbook of Research Synthesis and Meta-Analysis (2nd ed., pp. 357-376). New York, NY: Russell Sage Foundation.\r\r\r","date":1442448000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1442448000,"objectID":"4254d0a45a2d320d2bb1f066dadcdc5d","permalink":"/correlations-between-smds/","publishdate":"2015-09-17T00:00:00Z","relpermalink":"/correlations-between-smds/","section":"post","summary":"Several students and colleagues have asked me recently about an issue that comes up in multivariate meta-analysis when some of the studies include multiple treatment groups and multiple outcome measures.","tags":["meta-analysis","effect size","standardized mean difference","distribution theory"],"title":"Correlations between standardized mean differences","type":"post"},{"authors":["James E. Pustejovsky"],"categories":null,"content":"","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"d378e9964767eccc8e46a5aa6a7aa026","permalink":"/publication/measurement-comparable-effect-sizes/","publishdate":"2015-09-01T00:00:00Z","relpermalink":"/publication/measurement-comparable-effect-sizes/","section":"publication","summary":"Single-case research comprises a set of designs and methods for evaluating the effects of interventions, practices, or programs on individual cases, through comparison of outcomes measured at different points in time. Although there has long been interest in meta-analytic techniques for synthesizing single-case research, there has been little scrutiny of whether proposed effect sizes remain on a directly comparable metric when outcomes are measured using different operational procedures. Much of single-case research focuses on behavioral outcomes in free-operant contexts, which may be measured using a variety of different direct observation procedures. This article describes a suite of effect sizes for quantifying changes in free-operant behavior, motivated by an alternating renewal process model that allows measurement comparability to be established in precise terms. These effect size metrics have the advantage of comporting with how direct observation data are actually collected and summarized. Effect size estimators are proposed that are applicable when the behavior being measured remains stable within a given treatment condition. The methods are illustrated by 2 examples, including a re-analysis of a systematic review of the effects of choice-making opportunities on problem behavior.","tags":["alternating renewal process","effect size","response ratio","single-case design","behavioral observation"],"title":"Measurement-comparable effect sizes for single-case studies of free-operant behavior","type":"publication"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rI have been hearing quite a bit lately about how there have been an unusually large number of fatal automobile crashes in Austin this year, resulting in a total of 69 fatalities (as of August 19th). Terrence Henry (of KUT) recently did a story on this problem, and the City of Austin has convened the Vision Zero Task Force to figure out what policies to implement in order to prevent these deaths. KUT published an interactive map showing the locations of the fatal crashes and the Vision Zero Task Force put together a heat map showing the locations of crashes over the past five years.\nI was curious to understand more about how fatalities have changed over time, but the only data I could find on the time trends was this graphic on the Vision Zero website. After a bit of digging, I found that I could get annual data for Austin (2006-2014) and for Travis County (2003-2014) from the Texas Motor Vehicle Crash Statistics reports provided by TXDOT (though the data are trapped in pdfs). It’s also possible to get disaggregated data for the time period of 2010 through the present from the TXDOT CRIS database Public File Extract, which gets updated with new information as it comes in, and so will presumably be more current than the annual reports.\nThe chart below plots the annual number of fatal crashes, fatalities, crashes in which incapacitating injuries occurred, incapacitating injuries, and total crashes, for both Austin and Travis County as a whole. For the current year data, I plotted both the actual numbers (through July 31, 2015) and very simple projections. (Details on how I put the figures together are at the end of this post.)\nThe first thing you can see from these graphs is that the projected number of fatal crashes and total number of fatalities is substantially higher than in previous years. In contrast, the projected number of incapacitating crashes, number of incapacitating injuries, and total number of crashes all appear to be (very roughly) consistent with the linear trends from previous years. Taken together, these trends suggest that the proportion of crashes that are fatal is higher than would be expected. Here’s a graph of the fatality rate over time:\n2015 is clearly an outlier, though not as high as the proportion of fatal crashes in Travis County during 2003 and 2004 (unfortunately the data for Austin don’t go back that far). These years have higher proportions because there were fewer crashes overall in these initial years. Also note that Travis County as a whole has a higher fatality rate than the city of Austin, probably because the non-Austin roads in Travis county are larger and have higher speed limits.\nI think this second graph provides good justification for one of the principles of the Vision Zero task force, which is to focus on infrastructure improvements to improve the safety of the transformation system for all of the people who interact with it, including pedestrians—in short, to make our streets and roads safe for humans. The graphs suggests that there’s more to the increase in fatal crashes than just population growth, more than just increases in vehicle miles travelled.\nThere’s a big limitation to using annual data for this sort of simple, “eyeballing” sort of analysis. If there are seasonal patterns in automobile crashes overall (such as more crashes during the colder months) or, more specifically, in fatal crashes, then my simple back-of-the-envelope projections could be somewhat misleading. To develop more nuanced projections, I would need to get finer-grained data on when crashes occur. unfortunately, for some reasons the public version of the underlying data does not include dates of individual crashes. (This is rather perplexing, considering that the interface will let you query a date range, even down to a single day.) More to come if I can figure out how to get access to data with dates on it.\nMethods\rHere’s how I constructed these figures:\n\rThe data for 2003 through 2009 are drawn from the Crash Statistics reports, and the data for 2010 through 2015 are drawn from the CRIS Public File Extract.\rThere are some discrepancies between the annual reports and the CRIS database for the latter period, so I am assuming that the latter is more accurate. Curiously, the number’s don’t quite match the Vision Zero graphic either.\rThe incapacitating crashes and injuries numbers are only available starting in 2010, as prior to that time a different set of classifications was used that does not appear to be directly comparable.\rThe dashed lines in each graph represent estimated linear trends, fit by ordinary least squares.\rThe actual figures are plotted with circles. The projections for 2015 are plotted with triangles.\rFor 2015, the projections were calculated by multiplying the actual number by 12 / 7 = 1.71 because the actuals are based on 7 out of 12 months. (Using 211 out of 365 days leads to a very similar multiplier of 1.73.)\rThe underlying data (drawing from both the annual reports and the CRIS database) are available here.\rThe code to re-create the figures is available in this Gist.\r\r\r","date":1440028800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1440028800,"objectID":"c7e2668af4974041153cd8876fa58858","permalink":"/fatal-crashes-in-austin/travis-county/","publishdate":"2015-08-20T00:00:00Z","relpermalink":"/fatal-crashes-in-austin/travis-county/","section":"post","summary":"I have been hearing quite a bit lately about how there have been an unusually large number of fatal automobile crashes in Austin this year, resulting in a total of 69 fatalities (as of August 19th).","tags":["transportation"],"title":"Fatal crashes in Austin/Travis County","type":"post"},{"authors":["John A. Salsman","James E. Pustejovsky","Heather S. Jim","Alexis R. Munoz","Thomas V. Merluzzi","Logan George","Crystal L. Park","Suzanne C. Danhauer","Allen C. Sherman","Mallory A. Snyder","George Fitchett"],"categories":null,"content":"","date":1439164800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1439164800,"objectID":"c3e340618d62f7928b44552ac199c222","permalink":"/publication/religion-spirituality-mental-health/","publishdate":"2015-08-10T00:00:00Z","relpermalink":"/publication/religion-spirituality-mental-health/","section":"publication","summary":"Religion and spirituality (R/S) are patient-centered factors and often are resources for managing the emotional sequelae of the cancer experience. Studies investigating the correlation between R/S (eg, beliefs, experiences, coping) and mental health (eg, depression, anxiety, well being) in cancer have used very heterogeneous measures and have produced correspondingly inconsistent results. A meaningful synthesis of these findings has been lacking; thus, the objective of this review was to conduct a meta-analysis of the research on R/S and mental health. Four electronic databases were systematically reviewed, and 2073 abstracts met initial selection criteria. Reviewer pairs applied standardized coding schemes to extract indices of the correlation between R/S and mental health. In total, 617 effect sizes from 148 eligible studies were synthesized using meta-analytic generalized estimating equations, and subgroup analyses were performed to examine moderators of effects. The estimated mean correlation (Fisher z) was 0.19 (95% confidence interval [CI], 0.16-0.23), which varied as a function of R/S dimensions: affective R/S (z=0.38; 95% CI, 0.33-0.43), behavioral R/S (z=0.03; 95% CI, 20.02-0.08), cognitive R/S (z=0.10; 95% CI, 0.06-0.14), and ‘other’ R/S (z=0.08; 95% CI, 0.03-0.13). Aggregate, study-level demographic and clinical factors were not predictive of the relation between R/S and mental health. There was little indication of publication or reporting biases. The correlation between R/S and mental health generally was positive. The strength of that correlation was modest and varied as a function of the R/S dimensions and mental health domains assessed. The identification of optimal R/S measures and more sophisticated methodological approaches are needed to advance research.","tags":["cancer","correlation","mental health","meta-analysis","religion/spirituality","systematic review"],"title":"A meta-analytic approach to examine the relationship between religion/spirituality and mental health in cancer","type":"publication"},{"authors":["Allen C. Sherman","Thomas V. Merluzzi","James E. Pustejovsky","Crystal L. Park","Logan George","George Fitchett","Heather S. Jim","Alexis R. Munoz","Suzanne C. Danhauer","Mallory A. Snyder","John A. Salsman"],"categories":null,"content":"","date":1439164800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1439164800,"objectID":"00af87e3b8309ec639ce305037e4a534","permalink":"/publication/religion-spirituality-social-health/","publishdate":"2015-08-10T00:00:00Z","relpermalink":"/publication/religion-spirituality-social-health/","section":"publication","summary":"Religion and spirituality (R/S) play an important role in the daily lives of many cancer patients. There has been great interest in determining whether R/S factors are related to clinically relevant health outcomes. In this meta-analytic review, the authors examined associations between dimensions of R/S and social health (eg, social roles and relationships). A systematic search of the PubMed, PsycINFO, Cochrane Library, and Cumulative Index to Nursing and Allied Health Literature databases was conducted, and data were extracted by 4 pairs of investigators. Bivariate associations between specific R/S dimensions and social health outcomes were examined in a meta-analysis using a generalized estimating equation approach. In total, 78 independent samples encompassing 14,277 patients were included in the meta-analysis. Social health was significantly associated with overall R/S (Fisher z effect size = .20; P ","tags":["cancer","correlation","meta-analysis","religion/spirituality","social health","systematic review"],"title":"A meta-analytic review of religious or spiritual involvement and social health among cancer patients","type":"publication"},{"authors":["Heather S. Jim","James E. Pustejovsky","Crystal L. Park","Suzanne C. Danhauer","Allen C. Sherman","George Fitchett","Thomas V. Merluzzi","Alexis R. Munoz","Logan George","Mallory A. Snyder","John A. Salsman"],"categories":null,"content":"","date":1439164800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1439164800,"objectID":"64d9810febd7e812be210d328676a9c6","permalink":"/publication/religion-spirituality-physical-health/","publishdate":"2015-08-10T00:00:00Z","relpermalink":"/publication/religion-spirituality-physical-health/","section":"publication","summary":"Although religion/spirituality (R/S) is important in its own right for many cancer patients, a large body of research has examined whether R/S is also associated with better physical health outcomes. This literature has been characterized by heterogeneity in sample composition, measures of R/S, and measures of physical health. In an effort to synthesize previous findings, a meta-analysis of the relation between R/S and patient-reported physical health in cancer patients was performed. A search of PubMed, PsycINFO, the Cumulative Index to Nursing and Allied Health Literature, and the Cochrane Library yielded 2073 abstracts, which were independently evaluated by pairs of raters. The meta-analysis was conducted for 497 effect sizes from 101 unique samples encompassing more than 32,000 adult cancer patients. R/S measures were categorized into affective, behavioral, cognitive, and ‘other’ dimensions. Physical health measures were categorized into physical well-being, functional well-being, and physical symptoms. Average estimated correlations (Fisher z scores) were calculated with generalized estimating equations with robust variance estimation. Overall R/S was associated with overall physical health (z=0.153, P","tags":["cancer","correlation","meta-analysis","physical health","religion/spirituality","systematic review"],"title":"Religion, spirituality, and physical health in cancer patients: A meta-analysis","type":"publication"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rI’ve recently been working on small-sample correction methods for hypothesis tests in linear regression models with cluster-robust variance estimation. My colleague (and grad-schoolmate) Beth Tipton has developed small-sample adjustments for t-tests (of single regression coefficients) in the context of meta-regression models with robust variance estimation, and together we have developed methods for multiple-contrast hypothesis tests. We have an R package (called clubSandwich) that implements all this stuff, not only for meta-regression models but also for other models and contexts where cluster-robust variance estimation is often used.\nThe alpha-version of the package is currently available on Github. See the Github README for instructions on how to install it in R. Below I demonstrate how to use the package to get robust variance estimates, t-tests, and F-tests, all with small-sample corrections. The example uses a dataset of effect sizes from a Campbell Collaboration systematic review of dropout prevention programs, conducted by Sandra Jo Wilson and her colleagues.\nThe original analysis included a meta-regression with covariates that capture methodological, participant, and program characteristics. I’ll use a regression specification that is similar to Model III from Wilson et al. (2011), but treat the evaluator_independence and implementation_quality variables as categorical rather than interval-level; the original analysis clustered at the level of the sample (some studies reported results from multiple samples), whereas I will cluster at the study level.\rI fit the model two ways, first using the robumeta package and then using metafor.\nrobumeta model\roptions(width=150)\rlibrary(robumeta)\rlibrary(clubSandwich)\r## Registered S3 method overwritten by \u0026#39;clubSandwich\u0026#39;:\r## method from ## bread.mlm sandwich\rdata(dropoutPrevention)\rm3_robu \u0026lt;- robu(LOR1 ~ study_design + attrition + group_equivalence + adjusted\r+ outcome + evaluator_independence\r+ male_pct + white_pct + average_age\r+ implementation_quality + program_site + duration + service_hrs, data = dropoutPrevention, studynum = studyID, var.eff.size = varLOR, modelweights = \u0026quot;HIER\u0026quot;)\rprint(m3_robu)\r## RVE: Hierarchical Effects Model with Small-Sample Corrections ## ## Model: LOR1 ~ study_design + attrition + group_equivalence + adjusted + outcome + evaluator_independence + male_pct + white_pct + average_age + implementation_quality + program_site + duration + service_hrs ## ## Number of clusters = 152 ## Number of outcomes = 385 (min = 1 , mean = 2.53 , median = 1 , max = 30 )\r## Omega.sq = 0.24907 ## Tau.sq = 0.1024663 ## ## Estimate StdErr t-value dfs P(|t|\u0026gt;) 95% CI.L 95% CI.U Sig\r## 1 X.Intercept. 0.016899 0.615399 0.0275 16.9 0.97841541 -1.28228 1.31608 ## 2 study_designNon.random..non.matched -0.002626 0.185142 -0.0142 40.5 0.98875129 -0.37667 0.37141 ## 3 study_designRandomized -0.086872 0.140044 -0.6203 38.6 0.53869676 -0.37024 0.19650 ## 4 attrition 0.118889 0.247228 0.4809 15.5 0.63732597 -0.40666 0.64444 ## 5 group_equivalence 0.502463 0.195838 2.5657 28.7 0.01579282 0.10174 0.90318 **\r## 6 adjustedadjusted.data -0.322480 0.125413 -2.5713 33.8 0.01470796 -0.57741 -0.06755 **\r## 7 outcomeenrolled 0.097059 0.139842 0.6941 16.5 0.49727848 -0.19862 0.39274 ## 8 outcomegraduation 0.147643 0.134938 1.0942 30.2 0.28253825 -0.12786 0.42315 ## 9 outcomegraduation.ged 0.258034 0.169134 1.5256 16.3 0.14632629 -0.10006 0.61613 ## 10 evaluator_independenceIndirect..influential -0.765085 0.399109 -1.9170 6.2 0.10212896 -1.73406 0.20389 ## 11 evaluator_independencePlanning -0.920874 0.346536 -2.6574 5.6 0.04027061 -1.78381 -0.05794 **\r## 12 evaluator_independenceDelivery -0.916673 0.304303 -3.0124 4.7 0.03212299 -1.71432 -0.11903 **\r## 13 male_pct 0.167965 0.181538 0.9252 16.4 0.36824526 -0.21609 0.55202 ## 14 white_pct 0.022915 0.149394 0.1534 21.8 0.87950385 -0.28704 0.33287 ## 15 average_age 0.037102 0.027053 1.3715 21.2 0.18458247 -0.01913 0.09333 ## 16 implementation_qualityPossible.problems 0.411779 0.128898 3.1946 26.7 0.00358205 0.14714 0.67642 ***\r## 17 implementation_qualityNo.apparent.problems 0.658570 0.123874 5.3164 34.6 0.00000635 0.40699 0.91015 ***\r## 18 program_sitemixed 0.444384 0.172635 2.5741 28.6 0.01550504 0.09109 0.79768 **\r## 19 program_siteschool.classroom 0.426658 0.159773 2.6704 37.4 0.01115192 0.10303 0.75028 **\r## 20 program_siteschool..outside.of.classroom 0.262517 0.160519 1.6354 30.1 0.11236814 -0.06525 0.59028 ## 21 duration 0.000427 0.000873 0.4895 36.7 0.62736846 -0.00134 0.00220 ## 22 service_hrs -0.003434 0.005012 -0.6852 36.7 0.49752503 -0.01359 0.00672 ## ---\r## Signif. codes: \u0026lt; .01 *** \u0026lt; .05 ** \u0026lt; .10 *\r## ---\r## Note: If df \u0026lt; 4, do not trust the results\rNote that robumeta produces small-sample corrected standard errors and t-tests, and so there is no need to repeat those calculations with clubSandwich. The evaluator_independence variable has four levels, and it might be of interest to test whether the average program effects differ by the degree of evaluator independence. The null hypothesis in this case is that the 10th, 11th, and 12th regression coefficients are all equal to zero. A small-sample adjusted F-test for this hypothesis can be obtained as follows.\r(The vcov = \"CR2\" option means that the standard errors will be corrected using the bias-reduced linearization method proposed by McCaffrey, Bell, and Botts, 2001.)\nWald_test(m3_robu, constraints = 10:12, vcov = \u0026quot;CR2\u0026quot;)\r## Test F d.f. p.val\r## HTZ 2.78 16.8 0.0732\rBy default, the Wald_test function provides an F-type test with degrees of freedom estimated using the approximate Hotelling’s \\(T^2_Z\\) method. The test has less than 17 degrees of freedom, even though there are 152 independent studies in the data, and has a p-value of .07, so not-quite-significant at conventional levels. The low degrees of freedom are a consequence of the fact that one of the levels of evaluator independence has only a few effect sizes in it:\ntable(dropoutPrevention$evaluator_independence)\r## ## Independent Indirect, influential Planning Delivery ## 6 33 43 303\r\rmetafor model\rOur package also works with models fit using the metafor package. Here I re-fit the same regression specification, but use REML to estimate the variance components (robumeta uses a method-of-moments estimator) and use a somewhat different weighting scheme than that used in robumeta.\nlibrary(metafor)\rm3_metafor \u0026lt;- rma.mv(LOR1 ~ study_design + attrition + group_equivalence + adjusted\r+ outcome + evaluator_independence\r+ male_pct + white_pct + average_age\r+ implementation_quality + program_site + duration + service_hrs, V = varLOR, random = list(~ 1 | studyID, ~ 1 | studySample),\rdata = dropoutPrevention)\rsummary(m3_metafor)\r## ## Multivariate Meta-Analysis Model (k = 385; method: REML)\r## ## logLik Deviance AIC BIC AICc ## -489.0357 978.0714 1026.0714 1119.5371 1029.6217 ## ## Variance Components:\r## ## estim sqrt nlvls fixed factor ## sigma^2.1 0.2274 0.4769 152 no studyID ## sigma^2.2 0.1145 0.3384 317 no studySample ## ## Test for Residual Heterogeneity:\r## QE(df = 363) = 1588.4397, p-val \u0026lt; .0001\r## ## Test of Moderators (coefficients 2:22):\r## QM(df = 21) = 293.8694, p-val \u0026lt; .0001\r## ## Model Results:\r## ## estimate se zval pval ci.lb ci.ub ## intrcpt 0.5296 0.7250 0.7304 0.4651 -0.8915 1.9506 ## study_designNon-random, non-matched -0.0494 0.1722 -0.2871 0.7741 -0.3870 0.2881 ## study_designRandomized 0.0653 0.1628 0.4010 0.6884 -0.2538 0.3843 ## attrition -0.1366 0.2429 -0.5623 0.5739 -0.6126 0.3395 ## group_equivalence 0.4071 0.1573 2.5877 0.0097 0.0988 0.7155 ** ## adjustedadjusted data -0.3581 0.1532 -2.3371 0.0194 -0.6585 -0.0578 * ## outcomeenrolled -0.2831 0.0771 -3.6709 0.0002 -0.4343 -0.1320 *** ## outcomegraduation -0.0913 0.0657 -1.3896 0.1646 -0.2201 0.0375 ## outcomegraduation/ged 0.6983 0.0805 8.6750 \u0026lt;.0001 0.5406 0.8561 *** ## evaluator_independenceIndirect, influential -0.7530 0.4949 -1.5214 0.1282 -1.7230 0.2171 ## evaluator_independencePlanning -0.7700 0.4869 -1.5814 0.1138 -1.7242 0.1843 ## evaluator_independenceDelivery -1.0016 0.4600 -2.1774 0.0294 -1.9033 -0.1000 * ## male_pct 0.1021 0.1715 0.5951 0.5518 -0.2341 0.4382 ## white_pct 0.1223 0.1804 0.6777 0.4979 -0.2313 0.4758 ## average_age 0.0061 0.0291 0.2091 0.8344 -0.0509 0.0631 ## implementation_qualityPossible problems 0.4738 0.1609 2.9445 0.0032 0.1584 0.7892 ** ## implementation_qualityNo apparent problems 0.6318 0.1471 4.2965 \u0026lt;.0001 0.3436 0.9201 *** ## program_sitemixed 0.3289 0.2413 1.3631 0.1729 -0.1440 0.8019 ## program_siteschool classroom 0.2920 0.1736 1.6821 0.0926 -0.0482 0.6321 . ## program_siteschool, outside of classroom 0.1616 0.1898 0.8515 0.3945 -0.2104 0.5337 ## duration 0.0013 0.0009 1.3423 0.1795 -0.0006 0.0031 ## service_hrs -0.0003 0.0047 -0.0654 0.9478 -0.0096 0.0090 ## ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rmetafor produces model-based standard errors, t-tests, and confidence intervals. The coef_test function from clubSandwich will calculate robust standard errors and robust t-tests for each of the coefficients:\ncoef_test(m3_metafor, vcov = \u0026quot;CR2\u0026quot;)\r## Coef. Estimate SE t-stat d.f. p-val (Satt) Sig.\r## 1 intrcpt 0.529569 0.724851 0.7306 20.08 0.47347 ## 2 study_designNon-random, non-matched -0.049434 0.204152 -0.2421 58.42 0.80952 ## 3 study_designRandomized 0.065272 0.149146 0.4376 53.17 0.66342 ## 4 attrition -0.136575 0.306429 -0.4457 10.52 0.66485 ## 5 group_equivalence 0.407108 0.210917 1.9302 23.10 0.06595 .\r## 6 adjustedadjusted data -0.358124 0.136132 -2.6307 43.20 0.01176 *\r## 7 outcomeenrolled -0.283124 0.237199 -1.1936 7.08 0.27108 ## 8 outcomegraduation -0.091295 0.091465 -0.9981 9.95 0.34188 ## 9 outcomegraduation/ged 0.698328 0.364882 1.9138 8.02 0.09188 .\r## 10 evaluator_independenceIndirect, influential -0.752994 0.447670 -1.6820 6.56 0.13929 ## 11 evaluator_independencePlanning -0.769968 0.403898 -1.9063 6.10 0.10446 ## 12 evaluator_independenceDelivery -1.001648 0.355989 -2.8137 4.89 0.03834 *\r## 13 male_pct 0.102055 0.148410 0.6877 9.68 0.50782 ## 14 white_pct 0.122255 0.141470 0.8642 16.88 0.39961 ## 15 average_age 0.006084 0.033387 0.1822 15.79 0.85772 ## 16 implementation_qualityPossible problems 0.473789 0.148660 3.1871 22.44 0.00419 **\r## 17 implementation_qualityNo apparent problems 0.631842 0.138073 4.5761 28.68 \u0026lt; 0.001 ***\r## 18 program_sitemixed 0.328941 0.196848 1.6710 27.47 0.10607 ## 19 program_siteschool classroom 0.291952 0.146014 1.9995 42.70 0.05195 .\r## 20 program_siteschool, outside of classroom 0.161640 0.171700 0.9414 29.27 0.35420 ## 21 duration 0.001270 0.000978 1.2988 31.96 0.20332 ## 22 service_hrs -0.000309 0.004828 -0.0641 49.63 0.94915\rNote that coef_test assumed that it should cluster based on studyID, which is the outer-most random effect in the metafor model. This can also be specified explicitly by including the option cluster = dropoutPrevention$studyID in the call.\nThe F-test for degree of evaluator independence uses the same syntax as before:\nWald_test(m3_metafor, constraints = 10:12, vcov = \u0026quot;CR2\u0026quot;)\r## Test F d.f. p.val\r## HTZ 2.71 18.3 0.0753\rDespite some differences in weighting schemes, the p-value is very close to the result obtained using robumeta.\n\r","date":1436486400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1436486400,"objectID":"9d49343d4c58d37f29a56841677d6cfe","permalink":"/clubsandwich-for-rve-meta-analysis/","publishdate":"2015-07-10T00:00:00Z","relpermalink":"/clubsandwich-for-rve-meta-analysis/","section":"post","summary":"I’ve recently been working on small-sample correction methods for hypothesis tests in linear regression models with cluster-robust variance estimation. My colleague (and grad-schoolmate) Beth Tipton has developed small-sample adjustments for t-tests (of single regression coefficients) in the context of meta-regression models with robust variance estimation, and together we have developed methods for multiple-contrast hypothesis tests.","tags":["meta-analysis","robust variance estimation","sandwiches","Rstats"],"title":"The clubSandwich package for meta-analysis with RVE","type":"post"},{"authors":[],"categories":null,"content":"","date":1436313600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1436313600,"objectID":"d3885aeaf03db755a2c90020042b3055","permalink":"/talk/srsm-2015-small-sample-adjustments/","publishdate":"2018-04-15T08:15:00Z","relpermalink":"/talk/srsm-2015-small-sample-adjustments/","section":"talk","summary":"","tags":[],"title":"Small-sample adjustments for multiple-contrast hypothesis tests of meta-regressions using robust variance estimation","type":"talk"},{"authors":["James E. Pustejovsky","Daniel M. Swan"],"categories":null,"content":"","date":1434672000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1434672000,"objectID":"6573bb277639c0405e8d744b68ef89be","permalink":"/publication/four-methods-for-pir/","publishdate":"2015-06-19T00:00:00Z","relpermalink":"/publication/four-methods-for-pir/","section":"publication","summary":"Partial interval recording (PIR) is a procedure for collecting measurements during direct observation of behavior. It is used in several areas of educational and psychological research, particularly in connection with single-case research. Measurements collected using partial interval recording suffer from construct invalidity because they are not readily interpretable in terms of the underlying characteristics of the behavior. Using an alternating renewal process model for the behavior under observation, we demonstrate that ignoring the construct invalidity of PIR data can produce misleading inferences, such as inferring that an intervention reduces the prevalence of an undesirable behavior when in fact it has the opposite effect. We then propose four different methods for analyzing PIR summary measurements, each of which can be used to draw inferences about interpretable behavioral parameters. We demonstrate the methods by applying them to data from two single-case studies of problem behavior.","tags":["alternating renewal process","effect size","response ratio","single-case design","behavioral observation"],"title":"Four methods for analyzing partial interval recording data, with application to single-case research","type":"publication"},{"authors":[],"categories":null,"content":"","date":1429401600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1429401600,"objectID":"1ebbe83dc2060381febebc20586f8379","permalink":"/talk/aera-2015-operational-sensitivities/","publishdate":"2018-04-15T08:15:00Z","relpermalink":"/talk/aera-2015-operational-sensitivities/","section":"talk","summary":"","tags":[],"title":"Operational sensitivities of non-overlap effect sizes for single-case experimental designs","type":"talk"},{"authors":[],"categories":null,"content":"","date":1429315200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1429315200,"objectID":"3a5b69bd6ceec0e94900a8c3856ed318","permalink":"/talk/aera-2015-small-sample-adjustments/","publishdate":"2018-04-15T08:15:00Z","relpermalink":"/talk/aera-2015-small-sample-adjustments/","section":"talk","summary":"","tags":[],"title":"Small-sample adjustments for F-tests using robust variance estimation in meta-regression","type":"talk"},{"authors":[],"categories":null,"content":"","date":1429142400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1429142400,"objectID":"958b3fbda694dedb9612db94b8ce1aa9","permalink":"/talk/aera-2015-observation-procedures/","publishdate":"2018-04-15T08:15:00Z","relpermalink":"/talk/aera-2015-observation-procedures/","section":"talk","summary":"","tags":[],"title":"Observation procedures and Markov Chain models for estimating the prevalence and incidence of a state behavior","type":"talk"},{"authors":[],"categories":null,"content":"","date":1425600000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1425600000,"objectID":"4eefc0b62a61a1c284eb2def661ebcd8","permalink":"/talk/sree-2015-small-sample-adjustments/","publishdate":"2018-04-15T08:15:00Z","relpermalink":"/talk/sree-2015-small-sample-adjustments/","section":"talk","summary":"","tags":[],"title":"Small-sample adjustments for tests of moderators and model fit using robust variance estimation in meta-regression","type":"talk"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rMy article with Daniel Swan, “Four methods for analyzing partial interval recording data, with application to single-case research” has been accepted for publication in Multivariate Behavioral Research. In an extension of my earlier paper on measurement-comparable effect sizes for single-case studies, this article provides some approaches to estimating effect sizes from single-case studies that use partial interval or whole interval recording to measure behavioral outcomes. The full abstract is below. Preprint and supporting materials are available. R functions that implement the proposed methods are available in the package ARPobservation.\nPartial interval recording is a procedure for collecting measurements during direct observation of behavior. It is used in several areas of educational and psychological research, particularly in connection with single-case research. Measurements collected using partial interval recording suffer from construct invalidity because they are not readily interpretable in terms of the underlying characteristics of the behavior. Using an alternating renewal process model for the behavior under observation, we demonstrate that ignoring the construct invalidity of PIR data can produce misleading inferences, such as inferring that an intervention reduces the prevalence of an undesirable behavior when in fact it has the opposite effect. We then propose four different methods for analyzing PIR summary measurements, each of which can be used to draw inferences about interpretable behavioral parameters. We demonstrate the methods by applying them to data from two single-case studies of problem behavior.\n","date":1423612800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1423612800,"objectID":"c2297ead7976659c5ed4e903d3387ded","permalink":"/four-methods-for-analyzing-pir-data/","publishdate":"2015-02-11T00:00:00Z","relpermalink":"/four-methods-for-analyzing-pir-data/","section":"post","summary":"My article with Daniel Swan, “Four methods for analyzing partial interval recording data, with application to single-case research” has been accepted for publication in Multivariate Behavioral Research. In an extension of my earlier paper on measurement-comparable effect sizes for single-case studies, this article provides some approaches to estimating effect sizes from single-case studies that use partial interval or whole interval recording to measure behavioral outcomes.","tags":["behavioral observation","single-case design"],"title":"New article: Four methods for analyzing PIR data","type":"post"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rUPDATED 10/2/2016 after posting the package to CRAN\nHere are step-by-step instructions on how to download and install the scdhlm package for R. You’ll need to have a copy of R installed. There are two ways to do the installation: through the Comprehensive R Archive Network (CRAN) or from the source code on Github. I describe each approach in turn.\nOption 1: Via CRAN\rGo via CRAN to install the most recent stable version of the package. Type the following commands at the R prompt:\ninstall.packages(\u0026quot;scdhlm\u0026quot;)\rlibrary(scdhlm)\r\rOption 2: Via Github\rGo via Github to get the latest development version of the package. For this option, you will first need to install the devtools package:\ninstall.packages(\u0026quot;devtools\u0026quot;)\rOnce you have successfully installed this package, type the following:\nlibrary(devtools)\rinstall_github(\u0026quot;jepusto/scdhlm\u0026quot;)\rlibrary(scdhlm)\r\rFurther instructions\rYou’ll only need to do the installation once. Once you’ve got the package installed, type the following in order to access the package within an R session: library(scdhlm).\nTo open the package documentation, type package?scdhlm. To access the documentation for an individual function in this package, just type ? followed by the name of the function. For instance, one of the main functions in the package is called g_REML; to access its documentation, type ?g_REML.\n\rweb-interface for calculating effect sizes\rThe package includes an interactive app (written with shiny) for calculating design-comparable standardized mean differences. To run this app on your computer, you will first need to install RStudio (if you don’t already have it). Then ensure that you have the shiny, markdown, and ggplot2 packages installed by running the following:\ninstall.packages(\u0026quot;shiny\u0026quot;)\rinstall.packages(\u0026quot;markdown\u0026quot;)\rinstall.packages(\u0026quot;ggplot2\u0026quot;)\rFinally, open the app by typing the following at the prompt within RStudio:\nlibrary(scdhlm)\rshine_scd()\rThe app should now open in your web browser.\n\r","date":1413676800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1413676800,"objectID":"4992256430de5a3aea40eb2b32112f84","permalink":"/getting-started-with-scdhlm/","publishdate":"2014-10-19T00:00:00Z","relpermalink":"/getting-started-with-scdhlm/","section":"post","summary":"UPDATED 10/2/2016 after posting the package to CRAN\nHere are step-by-step instructions on how to download and install the scdhlm package for R. You’ll need to have a copy of R installed.","tags":["single-case design","design-comparable SMD","Rstats"],"title":"Getting started with scdhlm","type":"post"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rMy article with Chris Runyon, titled “Alternating renewal process models for behavioral observation: Simulation methods, software , and validity illustrations” has been published in Behavioral Disorders. The abstract is below. Postprint available here. All of the examples in the paper are available in the R package ARPobservation.\n\rDirect observation recording procedures produce reductive summary measurements of an underlying stream of behavior. Previous methodological studies of these recording procedures have employed simulation methods for generating random behavior streams, many of which amount to special cases of a statistical model known as the alternating renewal process. This paper describes the alternating renewal process model in its general form, demonstrates how it provides an organizing framework for most past simulation research on direct observation procedures, and introduces a freely available software package that implements the model. The software can be used to simulate behavior streams as well as data from many common recording procedures, including continuous recording, momentary time sampling, event counting, and interval recording procedures. Several examples illustrate how the software can be used to study the validity and reliability of direct observation data and to develop measurement strategies during the planning phases of empirical studies.\n\r","date":1413417600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1413417600,"objectID":"8997df27ee94af17428818e7a4afa8cc","permalink":"/new-article-alternating-renewal-process-models-for-behavioral-observation/","publishdate":"2014-10-16T00:00:00Z","relpermalink":"/new-article-alternating-renewal-process-models-for-behavioral-observation/","section":"post","summary":"My article with Chris Runyon, titled “Alternating renewal process models for behavioral observation: Simulation methods, software , and validity illustrations” has been published in Behavioral Disorders. The abstract is below.","tags":["behavioral observation","alternating renewal process","Rstats"],"title":"New article: Alternating renewal process models for behavioral observation","type":"post"},{"authors":["James E. Pustejovsky","Larry V. Hedges","William R. Shadish"],"categories":null,"content":"","date":1412121600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1412121600,"objectID":"35740f500912db17c523acb60749bca3","permalink":"/publication/design-comparable-effect-sizes/","publishdate":"2014-10-01T00:00:00Z","relpermalink":"/publication/design-comparable-effect-sizes/","section":"publication","summary":"In single-case research, the multiple baseline design is a widely used approach for evaluating the effects of interventions on individuals. Multiple baseline designs involve repeated measurement of outcomes over time and the controlled introduction of a treatment at different times for different individuals. This article outlines a general framework for defining effect sizes in multiple baseline designs that are directly comparable to the standardized mean difference from a between-subjects randomized experiment. The target, design-comparable effect size parameter can be estimated using restricted maximum likelihood together with a small sample correction analogous to Hedges’s g. The approach is demonstrated using hierarchical linear models that include baseline time trends and treatment-by-time interactions. A simulation compares the performance of the proposed estimator to that of an alternative, and an application illustrates the model-fitting process.","tags":["single-case design","design-comparable SMD","effect size","hierarchical models"],"title":"Design-comparable effect sizes in multiple baseline designs: A general modeling framework","type":"publication"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rPartial interval recording (PIR) is one method for recording data during systematic direct observation of a behavior. While a convenient method, PIR has the key drawback that it systematically over-states the prevalence of the behavior under observation. When used in single-case research to measure changes in behavior resulting from intervention, the systematic bias in PIR data can lead to deceptive results, such as inferring that an intervention reduces the prevalence of a problem behavior when in fact the opposite is true.\nWith my student Daniel Swan, I am currently working on developing methods for analyzing partial interval recording data that take its systematic bias into account. Some of these methods can be used with session-level summary PIR measurements (i.e., the percentage of intervals with the behavior), which are easily extracted from published single-case graphs. See here for the paper describing these methods.\nWe are now turning our attention to methods that use the finer-grained, interval-by-interval PIR data to obtain better estimates of the prevalence and incidence (frequency per unit time) of the behavior. For instance, if the observer uses 15 s partial interval recording, with 5 s for recording, for a 20 min session, this is a total of 60 intervals, for each of which the presence or absence of the behavior is recorded. The methods we’re working on make use of the full set of 60 ordered data points from the session. The general idea our work is similar to the post-hoc correction techniques proposed by Suen \u0026amp; Ary (1986), but we think we can greatly improve on their proposal.\nTo fully validate the methods we are developing, we need to test them out on real-world data. If you, dear reader, have access to PIR data and would be willing to share it with us, I would love to hear from you. We are looking specifically for:\n\rFine-grained (interval-by-interval) PIR data collected in real research contexts, such as single-case studies or observational studies involving students with behavioral disorders, children with autism-spectrum disorders, etc.\rAlternately, continuously-recorded behavioral observation data (e.g., as collected through MOOSES, the Direct Assessment Tracking Application, or ProCoderDV) that we could then convert into PIR data.\rAlong with either type of behavioral observation data, a brief (or lengthier) description of the participant(s) whose behavior was measured and the context in which the measurements were collected.\r\rWe can work with data in whatever format you might be willing to provide–whether that means photo-copied, paper observation forms, an Excel workbook, or a bunch of ProCoderDV data files. In return for sharing data, we will share with you the examples that we develop based on the data, which could also provide a basis for further collaboration. If you are interested in seeing your data analyzed and helping to advance this methodological work, please contact me.\n","date":1409702400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1409702400,"objectID":"bbe9886e835171159d590cc40825e6fd","permalink":"/wanted-pir-data/","publishdate":"2014-09-03T00:00:00Z","relpermalink":"/wanted-pir-data/","section":"post","summary":"Partial interval recording (PIR) is one method for recording data during systematic direct observation of a behavior. While a convenient method, PIR has the key drawback that it systematically over-states the prevalence of the behavior under observation.","tags":["behavioral observation"],"title":"Wanted: PIR data","type":"post"},{"authors":["James E. Pustejovsky","Christopher Runyon"],"categories":null,"content":"","date":1406851200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1406851200,"objectID":"940b85c936b016bc1b9b1dde2375b5f7","permalink":"/publication/arp-for-behavioral-observation/","publishdate":"2014-08-01T00:00:00Z","relpermalink":"/publication/arp-for-behavioral-observation/","section":"publication","summary":"Direct observation recording procedures produce reductive summary measurements of an underlying stream of behavior. Previous methodological studies of these recording procedures have employed simulation methods for generating random behavior streams, many of which amount to special cases of a statistical model known as the alternating renewal process. This paper describes the alternating renewal process model in its general form, demonstrates how it provides an organizing framework for most past simulation research on direct observation procedures, and introduces a freely available software package that implements the model. The software can be used to simulate behavior streams as well as data from many common recording procedures, including continuous recording, momentary time sampling, event counting, and interval recording procedures. Several examples illustrate how the software can be used to study the validity and reliability of direct observation data and to develop measurement strategies during the planning phases of empirical studies.","tags":["single-case design","behavioral observation","alternating renewal process"],"title":"Alternating renewal process models for behavioral observation: Simulation methods and validity implications","type":"publication"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rMy article with Larry Hedges and Will Shadish, titled “Design-comparable effect sizes in multiple baseline designs: A general modeling framework” has been accepted at Journal of Educational and Behavioral Statistics. The abstract is below. Here’s the article at the journal website. Postprint and supporting materials are available. An R package that implements the proposed methods is available here.\nIn single-case research, the multiple baseline design is a widely used approach for evaluating the effects of interventions on individuals. Multiple baseline designs involve repeated measurement of outcomes over time and the controlled introduction of a treatment at different times for different individuals. This article outlines a general approach for defining effect sizes in multiple baseline designs that are directly comparable to the standardized mean difference from a between-subjects randomized experiment. The target, design-comparable effect size parameter can be estimated using restricted maximum likelihood together with a small-sample correction analogous to Hedges’ g. The approach is demonstrated using hierarchical linear models that include baseline time trends and treatment-by-time interactions. A simulation compares the performance of the proposed estimator to that of an alternative, and an application illustrates the model-fitting process.\n","date":1405814400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1405814400,"objectID":"d453ead4d5d83f3826fd3c1fa5763b04","permalink":"/design-comparable-effect-sizes-in-multiple-baseline-designs/","publishdate":"2014-07-20T00:00:00Z","relpermalink":"/design-comparable-effect-sizes-in-multiple-baseline-designs/","section":"post","summary":"My article with Larry Hedges and Will Shadish, titled “Design-comparable effect sizes in multiple baseline designs: A general modeling framework” has been accepted at Journal of Educational and Behavioral Statistics.","tags":["single-case design","effect size","hierarchical models","design-comparable SMD"],"title":"New article: Design-comparable effect sizes in multiple baseline designs: A general modeling framework","type":"post"},{"authors":["William R. Shadish","Larry V. Hedges","James E. Pustejovsky","David Rindskopf","Jonathan G. Boyajian","Kristynn J. Sullivan"],"categories":null,"content":"","date":1405641600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1405641600,"objectID":"2d4efd749e36b118eff4355fc29f524e","permalink":"/publication/analyzing-scd-hopes-and-fears/","publishdate":"2014-07-18T00:00:00Z","relpermalink":"/publication/analyzing-scd-hopes-and-fears/","section":"publication","summary":"New approaches to the analyses of single-case designs are proliferating, which some single-case design researchers welcome and others view with skepticism. In this chapter we describe some of the analyses that we have been exploring, all of which can be conceptualized as versions of hierarchical models as a unifying framework. The approaches include a d-statistic for the (AB)k design that estimates the same parameter as the usual between-groups d-statistic, Bayesian approaches to the same and similar models, hierarchical generalized linear models that model outcomes as binomial or Poisson rather than the usual assumptions of normality, and semiparametric generalized additive models that allow diagnosis of trend and linearity. Throughout, we illustrate the analyses using a common example and show how the different analyses provide different insights into the data. We conclude with a discussion of potential criticisms and skepticism expressed by some researchers about such analyses, along with reasons why the field is increasingly likely to develop and use such analyses despite the criticisms.","tags":["single-case design","design-comparable SMD","effect size","hierarchical models"],"title":"Analyzing single-case designs: d, G, hierarchical models, Bayesian estimators, generalized additive models, and the hopes and fears of researchers about analyses","type":"publication"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rVersion 1.0 of the ARPobservation package is now available on the Comprehensive R Archive Network. This makes it even easier to install. Here’s the package description:\nARPobservation: Tools for simulating different methods of observing behavior based on alternating renewal processes\nARPobservation provides a set of tools for simulating data based on direct observation of behavior. It works by first simulating a behavior stream based on an alternating renewal process, given specified distributions of event durations and interim times. Different procedures for recording data can then be applied to the simulated behavior stream. Currently, functions are provided for the following recording methods: continuous duration recording, event counting, momentary time sampling, partial interval recording, and whole interval recording.\n","date":1401494400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1401494400,"objectID":"f3198771e3a12e7a95b5396facacda7f","permalink":"/arpobservation-now-on-cran/","publishdate":"2014-05-31T00:00:00Z","relpermalink":"/arpobservation-now-on-cran/","section":"post","summary":"Version 1.0 of the ARPobservation package is now available on the Comprehensive R Archive Network. This makes it even easier to install. Here’s the package description:\nARPobservation: Tools for simulating different methods of observing behavior based on alternating renewal processes","tags":["behavioral observation","alternating renewal process","Rstats"],"title":"ARPobservation now on CRAN","type":"post"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rIn an earlier post about sandwich standard errors for multi-variate meta-analysis, I mentioned that Beth Tipton has recently proposed small-sample corrections for the covariance estimators and t-tests, based on the bias-reduced linearization approach of McCaffrey, Bell, and Botts (2001).\rYou can find her forthcoming paper on the adjustments here.\rMy understanding is that these small-sample corrections are important because the uncorrected sandwich estimators can lead to under-statement of uncertainty and inflated type I error rates when a given meta-regression coefficient is estimated from only a small or moderately sized sample of independent studies (or clusters of studies).\rMoreover, it can be difficult to determine exactly when you have a large enough sample to trust the uncorrected sandwiches.\nI wanted to try out these small-sample corrected sandwich estimators for a meta-analyses project that I’m working on. Beth and one of her students have written an R package called robumeta that implements the sandwich covariance estimator and small-sample corrections as described in her paper.\rHowever, for my project I want to use the metafor package, which doesn’t provide these methods.\rI’ve therefore created a set of functions that implement the sandwich covariance estimators and small-sample corrections for models estimated using the rma.mv function in metafor.\rHere is the complete code. Sorry, there’s no further documentation at the moment (beyond the rest of this post).\nConsistency with robumeta\rIn order to check that the functions are correct, I compared the results generated by robumeta with the results from metafor plus my functions. Here’s one example (I looked at a few others as well). First, the robumeta results:\nlibrary(grid)\rlibrary(robumeta)\rdata(hierdat)\rrobu_hier \u0026lt;- robu(effectsize ~ males + binge,\rdata = hierdat, modelweights = \u0026quot;HIER\u0026quot;,\rstudynum = studyid,\rvar.eff.size = var, small = TRUE)\rrobu_hier\r## RVE: Hierarchical Effects Model with Small-Sample Corrections ## ## Model: effectsize ~ males + binge ## ## Number of clusters = 15 ## Number of outcomes = 68 (min = 1 , mean = 4.53 , median = 2 , max = 29 )\r## Omega.sq = 0.1146972 ## Tau.sq = 0.06797866 ## ## Estimate StdErr t-value dfs P(|t|\u0026gt;) 95% CI.L 95% CI.U Sig\r## 1 X.Intercept. -0.0989 0.32140 -0.308 1.79 0.79045 -1.6511 1.4533 ## 2 males 0.0020 0.00441 0.454 1.88 0.69689 -0.0182 0.0222 ## 3 binge 0.6799 0.12156 5.594 4.18 0.00439 0.3482 1.0117 ***\r## ---\r## Signif. codes: \u0026lt; .01 *** \u0026lt; .05 ** \u0026lt; .10 *\r## ---\r## Note: If df \u0026lt; 4, do not trust the results\rTo maintain consistency, I first need to calculate the approximate weights used in robumeta and then fit the model in metafor using these fixed weights.\ndevtools::source_gist(id = \u0026quot;11302318\u0026quot;, filename = \u0026quot;metafor-BRL.R\u0026quot;)\rhierdat$var_HTJ \u0026lt;- hierdat$var + as.numeric(robu_hier$mod_info$omega.sq) + as.numeric(robu_hier$mod_info$tau.sq)\rmeta_hier \u0026lt;- rma.mv(yi = effectsize ~ males + binge, V = var_HTJ, data = hierdat, method = \u0026quot;FE\u0026quot;)\rmeta_hier$cluster \u0026lt;- hierdat$studyid\rRobustResults(meta_hier)\r## Estimate Std. Error t value df Pr(\u0026gt;|t|)\r## intrcpt -0.098869582 0.321400179 -0.3076214 1.788350 0.790446059\r## males 0.002002043 0.004410552 0.4539212 1.879142 0.696887075\r## binge 0.679929801 0.121556887 5.5935111 4.182783 0.004385654\rThe estimated covariance matrices match:\nall.equal(sandwich(meta_hier, meat.=meatBRL), robu_hier$VR.r, check.attributes=FALSE)\r## [1] TRUE\rIt can also be verified that the p-values based on the Satterthwaite degrees of freedom agree.\n\rUse with metafor\rOf course, the point of writing functions that work with rma.mv objects is not to replicate robumeta results, but to take advantage of metafor’s flexibility. Rather than estimate the model with robumeta, typically one would estimate the variance components in metafor and then calculate the sandwich covariance estimates and small-sample corrections. For instance:\nmeta_REML \u0026lt;- rma.mv(yi = effectsize ~ males + binge, V = var, random = list(~ 1 | esid, ~ 1 | studyid), data = hierdat,\rmethod = \u0026quot;REML\u0026quot;)\rmeta_REML\r## ## Multivariate Meta-Analysis Model (k = 68; method: REML)\r## ## Variance Components:\r## ## estim sqrt nlvls fixed factor ## sigma^2.1 0.1566 0.3957 68 no esid ## sigma^2.2 0.0000 0.0000 15 no studyid ## ## Test for Residual Heterogeneity:\r## QE(df = 65) = 297.0172, p-val \u0026lt; .0001\r## ## Test of Moderators (coefficients 2:3):\r## QM(df = 2) = 27.2659, p-val \u0026lt; .0001\r## ## Model Results:\r## ## estimate se zval pval ci.lb ci.ub ## intrcpt -0.1118 0.2474 -0.4520 0.6513 -0.5966 0.3730 ## males 0.0022 0.0034 0.6467 0.5178 -0.0044 0.0088 ## binge 0.6744 0.1313 5.1349 \u0026lt;.0001 0.4170 0.9319 *** ## ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rRobustResults(meta_REML)\r## Estimate Std. Error t value df Pr(\u0026gt;|t|)\r## intrcpt -0.111796564 0.318156355 -0.3513888 1.794988 0.762200367\r## males 0.002173683 0.004380026 0.4962718 1.882842 0.671549040\r## binge 0.674435042 0.121660936 5.5435628 4.167780 0.004585142\rOne advantage here is that it’s possible to compare the model-based standard errors to the robust ones. In this instance, the two are fairly similar. However, the degrees of freedom estimated in the robust results indicate that the model-based standard errors (based on normal approximations) may be much too narrow.\n\rDifferences between robumeta and my implementation\rThere are two important differences between the approach implemented in robumeta and the approach based on metafor and the code that I’ve provided. The first is that robumeta uses moment estimators for the variance components, whereas metafor uses restricted- or full maximum likelihood. The estimated between-study heterogeneity (and for the hierarchical effects model, the within-study heterogeneity as well) will therefore differ to some degree.\nThe second, and perhaps more crucial, distinction has to do with the choice of weights. Weights are used for two purposes: to estimate the fixed effects and to calculate the small-sample correction. The robumeta package uses diagonal weights for both purposes. Using diagonal weights in calculating the fixed effects means that the resulting point estimates will be equivalent to those from a weighted ordinary least squares regression:\nWOLS \u0026lt;- lm(effectsize ~ males + binge, data = hierdat, weights = 1 / var_HTJ)\rcoef(WOLS)\r## (Intercept) males binge ## -0.098869582 0.002002043 0.679929801\rall.equal(coef(WOLS), as.numeric(robu_hier$b.r), check.attributes = FALSE)\r## [1] TRUE\rA subtler point is that robumeta uses the inverse weights for purposes of calculating the small sample-correction. The small sample correction involves choosing a “working” or “target” covariance matrix towards which to adjust the sandwich estimator. If the working covariance model is correct, then the BRL covariance estimator is exactly unbiased. The working matrix is also used to determine the Satterthwaite degrees of freedom. In robumeta, the working covariance matrix is taken to be inverse of the weights, which is also a diagonal matrix. Thus, the BRL correction amounts to assuming independence among all of the effect sizes. This may sound somewhat counter-intuitive, but some simulation results (reported in Beth’s paper, referenced above) suggest that the resulting estimators perform well even when the working independence assumption is not correct.\nIn contrast to the robumeta weights, metafor calculates the fixed effects based on a weighting matrix that is exactly inverse variance for given estimates of the variance components. Typically, the weighting matrix will be block-diagonal but may have off-diagonal entries corresponding to effect sizes drawn from the same study. Furthermore, my implementation of BRL uses the estimated covariance matrix derived from the posited random effects structure; in other words, the working covariance structure is taken to be the same as the model specified in the metafor call. This seems sensible to me, although I do not have any evidence regarding its performance relative to the alternatives. It is possible that any gains in asymptotic efficiency from using exactly inverse variance weights are outweighed by some sort of instability in small samples. It’s also possible that the performance of the different approaches to weighting might depend on which variance component estimators are used (i.e., MOM vs. REML).\nNeither implementation that I’ve described above is fully general. Following the generalized estimating equation framework, a fully general implementation would allow the user to specify an arbitrary weight matrix in addition to a working covariance structure. The weighting matrix would be used for purposes of estimating the fixed effects. The working covariance model would be estimated (based on MOM or REML or what-not) and then used for purposes of BRL adjustment. Of course, this fully general formulation may well be more complicated than what most analysts would actually need or use (especially for linear mixed models), except perhaps when dealing with complex survey data.\n\r","date":1398470400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1398470400,"objectID":"e5917683b8a89c22a39bbbff6178a45c","permalink":"/robust-meta-analysis-3/","publishdate":"2014-04-26T00:00:00Z","relpermalink":"/robust-meta-analysis-3/","section":"post","summary":"In an earlier post about sandwich standard errors for multi-variate meta-analysis, I mentioned that Beth Tipton has recently proposed small-sample corrections for the covariance estimators and t-tests, based on the bias-reduced linearization approach of McCaffrey, Bell, and Botts (2001).","tags":["meta-analysis","sandwiches","Rstats","robust variance estimation"],"title":"Meta-sandwich with extra mustard","type":"post"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rIn a previous post, I provided some code to do robust variance estimation with metafor and sandwich.\rHere’s another example, replicating some more of the calculations from Tanner-Smith \u0026amp; Tipton (2013).\r(See here for the complete code.)\nAs a starting point, here are the results produced by the robumeta package:\nlibrary(grid)\rlibrary(robumeta)\rdata(corrdat)\rrho \u0026lt;- 0.8\rHTJ \u0026lt;- robu(effectsize ~ males + college + binge,\rdata = corrdat, modelweights = \u0026quot;CORR\u0026quot;, rho = rho,\rstudynum = studyid,\rvar.eff.size = var, small = FALSE)\rHTJ\r## RVE: Correlated Effects Model ## ## Model: effectsize ~ males + college + binge ## ## Number of studies = 39 ## Number of outcomes = 172 (min = 1 , mean = 4.41 , median = 4 , max = 18 )\r## Rho = 0.8 ## I.sq = 75.08352 ## Tau.sq = 0.1557714 ## ## Estimate StdErr t-value dfs P(|t|\u0026gt;) 95% CI.L 95% CI.U Sig\r## 1 X.Intercept. 0.31936 0.27784 1.149 35 0.258 -0.2447 0.88340 ## 2 males -0.00331 0.00376 -0.882 35 0.384 -0.0109 0.00431 ## 3 college 0.41226 0.18685 2.206 35 0.034 0.0329 0.79159 **\r## 4 binge 0.13774 0.12586 1.094 35 0.281 -0.1178 0.39326 ## ---\r## Signif. codes: \u0026lt; .01 *** \u0026lt; .05 ** \u0026lt; .10 *\r## ---\rTo exactly re-produce the results with metafor, I’ll need to use the weights proposed by HTJ. In their approach to the correlated effects case, effect size \\(i\\) from study \\(j\\) receives weight equal to \\(\\left[\\left(v_{\\cdot j} + \\hat\\tau^2\\right)(1 + (k_j - 1) \\rho)\\right]^{-1}\\), where \\(v_{\\cdot j}\\) is the average sampling variance of the effect sizes from study \\(j\\), \\(\\hat\\tau^2\\) is an estimate of the between-study variance, \\(k_j\\) is the number of correlated effects in study \\(j\\), and \\(\\rho\\) is a user-specified value of the intra-study correlation. However, it appears that robumeta actually uses a slightly different set weights, which are equivalent to taking \\(\\rho = 1\\). I calculate the latter weights, fit the model in metafor, and output the robust standard errors and \\(t\\)-tests:\ndevtools::source_gist(id = \u0026quot;11144005\u0026quot;, filename = \u0026quot;metafor-sandwich.R\u0026quot;)\rcorrdat \u0026lt;- within(corrdat, {\rvar_mean \u0026lt;- tapply(var, studyid, mean)[studyid]\rk \u0026lt;- table(studyid)[studyid]\rvar_HTJ \u0026lt;- as.numeric(k * (var_mean + as.numeric(HTJ$mod_info$tau.sq)))\r})\rmeta1 \u0026lt;- rma.mv(effectsize ~ males + college + binge, V = var_HTJ, data = corrdat, method = \u0026quot;FE\u0026quot;)\rmeta1$cluster \u0026lt;- corrdat$studyid\rRobustResults(meta1)\r## ## t test of coefficients:\r## ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## intrcpt 0.3193586 0.2778360 1.1494 0.25816 ## males -0.0033143 0.0037573 -0.8821 0.38374 ## college 0.4122631 0.1868489 2.2064 0.03401 *\r## binge 0.1377393 0.1258637 1.0944 0.28127 ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rOne could specify a similar (though not exactly identical model) in metafor as follows. In the HTJ approach, \\(\\rho\\) represents the total correlation induced by both the within-study sampling error and intra-study correlation in true effects. In contrast, the metafor approach would take \\(\\rho\\) to be correlation due to within-study sampling error alone. I’ll first need to create a block-diagonal covariance matrix given a user-specified value of \\(\\rho\\).\nlibrary(Matrix)\requicorr \u0026lt;- function(x, rho) {\rcorr \u0026lt;- rho + (1 - rho) * diag(nrow = length(x))\rtcrossprod(x) * corr } covMat \u0026lt;- as.matrix(bdiag(with(corrdat, tapply(var_mean, studyid, equicorr, rho = 0.8, simplify = FALSE))))\rPassing this block-diagonal covariance matrix to rma.mv, I now estimate the model\n\\[T_{ij} = \\mathbf{X}_{ij} \\beta + \\nu_i + e_{ij},\\]\nwhere \\(Var(\\nu_i) = \\sigma^2\\), \\(Var(e_{ij}) = v_{ij}\\), and \\(Cor(e_{ij}, e_{ik}) = \\rho\\). Note that \\(\\sigma^2\\) is now estimated via REML.\nmeta2 \u0026lt;- rma.mv(yi = effectsize ~ males + college + binge, V = covMat, random = ~ 1 | studyid, data = corrdat,\rmethod = \u0026quot;REML\u0026quot;)\rc(sigma.sq = meta2$sigma2)\r## sigma.sq ## 0.2477825\rThe between-study heterogeneity estimate is considerably larger than the moment estimate from robumeta. Together with the difference in weighting, this leads to some changes in the coefficient estimates and their estimated precision:\nRobustResults(meta2)\r## ## t test of coefficients:\r## ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## intrcpt -0.8907096 0.4148219 -2.1472 0.038783 * ## males 0.0163074 0.0055805 2.9222 0.006052 **\r## college 0.3180139 0.2273396 1.3988 0.170658 ## binge -0.0984026 0.0897269 -1.0967 0.280265 ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rIt is important to keep in mind that the estimate of between-study heterogeneity depends on the posited model for the covariance structure, including the assumed value of \\(\\rho\\). HTJ recommend conducting sensitivity analysis across a range of values for the within-study effect correlation. Re-calculating the value of \\(\\sigma^2\\) for \\(\\rho\\) between 0.0 and 0.9 yields the following:\nsigma2 \u0026lt;- function(rho) {\rcovMat \u0026lt;- as.matrix(bdiag(with(corrdat, tapply(var_mean, studyid, equicorr, rho = rho, simplify = FALSE))))\rrma.mv(yi = effectsize ~ males + college + binge, V = covMat, random = ~ 1 | studyid, data = corrdat,\rmethod = \u0026quot;REML\u0026quot;)$sigma2\r}\rrho_sens \u0026lt;- seq(0,0.9,0.1)\rsigma2_sens \u0026lt;- sapply(rho_sens, sigma2)\rcbind(rho = rho_sens, sigma2 = round(sigma2_sens, 4))\r## rho sigma2\r## [1,] 0.0 0.2519\r## [2,] 0.1 0.2513\r## [3,] 0.2 0.2507\r## [4,] 0.3 0.2502\r## [5,] 0.4 0.2497\r## [6,] 0.5 0.2492\r## [7,] 0.6 0.2487\r## [8,] 0.7 0.2482\r## [9,] 0.8 0.2478\r## [10,] 0.9 0.2474\rThe between-study heterogeneity is quite insensitive to the assumed value of \\(\\rho\\).\nThe difference between the results based on metafor versus on robumeta appears to be due to the subtle difference in the weighting approach: metafor uses block-diagonal weights that contain off-diagonal terms for effects drawn from a common study, whereas robumeta uses entirely diagonal weights.\n","date":1398211200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1398211200,"objectID":"81d87072f5b6d7d910e951176e44c892","permalink":"/robust-meta-analysis-2/","publishdate":"2014-04-23T00:00:00Z","relpermalink":"/robust-meta-analysis-2/","section":"post","summary":"In a previous post, I provided some code to do robust variance estimation with metafor and sandwich.\rHere’s another example, replicating some more of the calculations from Tanner-Smith \u0026amp; Tipton (2013).","tags":["meta-analysis","sandwiches","Rstats","robust variance estimation"],"title":"Another meta-sandwich","type":"post"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rA common problem arising in many areas of meta-analysis is how to synthesize a set of effect sizes when the set includes multiple effect size estimates from the same study. It’s often not possible to obtain all of the information you’d need in order to estimate the sampling covariances between those effect sizes, yet without that information, established approaches to modeling dependent effect sizes become inaccurate. Hedges, Tipton, \u0026amp; Johnson (2010, HTJ hereafter) proposed the use of cluster-robust standard errors for multi-variate meta-analysis. (These are also called “sandwich” standard errors, which is up there on the list of great and evocative names for statistical procedures.) The great advantage of the sandwich approach is that it permits valid inferences for average effect sizes and meta-regression coefficients even if you don’t have correct covariance estimates (or variance estimates, for that matter).\nI recently heard from Beth Tipton (who’s a graduate-school buddy) that she and her student have written an R package implementing the HTJ methods, including moment estimators for the between-study variance components. I want to try out the cluster-robust standard errors for a project I’m working on, but I also need to use REML estimators rather than the moment estimators. It turns out, it’s easy enough to do that by writing a couple of short functions. Here’s how.\nFirst, the metafor package contains a very rich suite of meta-analytic methods, including for multi-variate meta-analysis. The only thing it lacks is sandwich standard errors. However, the sandwich package provides an efficient, well-structured framework for calculating all sorts of robust standard errors. All that’s needed are a few functions to make the packages talk to each other. Each of the functions described below takes as input a fitted multi-variate meta-analysis model, which is represented in R by an object of class rma.mv.\nFirst load up the packages:\nlibrary(metafor)\rlibrary(sandwich)\rlibrary(lmtest)\rNext, I need a bread method for objects of class rma.mv, which is a function that returns the \\(p \\times p\\) matrix \\(\\displaystyle{m \\left(\\sum_{i=1}^m \\mathbf{X}_j\u0026#39; \\mathbf{W}_j \\mathbf{X}_j\\right)^{-1}}\\). The bread function is straight-forward because it is just a multiple of the model-based covariance matrix, which rma.mv objects store in the vb component:\nbread.rma.mv \u0026lt;- function(obj) {\rcluster \u0026lt;- findCluster(obj)\rlength(unique(cluster)) * obj$vb }\rI also need an estfun method for objects of class rma.mv, which is a function that returns an \\(m \\times p\\) matrix where row \\(j\\) is equal to \\(\\mathbf{e}_j\u0026#39; \\mathbf{W}_j \\mathbf{X}_j\\), \\(j = 1,...,m\\). The necessary pieces for the estfun method can also be pulled out of the components of rma.mv:\nestfun.rma.mv \u0026lt;- function(obj) {\rcluster \u0026lt;- droplevels(as.factor(findCluster(obj)))\rres \u0026lt;- residuals(obj)\rWX \u0026lt;- chol2inv(chol(obj$M)) %*% obj$X\rrval \u0026lt;- by(cbind(res, WX), cluster, function(x) colSums(x[,1] * x[,-1, drop = FALSE]))\rrval \u0026lt;- matrix(unlist(rval), length(unique(cluster)), obj$p, byrow=TRUE)\rcolnames(rval) \u0026lt;- colnames(obj$X)\rrval\r}\rThe remaining question is how to determine which of the components in the model should be used to define independent clusters. This is a little bit tricky because there are several different methods of specifying random effects in the rma.mv function. One way involves providing a list of formulas, each containing a factor associated with a unique random effect, such as random = list( ~ 1 | classroom, ~ 1 | school). If this method of specifying random effects is used, the rma.mv object will have the component withS set to TRUE, and my approach is to simply take the factor with the smallest number of unique levels. This is perhaps a little bit presumptious, because the withS method could potentially be used to specify arbitrary random effects, where one level is not strictly nested inside another. However, probably the most common use will involve nested factors, so my assumption seems like a good starting point at least.\nAnother approach to specifying random effects is to use a formula of the form random = inner | outer, in which case the rma.mv object will have the component withG set to TRUE. Here, it seems reasonable to use the outer factor for defining clusters. If both the withS and withG methods are used together, I’ll assume that the withS factors contain the outermost level.\nFinally, if rma.mv is used to estimate a fixed effects model without any random components, the clustering factor will have to be manually added to the rma.mv object in a component called cluster. For example, if you want to cluster on the variable studyID in the dataframe dat:\nrma_fit$cluster \u0026lt;- dat$studyID\rHere’s code that implements these assumptions:\nfindCluster \u0026lt;- function(obj) {\rif (is.null(obj$cluster)) {\rif (obj$withS) {\rr \u0026lt;- which.min(obj$s.nlevels)\rcluster \u0026lt;- obj$mf.r[[r]][[obj$s.names[r]]]\r} else if (obj$withG) {\rcluster \u0026lt;- obj$mf.r[[1]][[obj$g.names[2]]]\r} else {\rstop(\u0026quot;No clustering variable specified.\u0026quot;)\r}\r} else {\rcluster \u0026lt;- obj$cluster\r}\rcluster\r}\rWith these three functions, you can then use metafor to fit a random effects model, sandwich to calculate the standard errors, and functions like coeftest from the package lmtest to run \\(t\\)-tests. As a little bonus, here’s a function for probably the most common case of how you’d use the sandwich standard errors:\nRobustResults \u0026lt;- function(obj, adjust = TRUE) {\rcluster \u0026lt;- findCluster(obj) vcov. \u0026lt;- sandwich(obj, adjust = adjust)\rdf. \u0026lt;- length(unique(cluster)) - obj$p\rcoeftest(obj, vcov. = vcov., df = df.)\r}\rSee here for a file containing the full code.\nExample\rTanner-Smith \u0026amp; Tipton (2013) provide an application of the cluster-robust method to a fictional dataset with 68 effect sizes nested within 15 studies. They call this a “hierarchical” dependence example because each effect size estimate is drawn from an independent sample, but dependence is induced because the experiments were all done in the same lab. For comparison purposes, here are the results produced by robumeta:\nlibrary(grid)\rlibrary(robumeta)\rdata(hierdat)\rHTJ \u0026lt;- robu(effectsize ~ 1,\rdata = hierdat, modelweights = \u0026quot;HIER\u0026quot;,\rstudynum = studyid,\rvar.eff.size = var, small = FALSE)\rHTJ\r## RVE: Hierarchical Effects Model ## ## Model: effectsize ~ 1 ## ## Number of clusters = 15 ## Number of outcomes = 68 (min = 1 , mean = 4.53 , median = 2 , max = 29 )\r## Omega.sq = 0.1560802 ## Tau.sq = 0.06835547 ## ## Estimate StdErr t-value dfs P(|t|\u0026gt;) 95% CI.L 95% CI.U Sig\r## 1 X.Intercept. 0.25 0.0598 4.18 14 0.000925 0.122 0.378 ***\r## ---\r## Signif. codes: \u0026lt; .01 *** \u0026lt; .05 ** \u0026lt; .10 *\r## ---\rTo exactly re-produce the results with metafor, I’ll need to use the weights proposed by HTJ. In their approach, effect size \\(i\\) from study \\(j\\) receives weight equal to \\(\\left(v_{ij} + \\hat\\omega^2 + \\hat\\tau^2\\right)^{-1}\\), where \\(v_{ij}\\) is the sampling variance of the effect size, \\(\\hat\\omega^2\\) is an estimate of the between-sample within-study variance, and \\(\\hat\\tau^2\\) is an estimate of the between-study variance. After calculating these weights, I fit the model in metafor, calculate the sandwich covariance matrix, and replay the results:\nhierdat$var_HTJ \u0026lt;- hierdat$var + HTJ$mod_info$omega.sq + HTJ$mod_info$tau.sq # calculate weights\r## Warning in hierdat$var + HTJ$mod_info$omega.sq: Recycling array of length 1 in vector-array arithmetic is deprecated.\r## Use c() or as.vector() instead.\r## Warning in hierdat$var + HTJ$mod_info$omega.sq + HTJ$mod_info$tau.sq: Recycling array of length 1 in vector-array arithmetic is deprecated.\r## Use c() or as.vector() instead.\rmeta1 \u0026lt;- rma.mv(yi = effectsize ~ 1, V = var_HTJ, data = hierdat, method = \u0026quot;FE\u0026quot;)\rmeta1$cluster \u0026lt;- hierdat$studyid # add clustering variable to the fitted model\rRobustResults(meta1)\r## ## t test of coefficients:\r## ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## intrcpt 0.249826 0.059762 4.1803 0.0009253 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rThe HTJ weights are not the only alternative–one could instead use weights that are exactly inverse variance under the posited model. For effect \\(i\\) from study \\(j\\), these weights would be closer to \\(\\left(v_{ij} + \\hat\\omega^2 + k_j \\hat\\tau^2 \\right)^{-1}\\). For \\(\\hat\\tau^2 \u0026gt; 0\\), the inverse-variance weights put proportionately less weight on studies containing many effects. These weights can be calculated in metafor as follows:\nmeta2 \u0026lt;- rma.mv(yi = effectsize ~ 1, V = var, random = list(~ 1 | esid, ~ 1 | studyid), sigma2 = c(HTJ$mod_info$omega.sq, HTJ$mod_info$tau.sq),\rdata = hierdat)\rRobustResults(meta2)\r## ## t test of coefficients:\r## ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## intrcpt 0.264422 0.086688 3.0503 0.008645 **\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rCuriously, the robust standard error increases under a weighting scheme that is more efficient if the model is correct.\nFinally, metafor provides ML and REML estimators for the between-sample and between-study random effects (the HTJ moment estimators are not available though). Here are the results based on REML estimators and the corresponding inverse-variance weights:\nmeta3 \u0026lt;- rma.mv(yi = effectsize ~ 1, V = var, random = list(~ 1 | esid, ~ 1 | studyid), data = hierdat,\rmethod = \u0026quot;REML\u0026quot;)\rmeta3\r## ## Multivariate Meta-Analysis Model (k = 68; method: REML)\r## ## Variance Components:\r## ## estim sqrt nlvls fixed factor ## sigma^2.1 0.2263 0.4757 68 no esid ## sigma^2.2 0.0000 0.0000 15 no studyid ## ## Test for Heterogeneity:\r## Q(df = 67) = 370.1948, p-val \u0026lt; .0001\r## ## Model Results:\r## ## estimate se zval pval ci.lb ci.ub ## 0.2501 0.0661 3.7822 0.0002 0.1205 0.3797 *** ## ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rRobustResults(meta3)\r## ## t test of coefficients:\r## ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## intrcpt 0.250071 0.059796 4.1821 0.0009222 ***\r## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rThe between-study variance estimate is tiny, particularly when compared to the between-sample within-study estimate. Despite the difference in variance estimates, the average effect size estimate is nearly identical to the estimate based on the HTJ approach.\nSee here for the full code to reproduce this example.\n\rNotes\rIt would be straight-forward to add a few more functions that provide robust standard errors for univariate meta-analysis models as well. All that it would take is to write bread and estfun methods for the class rma.uni.\nAlso, Beth has recently proposed\rsmall-sample corrections to the cluster-robust estimators, based on the bias-reduced linearization (BRL) approach of McCaffrey, Bell, \u0026amp; Botts (2001). It seems to me that these small-sample corrections could also be implemented using an approach similar to what I’ve done here, by building out the estfun method to provide BRL results. It would take a little more thought, but actually it would be worth doing–and treating the general case–because BRL seems like it would be useful for all sorts of models besides multi-variate meta-analysis.\n\r","date":1398038400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1398038400,"objectID":"74016240b407355f756320c43bc1b61f","permalink":"/robust-meta-analysis-1/","publishdate":"2014-04-21T00:00:00Z","relpermalink":"/robust-meta-analysis-1/","section":"post","summary":"A common problem arising in many areas of meta-analysis is how to synthesize a set of effect sizes when the set includes multiple effect size estimates from the same study.","tags":["meta-analysis","sandwiches","Rstats","robust variance estimation"],"title":"A meta-sandwich","type":"post"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rDr. Marcia Barnes from the department of Special Education invited me to visit her pro-seminar this afternoon and talk about some of my work on meta-analytic methods for single-case research. Thanks very much to the students for asking such thoughtful and engaging questions. Here are the slides, which include some additional material that we didn’t get to talk about.\n","date":1397088000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1397088000,"objectID":"2bcbe6db058bc824e3c9c6d813ea3f96","permalink":"/sped-pro-sem/","publishdate":"2014-04-10T00:00:00Z","relpermalink":"/sped-pro-sem/","section":"post","summary":"Dr. Marcia Barnes from the department of Special Education invited me to visit her pro-seminar this afternoon and talk about some of my work on meta-analytic methods for single-case research.","tags":["meta-analysis","single-case design","effect size"],"title":"Special Education Pro-Sem","type":"post"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rI have learned from Mr. Yaakoub El Khamra that he and the good folks at TACC have made some modifications to TACC’s custom MPI implementation and R build in order to correct bugs in Rmpi and snow that were causing crashes. My earlier post has been updated to reflect the modifications. The main changes are:\nThe version of MVAPICH2 has changed to 2.0b\rChanges to the Rmpi and snow packages necessitate using the latest version of R (Warm Puppy, 3.0.3). This version is available in the Rstats module.\rFor improved reproducibility, I modified the R code so that the simulation driver function uses a seed value.\rI had to switch from maply to mdply as a result of (3).\r\r","date":1396915200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1396915200,"objectID":"908beb75ee8d41728bc6f37166cdb69a","permalink":"/parallel-r-on-tacc-update/","publishdate":"2014-04-08T00:00:00Z","relpermalink":"/parallel-r-on-tacc-update/","section":"post","summary":"I have learned from Mr. Yaakoub El Khamra that he and the good folks at TACC have made some modifications to TACC’s custom MPI implementation and R build in order to correct bugs in Rmpi and snow that were causing crashes.","tags":["Rstats","programming","simulation","TACC"],"title":"Update: parallel R on the TACC","type":"post"},{"authors":[],"categories":null,"content":"","date":1396569600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1396569600,"objectID":"a5a53a0e820c5549df539778c95c97b4","permalink":"/talk/aera-2014-four-methods-for-pir/","publishdate":"2018-04-15T08:15:00Z","relpermalink":"/talk/aera-2014-four-methods-for-pir/","section":"talk","summary":"","tags":[],"title":"Four methods of analyzing partial interval recording data, with application to single-case research","type":"talk"},{"authors":["William R. Shadish","Larry V. Hedges","James E. Pustejovsky"],"categories":null,"content":"","date":1396310400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1396310400,"objectID":"3e103f4a3d5b5ccf53045c6aba22c26c","permalink":"/publication/bc-smd-primer-and-applications/","publishdate":"2014-04-01T00:00:00Z","relpermalink":"/publication/bc-smd-primer-and-applications/","section":"publication","summary":"This article presents a d-statistic for single-case designs that is in the same metric as the d-statistic used in between-subjects designs such as randomized experiments and offers some reasons why such a statistic would be useful in SCD research. The d has a formal statistical development, is accompanied by appropriate power analyses, and can be estimated using user-friendly SPSS macros. We discuss both advantages and disadvantages of d compared to other approaches such as previous d-statistics, overlap statistics, and multilevel modeling. It requires at least three cases for computation and assumes normally distributed outcomes and stationarity, assumptions that are discussed in some detail. We also show how to test these assumptions. The core of the article then demonstrates in depth how to compute d for one study, including estimation of the autocorrelation and the ratio of between case variance to total variance (between case plus within case variance), how to compute power using a macro, and how to use the d to conduct a meta-analysis of studies using single-case designs in the free program R, including syntax in an appendix. This syntax includes how to read data, compute fixed and random effect average effect sizes, prepare a forest plot and a cumulative meta-analysis, estimate various influence statistics to identify studies contributing to heterogeneity and effect size, and do various kinds of publication bias analyses. This d may prove useful for both the analysis and meta-analysis of data from SCDs.","tags":["single-case design","design-comparable SMD","effect size"],"title":"Analysis and meta-analysis of single-case designs with a standardized mean difference statistic: A primer and applications","type":"publication"},{"authors":[],"categories":null,"content":"","date":1395360000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1395360000,"objectID":"1f4a527011d1c324169999dee1b0a00e","permalink":"/talk/tuesap-2014-construct-invalidity-of-pir/","publishdate":"2018-04-15T08:15:00Z","relpermalink":"/talk/tuesap-2014-construct-invalidity-of-pir/","section":"talk","summary":"","tags":[],"title":"Addressing construct invalidity in partial interval recording data","type":"talk"},{"authors":[],"categories":null,"content":"","date":1394064000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1394064000,"objectID":"7df5dad632d26b5557eab1f5bb994d65","permalink":"/talk/sree-2014-internal-validity-of-mbd/","publishdate":"2018-04-15T08:15:00Z","relpermalink":"/talk/sree-2014-internal-validity-of-mbd/","section":"talk","summary":"","tags":[],"title":"On internal validity in multiple baseline designs","type":"talk"},{"authors":["James E. Pustejovsky"],"categories":null,"content":"","date":1393632000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1393632000,"objectID":"9e3f1c36373178a241f23443435d99a1","permalink":"/publication/converting-from-d-to-r-to-z/","publishdate":"2014-03-01T00:00:00Z","relpermalink":"/publication/converting-from-d-to-r-to-z/","section":"publication","summary":"Meta-analyses of the relationship between 2 continuous variables sometimes involves conversions between different effect sizes, but methodological literature offers conflicting guidance about how to make such conversions. This article provides methods for converting from a standardized mean difference to a correlation coefficient (and from there to Fisher’s z) under 3 types of study designs: extreme groups, dichotomization of a continuous variable, and controlled experiments. Also provided are formulas and recommendations regarding how the sampling variance of effect size statistics should be estimated in each of these cases. The conversion formula for extreme groups designs, originally due to Feldt (1961), can be viewed as a generalization of Hunter and Schmidt’s (1990) method for dichotomization designs. A simulation study examines the finite-sample properties of the proposed methods. The conclusion highlights areas where current guidance in the literature should be amended or clarified.","tags":["effect size","meta-analysis","correlation"],"title":"Converting from d to r to z when the design uses extreme groups, dichotomization, or experimental control","type":"publication"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rMy article “Measurement-comparable effect sizes for single-case studies of free-operant behavior” has been accepted at Psychological Methods. Postprint and supporting materials are available. Here’s the abstract:\nSingle-case research comprises a set of designs and methods for evaluating the effects of interventions, practices, or programs on individual cases, through comparison of outcomes measured at different points in time. Although there has long been interest in meta-analytic technique for synthesizing single-case research, there has been little scrutiny of whether proposed effect sizes remain on a directly comparable metric when outcomes are measured using different operational procedures. Much of single-case research focuses on behavioral outcomes in free-operant contexts, which may be measured using a variety of different direct observation procedures. This article describes a suite of effect sizes for quantifying changes in free-operant behavior, motivated by an alternating renewal process model that allows measurement comparability to be established in precise terms. These effect size metrics have the advantage of comporting with how direct observation data are actually collected and summarized. Effect size estimators are proposed that are applicable when the behavior being measured remains stable within a given treatment condition. The methods are illustrated by two examples, including a re-analysis of a systematic review of the effects of choice-making opportunities on problem behavior.\n","date":1391472000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1391472000,"objectID":"079b8acaea2ae8e0f0d2949061989847","permalink":"/measurement-comparable-effect-sizes/","publishdate":"2014-02-04T00:00:00Z","relpermalink":"/measurement-comparable-effect-sizes/","section":"post","summary":"My article “Measurement-comparable effect sizes for single-case studies of free-operant behavior” has been accepted at Psychological Methods. Postprint and supporting materials are available. Here’s the abstract:\nSingle-case research comprises a set of designs and methods for evaluating the effects of interventions, practices, or programs on individual cases, through comparison of outcomes measured at different points in time.","tags":["behavioral observation","single-case design","effect size","response ratio"],"title":"New article: Measurement-comparable effect sizes for single-case studies of free-operant behavior","type":"post"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rUPDATE (4/8/2014): I have learned from Mr. Yaakoub El Khamra that he and the good folks at TACC have made some modifications to TACC’s custom MPI implementation and R build in order to correct bugs in Rmpi and snow that were causing crashes. This post has been updated to reflect the modifications.\nI’ve started to use the Texas Advanced Computing Cluster to run statistical simulations in R. It takes a little bit of time to get up and running, but once you do it is an amazing tool. To get started, you’ll need\nAn account on the TACC and an allocation of computing time.\rAn ssh client like PUTTY.\rSome R code that can be adapted to run in parallel.\rA SLURM script that tells the server (called Stampede) how to run the R.\r\rThe R script\rI’ve been running my simulations using a combination of several packages that provide very high-level functionality for parallel computing, namely foreach, doSNOW, and the maply function in plyr. All of this runs on top of an Rmpi implementation developed by the folks at TACC (more details here).\nIn an earlier post, I shared code for running a very simple simulation of the Behrens-Fisher problem. Here’s adapted code for running the same simulation on Stampede. The main difference is that there are a few extra lines of code to set up a cluster, seed a random number generator, and pass necessary objects (saved in source_func) to the nodes of the cluster:\nlibrary(Rmpi)\rlibrary(snow)\rlibrary(foreach)\rlibrary(iterators)\rlibrary(doSNOW)\rlibrary(plyr)\r# set up parallel processing\rcluster \u0026lt;- getMPIcluster()\rregisterDoSNOW(cluster)\r# export source functions\rclusterExport(cluster, source_func)\rOnce it is all set up, running the code is just a matter of turning on the parallel option in mdply:\nBFresults \u0026lt;- mdply(parms, .fun = run_sim, .drop=FALSE, .parallel=TRUE)\rI fully admit that my method of passing source functions is rather kludgy. One alternative would be to save all of the source functions in a separate file (say, source_functions.R), then source the file at the beginning of the simulation script:\nrm(list=ls())\rsource(\u0026quot;source_functions.R\u0026quot;)\rprint(source_func \u0026lt;- ls())\rAnother, more elegant alternative would be to put all of your source functions in a little package (say, BehrensFisher), install the package, and then pass the package in the maply call:\nBFresults \u0026lt;- mdply(parms, .fun = run_sim, .drop=FALSE, .parallel=TRUE, .paropts = list(.packages=\u0026quot;BehrensFisher\u0026quot;))\rOf course, developing a package involves a bit more work on the front end.\n\rThe SLURM script\rSuppose that you’ve got your R code saved in a file called Behrens_Fisher.R. Here’s an example of a SLURM script that runs the R script after configuring an Rmpi cluster:\n#!/bin/bash\r#SBATCH -J Behrens # Job name\r#SBATCH -o Behrens.o%j # Name of stdout output file (%j expands to jobId)\r#SBATCH -e Behrens.o%j # Name of stderr output file(%j expands to jobId)\r#SBATCH -n 32 # Total number of mpi tasks requested\r#SBATCH -p normal # Submit to the \u0026#39;normal\u0026#39; or \u0026#39;development\u0026#39; queue\r#SBATCH -t 0:20:00 # Run time (hh:mm:ss)\r#SBATCH -A A-yourproject # Allocation name to charge job against\r#SBATCH --mail-user=you@email.address # specify email address for notifications\r#SBATCH --mail-type=begin # email when job begins\r#SBATCH --mail-type=end # email when job ends\r# load R module\rmodule load Rstats # call R code from RMPISNOW\ribrun RMPISNOW \u0026lt; Behrens_Fisher.R \rThe file should be saved in a plain text file called something like run_BF.slurm. The file has to use ANSI encoding and Unix-type end-of-line encoding; Notepad++ is a text editor that can create files in this format.\nNote that for full efficiency, the -n option should be a multiple of 16 because their are 16 cores per compute node. Further details about SBATCH options can be found here.\n\rRunning on Stampede\rFollow these directions to log in to the Stampede server. Here’s the User Guide for Stampede. The first thing you’ll need to do is ensure that you’ve got the proper version of MVAPICH loaded. To do that, type\nmodule swap intel intel/14.0.1.106\rmodule setdefault\rThe second line sets this as the default, so you won’t need to do this step again.\nSecond, you’ll need to install whatever R packages you’ll need to run your code. To do that, type the following at the login4$ prompt:\nlogin4$module load Rstats\rlogin4$R\rThis will start an interactive R session. From the R prompt, use install.packages to download and install, e.g.\ninstall.packages(\u0026quot;plyr\u0026quot;,\u0026quot;reshape\u0026quot;,\u0026quot;doSNOW\u0026quot;,\u0026quot;foreach\u0026quot;,\u0026quot;iterators\u0026quot;)\rThe packages will be installed in a local library. Now type q() to quit R.\nNext, make a new directory for your project:\nlogin4$mkdir project_name\rlogin4$cd project_name\rUpload your files to the directory (using psftp, for instance). Check that your R script is properly configured by viewing it in Vim.\nFinally, submit your job by typing\nlogin4$sbatch run_BF.slurm\ror whatever your SLURM script is called. To check the status of the submitted job, type showq -u followed by your TACC user name (more details here).\n\rFurther thoughts\rTACC accounts come with a limited number of computing hours, so you should be careful to write efficient code. Before you even start worrying about running on TACC, you should profile your code and try to find ways to speed up the computations. (Some simple improvements in my Behrens-Fisher code would make it run MUCH faster.) Once you’ve done what you can in terms of efficiency, you should do some small test runs on Stampede. For example, you could try running only a few iterations for each combination of factors, and/or running only some of the combinations rather than the full factorial design. Based on the run-time for these jobs, you’ll then be able to estimate how long the full code would take. If it’s acceptable (and within your allocation), then go ahead and sbatch the full job. If it’s not, you might reconsider the number of factor levels in your design or the number of iterations you need. I might have more comments about those some other time.\nComments? Suggestions? Corrections? Drop a comment.\n\r","date":1387497600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1387497600,"objectID":"1c0993aeb5e60f20f64b52b96fd74663","permalink":"/parallel-r-on-tacc/","publishdate":"2013-12-20T00:00:00Z","relpermalink":"/parallel-r-on-tacc/","section":"post","summary":"UPDATE (4/8/2014): I have learned from Mr. Yaakoub El Khamra that he and the good folks at TACC have made some modifications to TACC’s custom MPI implementation and R build in order to correct bugs in Rmpi and snow that were causing crashes.","tags":["Rstats","programming","simulation","TACC"],"title":"Running R in parallel on the TACC","type":"post"},{"authors":null,"categories":[],"content":"\rHere are the slides from my presentation at this afternoon’s Quant. Methods brown bag. I gave a very quick introduction to using R for conducting simulation studies. I hope it was enough to get people intrigued about the possibilities of using R in their own work.\nThe second half of the presentation sketched out a quick-and-dirty simulation of the Behrens-Fisher problem, or more specifically the coverage rates of 95% confidence intervals using Welch’s degrees of freedom approximation, given independent samples with unequal variances. Here is the complete code. As I mentioned in the talk, there’s lots of room for improvement. The main point that I was trying to illustrate is that simulations have five distinct pieces:\na data generating model,\ran estimation procedure,\rperformance criteria,\ran experimental design (parameter values and sample dimensions), and\ranalysis and results.\r\rIt is useful to write simulation code that reflects the structure, so that it is easy for you (or other people) to read, revise, extend, or re-run it. And then post it on your blog.\n","date":1386288000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1386288000,"objectID":"a0e5e6c6f199dd9e444305fc742c71c7","permalink":"/designing-simulation-studies-using-r/","publishdate":"2013-12-06T00:00:00Z","relpermalink":"/designing-simulation-studies-using-r/","section":"post","summary":"Here are the slides from my presentation at this afternoon’s Quant. Methods brown bag. I gave a very quick introduction to using R for conducting simulation studies. I hope it was enough to get people intrigued about the possibilities of using R in their own work.","tags":["Rstats","simulation"],"title":"Designing simulation studies using R","type":"post"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rIt is well known that the partial interval recording procedure produces an over-estimate of the prevalence of a behavior. Here I will demonstrate how to use the ARPobservation package to study the extent of this bias. First though, I’ll need to define the terms prevalence and incidence and also take a detour through continuous duration recording.\nPrevalence and incidence\rFirst off, what do I mean by prevalence? In an alternating renewal process, prevalence is the long-run proportion of time that the behavior occurs. I’ll call prevalence \\(\\phi\\) (“phi”). So far, I’ve described alternating renewal processes in terms of their average event duration (which I’ll call \\(\\mu\\) or “mu”) and the average interim time (which I’ll call \\(\\lambda\\) or “lambda”). Prevalence is related to these quantities mathematically as follows:\n\\[ \\phi = \\frac{\\mu}{\\mu + \\lambda}. \\]\nSo given \\(\\mu\\) and \\(\\lambda\\), we can figure out \\(\\phi\\).\nAnother characteristic of behavior that can be determined by the average event duration and average interim time is incidence, or the rate of event occurrence per unit of time. I’ll call incidence \\(\\zeta\\) (“zeta”). In an alternating renewal process,\n\\[ \\zeta = \\frac{1}{\\mu + \\lambda}. \\]\nThis makes intuitive sense, because \\(\\mu + \\lambda\\) is the average time in between the start of each event, so its inverse should be the average number of times that an event starts per unit of time. (Note that though this is quite intuitive, it’s also very difficult to prove mathematically.) Given \\(\\mu\\) and \\(\\lambda\\), we can figure out \\(\\zeta\\). Conversely, if we know \\(\\phi\\) and \\(\\zeta\\), we can solve for \\(\\mu = \\phi / \\zeta\\) and \\(\\lambda = (1 - \\phi) / \\zeta\\).\n\rContinuous duration recording\rIt can be shown mathematically that, on average, data produced by continuous duration recording (CDR) will be equal to the prevalence of the behavior. In statistical parlance, CDR data produces an unbiased estimate of prevalence. Since this is a mathematical fact, it’s a good idea to check that the software gives the same result (if it doesn’t, there must be something wrong with the code).\nIn order to simulate behavior streams, the software needs values for the average event duration and average interim time. But I want to think in terms of prevalence and incidence, so I’ll first pick a value for incidence. Say that a new behavioral event starts once per minute on average, so incidence (in events per second) would be \\(\\zeta = 1 / 60\\). I’ll then vary prevalence across the range from zero to one. For each value of prevalence, I’ll generate 10 behavior streams (if you’d like to do more, go ahead!).\nlibrary(ARPobservation)\rset.seed(8)\rzeta \u0026lt;- 1 / 60\rphi \u0026lt;- rep(seq(0.01, 0.99, 0.01), each = 10)\r# Now solve for mu and lambda\rmu \u0026lt;- phi / zeta\rlambda \u0026lt;- (1 - phi) / zeta\riterations \u0026lt;- length(phi) # total number of behavior streams to generate\rTwo last elements are needed before I can get to the simulating: I need to decide what distributions to use for event durations and interim times, and I need to decide how long the observation session should last. To keep things simple, for the time being I’ll use exponential distributions. I’ll also suppose that we observe for 10 min = 600 s, so that on average we should observe 10 events per session. Now I can simulate a bunch of behavior streams and apply the CDR procedure to them.\nBS \u0026lt;- r_behavior_stream(n = iterations, mu = mu, lambda = lambda, F_event = F_exp(), F_interim = F_exp(), stream_length = 600)\rCDR \u0026lt;- continuous_duration_recording(BS)\rTo check that the CDR procedure is unbiased, I’ll plot the CDR data versus the true value of prevalence, and run a smoothing line through the cloud of data-points:\nlibrary(ggplot2)\rqplot(x = phi, y = CDR, geom = \u0026quot;point\u0026quot;) + geom_smooth(method = \u0026quot;loess\u0026quot;)\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\rThe blue line is nearly identical to the line y = x, meaning that the average of CDR data is equal to prevalence. Good news–the software appears to be working correctly!\n\rPartial interval recording\rNow to partial interval recording (PIR). There are two different ways to think about how PIR data over-estimates prevalence. The conventional statistical approach follows the same logic as above, comparing the average value of PIR data to the true value of prevalence, \\(\\phi\\). Using the same simulated data streams as above, with 15 s intervals and 5 s of rest time after each interval…\nPIR \u0026lt;- interval_recording(BS, interval_length = 20, rest_length = 5)\rqplot(x = phi, y = PIR, geom = \u0026quot;point\u0026quot;, ylim = c(-0.02,1.02)) + geom_smooth(method = \u0026quot;loess\u0026quot;, se = FALSE) + geom_abline(intercept = 0, slope = 1, linetype = \u0026quot;dashed\u0026quot;)\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\rThe blue line indicates the average value of PIR data across the simulations for a given value of prevalence. The dashed line indicates y = x, so clearly PIR data over-estimates prevalence.\nPrevious studies in the Applied Behavior Analysis literature have taken a slightly different approach to thinking about over-estimation. Rather than comparing PIR data to the prevalence parameter \\(\\phi\\), PIR data is instead compared to the sample value of prevalence, which is equivalent to the CDR proportion. Following this logic, I apply the PIR and CDR procedures to the same simulated behavior streams, then plot PIR versus CDR.\nobs_data \u0026lt;- reported_observations(BS, data_types = c(\u0026quot;C\u0026quot;,\u0026quot;P\u0026quot;), interval_length = 20, rest_length = 5)\rqplot(x = CDR, y = PIR, data = obs_data, geom = \u0026quot;point\u0026quot;, ylim = c(-0.02,1.02)) + geom_smooth(method = \u0026quot;loess\u0026quot;, se = FALSE) + geom_abline(intercept = 0, slope = 1, linetype = \u0026quot;dashed\u0026quot;)\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\rThe blue fitted line is slightly different than with the other approach, but the general conclusion is the same: PIR data over-estimates prevalence.\nBut by how much? That’s actually a tricky question to answer, because the extent of the bias depends on a bunch of factors:\n\rthe true prevalence \\(\\phi\\),\rthe true incidence \\(\\zeta\\),\rthe length of the intervals, and\rthe distribution of interim times F_lambda.\r\r(Curiously enough, the bias doesn’t depend on the distribution of event durations F_mu.)\nInterval length\rTo see that the bias depends on the length of intervals used, I’ll compare 15 s intervals with 5 s rest times versus 25 s intervals with 5 s rest times. For a session of length 600 s, the latter procedure will yield 20 intervals.\nPIR_25 \u0026lt;- interval_recording(BS, interval_length = 30, rest_length = 5)\robs_data \u0026lt;- cbind(obs_data, PIR_25)\rqplot(x = CDR, y = PIR, data = obs_data, geom = \u0026quot;smooth\u0026quot;, method = \u0026quot;loess\u0026quot;, ylim = c(-0.02,1.02)) + geom_smooth(aes(y = PIR_25), method = \u0026quot;loess\u0026quot;, se = FALSE, col = \u0026quot;red\u0026quot;) + geom_abline(intercept = 0, slope = 1, linetype = \u0026quot;dashed\u0026quot;)\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\rThe red line indicates that the longer interval time leads to a larger degree of over-estimation. (For clarity, I’ve removed the points in the scatter-plot.)\n\rInterim time distribution\rIt isn’t terribly troubling that the bias of PIR data depends on the interval length, because the observer will generally know (and will hopefully report in any write-up of their experiment) the interval length that was used. Much more troubling is the fact that the bias depends on the distribution of interim times, because this is something that the observer or analyst won’t usually have much information about. To see how this bias works, I’ll compare behavior streams generated using an exponential distribution for the interim times with thos generated using a gamma distribution with shape parameter 3 (this distribution is much less dispersed than the exponential).\nBS_exp \u0026lt;- r_behavior_stream(n = iterations, mu = mu, lambda = lambda, F_event = F_exp(), F_interim = F_exp(), stream_length = 600)\robs_exp \u0026lt;- reported_observations(BS_exp, data_types = c(\u0026quot;C\u0026quot;,\u0026quot;P\u0026quot;), interval_length = 20, rest_length = 5)\robs_exp$F_lambda \u0026lt;- \u0026quot;Exponential\u0026quot;\rBS_gam \u0026lt;- r_behavior_stream(n = iterations, mu = mu, lambda = lambda, F_event = F_exp(), F_interim = F_gam(shape = 3), stream_length = 600)\robs_gam \u0026lt;- reported_observations(BS_gam, data_types = c(\u0026quot;C\u0026quot;,\u0026quot;P\u0026quot;), interval_length = 20, rest_length = 5)\robs_gam$F_lambda \u0026lt;- \u0026quot;Gamma(3)\u0026quot;\robs_data \u0026lt;- rbind(obs_exp, obs_gam)\rqplot(x = C, y = P, color = F_lambda, data = obs_data, geom = \u0026quot;smooth\u0026quot;, method = \u0026quot;loess\u0026quot;, se = FALSE, ylim = c(-0.02, 1.02))\r## `geom_smooth()` using formula \u0026#39;y ~ x\u0026#39;\rThe gamma(3) interim time distribution leads to a slightly larger positive bias.\n\r\r","date":1382745600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1382745600,"objectID":"60df83d1509cb1dd0f11154cc7b5a384","permalink":"/pir-overestimates-prevalence/","publishdate":"2013-10-26T00:00:00Z","relpermalink":"/pir-overestimates-prevalence/","section":"post","summary":"It is well known that the partial interval recording procedure produces an over-estimate of the prevalence of a behavior. Here I will demonstrate how to use the ARPobservation package to study the extent of this bias.","tags":["behavioral observation","simulation","alternating renewal process"],"title":"To what extent does partial interval recording over-estimate prevalence?","type":"post"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rThe ARPobservation package provides a set of tools for simulating data generated by different procedures for direct observation of behavior. This is accomplished in two steps. The first step is to simulate a “behavior stream” itself, which is assumed to follow some type of alternating renewal process. The second step is to apply a procedure or “filter,” which turns the simulated behavior stream into the data recorded by a given observation procedure. Each of these steps is illustrated below.\nSimulating behavior streams\rBehavior streams are simulated according to an equilibrium alternating renewal process, which involves the following assumptions.\nEach instance of a behavior, termed an event, lasts a random amount of time, drawn from a specified distribution F_mu with mean mu.\n\rThe length of time in between instances of behavior, termed the interim time, also lasts a random amount of time, drawn from a specified distribution F_lambda with mean lambda.\n\rAll events and interim times are mutually independent.\n\rThe entire process is in equilibrium.\n\r\rThe function r_behavior_stream generates random behavior streams. As an initial example, suppose that both the events and the interim times are exponentially distributed, that events last on average 10 seconds, and that the average interim time is 30 seconds. Also suppose that the behavior stream is observed for 300 seconds. The following code will simulate a behavior stream with these parameters:\nlibrary(ARPobservation)\rset.seed(8) # for reproducibility\rr_behavior_stream(n = 1, mu = 10, lambda = 30, F_event = F_exp(), F_interim = F_exp(), stream_length = 300)\r## $stream_length\r## [1] 300\r## ## $b_streams\r## $b_streams[[1]]\r## $b_streams[[1]]$start_state\r## [1] 0\r## ## $b_streams[[1]]$b_stream\r## [1] 61.46643 67.45959 117.53097 120.56840 175.94950 185.74134 265.04376\r## [8] 269.42231 276.13827 284.70467 286.36179 290.82906\r## ## ## ## attr(,\u0026quot;class\u0026quot;)\r## [1] \u0026quot;behavior_stream\u0026quot;\rThe function returns an object of class behavior_stream, which isn’t terribly nice to look at. The first characteristic of the object is stream_length, which just reports back how long the behavior stream is. The second characteristic is b_streams, a list containing one or more simulated behavior streams. Each behavior stream is also a list. The first element indicate the initial state of the stream, so start_state =0 means that the behavior was not occuring when observation began. The second element is a vector of transition times. The first entry in the vector indicates that the first event began at time 61.47; the following entry indicates that the first event ended (and the next interim time began) at time 67.46. Similarly, the second event began at time 117.53 and ended at time 120.57.\nThe argument n controls the number of simulated behavior streams returned:\nr_behavior_stream(n = 3, mu = 10, lambda = 30, F_event = F_exp(), F_interim = F_exp(), stream_length = 300)\r## $stream_length\r## [1] 300\r## ## $b_streams\r## $b_streams[[1]]\r## $b_streams[[1]]$start_state\r## [1] 1\r## ## $b_streams[[1]]$b_stream\r## [1] 8.480116 34.311542 43.069956 49.912461 50.087867 85.046893\r## [7] 103.030351 116.377965 117.101992 140.227289 161.762642 180.640609\r## [13] 196.060432 201.493182 212.232970 236.486373 238.432946 276.824019\r## ## ## $b_streams[[2]]\r## $b_streams[[2]]$start_state\r## [1] 0\r## ## $b_streams[[2]]$b_stream\r## [1] 6.702804 23.820354 26.087981 33.461543 62.786605 74.705604\r## [7] 163.806646 164.761520 271.270557 283.207882 286.136103 297.587748\r## ## ## $b_streams[[3]]\r## $b_streams[[3]]$start_state\r## [1] 0\r## ## $b_streams[[3]]$b_stream\r## [1] 196.4605 203.7452 237.9514 245.2451 246.2089 254.6313 256.6439 258.5644\r## [9] 262.1140 265.3249 283.9702 298.7830\r## ## ## ## attr(,\u0026quot;class\u0026quot;)\r## [1] \u0026quot;behavior_stream\u0026quot;\rNote that now b_streams is a list with three entries, each of which contains a start_state and a b_stream.\nMost of the time, you won’t need to look at the simulated behavior streams directly. Instead, you’ll just simulate a bunch of streams and store them for later analysis. Let’s store 10 simulated behavior streams in an object called BS10:\nBS10 \u0026lt;- r_behavior_stream(n = 10, mu = 10, lambda = 30, F_event = F_exp(), F_interim = F_exp(), stream_length = 300)\r\rApplying observation procedures\rSeveral different functions are available to turn the behavior_stream object into familiar types of behavioral observation data. For example, the continuous recording procedure (CDR) involves summarizing the behavior stream by the overall proportion of observation time during which events occur. This can be accomplished by feeding BS into the function continuous_duration_recording:\ncontinuous_duration_recording(BS10)\r## [1] 0.1680877 0.4426930 0.1290537 0.3506492 0.2372437 0.3568621 0.2897521\r## [8] 0.2570101 0.1704727 0.2968024\rThe function returns a vector containing one number per simulated behavior stream. As expected all of the numbers are proportions between 0 and 1.\nMore interesting is to simulate many more behavior streams, apply CDR, and calculate the mean and variance of the results or plot them in a histogram:\nBS_lots \u0026lt;- r_behavior_stream(n = 10000, mu = 10, lambda = 30, F_event = F_exp(), F_interim = F_exp(), stream_length = 300)\rCDR \u0026lt;- continuous_duration_recording(BS_lots)\rc(mean = mean(CDR), var = var(CDR))\r## mean var ## 0.250140703 0.009567949\rhist(CDR)\rAnother well-known recording procedure is partial interval recording (PIR), which involves dividing the observation session into short intervals, then scoring each interval according to whether or not the behavior occurs at any point during the interval. The function interval_recording applies partial interval recording (or the closely related procedure of whole interval recording) to a set of simulated behavior streams. Suppose that the observer uses 20 s intervals, back-to-back for 300 s, for a total of 15 intervals. This procedure can be applied to the simulated behavior streams using\ninterval_recording(BS10, interval_length = 20, summarize = FALSE)\r## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\r## [1,] 1 0 1 0 1 1 0 0 0 1\r## [2,] 0 0 0 0 1 1 1 1 1 1\r## [3,] 1 1 1 1 1 1 1 1 1 1\r## [4,] 0 1 1 0 1 1 0 0 1 1\r## [5,] 0 1 0 1 0 1 1 0 1 0\r## [6,] 0 1 0 1 0 1 1 1 0 0\r## [7,] 0 1 0 1 0 1 1 1 0 0\r## [8,] 1 1 0 0 1 1 1 1 0 0\r## [9,] 0 1 0 1 0 1 0 1 0 0\r## [10,] 0 0 1 1 0 1 1 1 1 0\r## [11,] 1 0 0 1 0 1 1 1 0 1\r## [12,] 1 1 1 1 0 1 1 1 1 1\r## [13,] 1 1 0 1 0 1 1 1 0 1\r## [14,] 1 1 1 1 1 1 1 0 1 0\r## [15,] 1 1 1 1 1 1 0 1 0 1\rSince summarize is set to false, the function returns a 15 by 10 matrix, with one column for each behavior stream. Each column contains one entry for each interval, equal to one if any behavior occured during that interval (and zero otherwise). Typically, PIR data is summarized by calculating the proportion of intervals across the entire observation session. The summary proportion can be calculated automatically by setting the option summarize = TRUE.\ninterval_recording(BS10, interval_length = 20, summarize = TRUE)\r## [1] 0.5333333 0.7333333 0.4666667 0.7333333 0.4666667 1.0000000 0.7333333\r## [8] 0.7333333 0.4666667 0.5333333\rcolMeans(interval_recording(BS10, interval_length = 20, summarize = FALSE)) # compare to summarized results\r## [1] 0.5333333 0.7333333 0.4666667 0.7333333 0.4666667 1.0000000 0.7333333\r## [8] 0.7333333 0.4666667 0.5333333\rSometimes, the PIR procedure is used with a short amount of time in between each interval, which allows the observer to record data or notes. Typical use might involve 15 s intervals of active observation, each followed by 5 s of rest time. This procedure can be applied using the rest_proportion option. Since 5 s is 25% of the full interval length, the rest proportion is 0.25.\ninterval_recording(BS10, interval_length = 20, rest_length = 5, summarize = TRUE)\r## [1] 0.4000000 0.7333333 0.4000000 0.6000000 0.4666667 0.8666667 0.5333333\r## [8] 0.6666667 0.4000000 0.5333333\rThe whole interval recording procedure is implemented using interval_recording with partial = FALSE. Two other observation procedures are also available: momentary time recording (a.k.a. momentary time sampling), using the function momentary_time_recording, and event counting, using event_counting. See the documentation for these functions for usage and examples.\nFinally, a convenience function is available to apply multiple observation procedures to the same set of simulated behavior streams. Suppose that you want to compare the data generated by CDR with the data generated by PIR with 15 s active intervals and 5 s rest times. This can be accomplished using\nreported_observations(BS10, data_types = c(\u0026quot;C\u0026quot;, \u0026quot;P\u0026quot;), interval_length = 20, rest_length = 5)\r## C P\r## 1 0.1680877 0.4000000\r## 2 0.4426930 0.7333333\r## 3 0.1290537 0.4000000\r## 4 0.3506492 0.6000000\r## 5 0.2372437 0.4666667\r## 6 0.3568621 0.8666667\r## 7 0.2897521 0.5333333\r## 8 0.2570101 0.6666667\r## 9 0.1704727 0.4000000\r## 10 0.2968024 0.5333333\rThis function returns a data frame with one column for each procedure and one row for each simulated behavior stream. Say that you also want to include data based on momentary time recording, with 20 s in between each moment. Just add an \"M\" to the list of data types to include:\nreported_observations(BS10, data_types = c(\u0026quot;C\u0026quot;, \u0026quot;M\u0026quot;, \u0026quot;P\u0026quot;), interval_length = 20, rest_length = 5)\r## C M P\r## 1 0.1680877 0.20000000 0.4000000\r## 2 0.4426930 0.46666667 0.7333333\r## 3 0.1290537 0.06666667 0.4000000\r## 4 0.3506492 0.40000000 0.6000000\r## 5 0.2372437 0.26666667 0.4666667\r## 6 0.3568621 0.40000000 0.8666667\r## 7 0.2897521 0.26666667 0.5333333\r## 8 0.2570101 0.20000000 0.6666667\r## 9 0.1704727 0.06666667 0.4000000\r## 10 0.2968024 0.20000000 0.5333333\r\r","date":1382659200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1382659200,"objectID":"4249a21676a852592e12c9d852b49465","permalink":"/arpobservation-basic-use/","publishdate":"2013-10-25T00:00:00Z","relpermalink":"/arpobservation-basic-use/","section":"post","summary":"The ARPobservation package provides a set of tools for simulating data generated by different procedures for direct observation of behavior. This is accomplished in two steps. The first step is to simulate a “behavior stream” itself, which is assumed to follow some type of alternating renewal process.","tags":["behavioral observation","alternating renewal process","Rstats"],"title":"ARPobservation: Basic use","type":"post"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rUPDATED 5/29/2014 after posting the package to CRAN\nHere are step-by-step instructions on how to download and install ARPobservation. For the time being, ARPobservation is available as a pre-compiled binary for Windows. For Mac/Linux, you’ll have to download the source from Github.\nDownload and install R. R is free, open-source software that is used by many data analysts and statisticians. ARPobservation is a contributed package that runs within R, so you’ll need to get the base software first.\n\r(Optional but recommended) Download and install RStudio, which is a very nice front-end interface to R.\n\rOpen R or RStudio and type the following sequence of commands in the console:\n\r\rinstall.packages(\u0026quot;ARPobservation\u0026quot;)\rlibrary(ARPobservation)\rYou’ll only need to do the above once. Once you’ve got the package installed, type the following in order to access the package within an R session: library(ARPobservation).\nTo open the package documentation, type package?ARPobservation. To access the documentation for an individual function in this package, just type ? followed by the name of the function. For instance, one of the main functions in the package is called r_behavior_stream; to access its documentation, type ?r_behavior_stream.\n","date":1382572800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1382572800,"objectID":"16190a3fa455b660f033b0425fc5a49b","permalink":"/getting-started-with-arpobservation/","publishdate":"2013-10-24T00:00:00Z","relpermalink":"/getting-started-with-arpobservation/","section":"post","summary":"UPDATED 5/29/2014 after posting the package to CRAN\nHere are step-by-step instructions on how to download and install ARPobservation. For the time being, ARPobservation is available as a pre-compiled binary for Windows.","tags":["behavioral observation","simulation","Rstats"],"title":"Getting started with ARPobservation","type":"post"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rIn one example from my dissertation, I re-analyzed a systematic review by Shogren and colleagues, titled “The effect of choice-making as an intervention for problem behavior” (Shogren, et al., 2004). In order to do the analysis, I retrieved all of the original articles identified by the review, scanned in all of the graphs depicting the data, and used (actually, had an undergraduate use) a computer program called UnGraph to capture the data-points off of the graphs (see Shadish, et al., 2009 for details on this procedure).\nAs it turned out, Wim Van Den Noortgate and Patrick Onghena followed a similar procedure in analyzing the same systematic review (reported in Van Den Noorgate \u0026amp; Onghena, 2008). Wim and Patrick were kind enough to share their data so that I could calculate the reliability of this data extraction procedure, based on the two independent replications. After some initial data-munging, I arrived at a clean, merged dataset:\nShogren \u0026lt;- read.csv(\u0026quot;http://jepusto.com/data/Shogren_data_merged.csv\u0026quot;)\rhead(Shogren)\r## Study Case Setting Measure time choice Phase A B lowIntAxis\r## 1 Bambara Al Dessert Protests 1 0 A 5 5 1\r## 2 Bambara Al Dessert Protests 2 0 A 7 7 1\r## 3 Bambara Al Dessert Protests 3 0 A 4 4 1\r## 4 Bambara Al Dessert Protests 4 1 B 1 1 1\r## 5 Bambara Al Dessert Protests 5 1 B 0 0 1\r## 6 Bambara Al Dessert Protests 6 1 B 1 1 1\rThe variables are as follows:\n\rStudy - First author of original study included in the meta-analysis;\rCase - Name of individual case;\rSetting - some of the studies used multiple baselines on single individuals across multiple settings;\rMeasure - some of the studies used multiple outcome measures on each case;\rtime - sequential measurement occasion;\rchoice - indicator equal to one if the treatment condition allowed for choice;\rPhase - Factor indicating sequential phases (some of the designs were treatment reversals, such as ABA or ABAB or ABABAB);\rA - Wim’s outcome measurement;\rB - my outcome measurement;\rlowIntAxis - an idicator equal to one if the vertical axis of the graph was labeled with integers, and the axis maximum was \u0026lt;= 20.\r\rThe final variable distinguishes graphs that are particularly easy to capture. Wim/Patrick and I used slightly different exclusion criteria, so there are a total of 30 cases across 12 studies included in the merged dataset. To begin, here’s a plot of A versus B by study:\nlibrary(ggplot2)\rqplot(A, B, geom = \u0026quot;point\u0026quot;, color = Case, data = Shogren) + facet_wrap( ~ Study, scales = \u0026quot;free\u0026quot;) + theme(legend.position=\u0026quot;none\u0026quot;)\rClearly the two measurements are very correlated. You’ll notice that the studies (and sometimes cases within studies) used several different outcome measurement scales, so the overall correlation between A and B (r = 0.999767) isn’t really the best approach. Furthermore, some of the variation in the outcomes is presumably due to differences between phases, and it would be better to calculate a reliability based on the residual variation within phases.\nI accomplish this with a simple hierarchical model, fit separately to the data from each case. Denote the outcome as \\(y_{ijk}\\) for phase \\(i = 1,...,P\\), measurement occasion \\(j = 1,...,n_i\\), and replicate \\(k = 1,2\\). I model these outcomes as\n\\[y_{ijk} = \\beta_i + \\epsilon_{ij} + \\nu_{ijk}\\]\nwith the \\(\\beta\\)’s fixed, \\(\\epsilon_{ij} \\sim (0, \\tau^2)\\), and \\(\\nu_{ijk} \\sim (0, \\sigma^2)\\). Reliability is then captured by the intra-class correlation \\(\\rho = \\tau^2 / (\\tau^2 + \\sigma^2)\\).\nI calculate the reliabilities from each case using restricted maximum likelihood, then apply Fisher’s Z-transform:\nlibrary(reshape)\rlibrary(plyr)\rShogren_long \u0026lt;- melt(Shogren, measure.vars = c(\u0026quot;A\u0026quot;,\u0026quot;B\u0026quot;), variable_name = \u0026quot;observer\u0026quot;)\rFisher_Z \u0026lt;- function(x) 0.5 * (log(1 + x) - log(1 - x))\rlibrary(nlme)\rZ_ICC \u0026lt;- function(x, formula = value ~ Phase){\rfit \u0026lt;- lme(formula, random = ~ 1 | time, data = x)\rtau.sq.ratio \u0026lt;- as.double(coef(fit$modelStruct$reStruct, FALSE))\rrho \u0026lt;- tau.sq.ratio / (tau.sq.ratio + 1)\rZ \u0026lt;- Fisher_Z(rho)\rdf \u0026lt;- dim(x)[1] / 2 - length(fit$coefficients$fixed)\rreturn(c(rho = rho, Z = Z, df = df))\r}\rICC \u0026lt;- ddply(Shogren_long, .(Study, Case, Setting, Measure, lowIntAxis), Z_ICC)\rIt turns out that 5 of 6 cases with lowIntAxis==1 are perfectly correlated. The remainder of my analysis focuses on the cases with lowIntAxis==0. Here’s a histogram of the Z-transformed correlations:\nwith(subset(ICC, lowIntAxis==0), hist(Z))\rWith only 2 replicates per measurement occasion, the large-sample variance of the intra-class correlation is equivalent to that of the usual Pearson correlation (see Hedges, Hedberg, \u0026amp; Kuyper, 2013), except that I use \\(N - P\\) in the denominator to account for the fact that separate means are estimated for each of the \\(P\\) phases: \\[Var(\\hat\\rho) \\approx \\frac{(1 - \\rho^2)^2}{N - P},\\] where \\(N = \\sum_i n_i\\). Applying Fisher’s Z transform stabilizes the variance, so that it is appropriate to use inverse variance weights of simply \\(N - P\\). Turning to a random-effects meta-analysis:\nlibrary(metafor)\rsummary(rma_Z \u0026lt;- rma(yi = Z, vi = 1 / df, data = ICC, subset = lowIntAxis==0))\r## ## Random-Effects Model (k = 27; tau^2 estimator: REML)\r## ## logLik deviance AIC BIC AICc ## -26.1156 52.2313 56.2313 58.7475 56.7530 ## ## tau^2 (estimated amount of total heterogeneity): 0.3778 (SE = 0.1198)\r## tau (square root of estimated tau^2 value): 0.6146\r## I^2 (total heterogeneity / total variability): 88.15%\r## H^2 (total variability / sampling variability): 8.44\r## ## Test for Heterogeneity:\r## Q(df = 26) = 211.1324, p-val \u0026lt; .0001\r## ## Model Results:\r## ## estimate se zval pval ci.lb ci.ub ## 3.2596 0.1265 25.7670 \u0026lt;.0001 3.0117 3.5075 *** ## ## ---\r## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1\rThe average effect size corresponds to a reliability of 0.9970546 (95% CI: [0.9951684,0.9982051]). The reliabilities are heterogeneous, but because they are all at the extreme of the scale, the heterogeneity has little practical implication: approximating the population of reliabilities by a normal distribution, and based on the RML estimates, 84 percent of reliabilities will be greater than 0.9900. Though one could certainly imagine factors that might explain the variation in reliabilities–the resolution of the image file from which the data were captured, the size of the points used to graph each measurement, the number of outcomes represented on the same graph–it hardly seems worth exploring further because all of the reliabilities are so high. These results are very similar to those reported by Shadish, et al. (2009), who found a median reliability of 0.9993 based on a similar study of 91 single-case graphs.\nReferences\r\rHedges, L. V, Hedberg, E. C., \u0026amp; Kuyper, A. M. (2012). The variance of intraclass correlations in three- and four-level models. Educational and Psychological Measurement. doi:10.1177/0013164412445193\n\rShadish, W. R., Brasil, I. C. C., Illingworth, D. A., White, K. D., Galindo, R., Nagler, E. D., \u0026amp; Rindskopf, D. M. (2009). Using UnGraph to extract data from image files: Verification of reliability and validity. Behavior Research Methods, 41(1), 177-83. doi:10.3758/BRM.41.1.177\n\rShogren, K. A., Faggella-Luby, M. N., Bae, S. J., \u0026amp; Wehmeyer, M. L. (2004). The effect of choice-making as an intervention for problem behavior. Journal of Positive Behavior Interventions, 6(4), 228-237.\n\rVan den Noortgate, W., \u0026amp; Onghena, P. (2008). A multilevel meta-analysis of single-subject experimental design studies. Evidence-Based Communication Assessment and Intervention, 2(3), 142-151. doi:10.1080/17489530802505362\n\r\r\r","date":1382486400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1382486400,"objectID":"21dc5ac486481804c8344f8ed4036135","permalink":"/shogren-reliability-analysis/","publishdate":"2013-10-23T00:00:00Z","relpermalink":"/shogren-reliability-analysis/","section":"post","summary":"In one example from my dissertation, I re-analyzed a systematic review by Shogren and colleagues, titled “The effect of choice-making as an intervention for problem behavior” (Shogren, et al., 2004).","tags":["single-case design","inter-rater reliability"],"title":"Reliability of UnGraphed single-case data: An example using the Shogren dataset","type":"post"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rSeveral different approaches have been proposed for meta-analysis of correlation coefficients. One of the major differences between approaches is the choice of scale: whether effect sizes should be analyzed on the Pearson-r scale or first transformed to the Fisher-z scale. This project will study methods for modeling correlation coefficients on the r scale in the presence of between-study effect heterogeneity. Specific topics include:\n\rrefined methods for variance estimation;\rhierarchical modeling to capture differences between distinct operationalizations of the same construct; and\rapplication to a large correlational meta-analysis.\r\rThis project would be appropriate for a Quantitative Methods graduate student with interests in meta-analysis and hierarchical models.\n","date":1379030400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1379030400,"objectID":"05f63b35bd5eb7e8c29626b2e42e6388","permalink":"/another-project-idea/","publishdate":"2013-09-13T00:00:00Z","relpermalink":"/another-project-idea/","section":"post","summary":"Several different approaches have been proposed for meta-analysis of correlation coefficients. One of the major differences between approaches is the choice of scale: whether effect sizes should be analyzed on the Pearson-r scale or first transformed to the Fisher-z scale.","tags":["meta-analysis","robust variance estimation","correlation"],"title":"Another project idea: Meta-analytic methods for correlational data","type":"post"},{"authors":["Larry V. Hedges","James E. Pustejovsky","William R. Shadish"],"categories":null,"content":"","date":1377216000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1377216000,"objectID":"2d196f47606e342081a21bbe005425cb","permalink":"/publication/smd-for-mbd/","publishdate":"2013-08-23T00:00:00Z","relpermalink":"/publication/smd-for-mbd/","section":"publication","summary":"Single-case designs are a class of research methods for evaluating treatment effects by measuring outcomes repeatedly over time while systematically introducing different condition (e.g., treatment and control) to the same individual. The designs are used across fields such as behavior analysis, clinical psychology, special education, and medicine. Emerging standards for single-case designs have focused attention on methods for summarizing and meta-analyzing findings and on the need for effect sizes indices that are comparable to those used in between-subjects designs. In the previous work, we discussed how to define and estimate an effect size that is directly comparable to the standardized mean difference often used in between-subjects research based on the data from a particular type of single-case design, the treatment reversal or (AB)k design. This paper extends the effect size measure to another type of single-case study, the multiple baseline design. We propose estimation methods for the effect size and its variance, study the estimators using simulation, and demonstrate the approach in two applications.","tags":["design-comparable SMD","effect size","single-case design"],"title":"A standardized mean difference effect size for multiple baseline designs","type":"publication"},{"authors":["James E. Pustejovsky"],"categories":[],"content":"\rInterested in working with me? See below for descriptions of several potential projects. If you have interest and abilities that line up with one of these, feel free to contact me.\nReview of methods for direct observation of behavior. Several different methods for recording direct observations of behavior are commonly used in single-case research and other areas of psychology; prominent methods include continuous duration recording, momentary time sampling, and partial interval recording. Textbook advice about appropriate use of different methods is conflicting and often ambiguous, and simulation studies evaluating the operating characteristics of different methods also yield mixed results. The goals of this project are to: find and organize the current guidance about direct observation procedures; understand the basis of that guidance (e.g., simulation studies, heuristic models); and relate the guidance to a unifying statistical framework, by translating claims and conclusions into the terms of a parametric model (known as an alternating renewal process). This project would be appropriate either for a quantitative methods student who is interested in learning about direct observation methods for measuring behavior or for a student from school psychology, counseling psychology, or special education who is familiar with direct observation methods and interested in learning about statistical models for the data they generate.\n\rApplications of meta-analysis for single-case studies of free-operant behavior. I have recently proposed a suite of new effect size metrics for quantifying treatment effects in single-case studies of free-operant behavior. The crux of this line of work is that it is important to use effect size metrics that are comparable across different methods of recording direct observation data. This project will involve: reviewing several published systematic reviews that incorporate evidence from single-case studies, in order to determine what measurement procedures were used to collect data, then re-analyzing the data from one or more of these studies, using the newly proposed effect size metrics and methods. This project would be appropriate for a special education student who is familiar with meta-analysis.\n\rApplications of design-comparable effect size measures for longitudinal studies. Co-authors and I have recently proposed a method of estimating effect sizes from single-case studies (or other types of longitudinal designs) that are in the same metric as Cohen’s d-type effect sizes from conventional between-subjects experiments. The goals of this project are to: develop exemplar code that implements effect size calculations in several major statistical packages (including SPSS, SAS, Stata, and R); review the algorithms available in major statistical packages for estimating the uncertainty of variance components (i.e., information matrices); develop further applications and extensions to the proposed effect sizes. This project would be appropriate for a quantitative methods student who is familiar with estimation of hierarchical linear models in SPSS, SAS, and other major statistical software platforms.\n\rProgramming information matrices for hierarchical linear modeling. The Fisher information plays a pivotal role in hierarchical linear models, both as an approximate estimates of parameter uncertainty and as a key component of small-sample hypothesis tests such as those of Kenward and Roger (1997,2009). The goals of this project are to: create an R package for constructing analytic information matrices for HLM models estimated with the well-known nlme package; also add functions for the revised Kenward \u0026amp; Roger hypothesis tests; and evaluate the performance of different information matrices (expected, observed, and average) for calculating degrees-of-freedom adjustments in the context of effect size estimation. This project could be appropriate for a quantitative methods student or a statistics student who has strong programming skills and wants to 1) learn more about the statistical guts of HLM estimation and 2) level-up on their R programming by designing a publishable package.\n\rA discrete-time Markov chain model for partial interval recording data. Partial interval recording is a commonly used method for recording direct observations of human behavior. Data generated by this method is problematic because, as typically analyzed, it yields upwardly biased measures of prevalence (the proportion of time that a behavior occurs). This shortcoming can be addressed by modeling the data using a discrete-time Markov chain and using maximum likelihood methods to estimate parameters corresponding directly to prevalence and incidence (the frequency with which new behaviors occur). The goals of this project are to create an R package implementing maximum likelihood estimation (and possibly other methods) for partial interval recording data and evaluate this estimation approach using asymptotic theory and simulation. This project could be appropriate for an advanced quantitative methods student or statistics student who is interested in learning about Markov chain models and who has strong programming skills.\n\r\r","date":1376956800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1376956800,"objectID":"02aa0f0b0644884c7c24956aa942316b","permalink":"/current-projects/","publishdate":"2013-08-20T00:00:00Z","relpermalink":"/current-projects/","section":"post","summary":"Interested in working with me? See below for descriptions of several potential projects. If you have interest and abilities that line up with one of these, feel free to contact me.","tags":["behavioral observation","single-case design"],"title":"Current projects","type":"post"},{"authors":["William R. Shadish","Larry V. Hedges","James E. Pustejovsky","Jonathan G. Boyajian","Kristynn J. Sullivan","Alma Andrade","Jeannette Barrientos"],"categories":null,"content":"","date":1374105600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1374105600,"objectID":"23c2a67e273840f009c7439010fdeb04","permalink":"/publication/between-groups-d-statistic/","publishdate":"2013-07-18T00:00:00Z","relpermalink":"/publication/between-groups-d-statistic/","section":"publication","summary":"We describe a standardised mean difference statistic (d) for single-case designs that is equivalent to the usual d in between-groups experiments. We show how it can be used to summarise treatment effects over cases within a study, to do power analyses in planning new studies and grant proposals, and to meta-analyse effects across studies of the same question. We discuss limitations of this d-statistic, and possible remedies to them. Even so, this d-statistic is better founded statistically than other effect size measures for single-case design, and unlike many general linear model approaches such as multilevel modelling or generalised additive models, it produces a standardised effect size that can be integrated over studies with different outcome measures. SPSS macros for both effect size computation and power analysis are available.","tags":["single-case design","design-comparable SMD","effect size"],"title":"A d-statistic for single-case designs that is equivalent to the usual between-groups d-statistics","type":"publication"},{"authors":["James E. Pustejovsky"],"categories":null,"content":"This thesis studies quantitative methods for summarizing and synthesizing single-case studies, a class of research designs for evaluating the effects of interventions through repeated measurement of individuals. Despite long-standing interest in meta-analytic synthesis of single-case research, there remains a lack of consensus about appropriate methods, even about the most basic question of what effect size metrics are useful and appropriate. I argue that operational comparability, or invariance to heterogeneous operational procedures, is crucial property for an effect size metric. I then consider two problems with operational comparability that arise in single-case research. The first problem is to find effect sizes that can be applied across studies that use different research designs, such as single-case designs and two-group randomized experiments. The second problem is to find effect sizes that can be applied across studies that use varied operations for measuring the same construct. To address each of these problems, I propose structural models that capture essential features of multiple relevant operations (either design-related operations or measurement-related operations). I then use these structural models to precisely define target effect size parameters and to consider identification issues and estimation strategies.\nChapter 1 defines operational comparability and situates the concept within the broad methodological concerns of meta-analysis, then reviews relevant features of single-case research and previously proposed effect sizes. Chapter 2 describes an abstract set of modeling criteria for constructing design-comparable effect sizes. Chapters 3 applies the general criteria to the case of standardized mean differences and proposes an effect size estimator based on restricted maximum likelihood. Chapter 4 presents several applications of the proposed models and methods. Chapter 5 proposes measurement-comparability model and defines effect size measures for use in studies of free-operant behavior, one of the most common classes of outcomes in single-case research. Chapter 6 extends the proposed effect size models to incorporate more complex features, including time trends and serial dependence, and studies a method of estimating those models through a combination of marginal quasi-likelihood and Gaussian pseudo-likelihood estimating equations. Chapter 7 collects various further extensions, areas for further research, and concluding thoughts.\n","date":1370044800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1370044800,"objectID":"69191c30f124833ab0f3fc545dea0952","permalink":"/publication/operationally-comparable-effect-sizes/","publishdate":"2013-06-01T00:00:00Z","relpermalink":"/publication/operationally-comparable-effect-sizes/","section":"publication","summary":"This thesis studies quantitative methods for summarizing and synthesizing single-case studies, a class of research designs for evaluating the effects of interventions through repeated measurement of individuals. Despite long-standing interest in meta-analytic synthesis of single-case research, there remains a lack of consensus about appropriate methods, even about the most basic question of what effect size metrics are useful and appropriate.","tags":["alternating renewal process","design-comparable SMD","effect size","hierarchical models","response ratio","single-case design","behavioral observation"],"title":"Operationally comparable effect sizes for meta-analysis of single-case research","type":"publication"},{"authors":[],"categories":null,"content":"","date":1369785600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1369785600,"objectID":"424240a8138a04271f587ddae1f0407b","permalink":"/talk/nu-stats-2013-markov-models-for-direct-observation/","publishdate":"2018-04-15T08:15:00Z","relpermalink":"/talk/nu-stats-2013-markov-models-for-direct-observation/","section":"talk","summary":"","tags":[],"title":"Some Markov models for direct observation of behavior","type":"talk"},{"authors":[],"categories":null,"content":"","date":1369699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1369699200,"objectID":"443147e931b739796ba6fe635dc07f13","permalink":"/talk/abai-2013-effect-sizes/","publishdate":"2018-04-15T08:15:00Z","relpermalink":"/talk/abai-2013-effect-sizes/","section":"talk","summary":"","tags":[],"title":"Effect sizes and measurement comparability for meta-analysis of single-case research","type":"talk"},{"authors":[],"categories":null,"content":"","date":1367280000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1367280000,"objectID":"63a50ca61be1ca949b5b2d437b36819f","permalink":"/talk/aera-2013-observation-procedures/","publishdate":"2018-04-15T08:15:00Z","relpermalink":"/talk/aera-2013-observation-procedures/","section":"talk","summary":"","tags":[],"title":"Observation procedures and Markov chain models for estimating the prevalence and incidence of a behavior","type":"talk"},{"authors":[],"categories":null,"content":"","date":1362614400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1362614400,"objectID":"3594393d097ae431382d04698594f000","permalink":"/talk/sree-2013-operationally-comparable-effect-sizes/","publishdate":"2018-04-15T08:15:00Z","relpermalink":"/talk/sree-2013-operationally-comparable-effect-sizes/","section":"talk","summary":"","tags":[],"title":"Operationally comparable effect sizes for meta-analysis of single-case research","type":"talk"},{"authors":[],"categories":null,"content":"","date":1352851200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1352851200,"objectID":"fba0e7a509a70237e6e31cebf1645819","permalink":"/talk/ut-austin-2012-meta-analysis-of-single-case-research/","publishdate":"2018-04-15T08:15:00Z","relpermalink":"/talk/ut-austin-2012-meta-analysis-of-single-case-research/","section":"talk","summary":"","tags":[],"title":"Some implications of behavioral observation procedures for meta-analysis of single-case research","type":"talk"},{"authors":["Larry V. Hedges","James E. Pustejovsky","William R. Shadish"],"categories":null,"content":"","date":1344902400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1344902400,"objectID":"b6ec615f1636d8305297211e686eb3f4","permalink":"/publication/smd-for-scd/","publishdate":"2012-08-14T00:00:00Z","relpermalink":"/publication/smd-for-scd/","section":"publication","summary":"Single case designs are a set of research methods for evaluating treatment effects by assigning different treatments to the same individual and measuring outcomes over time and are used across fields such as behavior analysis, clinical psychology, special education, and medicine. Emerging standards for single case designs have focused attention on the need for effect sizes for summarizing and meta-analyzing findings from the designs; although many effect size measures have been proposed, there is little consensus regarding their use. This article proposes a new effect size measure for single case research that is directly comparable with the standardized mean difference (Cohen's d) often used in between-subjects designs. Techniques are provided for estimating the new effect size, as well as its variance, from balanced or unbalanced treatment reversal designs. The proposed estimation methods are further evaluated using a simulation study and then demonstrated in two applications.","tags":["design-comparable SMD","effect size","single-case design"],"title":"A standardized mean difference effect size for single case designs","type":"publication"},{"authors":["James E. Pustejovsky","James P. Spillane"],"categories":null,"content":"","date":1254355200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1254355200,"objectID":"f9f82b3efcb6b2e6774b3baccf9a427b","permalink":"/publication/question-order-effects/","publishdate":"2009-10-01T00:00:00Z","relpermalink":"/publication/question-order-effects/","section":"publication","summary":"Social network surveys are an important tool for empirical research in a variety of fields, including the study of social capital and the evaluation of educational and social policy. A growing body of methodological research sheds light on the validity and reliability of social network survey data regarding a single relation, but much less attention has been paid to the measurement of multiplex networks and the validity of comparisons among criterion relations. In this paper, we identify ways that surveys designed to collect multiplex social network data might be vulnerable to question-order effects. We then test several hypotheses using a split-ballot experiment embedded in an online multiple name generator survey of teachers’ advice networks, collected for a study of complete networks. We conclude by discussing implications for the design of multiple name generator social network surveys.","tags":["question-order","social networks"],"title":"Question-order effects in social network name generators","type":"publication"},{"authors":[],"categories":null,"content":"","date":1201046400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1201046400,"objectID":"359f5606683ab2140467da7c8b7a74b0","permalink":"/talk/issna-2008-question-order-effects/","publishdate":"2018-04-15T08:15:00Z","relpermalink":"/talk/issna-2008-question-order-effects/","section":"talk","summary":"","tags":[],"title":"Question-order effects in social network name generators","type":"talk"}]