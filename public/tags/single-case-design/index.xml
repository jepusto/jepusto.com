<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>single-case design | James E. Pustejovsky</title>
    <link>/tags/single-case-design/</link>
      <atom:link href="/tags/single-case-design/index.xml" rel="self" type="application/rss+xml" />
    <description>single-case design</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2021</copyright><lastBuildDate>Thu, 08 Jul 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>single-case design</title>
      <link>/tags/single-case-design/</link>
    </image>
    
    <item>
      <title>Multi-level meta-analysis of single-case experimental designs using robust variance estimation</title>
      <link>/publication/sced-mlma-rve/</link>
      <pubDate>Thu, 08 Jul 2021 00:00:00 +0000</pubDate>
      <guid>/publication/sced-mlma-rve/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Systematic review and meta-analysis of stay-play-talk interventions for improving social behaviors of young children</title>
      <link>/publication/stay-play-talk-meta-analysis/</link>
      <pubDate>Tue, 05 Jan 2021 00:00:00 +0000</pubDate>
      <guid>/publication/stay-play-talk-meta-analysis/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The impact of response-guided designs on count outcomes in single-case experimental design baselines</title>
      <link>/publication/response-guided-designs-in-sced-baselines/</link>
      <pubDate>Thu, 26 Mar 2020 00:00:00 +0000</pubDate>
      <guid>/publication/response-guided-designs-in-sced-baselines/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An examination of measurement procedures and characteristics of baseline outcome data in single-case research</title>
      <link>/publication/measurement-procedures-and-baseline-outcomes/</link>
      <pubDate>Fri, 02 Aug 2019 00:00:00 +0000</pubDate>
      <guid>/publication/measurement-procedures-and-baseline-outcomes/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Examining the effects of social stories on challenging behavior and prosocial skills in young children: A systematic review and meta-analysis</title>
      <link>/publication/effects-of-social-stories-on-challenging-behavior-and-prosocial-skills/</link>
      <pubDate>Wed, 15 May 2019 00:00:00 +0000</pubDate>
      <guid>/publication/effects-of-social-stories-on-challenging-behavior-and-prosocial-skills/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Procedural sensitivities of effect sizes for single-case designs with behavioral outcome measures</title>
      <link>/publication/procedural-sensitivities-of-scd-effect-sizes/</link>
      <pubDate>Mon, 01 Apr 2019 00:00:00 +0000</pubDate>
      <guid>/publication/procedural-sensitivities-of-scd-effect-sizes/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Easily simulate thousands of single-case designs</title>
      <link>/easily-simulate-thousands-of-single-case-designs/</link>
      <pubDate>Thu, 21 Jun 2018 00:00:00 +0000</pubDate>
      <guid>/easily-simulate-thousands-of-single-case-designs/</guid>
      <description>


&lt;p&gt;Earlier this month, I taught at the &lt;a href=&#34;https://scdinstitute2018.com/&#34;&gt;Summer Research Training Institute on Single-Case Intervention Design and Analysis workshop&lt;/a&gt;, sponsored by the Institute of Education Sciences’ National Center for Special Education Research.
While I was there, I shared &lt;a href=&#34;https://jepusto.shinyapps.io/ARPsimulator/&#34;&gt;a web-app for simulating data from a single-case design&lt;/a&gt;.
This is a tool that I put together a couple of years ago as part of my &lt;a href=&#34;/software/arpobservation/&#34;&gt;ARPobservation R package&lt;/a&gt;, but haven’t ever really publicized or done anything formal with.
It provides an easy way to simulate “mock” data from a single-case design where the dependent variable is measured using systematic direct observation of behavior.
The simulated data can be viewed in the form of a graph or downloaded as a csv file.
And it’s quite fast—simulating 1000’s of mock single-case designs takes only a few seconds.
The tool also provides a visualization of the distribution of effect size estimates that you could anticipate observing in a single-case design, given a set of assumptions about how the dependent variable is measured and how it changes in response to treatment.&lt;/p&gt;
&lt;div id=&#34;demo&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Demo&lt;/h1&gt;
&lt;p&gt;Here’s an example of the sort of data that the tool generates and the assumptions it asks you to make.
Say that you’re interested in evaluating the effect of a Social Stories intervention on the behavior of a child with autism spectrum disorder, and that you plan to use a treatment reversal design.
Your primary dependent variable is inappropriate play behavior, measured using frequency counts over ten minute observation sessions.&lt;br /&gt;
The initial baseline and treatment phases will be 7 sessions long.
At baseline, the child engages in inappropriate play at a rate of about 0.8 per minute.
You anticipate that the intervention could reduce inappropriate play by as much as 90% from baseline.
Enter all of these details and assumptions into the simulator, and it will generate a graph like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/Crozier-Tincani-simulated-data-A.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Hit the “Simulate!” button again and you might get something like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/Crozier-Tincani-simulated-data-B.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Or one of these:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/Crozier-Tincani-simulated-data-C.png&#34; /&gt;
&lt;img src=&#34;/img/Crozier-Tincani-simulated-data-A.png&#34; /&gt;
&lt;img src=&#34;/img/Crozier-Tincani-simulated-data-A.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/Crozier-Tincani-simulated-data-D.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/Crozier-Tincani-simulated-data-E.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;All of the above graphs were generated from the same hypothetical model—the variation in the clarity and strength of the functional relation is due to random error alone.
The simulator can also produce graphs that show multiple realizations of the data-generating process. Here’s one with five replications:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/Crozier-Tincani-simulated-data-5.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And here’s the same figure, but with trend lines added:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/img/Crozier-Tincani-simulated-data-trend.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The trend lines represent the overall average level of the dependent variable during each session, across infinitely replications of the study.
The variability around the trend line provides a sense of the extent of random error in the measurements of the dependent variable.&lt;/p&gt;
&lt;p&gt;I think it’s a rather interesting exercise to try and draw inferences based on visual inspection of randomly generated graphs like this—particularly because it forces you to grapple with random measurement error in a way that using only real data (or only hand-drawn mock data) doesn’t allow.
It seems like it could really help a visual analyst to calibrate their interpretations of single-case graphs with visually apparent time trends, outliers, etc.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;use-cases&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Use cases&lt;/h1&gt;
&lt;p&gt;So far, this tool is really only a toy—something that I’ve puttered with off and on for a while, but never developed or applied for any substantive purpose.
However, it occurs to me that it (or something similar to it) might have a number of purposes related to planning single-case studies, studying the process of visual inspection, or training single-case researchers.&lt;/p&gt;
&lt;p&gt;When I originally put the tool together, the leading case I imagined was to use the tool to help researchers make principled decisions about how to measure dependent variables in single-case designs.
By using the tool to simulate hypothetical single-case studies, a researcher would be able to experiment with different measurement strategies—such as using partial interval recording instead of continuous duration recording, using shorter or longer observation sessions, or using short or longer baseline phases—before collecting data on real-life behavior in the field.
I’m not sure if this is something that well-trained single-case researchers would actually find helpful, but it seems like it might help a novice (like me!) to temper one’s expectations or to move towards a more reliable measurement system.&lt;/p&gt;
&lt;p&gt;There’s been quite a bit of research examining the reliability and error rates of inferences based on visual inspection (see &lt;a href=&#34;http://dx.doi.org/10.1037/14376-004&#34;&gt;Chapter 4 of Kratochwill &amp;amp; Levin, 2014&lt;/a&gt; for a review of some of this literature).
Some of this work has compared the inferences drawn by novices versus experts or by un-aided visual inspection versus visual inspection supplemented with graphical guides (like trend lines).
But there are many other factors that could be investigated too, such as phase lengths (this could help to better justify the WWC single-case design standards around minimum phase lengths), use of different measurement systems, or use of different design elements on single-case graphs (can we get some color on these graphs, folks?!? And stop plotting 14 different dependent variables on the same graph?!?).
The simulator would be an easy way to generate the stimuli one would need to do this sort of work.&lt;/p&gt;
&lt;p&gt;A closely related use-case is to generate stimuli for training researchers to do systematic visual inspection.
Some of the SCD Institute instructors (including Tom Kratochwill, Rob Horner, Joel Levin, along with some of their other colleagues) have developed the website &lt;a href=&#34;http://www.singlecase.org&#34;&gt;www.singlecase.org&lt;/a&gt; with a bunch of exercises meant to help researchers develop and test their visual analysis skills.
It looks to me like the site uses simulated data (though I’m not entirely sure).
The ARPsimulator tool could be used to do something similar, but based on a data-generating process that captures many of the features of systematic direct observation data.
This might let researchers test their skills under more challenging and ambiguous, yet plausible, conditions, similar to what they will encounter when collecting real data in the field.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;future-directions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Future directions&lt;/h1&gt;
&lt;p&gt;A number of future directions for this project have crossed my mind:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Currently, the outcome data are simulated as independent across observation sessions (given the true time trend). It wouldn’t be too hard to add a further option to generate auto-correlated data, although this would further increase the complexity of the model. Perhaps there would be a way to add this as an “advanced” option that would be concealed unless the user asks for it (i.e., “Are you Really Sure you want to go down this rabbit hole?”). So far, I have avoided adding these features because I’m not sure what reasonable defaults would be.&lt;/li&gt;
&lt;li&gt;Joel Levin, John Ferron, and some of the other SCD Institute instructors are big proponents of incorporating randomization procedures into the design of single-case studies, at least when circumstances allow. Currently, the ARPsimulator generates data based on a fixed, pre-specified design, such as an ABAB design with 6 sessions per phase or a multiple baseline design with 25 sessions total and intervention start-times of 8, 14, and 20. It wouldn’t be too hard to incorporate randomized phase-changes into the simulator. This might make a nice, contained project for a student who wants to learn more about randomization designs.&lt;/li&gt;
&lt;li&gt;Along similar lines, John Ferron has &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.3200/JEXE.75.1.66-81&#34;&gt;developed&lt;/a&gt; and &lt;a href=&#34;https://doi.org/10.1002/jaba.410&#34;&gt;evaluated&lt;/a&gt; masked visual analysis procedures, which blend randomization and traditional response-guided approaches to designing single-case studies. It would take a bit more work, but it would be pretty nifty to incorporate these designs into ARPsimulator too.&lt;/li&gt;
&lt;li&gt;Currently, the model behind ARPsimulator asks the user to specify a fixed baseline level of behavior, and this level of behavior is used for every simulated case—even in designs involving multiple cases. A more realistic (albeit more complicated) data-generating model would allow for between-case variation in the baseline level of behavior.&lt;/li&gt;
&lt;li&gt;Perhaps the most important outstanding question about the premise of this work is just how well the alternating renewal process model captures the features of real single-case data. Validating the model against empirical data from single-case studies would allow use to assess whether it is really a realistic approach to simulation, at least for certain classes of behavior. Another product of such an investigation would be to develop realistic default assumptions for the model’s parameters.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At the moment I have no plans to implement any of these unless there’s a reasonably focused need (sadly, I don’t have time to putter and putz to the same extent that I used to).
If you, dear reader, would be interested in helping to pursue any of these directions, or if you have other, better ideas for how to make use of this tool, I would love to hear from you.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Between-case standardized effect size analysis of single case design: Examination of the two methods</title>
      <link>/publication/bcsmd-examination-of-two-methods/</link>
      <pubDate>Sat, 02 Jun 2018 00:00:00 +0000</pubDate>
      <guid>/publication/bcsmd-examination-of-two-methods/</guid>
      <description></description>
    </item>
    
    <item>
      <title>New paper: A gradual effects model for single-case designs</title>
      <link>/gradual-effects-model-paper/</link>
      <pubDate>Mon, 14 May 2018 00:00:00 +0000</pubDate>
      <guid>/gradual-effects-model-paper/</guid>
      <description>


&lt;p&gt;I’m very happy to share a new paper, co-authored with my student Danny Swan, “A gradual effects model for single-case designs,” which is now available online at &lt;em&gt;Multivariate Behavioral Research&lt;/em&gt;. You can access the published version &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1080/00273171.2018.1466681?journalCode=hmbr20&#34;&gt;at the journal website&lt;/a&gt; (&lt;a href=&#34;https://www.tandfonline.com/eprint/dkVfazwZaxnyaa9MfUjw/full&#34;&gt;click here for free access while supplies last&lt;/a&gt;) or &lt;a href=&#34;https://psyarxiv.com/vh964/&#34;&gt;the pre-print on PsyArxiv&lt;/a&gt; (always free!). Here’s &lt;a href=&#34;/publication/gradual-effects-model&#34;&gt;the abstract&lt;/a&gt; and &lt;a href=&#34;https://osf.io/uzkq6/&#34;&gt;the supplementary materials&lt;/a&gt;. Danny wrote R functions for fitting the model, (available as part of the &lt;a href=&#34;/software/singlecasees/&#34;&gt;SingleCaseES package&lt;/a&gt;) as well as a &lt;a href=&#34;https://jepusto.shinyapps.io/gem-scd/&#34;&gt;slick web interface&lt;/a&gt;, if you prefer to point-and-click.&lt;/p&gt;
&lt;p&gt;This paper grew out of Danny’s qualifying process (QP), which is the major exam that our doctoral students have to pass before they can begin their dissertation work. For the QP, students work with a faculty advisor to develop an extensive literature review and proposal for an original research project. They produce a written research proposal, then take written and oral exams on their work. For Danny’s QP, he picked up one of the many loose ends in &lt;a href=&#34;/publication/operationally-comparable-effect-sizes&#34;&gt;my dissertation&lt;/a&gt;, studied up on generalized linear models to understand how to express and fit the model, and developed a simulation study evaluating the model. After he successfully passed his QP, we worked together to refine the estimation methods and the simulation design, and then draft a manuscript (much of it cribbed from his QP proposal). I’m very proud and pleased that Danny continued to develop the work and saw it through to a first-authored publication.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A gradual effects model for single-case designs</title>
      <link>/publication/gradual-effects-model/</link>
      <pubDate>Tue, 01 May 2018 00:00:00 +0000</pubDate>
      <guid>/publication/gradual-effects-model/</guid>
      <description></description>
    </item>
    
    <item>
      <title>New paper: Using response ratios for meta-analyzing SCDs with behavioral outcomes</title>
      <link>/using-response-ratios-paper/</link>
      <pubDate>Fri, 16 Mar 2018 00:00:00 +0000</pubDate>
      <guid>/using-response-ratios-paper/</guid>
      <description>


&lt;p&gt;I’m pleased to announce that my article “Using response ratios for meta-analyzing SCDs with behavioral outcomes” has been accepted at &lt;em&gt;Journal of School Psychology&lt;/em&gt;. There’s a multitude of ways that you can access this work:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For the next 6 weeks or so, the published version of the article will be &lt;a href=&#34;https://authors.elsevier.com/a/1Wj5D56ZN7p98&#34;&gt;available at the journal website&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;The pre-print will always remain &lt;a href=&#34;https://psyarxiv.com/nj28d/&#34;&gt;available at PsyArXiv&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Some supporting materials and replication code are &lt;a href=&#34;https://osf.io/c3fe9/&#34;&gt;available on the Open Science Framework&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here’s the abstract of the paper:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Methods for meta-analyzing single-case designs (SCDs) are needed to inform evidence-based practice in clinical and school settingsand to draw broader and more defensible generalizations in areas where SCDs comprise a large part of the research base. The most widely used outcomesin single-case research are measures of behavior collected using systematic direct observation, which typically take the form of rates or proportions. For studies that use such measures, one simple and intuitive way to quantify effect sizes is in terms of proportionate change from baseline, using an effect size known as the log response ratio. This paper describes methods for estimating log response ratios and combining the estimates using meta-analysis. The methods are based on a simple model for comparing two phases, where the level of the outcome is stable within each phase and the repeated outcome measurements are independent. Although auto-correlation will lead to biased estimates of the sampling variance of the effect size, meta-analysis of response ratios can be conducted with robust variance estimation procedures that remain valid even when sampling variance estimates are biased. The methods are demonstrated using data from a recent meta-analysis on group contingency interventions for student problem behavior.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Single-case synthesis tools II: Comparing overlap measures and parametric effect sizes for synthesizing antecedent sensory-based interventions</title>
      <link>/publication/scd-synthesis-tools-ii/</link>
      <pubDate>Wed, 07 Mar 2018 00:00:00 +0000</pubDate>
      <guid>/publication/scd-synthesis-tools-ii/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Single-case synthesis tools I: Evaluating the quality and rigor of research on antecedent sensory-based interventions</title>
      <link>/publication/scd-synthesis-tools-i/</link>
      <pubDate>Sat, 03 Mar 2018 00:00:00 +0000</pubDate>
      <guid>/publication/scd-synthesis-tools-i/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Using response ratios for meta-analyzing single-case designs with behavioral outcomes</title>
      <link>/publication/using-response-ratios/</link>
      <pubDate>Thu, 01 Feb 2018 00:00:00 +0000</pubDate>
      <guid>/publication/using-response-ratios/</guid>
      <description></description>
    </item>
    
    <item>
      <title>New paper: procedural sensitivities of effect size measures for SCDs</title>
      <link>/procedural-sensitivities-paper/</link>
      <pubDate>Thu, 11 Jan 2018 00:00:00 +0000</pubDate>
      <guid>/procedural-sensitivities-paper/</guid>
      <description>


&lt;p&gt;I’m very happy to share that my article “Procedural sensitivities of effect sizes for single-case designs with directly observed behavioral outcome measures” has been accepted at &lt;em&gt;Psychological Methods&lt;/em&gt;. There’s no need to delay in reading it, since you can check out the &lt;a href=&#34;https://psyarxiv.com/vxa86&#34;&gt;pre-print&lt;/a&gt; and &lt;a href=&#34;https://osf.io/hkzsm/&#34;&gt;supporting materials&lt;/a&gt;. Here’s the abstract:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A wide variety of effect size indices have been proposed for quantifying the magnitude of treatment effects in single-case designs. Commonly used measures include parametric indices such as the standardized mean difference, as well as non-overlap measures such as the percentage of non-overlapping data, improvement rate difference, and non-overlap of all pairs. Currently, little is known about the properties of these indices when applied to behavioral data collected by systematic direct observation, even though systematic direct observation is the most common method for outcome measurement in single-case research. This study uses Monte Carlo simulation to investigate the properties of several widely used single-case effect size measures when applied to systematic direct observation data. Results indicate that the magnitude of the non-overlap measures and of the standardized mean difference can be strongly influenced by procedural details of the study’s design, which is a significant limitation to using these indices as effect sizes for meta-analysis of single-case designs. A less widely used parametric index, the log-response ratio, has the advantage of being insensitive to sample size and observation session length, although its magnitude is influenced by the use of partial interval recording.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This paper was a long time coming. The core idea came out of a grant proposal I wrote during the summer of 2014, which I fleshed out for a &lt;a href=&#34;/files/AERA-2015-poster-Non-overlap-measures.pdf&#34;&gt;poster presented at AERA&lt;/a&gt; in April of 2015. After finishing a draft of the paper, I tried to publish it in a special education journal, reasoning that the main audience for the paper is researchers interested in meta-analyzing single case research studies that are commonly used in some parts of special education. That turned out to be a non-starter. Four rejection letters later, I re-worked the paper a bit to give more technical details, then submitted it to a more methods-ish journal. This yielded an R&amp;amp;R, I revised the paper extensively, resubmitted it, and it was declined. Buying in fully to the sunk costs fallacy, I sent the paper to Psychological Methods. This time, I received very extensive and helpful feedback from several anonymous reviewers and an associate editor (thank you, anonymous peers!), which helped me to revise the paper yet again, and this time it was accepted. Sixth time is the charm, as they say.&lt;/p&gt;
&lt;p&gt;Here’s the complete time-line of submissions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;August 5, 2015: submitted to journal #1 (special education)&lt;/li&gt;
&lt;li&gt;August 28, 2015: desk reject decision from journal #1&lt;/li&gt;
&lt;li&gt;September 3, 2015: submitted to journal #2 (special education)&lt;/li&gt;
&lt;li&gt;November 6, 2015: reject decision (after peer review) from journal #2&lt;/li&gt;
&lt;li&gt;November 18, 2015: submitted to journal #3 (special education)&lt;/li&gt;
&lt;li&gt;November 22, 2015: desk reject decision from journal #3 as not appropriate for their audience. I was grateful to get a quick decision.&lt;/li&gt;
&lt;li&gt;November 23, 2015: submitted to journal #4 (special education)&lt;/li&gt;
&lt;li&gt;February 17, 2016: reject decision (after peer review) from journal #4&lt;/li&gt;
&lt;li&gt;April 19, 2016: submitted to journal #5 (methods)&lt;/li&gt;
&lt;li&gt;August 16, 2016: revise-and-resubmit decision from journal #5&lt;/li&gt;
&lt;li&gt;October 14, 2016: re-submitted to journal #5&lt;/li&gt;
&lt;li&gt;February 2, 2017: reject decision from journal #5&lt;/li&gt;
&lt;li&gt;May 10, 2017: submitted to &lt;em&gt;Psychological Methods&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;September 1, 2017: revise-and-resubmit decision from &lt;em&gt;Psychological Methods&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;September 26, 2017: re-submitted to &lt;em&gt;Psychological Methods&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;November 22, 2017: conditional acceptance&lt;/li&gt;
&lt;li&gt;December 6, 2017: re-submitted with minor revisions&lt;/li&gt;
&lt;li&gt;January 10, 2018: accepted at &lt;em&gt;Psychological Methods&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Back from the IES PI meeting</title>
      <link>/back-from-ies-pi-meeting/</link>
      <pubDate>Wed, 10 Jan 2018 00:00:00 +0000</pubDate>
      <guid>/back-from-ies-pi-meeting/</guid>
      <description>


&lt;p&gt;I’m just back from the Institute of Education Sciences’ Principle Investigators conference in Washington D.C. It was an envigorating trip for me, and not only because of the opportunity to catch up with colleagues and friends from across the country. A running theme across several of the keynote addresses was the importance of increasing the transparency and replicability of education research, and it was exciting to hear about promising reforms underway and to talk about how to change the norms of our discipline(s).&lt;/p&gt;
&lt;p&gt;I contributed to the conference in two ways. First, I gave a presentation on incorporating randomization and randomization inference into single-case designs, as part of a session on innovations in single-case research methods organized by Dr. Wendy Machalicek. You can a static version of &lt;a href=&#34;/files/Randomization-inference-for-SCED-2018-01-10.pdf&#34;&gt;my slides here&lt;/a&gt;; unfortunately the animations don’t work in pdf.&lt;/p&gt;
&lt;p&gt;Second, I brought &lt;a href=&#34;/files/Gradual-effects-model-poster-IES-2018-01-10.pdf&#34;&gt;a poster&lt;/a&gt; presenting some work from my IES-funded methods grant. Thanks very much to the folks who stopped by to talk during the poster session! Y’all gave me some very helpful feedback about technical aspects of the work and about how to better contextualize it for single case researchers.&lt;/p&gt;
&lt;p&gt;If you didn’t make it: this project was joint work with Danny Swan, a doctoral student in our Quantitative Methods program. It involved developing a model for estimating effect sizes from single-case designs where the effects of the intervention take time to reach full potency. Rather than assuming that the intervention produces immediate shifts in the level of the outcome, we model the effects using an impulse response function (cribbed from &lt;a href=&#34;http://www.jstor.org/stable/2285379&#34;&gt;an old paper by Box and Tiao&lt;/a&gt;) that leads to non-linear trends in response to the introduction of the intervention. Using an impulse response function also makes it possible to model more complex design patterns, like treatment reversal designs with returns-to-baseline and treatment re-introduction phases, in a very parsimonious way. Check out &lt;a href=&#34;https://osf.io/gaxrv/&#34;&gt;the full paper&lt;/a&gt;, the &lt;a href=&#34;https://github.com/jepusto/SingleCaseES&#34;&gt;accompanying R package&lt;/a&gt;, and Danny’s &lt;a href=&#34;https://jepusto.shinyapps.io/gem-scd/&#34;&gt;interactive web-app&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introduction to the special issue on single-case systematic reviews and meta-analysis</title>
      <link>/publication/rase-special-issue-introduction/</link>
      <pubDate>Wed, 01 Nov 2017 00:00:00 +0000</pubDate>
      <guid>/publication/rase-special-issue-introduction/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A meta-analysis of school-based group contingency interventions for students with challenging behavior: An update</title>
      <link>/publication/school-based-group-contingencies-meta-analysis/</link>
      <pubDate>Sat, 01 Jul 2017 00:00:00 +0000</pubDate>
      <guid>/publication/school-based-group-contingencies-meta-analysis/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A meta-analysis of technology-aided instruction and intervention for students with ASD</title>
      <link>/publication/taii-meta-analysis/</link>
      <pubDate>Sat, 01 Jul 2017 00:00:00 +0000</pubDate>
      <guid>/publication/taii-meta-analysis/</guid>
      <description></description>
    </item>
    
    <item>
      <title>New working paper: Using log response ratios for meta-analyzing SCDs with behavioral outcomes</title>
      <link>/using-log-response-ratios/</link>
      <pubDate>Wed, 26 Apr 2017 00:00:00 +0000</pubDate>
      <guid>/using-log-response-ratios/</guid>
      <description>


&lt;p&gt;One of the papers that came out of my dissertation work (&lt;a href=&#34;/files/Measurement-comparable-ES.pdf&#34;&gt;Pustejovsky, 2015&lt;/a&gt;) introduced an effect size metric called the &lt;strong&gt;log response ratio&lt;/strong&gt; (or LRR) for use in meta-analysis of single-case research—particularly for single-case studies that measure behavioral outcomes through systematic direct observation. The original paper was pretty technical since it focused mostly on a formal measurement model for behavioral observation data. I’ve just completed a tutorial paper that demonstrates how to use the LRR for meta-analyzing single-case studies with behavioral outcomes. In this paper, I’ve tried to present the methods in as accessible a manner as I could muster, to provide a sort of “user’s guide” for researchers wanting to apply the LRR for their own work. You can find the &lt;a href=&#34;https://osf.io/4fe6u/&#34;&gt;working paper&lt;/a&gt; and &lt;a href=&#34;https://osf.io/c3fe9/&#34;&gt;supplementary materials&lt;/a&gt; (including data and replication code) on the Open Science Framework. I would welcome your feedback and questions about this work!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Functional assessment-based interventions for students with or at-risk for high incidence disabilities: Field-testing single-case synthesis methods</title>
      <link>/publication/fabi-meta-analysis/</link>
      <pubDate>Wed, 01 Mar 2017 00:00:00 +0000</pubDate>
      <guid>/publication/fabi-meta-analysis/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Research synthesis and meta-analysis of single-case designs</title>
      <link>/publication/meta-analysis-of-scd/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>/publication/meta-analysis-of-scd/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Between-case standardized mean difference effect sizes for single-case designs: A primer and tutorial using the scdhlm web application</title>
      <link>/publication/bc-smd-primer-and-tutorial/</link>
      <pubDate>Mon, 19 Dec 2016 00:00:00 +0000</pubDate>
      <guid>/publication/bc-smd-primer-and-tutorial/</guid>
      <description></description>
    </item>
    
    <item>
      <title>New tutorial paper on BC-SMD effect sizes</title>
      <link>/scdhlm-tutorial/</link>
      <pubDate>Mon, 19 Dec 2016 00:00:00 +0000</pubDate>
      <guid>/scdhlm-tutorial/</guid>
      <description>


&lt;p&gt;I’m pleased to announce that the &lt;a href=&#34;https://www.campbellcollaboration.org/&#34;&gt;Campbell Collaboration&lt;/a&gt; has just published a new discussion paper that I wrote with my colleagues Jeff Valentine and Emily Tanner-Smith about &lt;a href=&#34;https://campbellcollaboration.org/library/effect-sizes-single-case-designs-campbell-discussion-paper-1.html&#34;&gt;between-case standardized mean difference effect sizes for single-case designs&lt;/a&gt;.
The paper provides a relatively non-technical introduction to BC-SMD effect sizes and a tutorial on how to use the &lt;a href=&#34;https://jepusto.shinyapps.io/scdhlm/&#34;&gt;scdhlm web-app&lt;/a&gt; for calculating estimates of the BC-SMD for user-provided data.
If you have any questions or feedback about the app, please feel free to contact me!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Presentation at IES 2016 PI meeting</title>
      <link>/ies-2016-pi-meeting/</link>
      <pubDate>Mon, 19 Dec 2016 00:00:00 +0000</pubDate>
      <guid>/ies-2016-pi-meeting/</guid>
      <description>


&lt;p&gt;I am just back from the Institute of Education Sciences 2016 Principal Investigators meeting. Rob Horner had organized a session titled “Single-case methods: Current status and needed directions” as a tribute to our colleague Will Shadish, who passed away this past year. Rob invited me to give some brief remarks about Will as a mentor, and then to present some of my work with Will and Larry Hedges on effect sizes for single-case research. Here are &lt;a href=&#34;/files/Single-case-methods-IES-PI-meeting-2016.pdf&#34;&gt;the slides from my part of the presentation&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>What is Tau-U?</title>
      <link>/what-is-tau-u/</link>
      <pubDate>Thu, 03 Nov 2016 00:00:00 +0000</pubDate>
      <guid>/what-is-tau-u/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;http://dx.doi.org/10.1016/j.beth.2010.08.006&#34;&gt;Parker, Vannest, Davis, and Sauber (2011)&lt;/a&gt; proposed the Tau-U index—actually several indices, rather—as effect size measures for single-case designs. The original paper describes several different indices that involve corrections for trend during the baseline phase, treatment phase, both phases, or neither phase. Without correcting for trends in either phase, the index is equal to the Mann-Whitney &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; statistic calculated by comparing every pair of observations containing one point from each phase, scaled by the total number of such pairs. This version, which I’ll call just “Tau”, is simply a &lt;a href=&#34;/NAP-SEs-and-CIs&#34;&gt;linear re-scaling of the NAP statistic&lt;/a&gt; to the range [-1,1].&lt;/p&gt;
&lt;p&gt;To correct for baseline trend, the original paper proposes to calculate Kendall’s rank correlation (&lt;span class=&#34;math inline&#34;&gt;\(\tau_A\)&lt;/span&gt;) between the phase A outcome data and the session numbers and use the result to make an adjustment to Tau. The other analyses presented in the original paper (incorporating adjustments for time trends during the treatment phase) are not presented in subsequent review papers, nor are they implemented in &lt;a href=&#34;http://singlecaseresearch.org/calculators&#34;&gt;the web-calculator&lt;/a&gt; created by the authors, and so I won’t discuss them further here. Instead, in this post I will examine the calculation of the version of Tau-U that incorporates a baseline trend correction. This version seems to be the most widely applied in practice (likely due to the availability of the web-calculator) and is presented in several review papers by the same authors. It turns out though, that the definition of the index has shifted from the original paper to subsequent presentations.&lt;/p&gt;
&lt;p&gt;To make this concrete, let me first define a couple of things. Suppose that we have data from the baseline and treatment phases for a single case, where the baseline phase has &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; observations and treatment phase has &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; observations. Let &lt;span class=&#34;math inline&#34;&gt;\(y^A_1,...,y^A_m\)&lt;/span&gt; denote the baseline phase data and &lt;span class=&#34;math inline&#34;&gt;\(y^B_1,...,y^B_n\)&lt;/span&gt; denote the treatment phase data. Let &lt;span class=&#34;math inline&#34;&gt;\(S_P\)&lt;/span&gt; denote Kendall’s S statistic calculated for the comparison between phases and &lt;span class=&#34;math inline&#34;&gt;\(S_A\)&lt;/span&gt; denote Kendall’s S statistic calculated on the baseline trend. More precisely,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
S_P &amp;amp;= \sum_{i=1}^m \sum_{j=1}^n \left[I\left(y^B_j &amp;gt; y^A_i\right) - I\left(y^B_j &amp;lt; y^A_i\right)\right] \\
S_A &amp;amp;= \sum_{i=1}^{m - 1} \sum_{j = i + 1}^m \left[I\left(y^A_j &amp;gt; y^A_i\right) - I\left(y^A_j &amp;lt; y^A_i\right)\right].
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note that &lt;span class=&#34;math inline&#34;&gt;\(S_P\)&lt;/span&gt; is calculated from &lt;span class=&#34;math inline&#34;&gt;\(m \times n\)&lt;/span&gt; pairs of observations, and Tau (without trend correction) is equal to &lt;span class=&#34;math inline&#34;&gt;\(\text{Tau} = S_P / (m n)\)&lt;/span&gt;. Furthermore, &lt;span class=&#34;math inline&#34;&gt;\(S_A\)&lt;/span&gt; is calculated from &lt;span class=&#34;math inline&#34;&gt;\(m (m - 1) / 2\)&lt;/span&gt; pairs of observations and Kendall’s rank correlation coefficient for the baseline phase observations is &lt;span class=&#34;math inline&#34;&gt;\(t_A = S_A / [m (m - 1) / 2]\)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;the-original-version&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The original version&lt;/h2&gt;
&lt;p&gt;In the original paper, the authors explain that values of Tau-U can be calculated by adding or substracting values of &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt;, weighted by the corresponding number of pairs. Thus, Tau-U would be calculated as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Tau-U} = \frac{S_P - S_A}{mn + m(m - 1) / 2} = \frac{2n}{2n + m - 1} \text{Tau} - \frac{m - 1}{2n + m - 1} t_A.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Both &lt;span class=&#34;math inline&#34;&gt;\(\text{Tau}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(t_A\)&lt;/span&gt; have range [-1,1], and so Tau-U has the same range. This version of Tau-U can be calculated using &lt;a href=&#34;https://manolov.shinyapps.io/Overlap/&#34;&gt;this web app by Rumen Manolov&lt;/a&gt;, which is based on &lt;a href=&#34;https://dl.dropboxusercontent.com/u/2842869/Tau_U.R&#34;&gt;this R code by Kevin Tarlow&lt;/a&gt;. (The app and the R script also provide the other variants of Tau-U described in &lt;a href=&#34;http://dx.doi.org/10.1016/j.beth.2010.08.006&#34;&gt;Parker, Vannest, Davis, and Sauber (2011)&lt;/a&gt;.)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-revised-version&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The revised (?) version&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://dx.doi.org/10.1177/0145445511399147&#34;&gt;Parker, Vannest, and Davis (2011)&lt;/a&gt; reviewed nine different non-overlap indices for use with data from single-case designs, including Tau-U. Rather than describing all four variations from the original paper, the authors define the index as follows:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Tau-U (Parker et al., in press) extends [Tau] to control for undesirable positive baseline trend (monotonic trend). Monotonic trend is the upward progression of data points in any configuration, whether linear, curvilinear, or even in a mixed pattern of “fits and starts” (p. 11).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In this and subsequent review articles, Tau-U seems to refer exclusively to the variant involving comparison between phases A and B, with an adjustment for phase A trend. That seems a sensible enough choice, which could have been due to space limitations, guidance from the journal editor, or further refinement of the methods (i.e., recognizing which of the variants would be most useful in application). However, the presentation of Tau-U in this article involved more than a change in emphasis—the definition of the index also changed. Following the notation above, Tau-U was now defined as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Tau-U} = \frac{S_P - S_A}{mn} = \text{Tau} - \frac{m - 1}{2n} t_A.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The logical range of this version of the index is from &lt;span class=&#34;math inline&#34;&gt;\(-(2n + m - 1) / (2n)\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\((2n + m - 1) / (2n)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This is the version of Tau-U implemented in the &lt;a href=&#34;http://singlecaseresearch.org/calculators&#34;&gt;singlecaseresearch.org&lt;/a&gt; web calculator. It is also the version described in a later chapter by the same authors (&lt;a href=&#34;http://dx.doi.org/10.1037/14376-005&#34;&gt;Parker, Vannest, &amp;amp; Davis, 2014&lt;/a&gt;) and a review article by &lt;a href=&#34;http://dx.doi.org/10.1111/1467-8578.12091&#34;&gt;Rakap (2015)&lt;/a&gt;. &lt;a href=&#34;/Tau-U&#34;&gt;My previous post about Tau-U&lt;/a&gt; also presented this version of the index and noted that its magnitude is sensitive to the lengths of the baseline and treatment phases, which makes it rather difficult to interpret the Tau-U index as a measure of treatment effect magnitude.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;comparison&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Comparison&lt;/h2&gt;
&lt;p&gt;Here is an R function for calculating the original or revised versions of Tau-U:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Tau_U &amp;lt;- function(A_data, B_data, version = &amp;quot;revised&amp;quot;) {
    m &amp;lt;- length(A_data)
    n &amp;lt;- length(B_data)
    Q_A &amp;lt;- sapply(A_data, function(j) (j &amp;gt; A_data) - (j &amp;lt; A_data))
    Q_P &amp;lt;- sapply(B_data, function(j) (j &amp;gt; A_data) - (j &amp;lt; A_data))
    
    if (version==&amp;quot;original&amp;quot;) {
      (sum(Q_P) - sum(Q_A[upper.tri(Q_A)])) / (m * n + m * (m - 1) / 2)
    } else {
      (sum(Q_P) - sum(Q_A[upper.tri(Q_A)])) / (m * n)
    }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The papers I’ve mentioned above all provide examples of the calculation of Tau-U. The following table reports the data from each of these examples (&lt;a href=&#34;http://dx.doi.org/10.1016/j.beth.2010.08.006&#34;&gt;Parker, Vannest, Davis, and Sauber, 2011a&lt;/a&gt;; &lt;a href=&#34;http://dx.doi.org/10.1177/0145445511399147&#34;&gt;Parker, Vannest, and Davis, 2011b&lt;/a&gt;; &lt;a href=&#34;http://dx.doi.org/10.1037/14376-005&#34;&gt;Parker, Vannest, &amp;amp; Davis, 2014&lt;/a&gt;), along with the value of Tau-U based on the original and revised formulas. The differences in magnitude are non-trivial.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Source&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Phase A data&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Phase B data&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;original&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;revised&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2011a, Figure 2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2, 3, 5, 3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;4, 5, 5, 7, 6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5000000&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.6500000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2011b, Figure 1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;20, 20, 26, 25, 22, 23&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;28, 25, 24, 27, 30, 30, 29&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5438596&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7380952&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2011b, Table 1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;3, 3, 4, 5&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;4, 5, 6, 7, 7&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4230769&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5500000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;2014, Figure 4.1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;22, 21, 23, 23, 23, 22&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;24, 22, 23, 23, 24, 26, 25&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.4385965&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5952381&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;implications&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Implications&lt;/h2&gt;
&lt;p&gt;Rather than one effect size index called “Tau-U”, there are instead two different definitions, which can lead to quite different values of the index. Given this, researchers who apply Tau-U should endeavor to &lt;strong&gt;be clear and unambiguous about which version of the index they use&lt;/strong&gt;. This can be done by stating exactly which software routine, web-app, or formula was used in making the calculations. If the calculations are done using a computer script, then the script should be made available (e.g., through the &lt;a href=&#34;https://osf.io/&#34;&gt;Open Science Framework&lt;/a&gt;) so that other researchers can replicate the calculations.&lt;/p&gt;
&lt;p&gt;Furthermore, researchers need to &lt;strong&gt;be careful about applying interpretive guidelines for Tau-U&lt;/strong&gt;, since those guidelines will not apply uniformly across the different versions of the index.&lt;/p&gt;
&lt;p&gt;Finally, I would recommend that any researchers who conduct a meta-analysis of single-case research &lt;strong&gt;make available the raw data used for effect size calculations&lt;/strong&gt;, so that other researchers can scrutinize, replicate, and extend their analyses. The whole enterprise of research synthesis rests on the availability of data from primary studies (at least in summary form). It seems to me that meta-analysts thus have a duty to make the data that they assemble and organize readily accessible for others to use. Particularly in the context of meta-analysis of single-case research—where new methods are developing rapidly and there is not currently consensus around best practices—it seems especially appropriate and prudent to make one’s data available for future re-analysis.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>New working paper: Procedural sensitivities of SCD effect sizes</title>
      <link>/scd-effect-size-sensitivities/</link>
      <pubDate>Mon, 17 Oct 2016 00:00:00 +0000</pubDate>
      <guid>/scd-effect-size-sensitivities/</guid>
      <description>


&lt;p&gt;I’ve just posted a new version of my working paper, &lt;em&gt;Procedural sensitivities of effect sizes for single-case designs with behavioral outcome measures&lt;/em&gt;. The abstract is below. This version is a major update of an &lt;a href=&#34;/files/Pustejovsky-2015-Nov-Non-overlap-measures.pdf&#34;&gt;earlier paper&lt;/a&gt; that focused only on the non-overlap measures. The new version also includes analysis of two other effect sizes (the within-case standardized mean difference and the log response ratio) as well as additional results and more succinct summaries of the main findings.&lt;/p&gt;
&lt;p&gt;The paper itself is available on the Open Science Framework (&lt;a href=&#34;https://osf.io/pxn24/&#34;&gt;here&lt;/a&gt;), as are the &lt;a href=&#34;https://osf.io/hkzsm/&#34;&gt;supplementary materials&lt;/a&gt; and &lt;a href=&#34;https://osf.io/j4gvt/&#34;&gt;Source code&lt;/a&gt;. I also created interaction versions of the graphics from the main paper and the supplementary materials, which can be viewed in &lt;a href=&#34;https://jepusto.shinyapps.io/SCD-effect-size-sensitivities/&#34;&gt;this shiny app&lt;/a&gt;. I would welcome any comments, questions, or feedback that readers may have.&lt;/p&gt;
&lt;div id=&#34;abstract&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;A wide variety of effect size indices have been proposed for quantifying the magnitude of treatment effects in single-case designs. Commonly used measures include parametric indices such as the standardized mean difference, as well as non-overlap measures, such as the percentage of non-overlapping data, improvement rate difference, and non-overlap of all pairs. Currently, little is known about the properties of these indices when applied to behavioral data collected by systematic direct observation, even though systematic direct observation is the most common approach to outcome measurement in single-case research. This study uses computer simulation to investigate the properties of several single-case effect size measures when applied to systematic direct observation data. Results indicate that the magnitude of the non-overlap measures and of the standardized mean difference can be strongly influenced by procedural details of the study’s design, which is a significant limitation to using these indices as effect sizes for meta-analysis of single-case designs. A less widely used parametric index, the log-response ratio, has the advantage of being insensitive to sample size and observation session length, although its magnitude is influenced by the use of partial interval recording.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>scdhlm</title>
      <link>/software/scdhlm/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>/software/scdhlm/</guid>
      <description>&lt;p&gt;An R package implementing several methods of estimating a design-comparable standardized mean difference effect size based on data from a single-case design. Methods include those from Hedges, Pustejovsky, &amp;amp; Shadish (
&lt;a href=&#34;/publication/SMD-for-SCD&#34;&gt;2012&lt;/a&gt;, 
&lt;a href=&#34;/publication/SMD-for-MBD&#34;&gt;2013&lt;/a&gt;) and 
&lt;a href=&#34;/publication/design-comparable-effect-sizes/&#34;&gt;Pustejovsky, Hedges, &amp;amp; Shadish (2014)&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://CRAN.R-project.org/package=scdhlm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Available on the Comprehensive R Archive Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;/getting-started-with-scdhlm&#34;&gt;Installation instructions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/jepusto/scdhlm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Source code on Github&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://jepusto.shinyapps.io/scdhlm/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;scdhlm&lt;/a&gt;: An interactive web application for calculating design-comparable standardized mean difference effect sizes.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>SingleCaseES</title>
      <link>/software/singlecasees/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>/software/singlecasees/</guid>
      <description>&lt;p&gt;An R package for calculating basic effect size indices for single-case designs, including several non-overlap measures and parametric effect size measures, and for estimating the gradual effects model developed by Swan and Pustejovsky (2017).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://CRAN.R-project.org/package=SingleCaseES&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Available on the Comprehensive R Archive Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/jepusto/SingleCaseES&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Source code and installation instructions on Github&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://jepusto.shinyapps.io/SCD-effect-sizes/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Single case effect size calculator&lt;/a&gt;: An interactive web application for calculating basic effect size indices.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://jepusto.shinyapps.io/gem-scd/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gradual Effect Model calculator&lt;/a&gt;: An interactive web application for estimating effect sizes using the gradual effects model.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Tau-U</title>
      <link>/tau-u/</link>
      <pubDate>Wed, 23 Mar 2016 00:00:00 +0000</pubDate>
      <guid>/tau-u/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;http://doi.org/10.1016/j.beth.2010.08.006&#34;&gt;Parker, Vannest, Davis, and Sauber (2011)&lt;/a&gt; proposed Tau-U as an effect size measure for use in single-case designs that exhibit baseline trend. In their original paper, they actually conceptualize Tau-U as a family of four distinct indices, distinguished by a) whether the index includes an adjustment for the presence of baseline trend and b) whether the index incorporates information about trend during the intervention phase. However, in subsequent presentations the authors seem to have focused exclusively on the index that adjusts for baseline trend but not for intervention phase trend, and so I’ll do the same here. (This version is also the one available in the web-tool at &lt;a href=&#34;http://singlecaseresearch.org/calculators&#34;&gt;singlecaseresearch.org&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;Tau-U is an elaboration on their previously proposed effect sizes &lt;a href=&#34;/NAP-SEs-and-CIs&#34;&gt;NAP and Tau&lt;/a&gt;, which do not account for baseline trends. The index is calculated as follows. Suppose that we have data from A and B phases from a single case, where the baseline phase has &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; observations and treatment phase has &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; observations. Let &lt;span class=&#34;math inline&#34;&gt;\(y^A_1,...,y^A_m\)&lt;/span&gt; denote the baseline phase data and &lt;span class=&#34;math inline&#34;&gt;\(y^B_1,...,y^B_n\)&lt;/span&gt; denote the treatment phase data. Tau-U is then calculated as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Tau-U} = \frac{S_P - S_B}{mn}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(S_P\)&lt;/span&gt; is Kendall’s S statistic calculated for the comparison between phases and &lt;span class=&#34;math inline&#34;&gt;\(S_B\)&lt;/span&gt; is Kendall’s S statistic calculated on the baseline trend. More precisely,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
S_P &amp;amp;= \sum_{i=1}^m \sum_{j=1}^n \left[I\left(y^B_j &amp;gt; y^A_i\right) - I\left(y^B_j &amp;lt; y^A_i\right)\right] \\
S_B &amp;amp;= \sum_{i=1}^{m - 1} \sum_{j = i + 1}^m \left[I\left(y^A_j &amp;gt; y^A_i\right) - I\left(y^A_j &amp;lt; y^A_i\right)\right].
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Note that the first term in Tau-U is equivalent to &lt;span class=&#34;math inline&#34;&gt;\(\text{Tau} = S_P / (m n)\)&lt;/span&gt;, which in turn is a re-scaling of NAP. The second term is related to the rank-correlation between the measurement occasions and outcomes in the baseline phase. Subtracting the second from the first thus adjusts for baseline trend, in the sense that more pronounced baseline trends will lead to smaller values of Tau-U. But looking at the measure a bit more deeply, it has some very odd features. In this post, I’ll show that the distribution of Tau-U is sensitive to the number of observations in each phase.&lt;/p&gt;
&lt;div id=&#34;sample-size-sensitivity&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sample size sensitivity&lt;/h2&gt;
&lt;p&gt;Consider first the logical range of Tau-U. The minimum and maximum possible values of &lt;span class=&#34;math inline&#34;&gt;\(S_P\)&lt;/span&gt; are &lt;span class=&#34;math inline&#34;&gt;\(-m n\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(m n\)&lt;/span&gt;; the minimum and maximum of &lt;span class=&#34;math inline&#34;&gt;\(S_B\)&lt;/span&gt; are &lt;span class=&#34;math inline&#34;&gt;\(-m (m-1) / 2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(m (m - 1) / 2\)&lt;/span&gt;. Consequently, the logical range of Tau-U is from &lt;span class=&#34;math inline&#34;&gt;\(-(2n + m - 1) / (2n)\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\((2n + m - 1) / (2n)\)&lt;/span&gt;. If the treatment phase is quite long compared to the baseline phase, then this range will be close to [-1, 1]. On the other hand, in a study with a baseline that is twice as long as the treatment phase, the range of Tau-U will be closer to [-2, 2]. That’s a very odd property.&lt;/p&gt;
&lt;p&gt;The average magnitude of Tau-U is similarly influenced by the lengths of each phase. To see this, it’s helpful to think first about its target parameter–the quantity that is estimated when calculating Tau-U based on a sample of data. Since Tau-U is not defined in parametric terms, I will assume that the Tau-U statistic is an unbiased estimator of its target parameter &lt;span class=&#34;math inline&#34;&gt;\(\tau_U = \text{E}\left(\text{Tau-U}\right)\)&lt;/span&gt;. It follows that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\tau_U = \tau_P - \frac{m - 1}{2n} \tau_B,
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\tau_P\)&lt;/span&gt; is Kendall’s rank correlation between the outcomes and an indicator for the treatment phase and &lt;span class=&#34;math inline&#34;&gt;\(\tau_B\)&lt;/span&gt; is Kendall’s rank correlation between the measurement occasions and outcomes during baseline:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\tau_P &amp;amp;= \frac{1}{mn}\sum_{i=1}^m \sum_{j=1}^n \left[\text{Pr}\left(Y^B_j &amp;gt; Y^A_i\right) - \text{Pr}\left(Y^B_j &amp;lt; Y^A_i\right)\right] \\
\tau_B &amp;amp;= \frac{2}{m(m-1)} \sum_{i=1}^{m - 1} \sum_{j = i + 1}^m \left[\text{Pr}\left(Y^A_j &amp;gt; Y^A_i\right) - \text{Pr}\left(Y^A_j &amp;lt; Y^A_i\right)\right].
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now consider a positive a baseline trend, so that &lt;span class=&#34;math inline&#34;&gt;\(\tau_B &amp;gt; 0\)&lt;/span&gt;, and assume that &lt;span class=&#34;math inline&#34;&gt;\(\tau_P\)&lt;/span&gt; is constant. A longer baseline phase will then lead to smaller values of Tau-U (on average), while a longer treatment phase will lead to larger values of Tau-U (on average). Again, that’s really weird. This is not a good feature for an effect size measure because it means that Tau-U values from different cases are only on the same scale if the cases have identical baseline and treatment phase lengths. In a multiple baseline study, each case is necessarily observed for a different number of occasions in baseline (otherwise it wouldn’t be a multiple baseline). Thus, it seems inadvisable to use Tau-U to quantify the magnitude of treatment effects in a multiple baseline study.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sensitivity-under-a-parametric-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sensitivity under a parametric model&lt;/h2&gt;
&lt;p&gt;Things may be different if we allow for the magnitude of &lt;span class=&#34;math inline&#34;&gt;\(\tau_P\)&lt;/span&gt; to change along with the sample size. Such would be the case under a model where the intervention phase also exhibits a trend. For example, let’s suppose that the outcome follows a linear model with a non-zero trend and the intervention leads to an immediate shift in the outcome, as in the model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
y_t = \beta_0 + \beta_1 t + \beta_2 I(t &amp;gt; m) + \epsilon_t.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For simplicity, I’ll assume that the errors in this model are normally distributed with unit variance. Under this model,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\tau_B &amp;amp;= \frac{4}{m (m - 1)} \left[\sum_{i=1}^{m-1} \sum_{j=i+1}^m \Phi\left[\beta_1\left(j - i\right) / \sqrt{2}\right]\right] - 1, \\
\tau_P &amp;amp;= \frac{2}{m n} \left[\sum_{i=1}^m \sum_{j=1}^n \Phi\left[\left(\beta_1 (m + j - i) + \beta_2\right) / \sqrt{2}\right]\right] - 1,
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\Phi()\)&lt;/span&gt; is the standard normal cumulative distribution function. I can use the above formulas to calculate the average value of Tau-U for various degrees of baseline trend &lt;span class=&#34;math inline&#34;&gt;\((\beta_1)\)&lt;/span&gt;, level shift &lt;span class=&#34;math inline&#34;&gt;\((\beta_2)\)&lt;/span&gt;, and phase lengths &lt;span class=&#34;math inline&#34;&gt;\((m,n)\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;E_TauU &amp;lt;- function(b1, b2, m, n) {
  tau_B &amp;lt;- sum(sapply(1:(m - 1), function(i) 
    sum(pnorm(b1 * ((i+1):m - i) / sqrt(2))))) * 4 / (m * (m - 1)) - 1
  tau_P &amp;lt;- sum(sapply(1:m, function(i) 
    sum(pnorm((b1 * (m + 1:n - i) + b2) / sqrt(2))))) * 2 / (m * n) - 1
  tau_P - tau_B * (m - 1) / (2 * n)
}

library(dplyr)
library(tidyr)
b1 &amp;lt;- c(-0.2, -0.1, 0, 0.1, 0.2)
b2 &amp;lt;- c(0, 0.5, 1.0, 2.0)
m &amp;lt;- c(5, 10, 15, 20)
n &amp;lt;- 5:20

expand.grid(b1 = b1, b2 = b2, m = m, n = n) %&amp;gt;%
  group_by(b1, b2, m, n) %&amp;gt;% 
  mutate(TauU = E_TauU(b1, b2, m, n)) -&amp;gt;
  TauU_values
ex &amp;lt;- filter(TauU_values, b1 == -0.2 &amp;amp; b2 == 0)


library(ggplot2)
ggplot(TauU_values, aes(n, TauU, color = factor(m))) + 
  facet_grid(b1 ~ b2, labeller = &amp;quot;label_both&amp;quot;) + 
  geom_line() + 
  labs(y = &amp;quot;Expected magnitude of Tau-U&amp;quot;, color = &amp;quot;m&amp;quot;) + 
  theme_bw() + theme(legend.position = &amp;quot;bottom&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Tau-U_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the figure above, each plot corresponds to a different value of the baseline slope (&lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt;, ranging from -0.2 in the top row to 0.2 in the bottom row) and treatment shift (&lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt;, ranging from 0 in the first column to 2 in the last column). Within each plot, the x axis corresponds to treatment phase length and the different lines correspond to different baseline phase lengths. The thing to note is that, when the baseline slope is non-zero, the expected value of Tau-U ranges quite widely within each plot, depending on the values of &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;. For example, when &lt;span class=&#34;math inline&#34;&gt;\(\beta_2 = 0\)&lt;/span&gt; (in the first column), the data follow a simple linear trend with no shift. If the slope of the trend is equal to -0.2 (the first row), then the expected magnitude of Tau-U ranges from -0.8 to 0.3 depending on the phase lengths, which is quite a wide range.&lt;/p&gt;
&lt;p&gt;Generally, the degree of sample size sensitivity depends on the absolute magnitude of the baseline slope, with steeper slopes leading to increased sensitivity. For steeper values of slope, it appears that the degree to which the measure is affected by sample size even swamps the degree to which the measure is sensitive to the magnitude of the treatment effect. Very peculiar.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-final-thought&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A final thought&lt;/h2&gt;
&lt;p&gt;Of course, these results are contingent on the particular model under which I derived the expected magnitude of Tau-U. If the data followed some other model, such as a log-linear model with Poisson-distributed outcomes, then the behavior described above might change. Still, I think all of this raises the reasonable question: under what model (i.e., what sort of patterns of baseline trend, what sort of patterns of response to the intervention) does Tau-U provide a meaningful effect size measure that clearly quantifies the magnitude of treatment effects without being strongly affected by phase lengths? Unless and until such a model can be identified, I would be wary of interpreting Tau-U as a measure of treatment effect magnitude.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Standard errors and confidence intervals for NAP</title>
      <link>/nap-ses-and-cis/</link>
      <pubDate>Sun, 28 Feb 2016 00:00:00 +0000</pubDate>
      <guid>/nap-ses-and-cis/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;http://doi.org/10.1016/j.beth.2008.10.006&#34;&gt;Parker and Vannest (2009)&lt;/a&gt; proposed non-overlap of all pairs (NAP) as an effect size index for use in single-case research. NAP is defined in terms of all pair-wise comparisons between the data points in two different phases for a given case (i.e., a treatment phase versus a baseline phase). For an outcome that is desirable to increase, NAP is the proportion of all such pair-wise comparisons where the treatment phase observation exceeds the baseline phase observation, with pairs that are exactly tied getting a weight of 1/2. NAP belongs to the family of non-overlap measures, which also includes the percentage of non-overlapping data, the improvement rate difference, and several other indices. It is exactly equivalent to &lt;a href=&#34;http://doi.org/10.2307/1165329&#34;&gt;Vargha and Delaney’s (2000)&lt;/a&gt; modified Common Language Effect Size and has been proposed as an effect size index in other contexts too (e.g., &lt;a href=&#34;http://doi.org/10.1002/sim.2256&#34;&gt;Acion, Peterson, Temple, &amp;amp; Arndt, 2006&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The developers of NAP have created a &lt;a href=&#34;http://singlecaseresearch.org/calculators&#34;&gt;web-based tool&lt;/a&gt; for calculating it (as well as several other non-overlap indices), and I have the impression that the tool is fairly widely used. For example, &lt;a href=&#34;http://doi.org/10.1007%2Fs10864-013-9189-x&#34;&gt;Roth, Gillis, and DiGennaro Reed (2014)&lt;/a&gt; and &lt;a href=&#34;http://doi.org/10.1007/s10803-015-2373-1&#34;&gt;Whalon, Conroy, Martinez, and Welch (2015)&lt;/a&gt; both used NAP in their meta-analyses of single-case research, and both noted that they used &lt;a href=&#34;http://www.singlecaseresearch.org/calculators/nap&#34;&gt;singlecaseresearch.org&lt;/a&gt; for calculating the effect size measure. Given that the web tool is being used, it is worth scrutinizing the methods behind the calculations it reports. As of this writing, the standard error and confidence intervals reported along with the NAP statistic are incorrect, and should not be used. After introducing a bit of notation, I’ll explain why the existing methods are deficient. I’ll also suggest some methods for calculating standard errors and confidence intervals that are potentially more accurate.&lt;/p&gt;
&lt;div id=&#34;preliminaries&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preliminaries&lt;/h2&gt;
&lt;p&gt;Suppose that we have data from the baseline phase and treatment phase for a single case. Let &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; denote the number of baseline observations and &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; denote the number of treatment phase observations. Let &lt;span class=&#34;math inline&#34;&gt;\(y^A_1,...,y^A_m\)&lt;/span&gt; denote the baseline phase data and &lt;span class=&#34;math inline&#34;&gt;\(y^B_1,...,y^B_n\)&lt;/span&gt; denote the treatment phase data. Then NAP is calculated as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{NAP} = \frac{1}{m n} \sum_{i=1}^m \sum_{j=1}^n \left[I\left(y^B_j &amp;gt; y^A_i\right) + 0.5 I\left(y^B_j = y^A_i\right)\right]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;What is NAP an estimate of? The parameter of interest is the probability that a randomly selected treatment phase observation will exceed a randomly selected baseline phase observation (again, with an adjustment for ties):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\theta = \text{Pr}(Y^B &amp;gt; Y^A) + 0.5 \text{Pr}(Y^B = Y^A).
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Vargha and Delaney call &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; the &lt;em&gt;measure of stochastic superiority&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;NAP is very closely related to another non-overlap index called Tau (&lt;a href=&#34;http://doi.org/10.1016/j.beth.2010.08.006&#34;&gt;Parker, Vannest, Davis, &amp;amp; Sauber, 2011&lt;/a&gt;). Tau is nothing more than a linear re-scaling of NAP to the range of [-1, 1]:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Tau} = \frac{S}{m n} = 2 \times \text{NAP} - 1,
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
S = \sum_{i=1}^m \sum_{j=1}^n \left[I\left(y^B_j &amp;gt; y^A_i\right) - I\left(y^B_j &amp;lt; y^A_i\right)\right].
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; is Kendall’s S statistic, which is closely related to the Mann-Whitney &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; test.&lt;/p&gt;
&lt;p&gt;Here is an R function for calculating NAP:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;NAP &amp;lt;- function(yA, yB) {
  m &amp;lt;- length(yA)
  n &amp;lt;- length(yB)
  U &amp;lt;- sum(sapply(yA, function(i) sapply(yB, function(j) (j &amp;gt; i) + 0.5 * (j == i))))
  U / (m * n)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the data from the worked example in &lt;a href=&#34;http://doi.org/10.1016/j.beth.2008.10.006&#34;&gt;Parker and Vannest (2009)&lt;/a&gt;, the function result agrees with their reported NAP of 0.96:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;yA &amp;lt;- c(4, 3, 4, 3, 4, 7, 5, 2, 3, 2)
yB &amp;lt;- c(5, 9, 7, 9, 7, 5, 9, 11, 11, 10, 9)
NAP(yA, yB)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9636364&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;standard-errors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Standard errors&lt;/h2&gt;
&lt;p&gt;The webtool at &lt;a href=&#34;http://www.singlecaseresearch.org/calculators/nap&#34;&gt;singlecaseresearch.org&lt;/a&gt; reports a standard error for NAP (it is labelled as “SDnap”), which from what I can tell is based on the formula&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{SE}_{\text{Tau}} = \sqrt{\frac{m + n + 1}{3 m n}}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This formula appears to actually be the standard error for Tau, rather than for NAP. Since &lt;span class=&#34;math inline&#34;&gt;\(\text{NAP} = \left(\text{Tau} + 1\right) / 2\)&lt;/span&gt;, the standard error for NAP should be half as large:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{SE}_{null} = \sqrt{\frac{m + n + 1}{12 m n}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;(cf. &lt;a href=&#34;http://dx.doi.org/10.1037/1082-989X.6.2.135&#34;&gt;Grissom &amp;amp; Kim, 2001, p. 141&lt;/a&gt;). However, even the latter formula is not always correct. It is valid only when the observations are all mutually independent and when the treatment phase data are drawn from the same distribution as the baseline phase data—that is, when the treatment has no effect on the outcome. I’ve therefore denoted it as &lt;span class=&#34;math inline&#34;&gt;\(\text{SE}_{null}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;other-standard-error-estimators&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Other standard error estimators&lt;/h3&gt;
&lt;p&gt;Because an equivalent effect size measure is used in other contexts like clinical medicine, there has actually been a fair bit of research into better approaches for assessing the uncertainty in NAP. &lt;a href=&#34;http://dx.doi.org/10.1148/radiology.143.1.7063747&#34;&gt;Hanley and McNeil (1982)&lt;/a&gt; proposed an estimator for the sampling variance of NAP that is designed for continuous outcome measures, where exact ties are impossible. Modifying it slightly (and in entirely ad hoc fashion) to account for ties, let&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
Q_1 &amp;amp;= \frac{1}{m n^2}\sum_{i=1}^m \left[\sum_{j=1}^n I\left(y^B_j &amp;gt; y^A_i\right) + 0.5 I\left(y^B_j = y^A_i\right)\right]^2 \\
Q_2 &amp;amp;= \frac{1}{m^2 n}\sum_{j=1}^n \left[\sum_{i=1}^m I\left(y^B_j &amp;gt; y^A_i\right) + 0.5 I\left(y^B_j = y^A_i\right)\right]^2.
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Then the Hanley-McNeil variance estimator is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
V_{HM} = \frac{1}{mn} \left[\text{NAP}\left(1 - \text{NAP}\right) + (n - 1)\left(Q_1 - \text{NAP}^2\right) + (m - 1)\left(Q_2 - \text{NAP}^2\right)\right],
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with &lt;span class=&#34;math inline&#34;&gt;\(\text{SE}_{HM} = \sqrt{V_{HM}}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The same authors also propose a different estimator, which is based on the assumption that the outcome data are exponentially distributed. Even though this is a strong and often inappropriate assumption, there is evidence that this estimator works even for other, non-exponential distributions. &lt;a href=&#34;http://dx.doi.org/10.1002/sim.2324&#34;&gt;Newcombe (2006)&lt;/a&gt; suggested a further modification of their estimator, and I’ll describe his version. Let &lt;span class=&#34;math inline&#34;&gt;\(h = (m + n) / 2 - 1\)&lt;/span&gt;. Then&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
V_{New} = \frac{h}{mn} \text{NAP}\left(1 - \text{NAP}\right)\left[\frac{1}{h} + \frac{1 - \text{NAP}}{2 - \text{NAP}} + \frac{\text{NAP}}{1 + \text{NAP}}\right],
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with &lt;span class=&#34;math inline&#34;&gt;\(\text{SE}_{New} = \sqrt{V_{New}}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Here are R functions to calculate each of these variance estimators.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;V_HM &amp;lt;- function(yA, yB) {
  m &amp;lt;- length(yA)
  n &amp;lt;- length(yB)
  U &amp;lt;- sapply(yB, function(j) (j &amp;gt; yA) + 0.5 * (j == yA))
  t &amp;lt;- sum(U) / (m * n)
  Q1 &amp;lt;- sum(rowSums(U)^2) / (m * n^2)
  Q2 &amp;lt;- sum(colSums(U)^2) / (m^2 * n)
  (t * (1 - t) + (n - 1) * (Q1 - t^2) + (m - 1) * (Q2 - t^2)) / (m * n)
}

V_New &amp;lt;- function(yA, yB) {
  m &amp;lt;- length(yA)
  n &amp;lt;- length(yB)
  t &amp;lt;- NAP(yA, yB)
  h &amp;lt;- (m + n) / 2 - 1
  t * (1 - t) * (1 + h * (1 - t) / (2 - t) + h * t / (1 + t)) / (m * n)
}

sqrt(V_HM(yA, yB))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.03483351&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sqrt(V_New(yA, yB))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.04370206&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the worked example dataset from Parker and Vannest, the Newcombe estimator yields a standard error that is about 25% larger than the Hanley-McNeil estimator. Both of these are substantially smaller than the null standard error, which in this example is &lt;span class=&#34;math inline&#34;&gt;\(\text{SE}_{null} = 0.129\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-small-simulation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A small simulation&lt;/h3&gt;
&lt;p&gt;Simulation methods can be used to examine how well these various standard error formulas estimate the actual sampling variation of NAP. For simplicity, I’ll simulate normally distributed data where&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y^A \sim N(0, 1) \qquad \text{and} \qquad Y^B \sim N\left(\sqrt{2}\Phi^{-1}(\theta), 1\right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;for varying values of the effect size estimand (&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;) and a couple of different sample sizes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sample_NAP &amp;lt;- function(delta, m, n, iterations) {
  NAPs &amp;lt;- replicate(iterations, {
    yA &amp;lt;- rnorm(m)
    yB &amp;lt;- rnorm(n, mean = delta)
    c(NAP = NAP(yA, yB), V_HM = V_HM(yA, yB), V_New = V_New(yA, yB))
  })
  data.frame(sd = sd(NAPs[&amp;quot;NAP&amp;quot;,]), 
             SE_HM = sqrt(mean(NAPs[&amp;quot;V_HM&amp;quot;,])), 
             SE_New = sqrt(mean(NAPs[&amp;quot;V_New&amp;quot;,])))
}

library(dplyr)
library(tidyr)
theta &amp;lt;- seq(0.5, 0.95, 0.05)
m &amp;lt;- c(5, 10, 15, 20, 30)
n &amp;lt;- c(5, 10, 15, 20, 30)

expand.grid(theta = theta, m = m, n = n) %&amp;gt;%
  group_by(theta, m, n) %&amp;gt;% 
  mutate(delta = sqrt(2) * qnorm(theta)) -&amp;gt;
  params 

params %&amp;gt;%
  do(sample_NAP(.$delta, .$m, .$n, iterations = 2000)) %&amp;gt;%
  mutate(se_null = sqrt((m + n + 1) / (12 * m * n))) %&amp;gt;%
  gather(&amp;quot;sd&amp;quot;,&amp;quot;val&amp;quot;, sd, SE_HM, SE_New, se_null) -&amp;gt;
  NAP_sim&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
ggplot(NAP_sim, aes(theta, val, color = sd)) + 
  facet_grid(n ~ m, labeller = &amp;quot;label_both&amp;quot;) + 
  geom_line() + 
  theme_bw() + theme(legend.position = &amp;quot;bottom&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/NAP-SEs-and-CIs_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the above figure, the actual sampling standard deviation of NAP (in red) and the value of &lt;span class=&#34;math inline&#34;&gt;\(\text{SE}_{null}\)&lt;/span&gt; (in purple) are plotted against the true value of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, with separate plots for various combinations of &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;. The expected value of the standard errors &lt;span class=&#34;math inline&#34;&gt;\(\text{SE}_{HM}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\text{SE}_{New}\)&lt;/span&gt; (actually the square root of the expectation of the variance estimators) are depicted in green and blue, respectively. The value of &lt;span class=&#34;math inline&#34;&gt;\(\text{SE}_{null}\)&lt;/span&gt; agrees with the actual standard error when &lt;span class=&#34;math inline&#34;&gt;\(\delta = 0\)&lt;/span&gt;, but the two diverge when there is a positive treatment effect. It appears that &lt;span class=&#34;math inline&#34;&gt;\(\text{SE}_{HM}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\text{SE}_{New}\)&lt;/span&gt; both under-estimate the actual standard error when &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is equal to 5, and over-estimate for the largest values of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. However, both of these estimators offer a marked improvement over &lt;span class=&#34;math inline&#34;&gt;\(\text{SE}_{null}\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;confidence-intervals&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Confidence intervals&lt;/h2&gt;
&lt;p&gt;The webtool at &lt;a href=&#34;http://www.singlecaseresearch.org/calculators/nap&#34;&gt;singlecaseresearch.org&lt;/a&gt; also reports 85% and 90% confidence intervals for NAP. These confidence intervals appear to have the same two problems as the standard errors. First, they are constructed as CIs for Tau rather than for NAP. For the &lt;span class=&#34;math inline&#34;&gt;\(100\% \times (1 - \alpha)\)&lt;/span&gt; CI, let &lt;span class=&#34;math inline&#34;&gt;\(z_{\alpha / 2}\)&lt;/span&gt; be the appropriate critical value from a standard normal distribution. The CIs reported by the webtool are given by&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Tau} \pm \text{SE}_{\text{Tau}} \times z_{\alpha / 2}. 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is probably just an oversight in the programming, which could be corrected by instead using&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{NAP} \pm \text{SE}_{null} \times z_{\alpha / 2}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In parallel with the standard error formulas, I’ll call this formula the null confidence interval. Funnily enough, the upper bound of the null CI is the same as the upper bound of the Tau CI. However, the lower bound is going to be quite a bit larger than the lower bound for the Tau CI, so that the null CI will be much narrower.&lt;/p&gt;
&lt;p&gt;The second problem is that even the null CI has poor coverage properties because it is based on &lt;span class=&#34;math inline&#34;&gt;\(\text{SE}_{null}\)&lt;/span&gt;, which can drastically over-estimate the standard error of NAP for non-null values.&lt;/p&gt;
&lt;div id=&#34;other-confidence-intervals&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Other confidence intervals&lt;/h3&gt;
&lt;p&gt;As I noted above, there has been a fair amount of previous research into how to construct CIs for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, the parameter estimated by NAP. As is often the case with these sorts of problems, there are many different methods available, scattered across the literature. Fortunately, there are two (at least) fairly comprehensive simulation studies that compare the performance of various methods under a wide range of conditions. &lt;a href=&#34;http://dx.doi.org/10.1002/sim.2324&#34;&gt;Newcombe (2006)&lt;/a&gt; examined a range of methods based on inverting Wald-type test statistics (which give CIs of the form &lt;span class=&#34;math inline&#34;&gt;\(\text{estimate} \pm \text{SE} \times z_{\alpha / 2}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\text{SE}\)&lt;/span&gt; is some standard error estimate) and score-based methods (in which the standard error is estimated using the candidate parameter value). Based on an extensive simulation, he suggested a score-based method in which the end-points of the CI are defined the values of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; that satisfy:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
(\text{NAP} - \theta)^2 = \frac{z^2_{\alpha / 2} h \theta (1 - \theta)}{mn}\left[\frac{1}{h} + \frac{1 - \theta}{2 - \theta} + \frac{\theta}{1 + \theta}\right],
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(h = (m + n) / 2 - 1\)&lt;/span&gt;. This equation is a fourth-degree polynomial in &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, easily solved using a numerical root-finding algorithm.&lt;/p&gt;
&lt;p&gt;In a different simulation study, &lt;a href=&#34;http://dx.doi.org/10.1080/00273171.2012.658329&#34;&gt;Ruscio and Mullen (2012)&lt;/a&gt; examined the performance of a selection of different confidence intervals for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, including several methods not considered by Newcombe. Among the methods that they examined, they find that the bias-corrected, accelerated (BCa) bootstrap CI performs particularly well (and seems to outperform the score-based CI recommended by Newcombe).&lt;/p&gt;
&lt;p&gt;Neither &lt;a href=&#34;http://dx.doi.org/10.1002/sim.2324&#34;&gt;Newcombe (2006)&lt;/a&gt; nor &lt;a href=&#34;http://dx.doi.org/10.1080/00273171.2012.658329&#34;&gt;Ruscio and Mullen (2012)&lt;/a&gt; considered constructing a confidence interval by directly pivoting the Mann-Whitney U test (the same technique used to construct confidence intervals for the Hodges-Lehmann estimator of location shift), although it seems to me that this would be possible and potentially an attractive approach in the context of SCDs. The main caveat is that such a CI would require stronger distributional assumptions than those studied in the simulations, such as that the distributions of &lt;span class=&#34;math inline&#34;&gt;\(Y^A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y^B\)&lt;/span&gt; differ by an additive (or multiplicative) constant. In any case, it seems like it would be worth exploring this approach too.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;another-small-simulation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Another small simulation&lt;/h3&gt;
&lt;p&gt;Here is an R function for calculating several different CIs for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, including the null CI, Wald-type CIs based on &lt;span class=&#34;math inline&#34;&gt;\(V_{HM}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(V_{New}\)&lt;/span&gt;, and the score-type CI recommended by &lt;a href=&#34;http://dx.doi.org/10.1002/sim.2324&#34;&gt;Newcombe (2006)&lt;/a&gt;. I haven’t programmed the BCa bootstrap because it would take a bit more thought to figure out how to simulate it efficiently.&lt;/p&gt;
&lt;p&gt;The following code simulates the coverage rates of nominal 90% CIs based on each of these methods, following the same simulation set-up as above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;NAP_CIs &amp;lt;- function(yA, yB, alpha = .05) {
  m &amp;lt;- length(yA)
  n &amp;lt;- length(yB)
  U &amp;lt;- sapply(yB, function(j) (j &amp;gt; yA) + 0.5 * (j == yA))
  t &amp;lt;- sum(U) / (m * n)
  
  # variance estimators
  V_null &amp;lt;- (m + n + 1) / (12 * m * n)
  
  Q1 &amp;lt;- sum(rowSums(U)^2) / (m * n^2)
  Q2 &amp;lt;- sum(colSums(U)^2) / (m^2 * n)
  V_HM &amp;lt;- (t * (1 - t) + (n - 1) * (Q1 - t^2) + (m - 1) * (Q2 - t^2)) / (m * n)
  
  h &amp;lt;- (m + n) / 2 - 1
  V_New &amp;lt;- t * (1 - t) * (1 + h * (1 - t) / (2 - t) + h * t / (1 + t)) / (m * n)
  
  # Wald-type confidence intervals
  z &amp;lt;- qnorm(1 - alpha / 2)
  SEs &amp;lt;- sqrt(c(null = V_null, HM = V_HM, Newcombe = V_New))
  Wald_lower &amp;lt;- t - z * SEs
  Wald_upper &amp;lt;- t + z * SEs
  
  # score-type confidence interval
  f &amp;lt;- function(x) m * n * (t - x)^2 * (2 - x) * (1 + x) - 
    z^2 * x * (1 - x) * (2 + h + (1 + 2 * h) * x * (1 - x))
  score_lower &amp;lt;- if (t &amp;gt; 0) uniroot(f, c(0, t))$root else 0
  score_upper &amp;lt;- if (t &amp;lt; 1) uniroot(f, c(t, 1))$root else 1
  list(NAP = t, 
       CI = data.frame(lower = c(Wald_lower, score = score_lower), 
                       upper = c(Wald_upper, score = score_upper)))
}

NAP_CIs(yA, yB)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $NAP
## [1] 0.9636364
## 
## $CI
##              lower     upper
## null     0.7106061 1.2166666
## HM       0.8953639 1.0319088
## Newcombe 0.8779819 1.0492908
## score    0.7499741 0.9950729&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sample_CIs &amp;lt;- function(delta, m, n, alpha = .05, iterations) {
  NAPs &amp;lt;- replicate(iterations, {
    yA &amp;lt;- rnorm(m)
    yB &amp;lt;- rnorm(n, mean = delta)
    NAP_CIs(yA, yB, alpha = alpha)
  }, simplify = FALSE)
  theta &amp;lt;- mean(sapply(NAPs, function(x) x$NAP))
  coverage &amp;lt;- rowMeans(sapply(NAPs, function(x) (x$CI$lower &amp;lt; theta) &amp;amp; (theta &amp;lt; x$CI$upper)))
  data.frame(CI = rownames(NAPs[[1]]$CI), coverage = coverage)
}

params %&amp;gt;% 
  do(sample_CIs(delta = .$delta, m = .$m, n = .$n, alpha = .10, iterations = 5000)) -&amp;gt;
  NAP_CI_sim&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(NAP_CI_sim, aes(theta, coverage, color = CI)) + 
  facet_grid(n ~ m, labeller = &amp;quot;label_both&amp;quot;, scales = &amp;quot;free_y&amp;quot;) + 
  geom_line() + 
  labs(y = &amp;quot;SE&amp;quot;) + 
  geom_hline(yintercept=.90, linetype=&amp;quot;dashed&amp;quot;) +
  theme_bw() + theme(legend.position = &amp;quot;bottom&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/NAP-SEs-and-CIs_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The figure above plots the coverage rates of several different confidence intervals for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;: the naive CI (in blue), the HM Wald CI (red), the Newcombe Wald CI (green), and the Newcombe score CI (purple). The dashed horizontal line is the nominal coverage rate of 90%. It can be seen that the null CI has the correct coverage only when &lt;span class=&#34;math inline&#34;&gt;\(\theta \leq .6\)&lt;/span&gt;; for larger values of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, its coverage becomes too conservative (tending towards 100%). The Wald-type CIs have below-nominal coverage rates, which improve as the sample size in each phase increases but remain too liberal even at the largest sample size considered. Finally, Newcombe’s score CI maintains close-to-nominal coverage over a wider range of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; values. Although these CIs have below-nominal coverage for the smallest sample sizes, they generally have good coverage for &lt;span class=&#34;math inline&#34;&gt;\(\theta &amp;lt; .9\)&lt;/span&gt; and when the sample size in each phase is 10 or more. It is also notable that their coverage rates appear to become more accurate as the sample size in a given group increases, even if the sample size in the other group is fairly small and remains constant.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;caveats&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Caveats&lt;/h2&gt;
&lt;p&gt;My aim in this post was to highlight the problems with how &lt;a href=&#34;http://www.singlecaseresearch.org/calculators/nap&#34;&gt;singlecaseresearch.org&lt;/a&gt; calculates standard errors and CIs for the NAP statistic. Some of these issues could easily be resolved by correcting the relevant formulas so that they are appropriate for NAP rather than Tau. However, even with these corrections, better approaches exist for calculating standard errors and CIs. I’ve highlighted some promising ones above, which seem worthy of further investigation. But I should also emphasize that these methods do come with some important caveats too.&lt;/p&gt;
&lt;p&gt;First, all of the methods I’ve discussed are premised on having mutually independent observations. In the presence of serial correlation, I would anticipate that any of these standard errors will be too small and any of the confidence intervals will be too narrow. (This could readily be verified through simulation, although I have not done so here.)&lt;/p&gt;
&lt;p&gt;Second, my small simulations are based on the assumption of normally distributed, homoskedastic observations in each phase, which is not a particularly good model for the types of outcome measures commonly used in single case research. In some of my other work, I’ve developed statistical models for data collected by systematic direct observation of behavior, which is the most prevalent type of outcome data in single-case research. Before recommending any particular method, the performance of the standard error formulas (e.g., the Hanley-McNeil and Newcombe estimators) and CI methods (such as Newcombe’s score CI) should be examined under more realistic models for behavioral observation data.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Special Education Pro-Sem</title>
      <link>/sped-pro-sem-again/</link>
      <pubDate>Tue, 24 Nov 2015 00:00:00 +0000</pubDate>
      <guid>/sped-pro-sem-again/</guid>
      <description>


&lt;p&gt;Yesterday evening I again had the pleasure of visiting Dr. Barnes’ pro seminar for first year students in Special Education, where I shared some of my work on research synthesis and meta-analysis of single-case research. &lt;a href=&#34;/files/Barnes-Pro-Sem-2015-11.pdf&#34;&gt;Here are the slides&lt;/a&gt; from my presentation.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Measurement-comparable effect sizes for single-case studies of free-operant behavior</title>
      <link>/publication/measurement-comparable-effect-sizes/</link>
      <pubDate>Tue, 01 Sep 2015 00:00:00 +0000</pubDate>
      <guid>/publication/measurement-comparable-effect-sizes/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Four methods for analyzing partial interval recording data, with application to single-case research</title>
      <link>/publication/four-methods-for-pir/</link>
      <pubDate>Fri, 19 Jun 2015 00:00:00 +0000</pubDate>
      <guid>/publication/four-methods-for-pir/</guid>
      <description></description>
    </item>
    
    <item>
      <title>New article: Four methods for analyzing PIR data</title>
      <link>/four-methods-for-analyzing-pir-data/</link>
      <pubDate>Wed, 11 Feb 2015 00:00:00 +0000</pubDate>
      <guid>/four-methods-for-analyzing-pir-data/</guid>
      <description>


&lt;p&gt;My article with Daniel Swan, “Four methods for analyzing partial interval recording data, with application to single-case research” has been accepted for publication in Multivariate Behavioral Research. In an extension of my earlier paper on &lt;a href=&#34;/files/Measurement-comparable-ES.pdf&#34;&gt;measurement-comparable effect sizes&lt;/a&gt; for single-case studies, this article provides some approaches to estimating effect sizes from single-case studies that use partial interval or whole interval recording to measure behavioral outcomes. The full abstract is below. &lt;a href=&#34;/files/4-PIR-methods-MBR.pdf&#34;&gt;Preprint&lt;/a&gt; and &lt;a href=&#34;/files/4-PIR-Methods-Appendix.pdf&#34;&gt;supporting materials&lt;/a&gt; are available. R functions that implement the proposed methods are available in the package &lt;a href=&#34;https://cran.r-project.org/web/packages/ARPobservation/&#34;&gt;ARPobservation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Partial interval recording is a procedure for collecting measurements during direct observation of behavior. It is used in several areas of educational and psychological research, particularly in connection with single-case research. Measurements collected using partial interval recording suffer from construct invalidity because they are not readily interpretable in terms of the underlying characteristics of the behavior. Using an alternating renewal process model for the behavior under observation, we demonstrate that ignoring the construct invalidity of PIR data can produce misleading inferences, such as inferring that an intervention reduces the prevalence of an undesirable behavior when in fact it has the opposite effect. We then propose four different methods for analyzing PIR summary measurements, each of which can be used to draw inferences about interpretable behavioral parameters. We demonstrate the methods by applying them to data from two single-case studies of problem behavior.&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Getting started with scdhlm</title>
      <link>/getting-started-with-scdhlm/</link>
      <pubDate>Sun, 19 Oct 2014 00:00:00 +0000</pubDate>
      <guid>/getting-started-with-scdhlm/</guid>
      <description>


&lt;p&gt;&lt;strong&gt;UPDATED 10/2/2016 after posting the package to CRAN&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Here are step-by-step instructions on how to download and install the scdhlm package for R. You’ll need to have a &lt;a href=&#34;http://cran.us.r-project.org/&#34;&gt;copy of R installed&lt;/a&gt;. There are two ways to do the installation: through the Comprehensive R Archive Network (CRAN) or from the source code on Github. I describe each approach in turn.&lt;/p&gt;
&lt;div id=&#34;option-1-via-cran&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Option 1: Via CRAN&lt;/h3&gt;
&lt;p&gt;Go via CRAN to install the most recent stable version of the package. Type the following commands at the R prompt:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;scdhlm&amp;quot;)
library(scdhlm)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;option-2-via-github&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Option 2: Via Github&lt;/h3&gt;
&lt;p&gt;Go via Github to get the latest development version of the package. For this option, you will first need to install the devtools package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;devtools&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once you have successfully installed this package, type the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(devtools)
install_github(&amp;quot;jepusto/scdhlm&amp;quot;)
library(scdhlm)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;further-instructions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Further instructions&lt;/h3&gt;
&lt;p&gt;You’ll only need to do the installation once. Once you’ve got the package installed, type the following in order to access the package within an R session: &lt;code&gt;library(scdhlm)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To open the package documentation, type &lt;code&gt;package?scdhlm&lt;/code&gt;. To access the documentation for an individual function in this package, just type &lt;code&gt;?&lt;/code&gt; followed by the name of the function. For instance, one of the main functions in the package is called &lt;code&gt;g_REML&lt;/code&gt;; to access its documentation, type &lt;code&gt;?g_REML&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;web-interface-for-calculating-effect-sizes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;web-interface for calculating effect sizes&lt;/h3&gt;
&lt;p&gt;The package includes an interactive app (written with &lt;code&gt;shiny&lt;/code&gt;) for calculating design-comparable standardized mean differences. To run this app on your computer, you will first need to &lt;a href=&#34;https://www.rstudio.com/products/rstudio/download/&#34;&gt;install RStudio&lt;/a&gt; (if you don’t already have it). Then ensure that you have the &lt;code&gt;shiny&lt;/code&gt;, &lt;code&gt;markdown&lt;/code&gt;, and &lt;code&gt;ggplot2&lt;/code&gt; packages installed by running the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;shiny&amp;quot;)
install.packages(&amp;quot;markdown&amp;quot;)
install.packages(&amp;quot;ggplot2&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, open the app by typing the following at the prompt within RStudio:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(scdhlm)
shine_scd()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The app should now open in your web browser.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Design-comparable effect sizes in multiple baseline designs: A general modeling framework</title>
      <link>/publication/design-comparable-effect-sizes/</link>
      <pubDate>Wed, 01 Oct 2014 00:00:00 +0000</pubDate>
      <guid>/publication/design-comparable-effect-sizes/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Alternating renewal process models for behavioral observation: Simulation methods and validity implications</title>
      <link>/publication/arp-for-behavioral-observation/</link>
      <pubDate>Fri, 01 Aug 2014 00:00:00 +0000</pubDate>
      <guid>/publication/arp-for-behavioral-observation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>New article: Design-comparable effect sizes in multiple baseline designs: A general modeling framework</title>
      <link>/design-comparable-effect-sizes-in-multiple-baseline-designs/</link>
      <pubDate>Sun, 20 Jul 2014 00:00:00 +0000</pubDate>
      <guid>/design-comparable-effect-sizes-in-multiple-baseline-designs/</guid>
      <description>


&lt;p&gt;My article with Larry Hedges and Will Shadish, titled “Design-comparable effect sizes in multiple baseline designs: A general modeling framework” has been accepted at Journal of Educational and Behavioral Statistics. The abstract is below. Here’s the article at &lt;a href=&#34;http://doi.org/10.3102/1076998614547577&#34;&gt;the journal website&lt;/a&gt;. &lt;a href=&#34;/files/Effect-sizes-in-multiple-baseline-designs-JEBS.pdf&#34;&gt;Postprint&lt;/a&gt; and &lt;a href=&#34;/files/Effect-sizes-in-multiple-baseline-designs-Simulation-results.pdf&#34;&gt;supporting materials&lt;/a&gt; are available. An R package that implements the proposed methods is &lt;a href=&#34;/getting-started-with-scdhlm/&#34;&gt;available here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;In single-case research, the multiple baseline design is a widely used approach for evaluating the effects of interventions on individuals. Multiple baseline designs involve repeated measurement of outcomes over time and the controlled introduction of a treatment at different times for different individuals. This article outlines a general approach for defining effect sizes in multiple baseline designs that are directly comparable to the standardized mean difference from a between-subjects randomized experiment. The target, design-comparable effect size parameter can be estimated using restricted maximum likelihood together with a small-sample correction analogous to Hedges’ g. The approach is demonstrated using hierarchical linear models that include baseline time trends and treatment-by-time interactions. A simulation compares the performance of the proposed estimator to that of an alternative, and an application illustrates the model-fitting process.&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Analyzing single-case designs: d, G, hierarchical models, Bayesian estimators, generalized additive models, and the hopes and fears of researchers about analyses</title>
      <link>/publication/analyzing-scd-hopes-and-fears/</link>
      <pubDate>Fri, 18 Jul 2014 00:00:00 +0000</pubDate>
      <guid>/publication/analyzing-scd-hopes-and-fears/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Special Education Pro-Sem</title>
      <link>/sped-pro-sem/</link>
      <pubDate>Thu, 10 Apr 2014 00:00:00 +0000</pubDate>
      <guid>/sped-pro-sem/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;http://www.edb.utexas.edu/education/departments/sped/about/fac_dir/barnes/&#34;&gt;Dr. Marcia Barnes&lt;/a&gt; from the department of Special Education invited me to visit her pro-seminar this afternoon and talk about some of my work on meta-analytic methods for single-case research. Thanks very much to the students for asking such thoughtful and engaging questions. &lt;a href=&#34;/files/Barnes-Pro-Sem-2014-04-10.pdf&#34;&gt;Here are the slides&lt;/a&gt;, which include some additional material that we didn’t get to talk about.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Analysis and meta-analysis of single-case designs with a standardized mean difference statistic: A primer and applications</title>
      <link>/publication/bc-smd-primer-and-applications/</link>
      <pubDate>Tue, 01 Apr 2014 00:00:00 +0000</pubDate>
      <guid>/publication/bc-smd-primer-and-applications/</guid>
      <description></description>
    </item>
    
    <item>
      <title>New article: Measurement-comparable effect sizes for single-case studies of free-operant behavior</title>
      <link>/measurement-comparable-effect-sizes/</link>
      <pubDate>Tue, 04 Feb 2014 00:00:00 +0000</pubDate>
      <guid>/measurement-comparable-effect-sizes/</guid>
      <description>


&lt;p&gt;My article “Measurement-comparable effect sizes for single-case studies of free-operant behavior” has been accepted at &lt;em&gt;Psychological Methods&lt;/em&gt;. &lt;a href=&#34;/files/Measurement-comparable-ES.pdf&#34;&gt;Postprint&lt;/a&gt; and &lt;a href=&#34;/files/Measuerment-comparable-ES-Appendix.pdf&#34;&gt;supporting materials&lt;/a&gt; are available. Here’s the abstract:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Single-case research comprises a set of designs and methods for evaluating the effects of interventions, practices, or programs on individual cases, through comparison of outcomes measured at different points in time. Although there has long been interest in meta-analytic technique for synthesizing single-case research, there has been little scrutiny of whether proposed effect sizes remain on a directly comparable metric when outcomes are measured using different operational procedures. Much of single-case research focuses on behavioral outcomes in free-operant contexts, which may be measured using a variety of different direct observation procedures. This article describes a suite of effect sizes for quantifying changes in free-operant behavior, motivated by an alternating renewal process model that allows measurement comparability to be established in precise terms. These effect size metrics have the advantage of comporting with how direct observation data are actually collected and summarized. Effect size estimators are proposed that are applicable when the behavior being measured remains stable within a given treatment condition. The methods are illustrated by two examples, including a re-analysis of a systematic review of the effects of choice-making opportunities on problem behavior.&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Reliability of UnGraphed single-case data: An example using the Shogren dataset</title>
      <link>/shogren-reliability-analysis/</link>
      <pubDate>Wed, 23 Oct 2013 00:00:00 +0000</pubDate>
      <guid>/shogren-reliability-analysis/</guid>
      <description>


&lt;p&gt;In one example from my dissertation, I re-analyzed a systematic review by Shogren and colleagues, titled “The effect of choice-making as an intervention for problem behavior” (Shogren, et al., 2004). In order to do the analysis, I retrieved all of the original articles identified by the review, scanned in all of the graphs depicting the data, and used (actually, had an undergraduate use) a computer program called &lt;a href=&#34;http://www.biosoft.com/w/ungraph.htm&#34;&gt;UnGraph&lt;/a&gt; to capture the data-points off of the graphs (see Shadish, et al., 2009 for details on this procedure).&lt;/p&gt;
&lt;p&gt;As it turned out, &lt;a href=&#34;http://www.kuleuven.be/wieiswie/en/person/00006844&#34;&gt;Wim Van Den Noortgate&lt;/a&gt; and &lt;a href=&#34;http://www.kuleuven.be/wieiswie/en/person/00015697&#34;&gt;Patrick Onghena&lt;/a&gt; followed a similar procedure in analyzing the same systematic review (reported in Van Den Noorgate &amp;amp; Onghena, 2008). Wim and Patrick were kind enough to share their data so that I could calculate the reliability of this data extraction procedure, based on the two independent replications. After some initial data-munging, I arrived at a &lt;a href=&#34;%7B%7Bsite.url%7D%7D/data/Shogren_data_merged.csv&#34;&gt;clean, merged dataset&lt;/a&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Shogren &amp;lt;- read.csv(&amp;quot;http://jepusto.com/data/Shogren_data_merged.csv&amp;quot;)
head(Shogren)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##     Study Case Setting  Measure time choice Phase A B lowIntAxis
## 1 Bambara   Al Dessert Protests    1      0     A 5 5          1
## 2 Bambara   Al Dessert Protests    2      0     A 7 7          1
## 3 Bambara   Al Dessert Protests    3      0     A 4 4          1
## 4 Bambara   Al Dessert Protests    4      1     B 1 1          1
## 5 Bambara   Al Dessert Protests    5      1     B 0 0          1
## 6 Bambara   Al Dessert Protests    6      1     B 1 1          1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The variables are as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Study - First author of original study included in the meta-analysis;&lt;/li&gt;
&lt;li&gt;Case - Name of individual case;&lt;/li&gt;
&lt;li&gt;Setting - some of the studies used multiple baselines on single individuals across multiple settings;&lt;/li&gt;
&lt;li&gt;Measure - some of the studies used multiple outcome measures on each case;&lt;/li&gt;
&lt;li&gt;time - sequential measurement occasion;&lt;/li&gt;
&lt;li&gt;choice - indicator equal to one if the treatment condition allowed for choice;&lt;/li&gt;
&lt;li&gt;Phase - Factor indicating sequential phases (some of the designs were treatment reversals, such as ABA or ABAB or ABABAB);&lt;/li&gt;
&lt;li&gt;A - Wim’s outcome measurement;&lt;/li&gt;
&lt;li&gt;B - my outcome measurement;&lt;/li&gt;
&lt;li&gt;lowIntAxis - an idicator equal to one if the vertical axis of the graph was labeled with integers, and the axis maximum was &amp;lt;= 20.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The final variable distinguishes graphs that are particularly easy to capture. Wim/Patrick and I used slightly different exclusion criteria, so there are a total of 30 cases across 12 studies included in the merged dataset. To begin, here’s a plot of A versus B by study:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
qplot(A, B, geom = &amp;quot;point&amp;quot;, color = Case, data = Shogren) + facet_wrap( ~ Study, scales = &amp;quot;free&amp;quot;) + theme(legend.position=&amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Shogren-reliability-analysis_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Clearly the two measurements are very correlated. You’ll notice that the studies (and sometimes cases within studies) used several different outcome measurement scales, so the overall correlation between A and B (r = 0.999767) isn’t really the best approach. Furthermore, some of the variation in the outcomes is presumably due to differences between phases, and it would be better to calculate a reliability based on the residual variation within phases.&lt;/p&gt;
&lt;p&gt;I accomplish this with a simple hierarchical model, fit separately to the data from each case. Denote the outcome as &lt;span class=&#34;math inline&#34;&gt;\(y_{ijk}\)&lt;/span&gt; for phase &lt;span class=&#34;math inline&#34;&gt;\(i = 1,...,P\)&lt;/span&gt;, measurement occasion &lt;span class=&#34;math inline&#34;&gt;\(j = 1,...,n_i\)&lt;/span&gt;, and replicate &lt;span class=&#34;math inline&#34;&gt;\(k = 1,2\)&lt;/span&gt;. I model these outcomes as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y_{ijk} = \beta_i + \epsilon_{ij} + \nu_{ijk}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with the &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;’s fixed, &lt;span class=&#34;math inline&#34;&gt;\(\epsilon_{ij} \sim (0, \tau^2)\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\nu_{ijk} \sim (0, \sigma^2)\)&lt;/span&gt;. Reliability is then captured by the intra-class correlation &lt;span class=&#34;math inline&#34;&gt;\(\rho = \tau^2 / (\tau^2 + \sigma^2)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;I calculate the reliabilities from each case using restricted maximum likelihood, then apply Fisher’s Z-transform:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(reshape)
library(plyr)

Shogren_long &amp;lt;- melt(Shogren, measure.vars = c(&amp;quot;A&amp;quot;,&amp;quot;B&amp;quot;), variable_name = &amp;quot;observer&amp;quot;)

Fisher_Z &amp;lt;- function(x) 0.5 * (log(1 + x) - log(1 - x))

library(nlme)
Z_ICC &amp;lt;- function(x, formula = value ~ Phase){
  fit &amp;lt;- lme(formula, random = ~ 1 | time, data = x)
  tau.sq.ratio &amp;lt;- as.double(coef(fit$modelStruct$reStruct, FALSE))
  rho &amp;lt;- tau.sq.ratio / (tau.sq.ratio + 1)
  Z &amp;lt;- Fisher_Z(rho)
  df &amp;lt;- dim(x)[1] / 2 - length(fit$coefficients$fixed)
  return(c(rho = rho, Z = Z, df = df))
}
ICC &amp;lt;- ddply(Shogren_long, .(Study, Case, Setting, Measure, lowIntAxis), Z_ICC)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It turns out that 5 of 6 cases with lowIntAxis==1 are perfectly correlated. The remainder of my analysis focuses on the cases with lowIntAxis==0. Here’s a histogram of the Z-transformed correlations:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;with(subset(ICC, lowIntAxis==0), hist(Z))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Shogren-reliability-analysis_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;With only 2 replicates per measurement occasion, the large-sample variance of the intra-class correlation is equivalent to that of the usual Pearson correlation (see Hedges, Hedberg, &amp;amp; Kuyper, 2013), except that I use &lt;span class=&#34;math inline&#34;&gt;\(N - P\)&lt;/span&gt; in the denominator to account for the fact that separate means are estimated for each of the &lt;span class=&#34;math inline&#34;&gt;\(P\)&lt;/span&gt; phases: &lt;span class=&#34;math display&#34;&gt;\[Var(\hat\rho) \approx \frac{(1 - \rho^2)^2}{N - P},\]&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(N = \sum_i n_i\)&lt;/span&gt;. Applying Fisher’s Z transform stabilizes the variance, so that it is appropriate to use inverse variance weights of simply &lt;span class=&#34;math inline&#34;&gt;\(N - P\)&lt;/span&gt;. Turning to a random-effects meta-analysis:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(metafor)
summary(rma_Z &amp;lt;- rma(yi = Z, vi = 1 / df, data = ICC, subset = lowIntAxis==0))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Random-Effects Model (k = 27; tau^2 estimator: REML)
## 
##   logLik  deviance       AIC       BIC      AICc 
## -26.1156   52.2313   56.2313   58.7475   56.7530   
## 
## tau^2 (estimated amount of total heterogeneity): 0.3778 (SE = 0.1198)
## tau (square root of estimated tau^2 value):      0.6146
## I^2 (total heterogeneity / total variability):   88.15%
## H^2 (total variability / sampling variability):  8.44
## 
## Test for Heterogeneity:
## Q(df = 26) = 211.1324, p-val &amp;lt; .0001
## 
## Model Results:
## 
## estimate      se     zval    pval   ci.lb   ci.ub 
##   3.2596  0.1265  25.7670  &amp;lt;.0001  3.0117  3.5075  *** 
## 
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The average effect size corresponds to a reliability of 0.9970546 (95% CI: [0.9951684,0.9982051]). The reliabilities are heterogeneous, but because they are all at the extreme of the scale, the heterogeneity has little practical implication: approximating the population of reliabilities by a normal distribution, and based on the RML estimates, 84 percent of reliabilities will be greater than 0.9900. Though one could certainly imagine factors that might explain the variation in reliabilities–the resolution of the image file from which the data were captured, the size of the points used to graph each measurement, the number of outcomes represented on the same graph–it hardly seems worth exploring further because all of the reliabilities are so high. These results are very similar to those reported by Shadish, et al. (2009), who found a median reliability of 0.9993 based on a similar study of 91 single-case graphs.&lt;/p&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Hedges, L. V, Hedberg, E. C., &amp;amp; Kuyper, A. M. (2012). The variance of intraclass correlations in three- and four-level models. Educational and Psychological Measurement. &lt;a href=&#34;doi:10.1177/0013164412445193&#34; class=&#34;uri&#34;&gt;doi:10.1177/0013164412445193&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Shadish, W. R., Brasil, I. C. C., Illingworth, D. A., White, K. D., Galindo, R., Nagler, E. D., &amp;amp; Rindskopf, D. M. (2009). Using UnGraph to extract data from image files: Verification of reliability and validity. Behavior Research Methods, 41(1), 177-83. &lt;a href=&#34;doi:10.3758/BRM.41.1.177&#34; class=&#34;uri&#34;&gt;doi:10.3758/BRM.41.1.177&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Shogren, K. A., Faggella-Luby, M. N., Bae, S. J., &amp;amp; Wehmeyer, M. L. (2004). The effect of choice-making as an intervention for problem behavior. Journal of Positive Behavior Interventions, 6(4), 228-237.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Van den Noortgate, W., &amp;amp; Onghena, P. (2008). A multilevel meta-analysis of single-subject experimental design studies. Evidence-Based Communication Assessment and Intervention, 2(3), 142-151. &lt;a href=&#34;doi:10.1080/17489530802505362&#34; class=&#34;uri&#34;&gt;doi:10.1080/17489530802505362&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A standardized mean difference effect size for multiple baseline designs</title>
      <link>/publication/smd-for-mbd/</link>
      <pubDate>Fri, 23 Aug 2013 00:00:00 +0000</pubDate>
      <guid>/publication/smd-for-mbd/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Current projects</title>
      <link>/current-projects/</link>
      <pubDate>Tue, 20 Aug 2013 00:00:00 +0000</pubDate>
      <guid>/current-projects/</guid>
      <description>


&lt;p&gt;Interested in working with me? See below for descriptions of several potential projects. If you have interest and abilities that line up with one of these, feel free to contact me.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Review of methods for direct observation of behavior. Several different methods for recording direct observations of behavior are commonly used in single-case research and other areas of psychology; prominent methods include continuous duration recording, momentary time sampling, and partial interval recording. Textbook advice about appropriate use of different methods is conflicting and often ambiguous, and simulation studies evaluating the operating characteristics of different methods also yield mixed results. The goals of this project are to: find and organize the current guidance about direct observation procedures; understand the basis of that guidance (e.g., simulation studies, heuristic models); and relate the guidance to a unifying statistical framework, by translating claims and conclusions into the terms of a parametric model (known as an alternating renewal process). This project would be appropriate either for a quantitative methods student who is interested in learning about direct observation methods for measuring behavior or for a student from school psychology, counseling psychology, or special education who is familiar with direct observation methods and interested in learning about statistical models for the data they generate.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Applications of meta-analysis for single-case studies of free-operant behavior. I have recently proposed a suite of new effect size metrics for quantifying treatment effects in single-case studies of free-operant behavior. The crux of this line of work is that it is important to use effect size metrics that are comparable across different methods of recording direct observation data. This project will involve: reviewing several published systematic reviews that incorporate evidence from single-case studies, in order to determine what measurement procedures were used to collect data, then re-analyzing the data from one or more of these studies, using the newly proposed effect size metrics and methods. This project would be appropriate for a special education student who is familiar with meta-analysis.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Applications of design-comparable effect size measures for longitudinal studies. Co-authors and I have recently proposed a method of estimating effect sizes from single-case studies (or other types of longitudinal designs) that are in the same metric as Cohen’s d-type effect sizes from conventional between-subjects experiments. The goals of this project are to: develop exemplar code that implements effect size calculations in several major statistical packages (including SPSS, SAS, Stata, and R); review the algorithms available in major statistical packages for estimating the uncertainty of variance components (i.e., information matrices); develop further applications and extensions to the proposed effect sizes. This project would be appropriate for a quantitative methods student who is familiar with estimation of hierarchical linear models in SPSS, SAS, and other major statistical software platforms.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Programming information matrices for hierarchical linear modeling. The Fisher information plays a pivotal role in hierarchical linear models, both as an approximate estimates of parameter uncertainty and as a key component of small-sample hypothesis tests such as those of Kenward and Roger (1997,2009). The goals of this project are to: create an R package for constructing analytic information matrices for HLM models estimated with the well-known nlme package; also add functions for the revised Kenward &amp;amp; Roger hypothesis tests; and evaluate the performance of different information matrices (expected, observed, and average) for calculating degrees-of-freedom adjustments in the context of effect size estimation. This project could be appropriate for a quantitative methods student or a statistics student who has strong programming skills and wants to 1) learn more about the statistical guts of HLM estimation and 2) level-up on their R programming by designing a publishable package.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A discrete-time Markov chain model for partial interval recording data. Partial interval recording is a commonly used method for recording direct observations of human behavior. Data generated by this method is problematic because, as typically analyzed, it yields upwardly biased measures of prevalence (the proportion of time that a behavior occurs). This shortcoming can be addressed by modeling the data using a discrete-time Markov chain and using maximum likelihood methods to estimate parameters corresponding directly to prevalence and incidence (the frequency with which new behaviors occur). The goals of this project are to create an R package implementing maximum likelihood estimation (and possibly other methods) for partial interval recording data and evaluate this estimation approach using asymptotic theory and simulation. This project could be appropriate for an advanced quantitative methods student or statistics student who is interested in learning about Markov chain models and who has strong programming skills.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>A d-statistic for single-case designs that is equivalent to the usual between-groups d-statistics</title>
      <link>/publication/between-groups-d-statistic/</link>
      <pubDate>Thu, 18 Jul 2013 00:00:00 +0000</pubDate>
      <guid>/publication/between-groups-d-statistic/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Operationally comparable effect sizes for meta-analysis of single-case research</title>
      <link>/publication/operationally-comparable-effect-sizes/</link>
      <pubDate>Sat, 01 Jun 2013 00:00:00 +0000</pubDate>
      <guid>/publication/operationally-comparable-effect-sizes/</guid>
      <description>&lt;p&gt;This thesis studies quantitative methods for summarizing and synthesizing single-case studies, a class of research designs for evaluating the effects of interventions through repeated measurement of individuals. Despite long-standing interest in meta-analytic synthesis of single-case research, there remains a lack of consensus about appropriate methods, even about the most basic question of what effect size metrics are useful and appropriate. I argue that operational comparability, or invariance to heterogeneous operational procedures, is crucial property for an effect size metric. I then consider two problems with operational comparability that arise in single-case research. The first problem is to find effect sizes that can be applied across studies that use different research designs, such as single-case designs and two-group randomized experiments. The second problem is to find effect sizes that can be applied across studies that use varied operations for measuring the same construct. To address each of these problems, I propose structural models that capture essential features of multiple relevant operations (either design-related operations or measurement-related operations). I then use these structural models to precisely define target effect size parameters and to consider identification issues and estimation strategies.&lt;/p&gt;
&lt;p&gt;Chapter 1 defines operational comparability and situates the concept within the broad methodological concerns of meta-analysis, then reviews relevant features of single-case research and previously proposed effect sizes. Chapter 2 describes an abstract set of modeling criteria for constructing design-comparable effect sizes. Chapters 3 applies the general criteria to the case of standardized mean differences and proposes an effect size estimator based on restricted maximum likelihood. Chapter 4 presents several applications of the proposed models and methods. Chapter 5 proposes measurement-comparability model and defines effect size measures for use in studies of free-operant behavior, one of the most common classes of outcomes in single-case research. Chapter 6 extends the proposed effect size models to incorporate more complex features, including time trends and serial dependence, and studies a method of estimating those models through a combination of marginal quasi-likelihood and Gaussian pseudo-likelihood estimating equations. Chapter 7 collects various further extensions, areas for further research, and concluding thoughts.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A standardized mean difference effect size for single case designs</title>
      <link>/publication/smd-for-scd/</link>
      <pubDate>Tue, 14 Aug 2012 00:00:00 +0000</pubDate>
      <guid>/publication/smd-for-scd/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
