<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>response ratio | James E. Pustejovsky</title>
    <link>/tags/response-ratio/</link>
      <atom:link href="/tags/response-ratio/index.xml" rel="self" type="application/rss+xml" />
    <description>response ratio</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2021</copyright><lastBuildDate>Tue, 05 Jan 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>response ratio</title>
      <link>/tags/response-ratio/</link>
    </image>
    
    <item>
      <title>Systematic review and meta-analysis of stay-play-talk interventions for improving social behaviors of young children</title>
      <link>/publication/stay-play-talk-meta-analysis/</link>
      <pubDate>Tue, 05 Jan 2021 00:00:00 +0000</pubDate>
      <guid>/publication/stay-play-talk-meta-analysis/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Procedural sensitivities of effect sizes for single-case designs with behavioral outcome measures</title>
      <link>/publication/procedural-sensitivities-of-scd-effect-sizes/</link>
      <pubDate>Mon, 01 Apr 2019 00:00:00 +0000</pubDate>
      <guid>/publication/procedural-sensitivities-of-scd-effect-sizes/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A gradual effects model for single-case designs</title>
      <link>/publication/gradual-effects-model/</link>
      <pubDate>Tue, 01 May 2018 00:00:00 +0000</pubDate>
      <guid>/publication/gradual-effects-model/</guid>
      <description></description>
    </item>
    
    <item>
      <title>New paper: Using response ratios for meta-analyzing SCDs with behavioral outcomes</title>
      <link>/using-response-ratios-paper/</link>
      <pubDate>Fri, 16 Mar 2018 00:00:00 +0000</pubDate>
      <guid>/using-response-ratios-paper/</guid>
      <description>


&lt;p&gt;I’m pleased to announce that my article “Using response ratios for meta-analyzing SCDs with behavioral outcomes” has been accepted at &lt;em&gt;Journal of School Psychology&lt;/em&gt;. There’s a multitude of ways that you can access this work:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For the next 6 weeks or so, the published version of the article will be &lt;a href=&#34;https://authors.elsevier.com/a/1Wj5D56ZN7p98&#34;&gt;available at the journal website&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;The pre-print will always remain &lt;a href=&#34;https://psyarxiv.com/nj28d/&#34;&gt;available at PsyArXiv&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Some supporting materials and replication code are &lt;a href=&#34;https://osf.io/c3fe9/&#34;&gt;available on the Open Science Framework&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here’s the abstract of the paper:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Methods for meta-analyzing single-case designs (SCDs) are needed to inform evidence-based practice in clinical and school settingsand to draw broader and more defensible generalizations in areas where SCDs comprise a large part of the research base. The most widely used outcomesin single-case research are measures of behavior collected using systematic direct observation, which typically take the form of rates or proportions. For studies that use such measures, one simple and intuitive way to quantify effect sizes is in terms of proportionate change from baseline, using an effect size known as the log response ratio. This paper describes methods for estimating log response ratios and combining the estimates using meta-analysis. The methods are based on a simple model for comparing two phases, where the level of the outcome is stable within each phase and the repeated outcome measurements are independent. Although auto-correlation will lead to biased estimates of the sampling variance of the effect size, meta-analysis of response ratios can be conducted with robust variance estimation procedures that remain valid even when sampling variance estimates are biased. The methods are demonstrated using data from a recent meta-analysis on group contingency interventions for student problem behavior.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Single-case synthesis tools II: Comparing overlap measures and parametric effect sizes for synthesizing antecedent sensory-based interventions</title>
      <link>/publication/scd-synthesis-tools-ii/</link>
      <pubDate>Wed, 07 Mar 2018 00:00:00 +0000</pubDate>
      <guid>/publication/scd-synthesis-tools-ii/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Using response ratios for meta-analyzing single-case designs with behavioral outcomes</title>
      <link>/publication/using-response-ratios/</link>
      <pubDate>Thu, 01 Feb 2018 00:00:00 +0000</pubDate>
      <guid>/publication/using-response-ratios/</guid>
      <description></description>
    </item>
    
    <item>
      <title>New paper: procedural sensitivities of effect size measures for SCDs</title>
      <link>/procedural-sensitivities-paper/</link>
      <pubDate>Thu, 11 Jan 2018 00:00:00 +0000</pubDate>
      <guid>/procedural-sensitivities-paper/</guid>
      <description>


&lt;p&gt;I’m very happy to share that my article “Procedural sensitivities of effect sizes for single-case designs with directly observed behavioral outcome measures” has been accepted at &lt;em&gt;Psychological Methods&lt;/em&gt;. There’s no need to delay in reading it, since you can check out the &lt;a href=&#34;https://psyarxiv.com/vxa86&#34;&gt;pre-print&lt;/a&gt; and &lt;a href=&#34;https://osf.io/hkzsm/&#34;&gt;supporting materials&lt;/a&gt;. Here’s the abstract:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A wide variety of effect size indices have been proposed for quantifying the magnitude of treatment effects in single-case designs. Commonly used measures include parametric indices such as the standardized mean difference, as well as non-overlap measures such as the percentage of non-overlapping data, improvement rate difference, and non-overlap of all pairs. Currently, little is known about the properties of these indices when applied to behavioral data collected by systematic direct observation, even though systematic direct observation is the most common method for outcome measurement in single-case research. This study uses Monte Carlo simulation to investigate the properties of several widely used single-case effect size measures when applied to systematic direct observation data. Results indicate that the magnitude of the non-overlap measures and of the standardized mean difference can be strongly influenced by procedural details of the study’s design, which is a significant limitation to using these indices as effect sizes for meta-analysis of single-case designs. A less widely used parametric index, the log-response ratio, has the advantage of being insensitive to sample size and observation session length, although its magnitude is influenced by the use of partial interval recording.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This paper was a long time coming. The core idea came out of a grant proposal I wrote during the summer of 2014, which I fleshed out for a &lt;a href=&#34;/files/AERA-2015-poster-Non-overlap-measures.pdf&#34;&gt;poster presented at AERA&lt;/a&gt; in April of 2015. After finishing a draft of the paper, I tried to publish it in a special education journal, reasoning that the main audience for the paper is researchers interested in meta-analyzing single case research studies that are commonly used in some parts of special education. That turned out to be a non-starter. Four rejection letters later, I re-worked the paper a bit to give more technical details, then submitted it to a more methods-ish journal. This yielded an R&amp;amp;R, I revised the paper extensively, resubmitted it, and it was declined. Buying in fully to the sunk costs fallacy, I sent the paper to Psychological Methods. This time, I received very extensive and helpful feedback from several anonymous reviewers and an associate editor (thank you, anonymous peers!), which helped me to revise the paper yet again, and this time it was accepted. Sixth time is the charm, as they say.&lt;/p&gt;
&lt;p&gt;Here’s the complete time-line of submissions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;August 5, 2015: submitted to journal #1 (special education)&lt;/li&gt;
&lt;li&gt;August 28, 2015: desk reject decision from journal #1&lt;/li&gt;
&lt;li&gt;September 3, 2015: submitted to journal #2 (special education)&lt;/li&gt;
&lt;li&gt;November 6, 2015: reject decision (after peer review) from journal #2&lt;/li&gt;
&lt;li&gt;November 18, 2015: submitted to journal #3 (special education)&lt;/li&gt;
&lt;li&gt;November 22, 2015: desk reject decision from journal #3 as not appropriate for their audience. I was grateful to get a quick decision.&lt;/li&gt;
&lt;li&gt;November 23, 2015: submitted to journal #4 (special education)&lt;/li&gt;
&lt;li&gt;February 17, 2016: reject decision (after peer review) from journal #4&lt;/li&gt;
&lt;li&gt;April 19, 2016: submitted to journal #5 (methods)&lt;/li&gt;
&lt;li&gt;August 16, 2016: revise-and-resubmit decision from journal #5&lt;/li&gt;
&lt;li&gt;October 14, 2016: re-submitted to journal #5&lt;/li&gt;
&lt;li&gt;February 2, 2017: reject decision from journal #5&lt;/li&gt;
&lt;li&gt;May 10, 2017: submitted to &lt;em&gt;Psychological Methods&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;September 1, 2017: revise-and-resubmit decision from &lt;em&gt;Psychological Methods&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;September 26, 2017: re-submitted to &lt;em&gt;Psychological Methods&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;November 22, 2017: conditional acceptance&lt;/li&gt;
&lt;li&gt;December 6, 2017: re-submitted with minor revisions&lt;/li&gt;
&lt;li&gt;January 10, 2018: accepted at &lt;em&gt;Psychological Methods&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>New working paper: Using log response ratios for meta-analyzing SCDs with behavioral outcomes</title>
      <link>/using-log-response-ratios/</link>
      <pubDate>Wed, 26 Apr 2017 00:00:00 +0000</pubDate>
      <guid>/using-log-response-ratios/</guid>
      <description>


&lt;p&gt;One of the papers that came out of my dissertation work (&lt;a href=&#34;/files/Measurement-comparable-ES.pdf&#34;&gt;Pustejovsky, 2015&lt;/a&gt;) introduced an effect size metric called the &lt;strong&gt;log response ratio&lt;/strong&gt; (or LRR) for use in meta-analysis of single-case research—particularly for single-case studies that measure behavioral outcomes through systematic direct observation. The original paper was pretty technical since it focused mostly on a formal measurement model for behavioral observation data. I’ve just completed a tutorial paper that demonstrates how to use the LRR for meta-analyzing single-case studies with behavioral outcomes. In this paper, I’ve tried to present the methods in as accessible a manner as I could muster, to provide a sort of “user’s guide” for researchers wanting to apply the LRR for their own work. You can find the &lt;a href=&#34;https://osf.io/4fe6u/&#34;&gt;working paper&lt;/a&gt; and &lt;a href=&#34;https://osf.io/c3fe9/&#34;&gt;supplementary materials&lt;/a&gt; (including data and replication code) on the Open Science Framework. I would welcome your feedback and questions about this work!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Functional assessment-based interventions for students with or at-risk for high incidence disabilities: Field-testing single-case synthesis methods</title>
      <link>/publication/fabi-meta-analysis/</link>
      <pubDate>Wed, 01 Mar 2017 00:00:00 +0000</pubDate>
      <guid>/publication/fabi-meta-analysis/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Research synthesis and meta-analysis of single-case designs</title>
      <link>/publication/meta-analysis-of-scd/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>/publication/meta-analysis-of-scd/</guid>
      <description></description>
    </item>
    
    <item>
      <title>New working paper: Procedural sensitivities of SCD effect sizes</title>
      <link>/scd-effect-size-sensitivities/</link>
      <pubDate>Mon, 17 Oct 2016 00:00:00 +0000</pubDate>
      <guid>/scd-effect-size-sensitivities/</guid>
      <description>


&lt;p&gt;I’ve just posted a new version of my working paper, &lt;em&gt;Procedural sensitivities of effect sizes for single-case designs with behavioral outcome measures&lt;/em&gt;. The abstract is below. This version is a major update of an &lt;a href=&#34;/files/Pustejovsky-2015-Nov-Non-overlap-measures.pdf&#34;&gt;earlier paper&lt;/a&gt; that focused only on the non-overlap measures. The new version also includes analysis of two other effect sizes (the within-case standardized mean difference and the log response ratio) as well as additional results and more succinct summaries of the main findings.&lt;/p&gt;
&lt;p&gt;The paper itself is available on the Open Science Framework (&lt;a href=&#34;https://osf.io/pxn24/&#34;&gt;here&lt;/a&gt;), as are the &lt;a href=&#34;https://osf.io/hkzsm/&#34;&gt;supplementary materials&lt;/a&gt; and &lt;a href=&#34;https://osf.io/j4gvt/&#34;&gt;Source code&lt;/a&gt;. I also created interaction versions of the graphics from the main paper and the supplementary materials, which can be viewed in &lt;a href=&#34;https://jepusto.shinyapps.io/SCD-effect-size-sensitivities/&#34;&gt;this shiny app&lt;/a&gt;. I would welcome any comments, questions, or feedback that readers may have.&lt;/p&gt;
&lt;div id=&#34;abstract&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;A wide variety of effect size indices have been proposed for quantifying the magnitude of treatment effects in single-case designs. Commonly used measures include parametric indices such as the standardized mean difference, as well as non-overlap measures, such as the percentage of non-overlapping data, improvement rate difference, and non-overlap of all pairs. Currently, little is known about the properties of these indices when applied to behavioral data collected by systematic direct observation, even though systematic direct observation is the most common approach to outcome measurement in single-case research. This study uses computer simulation to investigate the properties of several single-case effect size measures when applied to systematic direct observation data. Results indicate that the magnitude of the non-overlap measures and of the standardized mean difference can be strongly influenced by procedural details of the study’s design, which is a significant limitation to using these indices as effect sizes for meta-analysis of single-case designs. A less widely used parametric index, the log-response ratio, has the advantage of being insensitive to sample size and observation session length, although its magnitude is influenced by the use of partial interval recording.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>SingleCaseES</title>
      <link>/software/singlecasees/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>/software/singlecasees/</guid>
      <description>&lt;p&gt;An R package for calculating basic effect size indices for single-case designs, including several non-overlap measures and parametric effect size measures, and for estimating the gradual effects model developed by Swan and Pustejovsky (2017).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://CRAN.R-project.org/package=SingleCaseES&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Available on the Comprehensive R Archive Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/jepusto/SingleCaseES&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Source code and installation instructions on Github&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://jepusto.shinyapps.io/SCD-effect-sizes/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Single case effect size calculator&lt;/a&gt;: An interactive web application for calculating basic effect size indices.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://jepusto.shinyapps.io/gem-scd/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gradual Effect Model calculator&lt;/a&gt;: An interactive web application for estimating effect sizes using the gradual effects model.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Measurement-comparable effect sizes for single-case studies of free-operant behavior</title>
      <link>/publication/measurement-comparable-effect-sizes/</link>
      <pubDate>Tue, 01 Sep 2015 00:00:00 +0000</pubDate>
      <guid>/publication/measurement-comparable-effect-sizes/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Four methods for analyzing partial interval recording data, with application to single-case research</title>
      <link>/publication/four-methods-for-pir/</link>
      <pubDate>Fri, 19 Jun 2015 00:00:00 +0000</pubDate>
      <guid>/publication/four-methods-for-pir/</guid>
      <description></description>
    </item>
    
    <item>
      <title>New article: Measurement-comparable effect sizes for single-case studies of free-operant behavior</title>
      <link>/measurement-comparable-effect-sizes/</link>
      <pubDate>Tue, 04 Feb 2014 00:00:00 +0000</pubDate>
      <guid>/measurement-comparable-effect-sizes/</guid>
      <description>


&lt;p&gt;My article “Measurement-comparable effect sizes for single-case studies of free-operant behavior” has been accepted at &lt;em&gt;Psychological Methods&lt;/em&gt;. &lt;a href=&#34;/files/Measurement-comparable-ES.pdf&#34;&gt;Postprint&lt;/a&gt; and &lt;a href=&#34;/files/Measuerment-comparable-ES-Appendix.pdf&#34;&gt;supporting materials&lt;/a&gt; are available. Here’s the abstract:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Single-case research comprises a set of designs and methods for evaluating the effects of interventions, practices, or programs on individual cases, through comparison of outcomes measured at different points in time. Although there has long been interest in meta-analytic technique for synthesizing single-case research, there has been little scrutiny of whether proposed effect sizes remain on a directly comparable metric when outcomes are measured using different operational procedures. Much of single-case research focuses on behavioral outcomes in free-operant contexts, which may be measured using a variety of different direct observation procedures. This article describes a suite of effect sizes for quantifying changes in free-operant behavior, motivated by an alternating renewal process model that allows measurement comparability to be established in precise terms. These effect size metrics have the advantage of comporting with how direct observation data are actually collected and summarized. Effect size estimators are proposed that are applicable when the behavior being measured remains stable within a given treatment condition. The methods are illustrated by two examples, including a re-analysis of a systematic review of the effects of choice-making opportunities on problem behavior.&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Operationally comparable effect sizes for meta-analysis of single-case research</title>
      <link>/publication/operationally-comparable-effect-sizes/</link>
      <pubDate>Sat, 01 Jun 2013 00:00:00 +0000</pubDate>
      <guid>/publication/operationally-comparable-effect-sizes/</guid>
      <description>&lt;p&gt;This thesis studies quantitative methods for summarizing and synthesizing single-case studies, a class of research designs for evaluating the effects of interventions through repeated measurement of individuals. Despite long-standing interest in meta-analytic synthesis of single-case research, there remains a lack of consensus about appropriate methods, even about the most basic question of what effect size metrics are useful and appropriate. I argue that operational comparability, or invariance to heterogeneous operational procedures, is crucial property for an effect size metric. I then consider two problems with operational comparability that arise in single-case research. The first problem is to find effect sizes that can be applied across studies that use different research designs, such as single-case designs and two-group randomized experiments. The second problem is to find effect sizes that can be applied across studies that use varied operations for measuring the same construct. To address each of these problems, I propose structural models that capture essential features of multiple relevant operations (either design-related operations or measurement-related operations). I then use these structural models to precisely define target effect size parameters and to consider identification issues and estimation strategies.&lt;/p&gt;
&lt;p&gt;Chapter 1 defines operational comparability and situates the concept within the broad methodological concerns of meta-analysis, then reviews relevant features of single-case research and previously proposed effect sizes. Chapter 2 describes an abstract set of modeling criteria for constructing design-comparable effect sizes. Chapters 3 applies the general criteria to the case of standardized mean differences and proposes an effect size estimator based on restricted maximum likelihood. Chapter 4 presents several applications of the proposed models and methods. Chapter 5 proposes measurement-comparability model and defines effect size measures for use in studies of free-operant behavior, one of the most common classes of outcomes in single-case research. Chapter 6 extends the proposed effect size models to incorporate more complex features, including time trends and serial dependence, and studies a method of estimating those models through a combination of marginal quasi-likelihood and Gaussian pseudo-likelihood estimating equations. Chapter 7 collects various further extensions, areas for further research, and concluding thoughts.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
