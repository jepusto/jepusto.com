<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Distribution Theory | James E. Pustejovsky</title>
    <link>http://localhost:4321/tags/distribution-theory/</link>
      <atom:link href="http://localhost:4321/tags/distribution-theory/index.xml" rel="self" type="application/rss+xml" />
    <description>Distribution Theory</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2024</copyright><lastBuildDate>Thu, 28 Mar 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://localhost:4321/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Distribution Theory</title>
      <link>http://localhost:4321/tags/distribution-theory/</link>
    </image>
    
    <item>
      <title>Distribution of the number of significant effect sizes</title>
      <link>http://localhost:4321/distribution-of-significant-effects/</link>
      <pubDate>Thu, 28 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/distribution-of-significant-effects/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;http://localhost:4321/number-of-significant-effects/&#34;&gt;A while back&lt;/a&gt;, I posted the outline of a problem about the number of significant effect size estimates in a study that reports multiple outcomes. This problem interests me because it connects to the issue of selective reporting of study results, which creates problems for meta-analysis.
Here, I’ll re-state the problem in slightly more general terms and then make some notes about what’s going on.&lt;/p&gt;
&lt;p&gt;Consider a study that assesses some effect size across &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; different outcomes. (We’ll be thinking about one study at a time here, so no need to index the study as we would in a meta-analysis problem.) Let &lt;span class=&#34;math inline&#34;&gt;\(T_i\)&lt;/span&gt; denote the effect size estimate for outcome &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, let &lt;span class=&#34;math inline&#34;&gt;\(V_i\)&lt;/span&gt; denote the sampling variance of the effect size estimate for outcome &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, and let &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt; denote the true effect size parameter for corresponding to outcome &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;. Assume that the study outcomes &lt;span class=&#34;math inline&#34;&gt;\(\left[T_i\right]_{i=1}^m\)&lt;/span&gt; follow a correlated-and-hierarchical effects model, in which
&lt;span class=&#34;math display&#34;&gt;\[T_i = \mu + u + v_i + e_i,\]&lt;/span&gt;
where the study-level error &lt;span class=&#34;math inline&#34;&gt;\(u \sim N\left(0,\tau^2\right)\)&lt;/span&gt;, the effect-specific error &lt;span class=&#34;math inline&#34;&gt;\(v_i \stackrel{iid}{\sim} N\left(0, \omega^2\right)\)&lt;/span&gt;, and the vector of sampling errors &lt;span class=&#34;math inline&#34;&gt;\(\left[e_i\right]_{i=1}^m\)&lt;/span&gt; is multivariate normal with mean &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{0}\)&lt;/span&gt;, known variances &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}(e_i) = \sigma^2\)&lt;/span&gt;, and compound symmetric correlation structure &lt;span class=&#34;math inline&#34;&gt;\(\text{cor}(e_h, e_i) = \rho\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Define &lt;span class=&#34;math inline&#34;&gt;\(A_i\)&lt;/span&gt; as an indicator that is equal to one if &lt;span class=&#34;math inline&#34;&gt;\(T_i\)&lt;/span&gt; is statistically significant at level &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; based on a one-sided test, and otherwise equal to zero. (Equivalently, let &lt;span class=&#34;math inline&#34;&gt;\(A_i\)&lt;/span&gt; be equal to one if the effect is statistically significant at level &lt;span class=&#34;math inline&#34;&gt;\(2 \alpha\)&lt;/span&gt; and in the theoretically expected direction.) Formally,
&lt;span class=&#34;math display&#34;&gt;\[A_i = I\left(\frac{T_i}{\sigma} &amp;gt; q_\alpha \right)\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(q_\alpha = \Phi^{-1}(1 - \alpha)\)&lt;/span&gt; is the critical value from a standard normal distribution (e.g., &lt;span class=&#34;math inline&#34;&gt;\(q_{.05} = 1.645\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(q_{.025} = 1.96\)&lt;/span&gt;). Let &lt;span class=&#34;math inline&#34;&gt;\(N_A = \sum_{i=1}^m A_i\)&lt;/span&gt; denote the total number of statistically significant effect sizes in the study. The question is: what is the distribution of &lt;span class=&#34;math inline&#34;&gt;\(N_A\)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;compound-symmetry-to-the-rescue&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Compound symmetry to the rescue&lt;/h2&gt;
&lt;p&gt;As I noted in the previous post, this set-up means that the effect size estimates have a compound symmetric distribution. We can make this a bit more explicit by writing the sampling errors in terms of the sum of a component that’s common acrosss outcomes and a component that’s specific to each outcome. Thus, let &lt;span class=&#34;math inline&#34;&gt;\(e_i = f + g_i\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(f \sim N\left(0, \rho \sigma^2 \right)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(g_i \stackrel{iid}{\sim} N \left(0, (1 - \rho) \sigma^2\right)\)&lt;/span&gt;. Let me also define &lt;span class=&#34;math inline&#34;&gt;\(\zeta = \mu + u + f\)&lt;/span&gt; as the conditional mean of the effects. It then follows that the effect size estimates are &lt;em&gt;conditionally independent&lt;/em&gt;, given the common components:
&lt;span class=&#34;math display&#34;&gt;\[
\left(T_i | \zeta \right) \stackrel{iid}{\sim} N\left(\zeta, \omega^2 + (1 - \rho) \sigma^2\right)
\]&lt;/span&gt;
Furthermore, the conditional probability of a significant effect is
&lt;span class=&#34;math display&#34;&gt;\[
\text{Pr}(A_i = 1 | \zeta) = \Phi\left(\frac{\zeta - q_{\alpha} \sigma}{\sqrt{\omega^2 + (1 - \rho)\sigma^2}}\right)
\]&lt;/span&gt;
and &lt;span class=&#34;math inline&#34;&gt;\(A_1,...,A_m\)&lt;/span&gt; are mutually independent, conditional on &lt;span class=&#34;math inline&#34;&gt;\(\zeta\)&lt;/span&gt;. Therefore, the conditional distribution of &lt;span class=&#34;math inline&#34;&gt;\(N_A\)&lt;/span&gt; is binomial,
&lt;span class=&#34;math display&#34;&gt;\[
\left(N_A | \zeta\right) \sim Bin(m, \pi)
\]&lt;/span&gt;
where
&lt;span class=&#34;math display&#34;&gt;\[
\pi = \Phi\left(\frac{\zeta - q_{\alpha} \sigma}{\sqrt{\omega^2 + (1 - \rho)\sigma^2}}\right).
\]&lt;/span&gt;
What about the unconditional distribution?&lt;/p&gt;
&lt;p&gt;To get rid of the &lt;span class=&#34;math inline&#34;&gt;\(\zeta\)&lt;/span&gt;, we need to integrate over its distribution, which leads to
&lt;span class=&#34;math display&#34;&gt;\[
\text{Pr}(N_A = a) = \text{E}\left[\text{Pr}\left(N_A | \zeta\right)\right] = \int f_{N_A}\left(a | \zeta, \omega, \sigma, \rho, m\right) \times f_\zeta(\zeta | \mu, \tau, \sigma, \rho) \ d \zeta,
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(f_{N_A}\left(a | \zeta, \omega, \sigma, \rho \right)\)&lt;/span&gt; is a binomial density with size &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; and probability &lt;span class=&#34;math inline&#34;&gt;\(\pi = \pi(\zeta, \omega, \sigma, \rho)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(f_\zeta(\zeta | \mu, \tau, \sigma, \rho)\)&lt;/span&gt; is a normal density with mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and variance &lt;span class=&#34;math inline&#34;&gt;\(\tau^2 + \rho \sigma^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This distribution is what you might call a binomial-normal convolution or a random-intercept probit model (where the random intercept is &lt;span class=&#34;math inline&#34;&gt;\(\zeta\)&lt;/span&gt;). As far as I know, the distribution cannot be evaluated analytically but instead must be calculated using some sort of numerical integration routine. Here is &lt;a href=&#34;http://localhost:4321/distribution-of-significant-effects-graph/&#34;&gt;an interactive graph of the probability mass function&lt;/a&gt; (the probability points are calculated using Gaussian quadrature).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;just-the-moments-please&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Just the moments, please&lt;/h2&gt;
&lt;p&gt;If all we care about is the expectation of &lt;span class=&#34;math inline&#34;&gt;\(N_A\)&lt;/span&gt;, we don’t need to bother with all the conditioning business and can just look at the marginal distribution of the effect size estimates taken individually. Marginally, &lt;span class=&#34;math inline&#34;&gt;\(T_i\)&lt;/span&gt; is normally distributed with mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and variance &lt;span class=&#34;math inline&#34;&gt;\(\tau^2 + \omega^2 + \sigma^2\)&lt;/span&gt;, so &lt;span class=&#34;math inline&#34;&gt;\(\text{Pr}(A_i = 1) = \psi\)&lt;/span&gt;, where
&lt;span class=&#34;math display&#34;&gt;\[
\psi = \Phi\left(\frac{\mu - q_{\alpha} \sigma}{\sqrt{\tau^2 + \omega^2 + \sigma^2}}\right).
\]&lt;/span&gt;
By the linearity of expectations,
&lt;span class=&#34;math display&#34;&gt;\[
\text{E}(N_A) = \sum_{i=1}^m \text{E}(A_i) = m \psi.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We can also get an approximation for the variance of &lt;span class=&#34;math inline&#34;&gt;\(N_A\)&lt;/span&gt; by working with its conditional distribution above. By the rule of variance decomposition,
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\text{Var}(N_A) &amp;amp;= \text{E}\left[\text{Var}\left(N_A | \zeta\right)\right] + \text{Var}\left[\text{E}\left(N_A | \zeta\right)\right] \\
&amp;amp;= m \times \text{E}\left[\pi (1 - \pi)\right] + m^2 \times \text{Var}\left[\pi\right]\\
&amp;amp;= m \times \text{E}\left[\pi\right] \left(1 - \text{E}\left[\pi\right]\right) + m (m - 1) \times \text{Var}\left[\pi\right],
\end{aligned}
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt; is, as defined above, a function of &lt;span class=&#34;math inline&#34;&gt;\(\zeta\)&lt;/span&gt; and thus a random variable. Now, &lt;span class=&#34;math inline&#34;&gt;\(\text{E}(\pi) = \psi\)&lt;/span&gt; and we can get something close to &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}(\pi)\)&lt;/span&gt; using a first-order approximation:
&lt;span class=&#34;math display&#34;&gt;\[
\text{Var}\left(\pi\right) \approx \left(\left.\frac{\delta \pi}{\delta \zeta}\right|_{\zeta = \mu}\right)^2 \times \text{Var}\left(\zeta\right) = \left[\phi\left(\frac{\mu - q_{\alpha} \sigma}{\sqrt{\omega^2 + (1 - \rho)\sigma^2}}\right)\right]^2 \times \frac{\tau^2 + \rho \sigma^2}{\omega^2 + (1 - \rho)\sigma^2}.
\]&lt;/span&gt;
Thus,
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\text{Var}(N_A) \approx m \times \psi \left(1 - \psi\right) + m (m - 1) \times \left[\phi\left(\frac{\mu - q_{\alpha} \sigma}{\sqrt{\omega^2 + (1 - \rho)\sigma^2}}\right)\right]^2 \times \frac{\tau^2 + \rho \sigma^2}{\omega^2 + (1 - \rho)\sigma^2}.
\end{aligned}
\]&lt;/span&gt;
If the amount of common variation is small, so &lt;span class=&#34;math inline&#34;&gt;\(\tau^2\)&lt;/span&gt; is near zero and &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt; is near zero, then the contribution of the second term will be small, and &lt;span class=&#34;math inline&#34;&gt;\(N_A\)&lt;/span&gt; will act more or less like a binomial random variable with size &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; and probability &lt;span class=&#34;math inline&#34;&gt;\(\psi\)&lt;/span&gt;. On the other hand, if the amount of independent variation in the effect sizes is small, so &lt;span class=&#34;math inline&#34;&gt;\(\omega^2\)&lt;/span&gt; is near zero and &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt; is near 1, then the term on the right will approach &lt;span class=&#34;math inline&#34;&gt;\(m(m - 1)\psi(1 - \psi)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}\left(N_A\right)\)&lt;/span&gt; will approach &lt;span class=&#34;math inline&#34;&gt;\(m^2 \psi(1 - \psi)\)&lt;/span&gt;, or the variance of &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; times a single Bernoulli variate. So you could say that &lt;span class=&#34;math inline&#34;&gt;\(N_A\)&lt;/span&gt; has anywhere between &lt;span class=&#34;math inline&#34;&gt;\(1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; variate’s worth of information in it, depending on the degree of correlation between the effect size estimates.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Approximating the distribution of cluster-robust Wald statistics</title>
      <link>http://localhost:4321/cluster-robust-wald-statistics/</link>
      <pubDate>Sun, 24 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/cluster-robust-wald-statistics/</guid>
      <description>


&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\def\Pr{{\text{Pr}}}
\def\E{{\text{E}}}
\def\Var{{\text{Var}}}
\def\Cov{{\text{Cov}}}
\def\cor{{\text{cor}}}
\def\bm{\mathbf}
\def\bs{\boldsymbol}
\]&lt;/span&gt;
In &lt;a href=&#34;http://doi.org/10.3102/1076998615606099&#34;&gt;Tipton and Pustejovsky (2015)&lt;/a&gt;, we examined several different small-sample approximations for cluster-robust Wald test statistics, which are like &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; statistics but based on cluster-robust variance estimators. These statistics are, frankly, kind of weird and awkward to work with, and the approximations that we examined were far from perfect. In this post, I will look in detail at the robust Wald statistic for a simple but common scenario: a one-way ANOVA problem with clusters of dependent observations.&lt;/p&gt;
&lt;p&gt;Consider a setup where clusters can be classified into one of &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt; categories, with each cluster of observations falling into a single category. Let &lt;span class=&#34;math inline&#34;&gt;\(\bs\mu = \left[\mu_c \right]_{c=1}^C\)&lt;/span&gt; denote the means of these categories. Suppose we have an estimator of those means &lt;span class=&#34;math inline&#34;&gt;\(\bs{\hat\mu} = \left[\hat\mu_c\right]_{c=1}^C\)&lt;/span&gt; and a corresponding cluster-robust variance estimator &lt;span class=&#34;math inline&#34;&gt;\(\bm{V}^R = \bigoplus_{c=1}^C V^R_c\)&lt;/span&gt;. Note that &lt;span class=&#34;math inline&#34;&gt;\(\bm{V}^R\)&lt;/span&gt; is diagonal because the estimators for each category are independent.
Assume that the robust variance estimator is unbiased so &lt;span class=&#34;math inline&#34;&gt;\(\E\left(V^R_c\right) = \Var\left( \hat\mu_c \right) = \psi_c\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(c = 1,...,C\)&lt;/span&gt;.
Let &lt;span class=&#34;math inline&#34;&gt;\(\bs\Psi = \bigoplus_{c=1}^C \psi_c\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Suppose that we want to test the null hypothesis that that means of the categories are all equal, &lt;span class=&#34;math inline&#34;&gt;\(H_0: \mu_1 = \mu_2 = \cdots = \mu_C\)&lt;/span&gt;. We can express this null using a &lt;span class=&#34;math inline&#34;&gt;\(q \times C\)&lt;/span&gt; contrast matrix &lt;span class=&#34;math inline&#34;&gt;\(\bm{C} = \left[-\bm{1}_q \ \bm{I}_q \right]\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(q = C - 1\)&lt;/span&gt;. The null hypothesis is then &lt;span class=&#34;math inline&#34;&gt;\(\bm{C} \bs\mu = \bm{0}_q\)&lt;/span&gt;. The corresponding cluster-robust Wald statistic is
&lt;span class=&#34;math display&#34;&gt;\[
Q = \bs{\hat\mu}&amp;#39; \bm{C}&amp;#39; \left(\bm{C} \bm{V}^R \bm{C}&amp;#39;\right)^{-1} \bm{C} \bs{\hat\mu}.
\]&lt;/span&gt;
Under the null hypothesis, the distribution of &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; will converge to a &lt;span class=&#34;math inline&#34;&gt;\(\chi^2_q\)&lt;/span&gt; as the number of clusters in each category grows large. However, with a limited number of clusters in some of the categories, this approximate reference distribution is not very accurate and tests based on it can have wildly inflated type I error rates.&lt;/p&gt;
&lt;p&gt;In the paper, we considered several different ways of approximating the distribution of &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; that work at smaller sample sizes.
One class of approaches to approximating the sampling distribution of &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; is to use a Hotelling’s &lt;span class=&#34;math inline&#34;&gt;\(T^2\)&lt;/span&gt; distribution with degrees of freedom &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt;. Given the degrees of freedom, Hotelling’s &lt;span class=&#34;math inline&#34;&gt;\(T^2\)&lt;/span&gt; is a multiple of an &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; distribution:
&lt;span class=&#34;math display&#34;&gt;\[
\frac{\eta - q + 1}{\eta q} Q \sim F(q, \eta - q + 1).
\]&lt;/span&gt;
The question is then how to determine &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Several of the approaches that we considered are based on representing the &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt; statistic as
&lt;span class=&#34;math display&#34;&gt;\[
Q = \bm{z}&amp;#39; \bm{D}^{-1} \bm{z},
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\bs\Omega = \bm{C} \bs\Psi \bm{C}&amp;#39;\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\bm{z} = \bs\Omega^{-1/2}\bm{C}\hat\mu_c\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\bm{G} = \bs\Omega^{-1/2} \bm{C}\)&lt;/span&gt;, and
&lt;span class=&#34;math display&#34;&gt;\[
\bm{D} = \bm{G} \bm{V}^R \bm{G}&amp;#39;.
\]&lt;/span&gt;
The various approaches we considered involve different ways of approximating the sampling distribution of &lt;span class=&#34;math inline&#34;&gt;\(\bm{D}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;One of the approximations involves finding degrees of freedom &lt;span class=&#34;math inline&#34;&gt;\(\eta\)&lt;/span&gt; by following a strategy suggested by Zhang (&lt;a href=&#34;https://doi.org/10.1016/j.jspi.2011.07.023&#34;&gt;2012&lt;/a&gt;, &lt;a href=&#34;https://doi.org/10.14419/ijasp.v1i2.908&#34;&gt;2013&lt;/a&gt;). These degrees of freedom are given by
&lt;span class=&#34;math display&#34;&gt;\[
\eta_Z = \frac{q(q + 1)}{\sum_{s=1}^q \sum_{t = 1}^q \Var(d_{st})},
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(d_{st}\)&lt;/span&gt; is the entry in row &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt;, column &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(\bm{D}\)&lt;/span&gt;. To find &lt;span class=&#34;math inline&#34;&gt;\(\eta_Z\)&lt;/span&gt;, we can compute the denominator using general formulas given in the paper. However, with a bit of analysis we can find a much simpler expression for the special case of one-way ANOVA.&lt;/p&gt;
&lt;p&gt;Before going further, it’s useful to observe that &lt;span class=&#34;math inline&#34;&gt;\(\bm{D}\)&lt;/span&gt; is invariant to linear transformations of &lt;span class=&#34;math inline&#34;&gt;\(\bm{C}\)&lt;/span&gt;. In particular, an equivalent way to write the null hypothesis is as &lt;span class=&#34;math inline&#34;&gt;\(H_0: \bs\Psi_{\circ}^{-1/2} \bm{C} = \bm{0}_q\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\bs\Psi_{\circ} = \bigoplus_{c=2}^C \psi_c\)&lt;/span&gt; is the diagonal of the true sampling variances of categories 2 through &lt;span class=&#34;math inline&#34;&gt;\(C\)&lt;/span&gt;, omitting the first category. Thus, let me redefine
&lt;span class=&#34;math display&#34;&gt;\[
\bs\Omega = \bs\Psi_{\circ}^{-1/2} \bm{C} \bs\Psi \bm{C}&amp;#39;\bs\Psi_{\circ}^{-1/2},
\]&lt;/span&gt;
&lt;span class=&#34;math inline&#34;&gt;\(\bm{z} = \bs\Omega^{-1/2}\bs\Psi_{\circ}^{-1/2}\bm{C}\hat\mu_c\)&lt;/span&gt;, and
&lt;span class=&#34;math display&#34;&gt;\[
\bm{G} = \bs\Omega^{-1/2} \bs\Psi_{\circ}^{-1/2} \bm{C}.
\]&lt;/span&gt;
This transformation of the constraint matrix will make it possible to find a closed-form expression for &lt;span class=&#34;math inline&#34;&gt;\(\bs\Omega^{-1/2}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Now, observe that
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\bs\Omega &amp;amp;= \bs\Psi_{\circ}^{-1/2} \bm{C} \bs\Psi \bm{C}&amp;#39;\bs\Psi_{\circ}^{-1/2} \\
&amp;amp;= \bs\Psi_{\circ}^{-1/2} \left(\bs\Psi_{\circ} + \psi_1 \bm{1}_q \bm{1}_q&amp;#39;\right)\bs\Psi_{\circ}^{-1/2} \\
&amp;amp;= \bm{I}_q + \psi_1 \bm{f} \bm{f}&amp;#39;,
\end{aligned}
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\bm{f} = \bs\Psi_{\circ}^{-1/2} \bm{1}_q = \left[ \psi_c^{-1/2}\right]_{c = 2}^C\)&lt;/span&gt;. From the &lt;a href=&#34;\bs\Psi_%7B\circ%7D%5E%7B-1/2%7D&#34;&gt;Woodbury identity&lt;/a&gt;,
&lt;span class=&#34;math display&#34;&gt;\[
\bs\Omega^{-1} = \bm{I} - \frac{1}{W} \bm{f} \bm{f}&amp;#39;,
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(W = \sum_{c=1}^C \frac{1}{\psi_c}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://doi.org/10.1137/22M1471559&#34;&gt;Fasi, Higham, and Liu (2023)&lt;/a&gt; provide formulas for &lt;span class=&#34;math inline&#34;&gt;\(p^{th}\)&lt;/span&gt; roots of low-rank updates to scaled identity matrices. Their results provide a neat closed-form expression for &lt;span class=&#34;math inline&#34;&gt;\(\bs\Omega^{-1/2}\)&lt;/span&gt;. From their Equation (1.9),
&lt;span class=&#34;math display&#34;&gt;\[
\bs\Omega^{-1/2} = \mathbf{I}_q - \kappa \ \bm{f} \bm{f}&amp;#39;,
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\kappa = \frac{\sqrt{\psi_1}}{W \sqrt{\psi_1} + \sqrt{W}}\)&lt;/span&gt;.
Further, we can write the &lt;span class=&#34;math inline&#34;&gt;\(q \times C\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(\bm{G}\)&lt;/span&gt; as
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\bm{G} &amp;amp;= \bs\Omega^{-1/2} \bs\Psi_{\circ}^{-1/2} \bm{C} \\
&amp;amp;= \left( \mathbf{I}_q - \kappa \ \bm{f} \bm{f}&amp;#39; \right) \bs\Psi_{\circ}^{-1/2} \left[-\bm{1}_q, \ \bm{I}_q \right] \\
&amp;amp;= \left[\frac{\kappa(W \psi_1 - 1) - \psi_1}{\psi_1} \bm{f},  \left( \mathbf{I}_q - \kappa \ \bm{f} \bm{f}&amp;#39; \right) \bs\Psi_{\circ}^{-1/2}\right],
\end{aligned}
\]&lt;/span&gt;
with entries given by
&lt;span class=&#34;math display&#34;&gt;\[
g_{sc} = \begin{cases}
\frac{\kappa(W \psi_1 - 1) - \psi_1}{\psi_1 \sqrt{\psi_{s+1}}} &amp;amp; \text{if} \quad c = 1 \\
\frac{I(s+1 = c)}{\sqrt{\psi_{c}}} - \frac{\kappa}{\psi_c \sqrt{\psi_{s+1}}} &amp;amp; \text{if} \quad c &amp;gt; 1.
\end{cases}
\]&lt;/span&gt;
Because &lt;span class=&#34;math inline&#34;&gt;\(\bm{D} = \bm{G} \bm{V}^R \bm{G}&amp;#39;\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\bm{V}^R\)&lt;/span&gt; is diagonal, we can write the entries of &lt;span class=&#34;math inline&#34;&gt;\(\bm{D}\)&lt;/span&gt; as
&lt;span class=&#34;math display&#34;&gt;\[
d_{st} = \sum_{c=1}^C g_{sc} g_{tc} V^R_c.
\]&lt;/span&gt;
And because the variance estimators for each category are independent,
&lt;span class=&#34;math display&#34;&gt;\[
\Var(d_{st}) = \sum_{c=1}^C g_{sc}^2 g_{tc}^2 \Var(V^R_c).
\]&lt;/span&gt;
In &lt;a href=&#34;http://localhost:4321/publication/power-approximations-for-dependent-effects/&#34;&gt;prior work&lt;/a&gt;, we derived expressions for the Satterthwaite degrees of freedom for variances of average effect sizes, and the same formulas can be applied here with the category-specific &lt;span class=&#34;math inline&#34;&gt;\(V^R_c\)&lt;/span&gt;. Let me write &lt;span class=&#34;math inline&#34;&gt;\(\nu_c = 2\left[\E(V^R_c)\right]^2 / \Var(V^R_c)\)&lt;/span&gt; for the degrees of freedom corresponding to category &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;. Then
&lt;span class=&#34;math display&#34;&gt;\[
\Var(d_{st}) = 2 \sum_{c=1}^C g_{sc}^2 g_{tc}^2 \frac{\psi_c^2}{\nu_c}.
\]&lt;/span&gt;
We can use this to obtain an expression for Zhang’s approximate degrees of freedom:
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
q(q + 1)\eta_Z^{-1} &amp;amp;= \sum_{s=1}^q \sum_{t = 1}^q \Var(d_{st}) \\
&amp;amp;= 2\sum_{s=1}^q \sum_{t = 1}^q \sum_{c=1}^C g_{sc}^2 g_{tc}^2 \frac{\psi_c^2}{\nu_c} \\
&amp;amp;= 2\sum_{c=1}^C \frac{\psi_c^2}{\nu_c} \left(\sum_{s=1}^q g_{sc}^2\right)^2.
\end{aligned}
\]&lt;/span&gt;
Now, all we need to do is simplify…
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\sum_{s=1}^q g_{s1}^2 &amp;amp;= \sum_{s=1}^q \frac{\left(\kappa(W \psi_1 - 1) - \psi_1\right)^2}{\psi_1^2 \psi_{s+1}} \\
&amp;amp;= \frac{\left(\kappa(W \psi_1 - 1) - \psi_1\right)^2}{\psi_1^2} \sum_{c=2}^C \frac{1}{\psi_{s+1}} \\
&amp;amp;= \frac{\left(\kappa(W \psi_1 - 1) - \psi_1\right)^2}{\psi_1^2} \frac{(W \psi_1 - 1)}{\psi_1} \\
&amp;amp;= \text{...a bunch of tedious algebra...} \\
&amp;amp;= \frac{1}{\psi_1^2} \left(\psi_1 - \frac{1}{W}\right)
\end{aligned}
\]&lt;/span&gt;
and, for &lt;span class=&#34;math inline&#34;&gt;\(c = 2,...,C\)&lt;/span&gt;,
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\sum_{s=1}^q g_{sc}^2 &amp;amp;= \sum_{s=1}^q \left(\frac{I(s+1 = c)}{\sqrt{\psi_{c}}} - \frac{\kappa}{\psi_c \sqrt{\psi_{s+1}}}\right)^2 \\
&amp;amp;= \frac{1}{\psi_c} - \frac{2 \kappa}{\psi_c^2} + \frac{\kappa^2}{\psi_c^2}\sum_{s=1}^q \frac{1}{\psi_{s+1}} \\
&amp;amp;= \frac{1}{\psi_c} - \frac{2 \kappa}{\psi_c^2} + \frac{\kappa^2}{\psi_c^2}\frac{(W \psi_1 - 1)}{\psi_1} \\
&amp;amp;= \text{...a bunch of tedious algebra...} \\
&amp;amp;= \frac{1}{\psi_c^2} \left(\psi_c - \frac{1}{W}\right)
\end{aligned}
\]&lt;/span&gt;
Thus,
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
q(q + 1)\eta_Z^{-1} &amp;amp;= 2\sum_{c=1}^C \frac{\psi_c^2}{\nu_c} \left(\sum_{s=1}^q g_{sc}^2\right)^2 \\
&amp;amp;= 2\sum_{c=1}^C \frac{1}{\nu_c \psi_c^2}\left(\psi_c - \frac{1}{W}\right)^2 \\
&amp;amp;= 2\sum_{c=1}^C \frac{1}{\nu_c}\left(1 - \frac{1}{\psi_c W}\right)^2
\end{aligned}
\]&lt;/span&gt;
or, rearranging,
&lt;span class=&#34;math display&#34;&gt;\[
\eta_Z = \frac{C(C - 1)}{2 \sum_{c=1}^C \frac{1}{\nu_c}\left(1 - \frac{1}{\psi_c W}\right)^2}.
\]&lt;/span&gt;
It’s a surprisingly clean formula!
Once these degrees of freedom are calculated, the degrees of freedom for the reference &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; distribution would be &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\eta_Z - q + 1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In the paper, we also considered two other degrees of freedom approximations, which involve not only the variances of &lt;span class=&#34;math inline&#34;&gt;\(d_{st}\)&lt;/span&gt; but also the covariances between entries. In principle, one could follow similar algebra to get expressions for these other degrees of freedom as well. However, our simulations indicated that the other degrees of freedom approximations tend to be overly conservative and produce type-I error rates way below the nominal level (essentially, hardly ever rejecting the null) and less accurate than HTZ. So, there’s not much reason to work through them unless you find algebra enjoyable for its own sake.&lt;/p&gt;
&lt;p&gt;A further question about this cluster-robust Wald statistic is how to approximate its sampling distribution under specific alternative hypotheses. In other words, given a vector of means &lt;span class=&#34;math inline&#34;&gt;\(\mu_1,...,\mu_C\)&lt;/span&gt; where the null does not hold, plus some information to determine &lt;span class=&#34;math inline&#34;&gt;\(\psi_c\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\nu_c\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(c = 1,...,C\)&lt;/span&gt;, how could we approximate the distribution of &lt;span class=&#34;math inline&#34;&gt;\(Q\)&lt;/span&gt;? We need something like a non-central Hotelling’s &lt;span class=&#34;math inline&#34;&gt;\(T^2\)&lt;/span&gt; distribution…&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>generalized Poisson versus double Poisson</title>
      <link>http://localhost:4321/generalized-poisson-vs-double-poisson/</link>
      <pubDate>Wed, 06 Dec 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/generalized-poisson-vs-double-poisson/</guid>
      <description>


&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\def\Pr{{\text{Pr}}}
\def\E{{\text{E}}}
\def\Var{{\text{Var}}}
\def\Cov{{\text{Cov}}}
\def\bm{\mathbf}
\def\bs{\boldsymbol}
\]&lt;/span&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(patchwork)   # composing figures
library(rstan)       # Stan interface to R
library(brms)        # fitting generalized linear models
library(bayesplot)   # Examine fitted models
library(loo)         # Model fit measures&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stancode_qr &amp;lt;- &amp;quot;
int gpo_quantile(real p, real mu, real phi) {
  int q = 0;
  real phi_sqrt = sqrt(phi);
  real mu_phi_sqrt = mu * phi_sqrt;
  real m;
  if (phi &amp;gt; 1) 
    m = mu / (1 - inv(phi_sqrt));
  else 
    m = positive_infinity();
  real lpmf = - mu_phi_sqrt;
  real cdf = exp(lpmf);
  real ln_inc;
  while (cdf &amp;lt; p &amp;amp;&amp;amp; q &amp;lt; m) {
    q += 1;
    ln_inc = (q - 1) * log(mu_phi_sqrt + q * (1 - phi_sqrt)) - (q - 2) * log(mu_phi_sqrt + (q - 1) * (1 - phi_sqrt)) + (phi_sqrt - 1) - log(q);
    lpmf += ln_inc;    
    cdf += exp(lpmf);
  }
  return q;
}

int gpo_rng(real mu, real phi) {
  real p = uniform_rng(0,1);
  int x = gpo_quantile(p, mu, phi);
  return x;
}
&amp;quot;
writeLines(paste(&amp;quot;functions {&amp;quot;, stancode_qr, &amp;quot;}&amp;quot;, sep = &amp;quot;\n&amp;quot;), &amp;quot;GPO-rng.stan&amp;quot;)
expose_stan_functions(&amp;quot;GPO-rng.stan&amp;quot;)
gpo_rng_sampler &amp;lt;- function(N, mu, phi) {
  replicate(N, gpo_rng(mu = mu, phi = phi))
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The research project for which we need these distributions involves models that are quite a bit more involved than the simple GLM that I simulated above. We’re especially interested in hierarchical models that allow for cluster-level heterogeneity in both the mean and the dispersion parameter of the distribution. These models go under the heading of generalized additive models for location, scale, and shape (GAMLSS) and have been developed in the likelihood framework with the &lt;a href=&#34;https://www.gamlss.com/&#34;&gt;gamlss package&lt;/a&gt; and in the Bayesian framework with the [bamlss package].&lt;/p&gt;
&lt;p&gt;I’ll test out my generalized Poisson implementation by simulating data from a model that has random variation in the means and in the variances. The data-generating process is as follows:
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
N_j &amp;amp;\sim 1 + Pois(15) \\
\ln \mu_j &amp;amp;\sim N(3.5, \ 1) \\
\ln \phi_j &amp;amp;\sim N(0.15, \ 0.15) \\
Y_{ij} &amp;amp;\sim GPO(\mu_j, \phi_j) \quad \text{for} \quad i = 1,...,N_j
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;all for &lt;span class=&#34;math inline&#34;&gt;\(j = 1,...,J\)&lt;/span&gt;. The specified distribution of &lt;span class=&#34;math inline&#34;&gt;\(\phi_j\)&lt;/span&gt;’s leads to dispersions ranging from about 0.61 to 1.22, with a median of 0.86 and and IQR of 0.78 to 0.95.&lt;/p&gt;
&lt;p&gt;Here’s a simulation from the model with &lt;span class=&#34;math inline&#34;&gt;\(J = 80\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(20231205)
J &amp;lt;- 80

dat &amp;lt;- 
  tibble(
    ID = 1:J, 
    N = 1L + rpois(J, lambda = 15), 
    log_mu = rnorm(J, mean = 3.5, sd = 1), 
    log_phi = rnorm(J, mean = 0.15, sd = 0.15)
  ) %&amp;gt;%
  mutate(
    Y = pmap(list(N = N, mu = exp(log_mu), phi = exp(log_phi)), gpo_rng_sampler)
  ) %&amp;gt;%
  unnest(Y)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let me try fitting some models. I’ll first try fitting some GLMMs, which include random intercepts on the mean term but not on the dispersions. Just for kicks, I’ll use both the generalized Poisson (i.e., the true distribution) and the double Poisson (which is mis-specified).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stancode_gpo &amp;lt;- &amp;quot;
real gpo_lpmf(int X, real mu, real phi) {
  real ans;
  real m = mu / (1 - inv(sqrt(phi)));
  real z = X + sqrt(phi) * (mu - X);
  if (phi &amp;gt; 1 &amp;amp;&amp;amp; X &amp;gt; m)
    ans = negative_infinity();
  else 
    ans = log(mu) + inv(2) * log(phi) + (X - 1) * log(z) - z - lgamma(X + 1);
  return ans;
}
int gpo_quantile(real p, real mu, real phi) {
  int q = 0;
  real phi_sqrt = sqrt(phi);
  real mu_phi_sqrt = mu * phi_sqrt;
  real m;
  if (phi &amp;gt; 1) 
    m = mu / (1 - inv(phi_sqrt));
  else 
    m = positive_infinity();
  real lpmf = - mu_phi_sqrt;
  real cdf = exp(lpmf);
  real ln_inc;
  while (cdf &amp;lt; p &amp;amp;&amp;amp; q &amp;lt; m) {
    q += 1;
    ln_inc = (q - 1) * log(mu_phi_sqrt + q * (1 - phi_sqrt)) - (q - 2) * log(mu_phi_sqrt + (q - 1) * (1 - phi_sqrt)) + (phi_sqrt - 1) - log(q);
    lpmf += ln_inc;    
    cdf += exp(lpmf);
  }
  return q;
}
int gpo_rng(real mu, real phi) {
  real p = uniform_rng(0,1);
  int x = gpo_quantile(p, mu, phi);
  return x;
}
&amp;quot;
generalized_Poisson &amp;lt;- custom_family(
  &amp;quot;gpo&amp;quot;, dpars = c(&amp;quot;mu&amp;quot;,&amp;quot;phi&amp;quot;),
  links = c(&amp;quot;log&amp;quot;,&amp;quot;log&amp;quot;),
  lb = c(0, 0), ub = c(NA, NA),
  type = &amp;quot;int&amp;quot;
)

generalized_Poisson_stanvars &amp;lt;- stanvar(scode = stancode_gpo, block = &amp;quot;functions&amp;quot;)

phi_prior &amp;lt;- prior(exponential(1), class = &amp;quot;phi&amp;quot;)

# Fit GPO model with mean random effects
glmm_gpo &amp;lt;- brm(
  Y ~ 1 + (1 | ID),
  data = dat,
  family = generalized_Poisson,
  stanvars = generalized_Poisson_stanvars,
  prior = phi_prior,
  warmup = 1000,
  iter = 3000,
  chains = 4, 
  cores = 4,
  control = list(adapt_delta = 0.80)
)

expose_functions(glmm_gpo, vectorize = TRUE)

log_lik_gpo &amp;lt;- function(i, prep) {
  mu &amp;lt;- brms::get_dpar(prep, &amp;quot;mu&amp;quot;, i = i)
  phi &amp;lt;- brms::get_dpar(prep, &amp;quot;phi&amp;quot;, i = i)
  y &amp;lt;- prep$data$Y[i]
  gpo_lpmf(y, mu, phi)
}

posterior_predict_gpo &amp;lt;- function(i, prep, maxval = NULL, ...) {
  mu &amp;lt;- brms::get_dpar(prep, &amp;quot;mu&amp;quot;, i = i)
  phi &amp;lt;- brms::get_dpar(prep, &amp;quot;phi&amp;quot;, i = i)
  gpo_rng(mu, phi)
}

summary(glmm_gpo)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gpo 
##   Links: mu = log; phi = identity 
## Formula: Y ~ 1 + (1 | ID) 
##    Data: dat (Number of observations: 1213) 
##   Draws: 4 chains, each with iter = 3000; warmup = 1000; thin = 1;
##          total post-warmup draws = 8000
## 
## Group-Level Effects: 
## ~ID (Number of levels: 80) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     1.03      0.08     0.89     1.22 1.01      184      392
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     3.31      0.11     3.07     3.52 1.06      120      203
## 
## Family Specific Parameters: 
##     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## phi     1.09      0.05     1.01     1.18 1.00     3276     4630
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stancode_dpo &amp;lt;- &amp;quot;
real dpo_lpmf(int X, real mu, real phi) {
  real ans;
  real A = inv(2) * log(phi) - phi * mu;
  if (X == 0)
    ans = A;
  else
    ans = A + X * (phi * (1 + log(mu)) - 1) - lgamma(X + 1) + (1 - phi) * X * log(X);
  return ans;
}
vector dpo_cdf(real mu, real phi, int maxval) {
  real d = exp(phi * (1 + log(mu)) - 1);
  real prob;
  int n = maxval + 1;
  vector[n] cdf;
  cdf[1] = sqrt(phi) * exp(-mu * phi);
  prob = cdf[1] * d;
  cdf[2] = cdf[1] + prob;
  for (i in 2:maxval) {
    prob = prob * d * exp((1 - phi) * (i - 1) * (log(i) - log(i - 1))) / (i^phi);
    cdf[i + 1] = cdf[i] + prob;
    if (prob / cdf[i + 1] &amp;lt; 1e-8) {
      n = i + 1;
      break;
    }
  }
  return cdf / cdf[n];
}
int dpo_quantile(real p, real mu, real phi, int maxval) {
  vector[maxval + 1] cdf_vec = dpo_cdf(mu, phi, maxval);
  int q = 0;
  while (cdf_vec[q + 1] &amp;lt; p) {
      q += 1;
    }
  return q;
}
int dpo_rng(real mu, real phi, int maxval) {
  real p = uniform_rng(0,1);
  int x = dpo_quantile(p, mu, phi, maxval);
  return x;
}
&amp;quot;
double_Poisson &amp;lt;- custom_family(
  &amp;quot;dpo&amp;quot;, dpars = c(&amp;quot;mu&amp;quot;,&amp;quot;phi&amp;quot;),
  links = c(&amp;quot;log&amp;quot;,&amp;quot;log&amp;quot;),
  lb = c(0, 0), ub = c(NA, NA),
  type = &amp;quot;int&amp;quot;
)

double_Poisson_stanvars &amp;lt;- stanvar(scode = stancode_dpo, block = &amp;quot;functions&amp;quot;)

# Fit DPO model with mean random effects
glmm_dpo &amp;lt;- brm(
  Y ~ 1 + (1 | ID),
  data = dat,
  family = double_Poisson,
  stanvars = double_Poisson_stanvars,
  prior = phi_prior,
  warmup = 1000,
  iter = 3000,
  chains = 4, 
  cores = 4,
  control = list(adapt_delta = 0.80)
)

expose_functions(glmm_dpo, vectorize = TRUE)

log_lik_dpo &amp;lt;- function(i, prep) {
  mu &amp;lt;- brms::get_dpar(prep, &amp;quot;mu&amp;quot;, i = i)
  phi &amp;lt;- brms::get_dpar(prep, &amp;quot;phi&amp;quot;, i = i)
  y &amp;lt;- prep$data$Y[i]
  dpo_lpmf(y, mu, phi)
}

posterior_predict_dpo &amp;lt;- function(i, prep, maxval = NULL, ...) {
  mu &amp;lt;- brms::get_dpar(prep, &amp;quot;mu&amp;quot;, i = i)
  phi &amp;lt;- brms::get_dpar(prep, &amp;quot;phi&amp;quot;, i = i)
  if (is.null(maxval)) maxval &amp;lt;- 20 * mu / min(phi, 1)
  dpo_rng(mu, phi, maxval = maxval)
}


summary(glmm_dpo)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: dpo 
##   Links: mu = log; phi = identity 
## Formula: Y ~ 1 + (1 | ID) 
##    Data: dat (Number of observations: 1213) 
##   Draws: 4 chains, each with iter = 3000; warmup = 1000; thin = 1;
##          total post-warmup draws = 8000
## 
## Group-Level Effects: 
## ~ID (Number of levels: 80) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     1.05      0.09     0.91     1.25 1.02      150      209
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     3.31      0.12     3.08     3.54 1.12       30      179
## 
## Family Specific Parameters: 
##     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## phi     1.08      0.04     0.99     1.17 1.00     1435     2708
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I’ll now fit the actual data-generating model, which has random dispersion terms in addition to the random intercepts on the mean. I’ll also try out the same model specification, but with the double Poisson distribution:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Fit GPO model with mean and dispersion random effects
gamlss_gpo &amp;lt;- brm(
  bf(
    Y ~ 1 + (1 | a | ID),
    phi ~ 1 + (1 | a | ID)
  ),
  data = dat,
  family = generalized_Poisson,
  stanvars = generalized_Poisson_stanvars,
  warmup = 1000,
  iter = 3000,
  chains = 4, 
  cores = 4
)

summary(gamlss_gpo)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: gpo 
##   Links: mu = log; phi = log 
## Formula: Y ~ 1 + (1 | a | ID) 
##          phi ~ 1 + (1 | a | ID)
##    Data: dat (Number of observations: 1213) 
##   Draws: 4 chains, each with iter = 3000; warmup = 1000; thin = 1;
##          total post-warmup draws = 8000
## 
## Group-Level Effects: 
## ~ID (Number of levels: 80) 
##                              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS
## sd(Intercept)                    1.04      0.08     0.90     1.22 1.00      577
## sd(phi_Intercept)                0.21      0.07     0.04     0.34 1.00     1987
## cor(Intercept,phi_Intercept)     0.22      0.24    -0.26     0.70 1.00     6380
##                              Tail_ESS
## sd(Intercept)                    1282
## sd(phi_Intercept)                1909
## cor(Intercept,phi_Intercept)     3532
## 
## Population-Level Effects: 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept         3.31      0.11     3.08     3.53 1.02      225      593
## phi_Intercept     0.11      0.05     0.02     0.22 1.00     5237     5580
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Fit DPO model with mean and dispersion random effects
gamlss_dpo &amp;lt;- brm(
  bf(
    Y ~ 1 + (1 | a | ID),
    phi ~ 1 + (1 | a | ID)
  ),
  data = dat,
  family = double_Poisson,
  stanvars = double_Poisson_stanvars,
  warmup = 1000,
  iter = 3000,
  chains = 4, 
  cores = 4
)

summary(gamlss_dpo)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: dpo 
##   Links: mu = log; phi = log 
## Formula: Y ~ 1 + (1 | a | ID) 
##          phi ~ 1 + (1 | a | ID)
##    Data: dat (Number of observations: 1213) 
##   Draws: 4 chains, each with iter = 3000; warmup = 1000; thin = 1;
##          total post-warmup draws = 8000
## 
## Group-Level Effects: 
## ~ID (Number of levels: 80) 
##                              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS
## sd(Intercept)                    1.04      0.09     0.88     1.24 1.02      413
## sd(phi_Intercept)                0.19      0.08     0.03     0.33 1.00     1209
## cor(Intercept,phi_Intercept)     0.39      0.26    -0.10     0.91 1.00     2846
##                              Tail_ESS
## sd(Intercept)                     795
## sd(phi_Intercept)                1115
## cor(Intercept,phi_Intercept)     1588
## 
## Population-Level Effects: 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept         3.31      0.13     3.06     3.56 1.03      112      191
## phi_Intercept     0.09      0.05    -0.00     0.19 1.00     3386     5390
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We get some rather odd results. The simpler GLMMs have better fit (as indicated by LOOIC) than the GAMLSS models:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;loo_comparison &amp;lt;- loo(glmm_gpo, glmm_dpo, gamlss_gpo, gamlss_dpo)
loo_comparison$diffs&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            elpd_diff se_diff
## gamlss_gpo  0.0       0.0   
## gamlss_dpo -1.3       0.9   
## glmm_gpo   -2.4       2.9   
## glmm_dpo   -3.9       2.7&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;All of the models also estimate some degree of over-dispersion (&lt;span class=&#34;math inline&#34;&gt;\(\phi &amp;lt; 1\)&lt;/span&gt;) on average. Curious.&lt;/p&gt;
&lt;div id=&#34;colophon&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Colophon&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.2.2 (2022-10-31 ucrt)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19045)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United States.utf8 
## [2] LC_CTYPE=English_United States.utf8   
## [3] LC_MONETARY=English_United States.utf8
## [4] LC_NUMERIC=C                          
## [5] LC_TIME=English_United States.utf8    
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] gamlss.dist_6.0-3   MASS_7.3-57         loo_2.5.1          
##  [4] bayesplot_1.9.0     brms_2.18.0         Rcpp_1.0.10        
##  [7] rstan_2.26.23       StanHeaders_2.26.27 patchwork_1.1.3    
## [10] lubridate_1.9.2     forcats_1.0.0       stringr_1.5.0      
## [13] dplyr_1.1.2         purrr_1.0.2         readr_2.1.4        
## [16] tidyr_1.3.0         tibble_3.2.1        ggplot2_3.4.3      
## [19] tidyverse_2.0.0    
## 
## loaded via a namespace (and not attached):
##   [1] TH.data_1.1-2        colorspace_2.1-0     RcppEigen_0.3.3.9.2 
##   [4] ellipsis_0.3.2       ggridges_0.5.3       estimability_1.3    
##   [7] markdown_1.7         QuickJSR_1.0.5       base64enc_0.1-3     
##  [10] rstudioapi_0.15.0    farver_2.1.1         DT_0.29             
##  [13] fansi_1.0.4          mvtnorm_1.1-3        bridgesampling_1.1-2
##  [16] codetools_0.2-18     splines_4.2.2        cachem_1.0.6        
##  [19] knitr_1.40           shinythemes_1.2.0    jsonlite_1.8.4      
##  [22] shiny_1.7.4          compiler_4.2.2       emmeans_1.7.3       
##  [25] backports_1.4.1      Matrix_1.6-3         fastmap_1.1.0       
##  [28] cli_3.6.1            later_1.3.0          htmltools_0.5.4     
##  [31] prettyunits_1.1.1    tools_4.2.2          igraph_1.3.5        
##  [34] coda_0.19-4          gtable_0.3.4         glue_1.6.2          
##  [37] reshape2_1.4.4       posterior_1.3.1      jquerylib_0.1.4     
##  [40] vctrs_0.6.3          nlme_3.1-157         blogdown_1.10       
##  [43] crosstalk_1.2.0      tensorA_0.36.2       xfun_0.40           
##  [46] ps_1.6.0             timechange_0.2.0     mime_0.12           
##  [49] miniUI_0.1.1.1       lifecycle_1.0.3      gtools_3.9.3        
##  [52] zoo_1.8-10           scales_1.2.1         colourpicker_1.1.1  
##  [55] hms_1.1.3            promises_1.2.0.1     Brobdingnag_1.2-9   
##  [58] parallel_4.2.2       sandwich_3.0-1       inline_0.3.19       
##  [61] shinystan_2.6.0      yaml_2.3.5           gridExtra_2.3       
##  [64] sass_0.4.5           stringi_1.7.12       dygraphs_1.1.1.6    
##  [67] checkmate_2.1.0      pkgbuild_1.3.1       rlang_1.1.1         
##  [70] pkgconfig_2.0.3      matrixStats_0.62.0   BH_1.78.0-0         
##  [73] distributional_0.3.1 evaluate_0.18        lattice_0.20-45     
##  [76] rstantools_2.2.0     htmlwidgets_1.6.2    processx_3.7.0      
##  [79] tidyselect_1.2.0     plyr_1.8.8           magrittr_2.0.3      
##  [82] bookdown_0.26        R6_2.5.1             generics_0.1.3      
##  [85] multcomp_1.4-23      pillar_1.9.0         withr_2.5.0         
##  [88] xts_0.12.1           survival_3.4-0       abind_1.4-5         
##  [91] crayon_1.5.2         utf8_1.2.3           tzdb_0.3.0          
##  [94] rmarkdown_2.18       grid_4.2.2           callr_3.7.2         
##  [97] threejs_0.3.3        digest_0.6.30        xtable_1.8-4        
## [100] httpuv_1.6.8         RcppParallel_5.1.5   stats4_4.2.2        
## [103] munsell_0.5.0        bslib_0.4.2          shinyjs_2.1.0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Implementing Consul&#39;s generalized Poisson distribution in Stan</title>
      <link>http://localhost:4321/generalized-poisson-in-stan/</link>
      <pubDate>Wed, 06 Dec 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/generalized-poisson-in-stan/</guid>
      <description>


&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\def\Pr{{\text{Pr}}}
\def\E{{\text{E}}}
\def\Var{{\text{Var}}}
\def\Cov{{\text{Cov}}}
\def\bm{\mathbf}
\def\bs{\boldsymbol}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For a project I am working on, we are using &lt;a href=&#34;https://mc-stan.org/&#34;&gt;Stan&lt;/a&gt; to fit generalized random effects location-scale models to a bunch of count data.
In &lt;a href=&#34;http://localhost:4321/double-poisson-in-Stan/&#34;&gt;a previous post&lt;/a&gt;, I walked through our implementation of &lt;a href=&#34;https://doi.org/10.2307/2289002&#34;&gt;Efron’s (1986)&lt;/a&gt; double-Poisson distribution, which we are interested in using because it allows for both over- and under-dispersion relative to the Poisson distribution.
Another distribution with these properties is the generalized Poisson distribution described by &lt;a href=&#34;https://doi.org/10.1080/00401706.1973.10489112&#34;&gt;Consul and Jain (1973)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this post, I’ll walk through my implementation of the GPO in Stan.
The &lt;a href=&#34;https://cran.r-project.org/package=gamlss.dist&#34;&gt;&lt;code&gt;gamlss.dist&lt;/code&gt; package&lt;/a&gt; provides a full set of distributional functions for the generalized Poisson distribution, including a sampler, but the functions are configured to only allow for over-dispersion. Since I’m interested in allowing for under-dispersion as well, I’ll need to write my own functions. As in my previous post, I can validate my Stan functions against the functions from &lt;code&gt;gamlss.dist&lt;/code&gt; (although only for over-dispersion scenarios).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(patchwork)   # composing figures
library(gamlss.dist) # DPO distribution functions
library(rstan)       # Stan interface to R
library(brms)        # fitting generalized linear models
library(bayesplot)   # Examine fitted models
library(loo)         # Model fit measures&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;the-generalized-poisson&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The generalized Poisson&lt;/h2&gt;
&lt;p&gt;Consul and Jain’s generalized Poisson distribution is a discrete distribution for non-negative counts, with support &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{S}_X = \{0, 1, 2, 3, ...\}\)&lt;/span&gt;.
The mean-variance relationship of the generalized Poisson is constant; for &lt;span class=&#34;math inline&#34;&gt;\(X \sim GPO(\mu, \phi)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\text{E}(X) = \mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}(X) = \mu / \phi\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(0 &amp;lt; \phi &amp;lt; 1\)&lt;/span&gt;; the expectation and variance are not exact but are close approximations when there is underdispersion, so &lt;span class=&#34;math inline&#34;&gt;\(\phi &amp;gt; 1\)&lt;/span&gt;. Thus, like the double-Poisson distribution, the generalized Poisson satisfies the assumptions of a quasi-Poisson generalized linear model (at least approximately).&lt;/p&gt;
&lt;p&gt;The density of the generalized Poisson distribution with mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and inverse-disperson &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; is:
&lt;span class=&#34;math display&#34;&gt;\[
f(x | \mu, \phi) = \mu \sqrt{\phi} \left( x + \sqrt{\phi}(\mu - x) \right)^{x-1} \frac{\exp \left[-\left( x + \sqrt{\phi}(\mu - x)\right)\right]}{x!}.
\]&lt;/span&gt;
We then have
&lt;span class=&#34;math display&#34;&gt;\[
\ln f(x | \mu, \phi) = \frac{1}{2} \ln \phi + \ln \mu + (x - 1) \ln \left( x + \sqrt{\phi}(\mu - x) \right) - \left( x + \sqrt{\phi}(\mu - x) \right) - \ln \left(x!\right).
\]&lt;/span&gt;
Using the GPO with under-dispersed data is a little bit more controversial (by statistical standards) than using the DPO.
This is because, for parameter values corresponding to under-dispersion, its probability mass function becomes negative for large counts. In particular, note that for values &lt;span class=&#34;math inline&#34;&gt;\(x &amp;gt; \frac{\mu\sqrt\phi}{\sqrt\phi - 1}\)&lt;/span&gt;, the quantity &lt;span class=&#34;math inline&#34;&gt;\(x + \sqrt{\phi}(\mu - x)\)&lt;/span&gt; becomes negative, and so &lt;span class=&#34;math inline&#34;&gt;\(f(x| \mu, \phi)\)&lt;/span&gt; is no longer a proper probability.
Consul suggested handling this situation by truncating the distribution at &lt;span class=&#34;math inline&#34;&gt;\(m = \left\lfloor \frac{\mu\sqrt\phi}{\sqrt\phi - 1}\right\rfloor\)&lt;/span&gt;. However, doing so makes the distribution only an approximation, such that &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is no longer exactly the mean and &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; is no longer exactly the inverse dispersion.
For modest under-dispersion of no less than 60% of the mean, &lt;span class=&#34;math inline&#34;&gt;\(1 &amp;lt; \phi &amp;lt; 5 / 3\)&lt;/span&gt; and the truncation point is fairly extreme, with &lt;span class=&#34;math inline&#34;&gt;\(m \approx 4.4 \mu\)&lt;/span&gt;, so I’m not too worried about this issue.
We’ll see how it plays out in application, of course.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;log-of-the-probability-mass-function&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Log of the probability mass function&lt;/h1&gt;
&lt;p&gt;Here’s a Stan function implementing the lpmf, with the truncation bit:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stancode_lpmf &amp;lt;- &amp;quot;
real gpo_lpmf(int X, real mu, real phi) {
  real ans;
  real m = mu / (1 - inv(sqrt(phi)));
  real z = X + sqrt(phi) * (mu - X);
  if (phi &amp;gt; 1 &amp;amp;&amp;amp; X &amp;gt; m)
    ans = negative_infinity();
  else 
    ans = log(mu) + inv(2) * log(phi) + (X - 1) * log(z) - z - lgamma(X + 1);
  return ans;
}
&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To check that this is accurate, I’ll compare the Stan function to the corresponding function from &lt;code&gt;gamlss.dist&lt;/code&gt; for a couple of different parameter values and for &lt;span class=&#34;math inline&#34;&gt;\(x = 0,...,100\)&lt;/span&gt;. If my function is accurate, my calculated log-probabilities should be equal to the results from &lt;code&gt;gamlss.dist::dGPO&lt;/code&gt;. Note that the &lt;code&gt;gamlss.dist&lt;/code&gt; function uses a different parameterization for the dispersion, with &lt;span class=&#34;math inline&#34;&gt;\(\sigma = \frac{\phi^{-1/2} - 1}{\mu}\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;writeLines(paste(&amp;quot;functions {&amp;quot;, stancode_lpmf, &amp;quot;}&amp;quot;, sep = &amp;quot;\n&amp;quot;), &amp;quot;GPO-lpmf.stan&amp;quot;)
expose_stan_functions(&amp;quot;GPO-lpmf.stan&amp;quot;)

test_lpmf &amp;lt;- 
  expand_grid(
    mu = c(2, 5, 10, 20),
    phi = seq(0.1, 0.9, 0.1),
    X = 0:100
  ) %&amp;gt;%
  mutate(
    sigma = (phi^(-1/2) - 1) / mu,
    gamlss_lpmf = dGPO(x = X, mu = mu, sigma = sigma, log = TRUE),
    my_lpmf = pmap_dbl(.l = list(X = X, mu = mu, phi = phi), .f = gpo_lpmf),
    diff = my_lpmf - gamlss_lpmf
  )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(test_lpmf, aes(factor(phi), diff, color = factor(phi))) + 
  geom_boxplot() + 
  facet_wrap( ~ mu, labeller = &amp;quot;label_both&amp;quot;, ncol = 2) + 
  theme_minimal() + 
  labs(x = &amp;quot;phi&amp;quot;) + 
  theme(legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:4321/post/Generalized-Poisson-in-Stan_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Checks out. Onward!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;quantile-function&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Quantile function&lt;/h1&gt;
&lt;p&gt;I’ll next implement the generalized Poisson quantile function, taking advantage of a recurrence relationship for sequential values. Note that
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(0 | \mu, \phi) &amp;amp;= \exp \left(-\mu \sqrt{\phi}\right) \\
f(x | \mu, \phi) &amp;amp;= f(x - 1 | \mu, \phi) \times \frac{\left(x + \sqrt{\phi}(\mu - x)\right)^{x - 1}}{\left(x - 1 + \sqrt{\phi}(\mu - (x - 1))\right)^{x - 2}} \times \frac{\exp(\sqrt{\phi} - 1)}{x}
\end{aligned}
\]&lt;/span&gt;
where the second expression holds for &lt;span class=&#34;math inline&#34;&gt;\(x \geq 1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The function below computes the quantile given a value &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; by computing the cumulative distribution function until &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is exceeded:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stancode_quantile &amp;lt;- &amp;quot; 
int gpo_quantile(real p, real mu, real phi) {
  int q = 0;
  real phi_sqrt = sqrt(phi);
  real mu_phi_sqrt = mu * phi_sqrt;
  real m;
  if (phi &amp;gt; 1) 
    m = mu / (1 - inv(phi_sqrt));
  else 
    m = positive_infinity();
  real lpmf = - mu_phi_sqrt;
  real cdf = exp(lpmf);
  real ln_inc;
  while (cdf &amp;lt; p &amp;amp;&amp;amp; q &amp;lt; m) {
    q += 1;
    ln_inc = (q - 1) * log(mu_phi_sqrt + q * (1 - phi_sqrt)) - (q - 2) * log(mu_phi_sqrt + (q - 1) * (1 - phi_sqrt)) + (phi_sqrt - 1) - log(q);
    lpmf += ln_inc;    
    cdf += exp(lpmf);
  }
  return q;
}
&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If my quantile function is accurate, it should match the value computed from &lt;code&gt;gamlss.dist::qDPO()&lt;/code&gt; exactly.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;writeLines(paste(&amp;quot;functions {&amp;quot;, stancode_quantile, &amp;quot;}&amp;quot;, sep = &amp;quot;\n&amp;quot;), &amp;quot;GPO-quantile.stan&amp;quot;)
expose_stan_functions(&amp;quot;GPO-quantile.stan&amp;quot;)

test_quantile &amp;lt;- 
  expand_grid(
    mu = c(2, 5, 10, 20),
    phi = seq(0.1, 0.9, 0.1),
  ) %&amp;gt;%
  mutate(
    sigma = (phi^(-1/2) - 1) / mu,
    p = map(1:n(), ~ runif(100)),
  ) %&amp;gt;%
  unnest(p) %&amp;gt;%
  mutate(
    my_q = pmap_dbl(list(p = p, mu = mu, phi = phi), .f = gpo_quantile),
    gamlss_q = qGPO(p, mu = mu, sigma = sigma),
    diff = my_q - gamlss_q
  )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(test_quantile, aes(factor(phi), diff, color = factor(phi))) + 
  geom_boxplot() + 
  facet_wrap( ~ mu, labeller = &amp;quot;label_both&amp;quot;, ncol = 2) + 
  theme_minimal() + 
  labs(x = &amp;quot;phi&amp;quot;) + 
  theme(legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:4321/post/Generalized-Poisson-in-Stan_files/figure-html/check-quantile-plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I should enter this figure in the competition for the world’s most boring statistical graphic.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sampler&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Sampler&lt;/h1&gt;
&lt;p&gt;The last thing I’ll need is a sampler, which I’ll implement by generating random points from a uniform distribution, then computing the generalized Poisson quantiles of these random points. My implementation just generates a single random variate:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stancode_qr &amp;lt;- &amp;quot;
int gpo_quantile(real p, real mu, real phi) {
  int q = 0;
  real phi_sqrt = sqrt(phi);
  real mu_phi_sqrt = mu * phi_sqrt;
  real m;
  if (phi &amp;gt; 1) 
    m = mu / (1 - inv(phi_sqrt));
  else 
    m = positive_infinity();
  real lpmf = - mu_phi_sqrt;
  real cdf = exp(lpmf);
  real ln_inc;
  while (cdf &amp;lt; p &amp;amp;&amp;amp; q &amp;lt; m) {
    q += 1;
    ln_inc = (q - 1) * log(mu_phi_sqrt + q * (1 - phi_sqrt)) - (q - 2) * log(mu_phi_sqrt + (q - 1) * (1 - phi_sqrt)) + (phi_sqrt - 1) - log(q);
    lpmf += ln_inc;    
    cdf += exp(lpmf);
  }
  return q;
}

int gpo_rng(real mu, real phi) {
  real p = uniform_rng(0,1);
  int x = gpo_quantile(p, mu, phi);
  return x;
}
&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To check this function, I’ll generate some large samples from the generalized Poisson with a few different parameter sets. If the sampler is working properly, the empirical cumulative distribution should line up closely with the cumulative distribution computed using &lt;code&gt;gamlss.dist::pGPO()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;writeLines(paste(&amp;quot;functions {&amp;quot;, stancode_qr, &amp;quot;}&amp;quot;, sep = &amp;quot;\n&amp;quot;), &amp;quot;GPO-rng.stan&amp;quot;)
expose_stan_functions(&amp;quot;GPO-rng.stan&amp;quot;)

gpo_rng_sampler &amp;lt;- function(N, mu, phi) {
  replicate(N, gpo_rng(mu = mu, phi = phi))
}

test_rng &amp;lt;- 
  expand_grid(
    mu = c(2, 5, 10, 20),
    phi = seq(0.1, 0.9, 0.1),
  ) %&amp;gt;%
  mutate(
    sigma = (phi^(-1/2) - 1) / mu,
    x = pmap(.l = list(N = 10000, mu = mu, phi = phi), .f = gpo_rng_sampler),
    tb = map(x, ~ as.data.frame(table(.x)))
  ) %&amp;gt;%
  dplyr::select(-x) %&amp;gt;%
  group_by(mu, phi) %&amp;gt;%
  unnest(tb) %&amp;gt;%
  mutate(
    .x = as.integer(levels(.x))[.x],
    Freq_cum = cumsum(Freq) / 10000,
    gamlss_F = pGPO(q = .x, mu = mu, sigma = sigma)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(test_rng, aes(gamlss_F, Freq_cum, color = factor(phi))) + 
  geom_abline(slope = 1, color = &amp;quot;blue&amp;quot;, linetype = &amp;quot;dashed&amp;quot;) + 
  geom_point() + geom_line() +  
  facet_grid(phi ~ mu, labeller = &amp;quot;label_both&amp;quot;) + 
  theme_minimal() + 
  labs(x = &amp;quot;Theoretical cdf (gamlss.dist)&amp;quot;, y = &amp;quot;Empirical cdf (my function)&amp;quot;) + 
  theme(legend.position = &amp;quot;none&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:4321/post/Generalized-Poisson-in-Stan_files/figure-html/check-rng-plot-1.png&#34; width=&#34;672&#34; /&gt;
Another approach to checking the sampler is to simulate a bunch of observations and check whether the empirical mean and variance match the theoretical moments. I’ll do this as well, using some values of &lt;span class=&#34;math inline&#34;&gt;\(\phi &amp;gt; 1\)&lt;/span&gt; to test whether the sampler still works when there’s under-dispersion.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_moments &amp;lt;- 
  expand_grid(
    mu = c(5, 10, 20, 40, 60),
    phi = seq(1, 2, 0.1),
  ) %&amp;gt;%
  mutate(
    x = pmap(.l = list(N = 1e5, mu = mu, phi = phi), .f = gpo_rng_sampler),
    M = map_dbl(x, mean),
    S = map_dbl(x, sd),
    M_ratio = M / mu,
    S_ratio = S / sqrt(mu / phi)
  ) %&amp;gt;%
  dplyr::select(-x) %&amp;gt;%
  pivot_longer(ends_with(&amp;quot;_ratio&amp;quot;),names_to = &amp;quot;moment&amp;quot;,values_to = &amp;quot;ratio&amp;quot;) %&amp;gt;%
  mutate(
    moment = factor(moment, levels = c(&amp;quot;M_ratio&amp;quot;, &amp;quot;S_ratio&amp;quot;), labels = c(&amp;quot;Sample mean&amp;quot;, &amp;quot;Standard deviation&amp;quot;)),
    mu = factor(mu)
  )

ggplot(test_moments, aes(phi, ratio, color = mu)) + 
  geom_point() + geom_line() + 
  facet_wrap(~ moment) + 
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:4321/post/Generalized-Poisson-in-Stan_files/figure-html/test-sample-moments-1.png&#34; width=&#34;768&#34; /&gt;
Looks like the sample moments closely match the parameter values, with deviations that look pretty much like random error. Nice!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;using-the-custom-distribution-functions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Using the custom distribution functions&lt;/h1&gt;
&lt;p&gt;To finish out my tests of these functions, I’ll try out a small simulation. Following my &lt;a href=&#34;http://localhost:4321/Double-Poisson-in-Stan/&#34;&gt;previous post&lt;/a&gt;, I’ll generate data based on a generalized linear model with a single predictor &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, where the outcome &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; follows a generalized Poisson distribution conditional on &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. The data-generating process is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
X &amp;amp;\sim \Gamma(6,2) \\
Y|X &amp;amp;\sim GPO(\mu(X), \phi) \\
\log \mu(X) &amp;amp;= 2 + 0.3 \times X
\end{aligned}
\]&lt;/span&gt;
with the dispersion parameter set to &lt;span class=&#34;math inline&#34;&gt;\(\phi = 6/10\)&lt;/span&gt; so that the outcome is &lt;em&gt;over&lt;/em&gt;-dispersed. I’m looking at over-dispersion here so that the negative binomial has a chance to keep up, since it doesn’t allow for any degree of under-dispersion.&lt;/p&gt;
&lt;p&gt;The following code generates a large sample from the data-generating process:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(20231206)
N &amp;lt;- 1500
X &amp;lt;- rgamma(N, shape = 6, rate = 2)
mu &amp;lt;- exp(2 + 0.3 * X)
phi &amp;lt;- 6 / 10
Y &amp;lt;- map_dbl(mu, gpo_rng, phi = phi)
dat &amp;lt;- data.frame(X = X, Y = Y)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s what the sample looks like, along with a smoothed regression estimated using a basic cubic spline:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(dat, aes(X, Y)) + 
  geom_point(alpha = 0.1) + 
  geom_smooth(method = &amp;#39;gam&amp;#39;, formula = y ~ s(x, bs = &amp;quot;cs&amp;quot;)) + 
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:4321/post/Generalized-Poisson-in-Stan_files/figure-html/glm-scatterplot-1.png&#34; width=&#34;576&#34; /&gt;
Here is a fit using quasi-likelihood estimation of a log-linear model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;quasi_fit &amp;lt;- glm(Y ~ X, family = quasipoisson(link = &amp;quot;log&amp;quot;), data = dat)
summary(quasi_fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = Y ~ X, family = quasipoisson(link = &amp;quot;log&amp;quot;), data = dat)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -5.3261  -0.9290  -0.0912   0.7371   5.0944  
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept) 1.981959   0.020609   96.17   &amp;lt;2e-16 ***
## X           0.306733   0.005525   55.52   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for quasipoisson family taken to be 1.680427)
## 
##     Null deviance: 7248.1  on 1499  degrees of freedom
## Residual deviance: 2520.5  on 1498  degrees of freedom
## AIC: NA
## 
## Number of Fisher Scoring iterations: 4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This approach recovers the data-generating parameters quite well, with a dispersion estimate of 1.68 compared to the true dispersion parameter of 1.67.&lt;/p&gt;
&lt;div id=&#34;candidate-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Candidate models&lt;/h2&gt;
&lt;p&gt;Now let me fit the same generalized linear model but assuming that the outcome follow a couple of different distributions, including a true Poisson (with unit dispersion), a negative binomial, the double-Poisson distribution from the previous post, and the generalized Poisson distribution. Here goes!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Poisson_fit &amp;lt;- 
  brm(
    Y ~ X, family = poisson(link = &amp;quot;log&amp;quot;),
    data = dat, 
    warmup = 500, 
    iter = 2500, 
    chains = 4, 
    cores = 4,
    seed = 20231204
  )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;negbin_fit &amp;lt;- 
  brm(
    Y ~ X, family = negbinomial(link = &amp;quot;log&amp;quot;),
    data = dat, 
    warmup = 500, 
    iter = 2500, 
    chains = 4, 
    cores = 4,
    seed = 20231204
  )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stancode_dpo &amp;lt;- &amp;quot;
real dpo_lpmf(int X, real mu, real phi) {
  real ans;
  real A = inv(2) * log(phi) - phi * mu;
  if (X == 0)
    ans = A;
  else
    ans = A + X * (phi * (1 + log(mu)) - 1) - lgamma(X + 1) + (1 - phi) * X * log(X);
  return ans;
}
vector dpo_cdf(real mu, real phi, int maxval) {
  real d = exp(phi * (1 + log(mu)) - 1);
  real prob;
  int n = maxval + 1;
  vector[n] cdf;
  cdf[1] = sqrt(phi) * exp(-mu * phi);
  prob = cdf[1] * d;
  cdf[2] = cdf[1] + prob;
  for (i in 2:maxval) {
    prob = prob * d * exp((1 - phi) * (i - 1) * (log(i) - log(i - 1))) / (i^phi);
    cdf[i + 1] = cdf[i] + prob;
    if (prob / cdf[i + 1] &amp;lt; 1e-8) {
      n = i + 1;
      break;
    }
  }
  return cdf / cdf[n];
}
int dpo_quantile(real p, real mu, real phi, int maxval) {
  vector[maxval + 1] cdf_vec = dpo_cdf(mu, phi, maxval);
  int q = 0;
  while (cdf_vec[q + 1] &amp;lt; p) {
      q += 1;
    }
  return q;
}
int dpo_rng(real mu, real phi, int maxval) {
  real p = uniform_rng(0,1);
  int x = dpo_quantile(p, mu, phi, maxval);
  return x;
}
&amp;quot;
double_Poisson &amp;lt;- custom_family(
  &amp;quot;dpo&amp;quot;, dpars = c(&amp;quot;mu&amp;quot;,&amp;quot;phi&amp;quot;),
  links = c(&amp;quot;log&amp;quot;,&amp;quot;log&amp;quot;),
  lb = c(0, 0), ub = c(NA, NA),
  type = &amp;quot;int&amp;quot;
)

double_Poisson_stanvars &amp;lt;- stanvar(scode = stancode_dpo, block = &amp;quot;functions&amp;quot;)

phi_prior &amp;lt;- prior(exponential(1), class = &amp;quot;phi&amp;quot;)

DPO_fit &amp;lt;- 
  brm(
    Y ~ X, family = double_Poisson,
    prior = phi_prior,
    stanvars = double_Poisson_stanvars,
    data = dat, 
    warmup = 500, 
    iter = 2500, 
    chains = 4, 
    cores = 4,
    seed = 20231204
  )

expose_functions(DPO_fit, vectorize = TRUE)

log_lik_dpo &amp;lt;- function(i, prep) {
  mu &amp;lt;- brms::get_dpar(prep, &amp;quot;mu&amp;quot;, i = i)
  phi &amp;lt;- brms::get_dpar(prep, &amp;quot;phi&amp;quot;, i = i)
  y &amp;lt;- prep$data$Y[i]
  dpo_lpmf(y, mu, phi)
}

posterior_predict_dpo &amp;lt;- function(i, prep, maxval = NULL, ...) {
  mu &amp;lt;- brms::get_dpar(prep, &amp;quot;mu&amp;quot;, i = i)
  phi &amp;lt;- brms::get_dpar(prep, &amp;quot;phi&amp;quot;, i = i)
  if (is.null(maxval)) maxval &amp;lt;- 20 * mu / min(phi, 1)
  dpo_rng(mu, phi, maxval = maxval)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stancode_gpo &amp;lt;- &amp;quot;
real gpo_lpmf(int X, real mu, real phi) {
  real ans;
  real m = mu / (1 - inv(sqrt(phi)));
  real z = X + sqrt(phi) * (mu - X);
  if (phi &amp;gt; 1 &amp;amp;&amp;amp; X &amp;gt; m)
    ans = negative_infinity();
  else 
    ans = log(mu) + inv(2) * log(phi) + (X - 1) * log(z) - z - lgamma(X + 1);
  return ans;
}
int gpo_quantile(real p, real mu, real phi) {
  int q = 0;
  real phi_sqrt = sqrt(phi);
  real mu_phi_sqrt = mu * phi_sqrt;
  real m;
  if (phi &amp;gt; 1) 
    m = mu / (1 - inv(phi_sqrt));
  else 
    m = positive_infinity();
  real lpmf = - mu_phi_sqrt;
  real cdf = exp(lpmf);
  real ln_inc;
  while (cdf &amp;lt; p &amp;amp;&amp;amp; q &amp;lt; m) {
    q += 1;
    ln_inc = (q - 1) * log(mu_phi_sqrt + q * (1 - phi_sqrt)) - (q - 2) * log(mu_phi_sqrt + (q - 1) * (1 - phi_sqrt)) + (phi_sqrt - 1) - log(q);
    lpmf += ln_inc;    
    cdf += exp(lpmf);
  }
  return q;
}
int gpo_rng(real mu, real phi) {
  real p = uniform_rng(0,1);
  int x = gpo_quantile(p, mu, phi);
  return x;
}
&amp;quot;
generalized_Poisson &amp;lt;- custom_family(
  &amp;quot;gpo&amp;quot;, dpars = c(&amp;quot;mu&amp;quot;,&amp;quot;phi&amp;quot;),
  links = c(&amp;quot;log&amp;quot;,&amp;quot;log&amp;quot;),
  lb = c(0, 0), ub = c(NA, NA),
  type = &amp;quot;int&amp;quot;
)

generalized_Poisson_stanvars &amp;lt;- stanvar(scode = stancode_gpo, block = &amp;quot;functions&amp;quot;)

phi_prior &amp;lt;- prior(exponential(1), class = &amp;quot;phi&amp;quot;)

GPO_fit &amp;lt;- 
  brm(
    Y ~ X, family = generalized_Poisson,
    prior = phi_prior,
    stanvars = generalized_Poisson_stanvars,
    data = dat, 
    warmup = 500, 
    iter = 2500, 
    chains = 4, 
    cores = 4,
    seed = 20231204
  )

expose_functions(GPO_fit, vectorize = TRUE)

log_lik_gpo &amp;lt;- function(i, prep) {
  mu &amp;lt;- brms::get_dpar(prep, &amp;quot;mu&amp;quot;, i = i)
  phi &amp;lt;- brms::get_dpar(prep, &amp;quot;phi&amp;quot;, i = i)
  y &amp;lt;- prep$data$Y[i]
  gpo_lpmf(y, mu, phi)
}

posterior_predict_gpo &amp;lt;- function(i, prep, maxval = NULL, ...) {
  mu &amp;lt;- brms::get_dpar(prep, &amp;quot;mu&amp;quot;, i = i)
  phi &amp;lt;- brms::get_dpar(prep, &amp;quot;phi&amp;quot;, i = i)
  gpo_rng(mu, phi)
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;model-comparison&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model comparison&lt;/h2&gt;
&lt;p&gt;Here is a comparison of LOOIC for all of the models:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;loo_comparison &amp;lt;- loo(Poisson_fit, negbin_fit, DPO_fit, GPO_fit)
loo_comparison$diffs&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##             elpd_diff se_diff
## DPO_fit        0.0       0.0 
## GPO_fit       -2.9       2.8 
## negbin_fit    -8.9       4.4 
## Poisson_fit -120.7      20.1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The model based on the double-Poisson distribution fits equally well to the true data-generating process here, suggesting that there’s really just not enough information to distriguish between the two models. The negative binomial distribution fit is substantially worse, and the Poisson distribution fit is awful.&lt;/p&gt;
&lt;p&gt;Here’s the posterior for the dispersion (i.e., &lt;span class=&#34;math inline&#34;&gt;\(1 / \phi\)&lt;/span&gt;) based on the GPO and DPO models:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;color_scheme_set(&amp;quot;green&amp;quot;)
GPO_dispersion &amp;lt;- 
  mcmc_areas(GPO_fit, pars = &amp;quot;phi&amp;quot;, transformations = \(x) 1 / x) + 
  theme_minimal() + 
  ggtitle(&amp;quot;Generalized Poisson&amp;quot;)

color_scheme_set(&amp;quot;brightblue&amp;quot;)
DPO_dispersion &amp;lt;- 
  mcmc_areas(DPO_fit, pars = &amp;quot;phi&amp;quot;, transformations = \(x) 1 / x) + 
  theme_minimal() + 
  ggtitle(&amp;quot;Double Poisson&amp;quot;)

DPO_dispersion / GPO_dispersion &amp;amp; 
  xlim(1.5, 2.0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:4321/post/Generalized-Poisson-in-Stan_files/figure-html/dispersion-comparison-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Right on par. To get a better sense of model fit, I’ll run some posterior predictive checks, using the quasi-likelihood dispersion as a summary statistic:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Yrep_Poisson &amp;lt;- posterior_predict(Poisson_fit, ndraws = 500) 
Yrep_negbin &amp;lt;- posterior_predict(negbin_fit, ndraws = 500)
Yrep_dpo &amp;lt;- posterior_predict(DPO_fit, ndraws = 500)
Yrep_gpo &amp;lt;- posterior_predict(GPO_fit, ndraws = 500)

dispersion_coef &amp;lt;- function(y) {
  quasi_fit &amp;lt;- glm(y ~ dat$X, family = quasipoisson(link = &amp;quot;log&amp;quot;))
  sum(residuals(quasi_fit, type = &amp;quot;pearson&amp;quot;)^2) / quasi_fit$df.residual
}

color_scheme_set(&amp;quot;blue&amp;quot;)
Poisson_disp &amp;lt;- ppc_stat(dat$Y, Yrep_Poisson, stat = dispersion_coef, binwidth = 0.02) + 
  labs(title = &amp;quot;Poisson&amp;quot;)

color_scheme_set(&amp;quot;purple&amp;quot;)
negbin_disp &amp;lt;- ppc_stat(dat$Y, Yrep_negbin, stat = dispersion_coef, binwidth = 0.02) + 
  labs(title = &amp;quot;Negative-binomial&amp;quot;)

color_scheme_set(&amp;quot;brightblue&amp;quot;)
dpo_disp &amp;lt;- ppc_stat(dat$Y, Yrep_dpo, stat = dispersion_coef, binwidth = 0.02) + 
  labs(title = &amp;quot;Double Poisson&amp;quot;)

color_scheme_set(&amp;quot;green&amp;quot;)
gpo_disp &amp;lt;- ppc_stat(dat$Y, Yrep_gpo, stat = dispersion_coef, binwidth = 0.02) + 
  labs(title = &amp;quot;Generalized Poisson&amp;quot;)

Poisson_disp / negbin_disp / dpo_disp / gpo_disp &amp;amp;
  theme_minimal() &amp;amp; 
  xlim(c(0.8, 2.1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:4321/post/Generalized-Poisson-in-Stan_files/figure-html/ppc-dispersion-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Both the double Poisson and the generalized Poisson models generate data with levels of dispersion similar to the observed data. The negative binomial distribution is not noticeably worse.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;marginal-posterior-predictive-densities&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Marginal posterior predictive densities&lt;/h2&gt;
&lt;p&gt;Here’s some rootograms for the posterior predictive density of the raw outcomes:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;color_scheme_set(&amp;quot;blue&amp;quot;)
Poisson_root &amp;lt;- ppc_rootogram(dat$Y, Yrep_Poisson, style = &amp;quot;hanging&amp;quot;) + labs(title = &amp;quot;Poisson&amp;quot;)
color_scheme_set(&amp;quot;purple&amp;quot;)
negbin_root &amp;lt;- ppc_rootogram(dat$Y, Yrep_negbin, style = &amp;quot;hanging&amp;quot;) + labs(title = &amp;quot;Negative-binomial&amp;quot;)
color_scheme_set(&amp;quot;brightblue&amp;quot;)
dpo_root &amp;lt;- ppc_rootogram(dat$Y, Yrep_dpo, style = &amp;quot;hanging&amp;quot;) + labs(title = &amp;quot;Double Poisson&amp;quot;)
color_scheme_set(&amp;quot;green&amp;quot;)
gpo_root &amp;lt;- ppc_rootogram(dat$Y, Yrep_gpo, style = &amp;quot;hanging&amp;quot;) + labs(title = &amp;quot;Generalized Poisson&amp;quot;)

Poisson_root / negbin_root / dpo_root / gpo_root &amp;amp;
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:4321/post/Generalized-Poisson-in-Stan_files/figure-html/ppd-1.png&#34; width=&#34;672&#34; /&gt;
You can see from these that the Poisson model maybe expects slightly fewer low counts and slightly fewer high counts than are present in the observed data. However, the figure doesn’t really capture the degree of mis-fit that is apparent with the dispersion summary statistics. I think this is because the distribution of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; changes so much depending on the value of the predictor &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;posterior-predictive-residual-densities&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Posterior predictive residual densities&lt;/h2&gt;
&lt;p&gt;One way to focus in on the distributional assumption is to examine the distribution of residuals rather than raw outcomes. I’ll do that here by looking the deviance residuals from the quasi-Poisson GLM model, treating the calculation of the residuals as merely a transformation of the raw data. Here are some posterior predictive density plots of these deviance residuals:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# quasi-Poisson deviance residuals
dat$resid &amp;lt;- residuals(quasi_fit)

# function to calculate quasi-Poisson deviance residuals
quasi_residuals &amp;lt;- function(y) as.numeric(residuals(glm(y ~ dat$X, family = quasipoisson(link = &amp;quot;log&amp;quot;))))

# transform posterior predictive data into residuals
R &amp;lt;- 50
resid_Poisson &amp;lt;- apply(Yrep_Poisson[1:R,], 1, quasi_residuals) |&amp;gt; t()
resid_negbin &amp;lt;- apply(Yrep_negbin[1:R,], 1, quasi_residuals) |&amp;gt; t()
resid_dpo &amp;lt;- apply(Yrep_dpo[1:R,], 1, quasi_residuals) |&amp;gt; t()
resid_gpo &amp;lt;- apply(Yrep_gpo[1:R,], 1, quasi_residuals) |&amp;gt; t()

# make density plots
color_scheme_set(&amp;quot;blue&amp;quot;)
Poisson_resid_density &amp;lt;- ppc_dens_overlay(dat$resid, resid_Poisson) + labs(title = &amp;quot;Poisson&amp;quot;)

color_scheme_set(&amp;quot;purple&amp;quot;)
negbin_resid_density &amp;lt;- ppc_dens_overlay(dat$resid, resid_negbin) + labs(title = &amp;quot;Negative-binomial&amp;quot;)

color_scheme_set(&amp;quot;brightblue&amp;quot;)
dpo_resid_density &amp;lt;- ppc_dens_overlay(dat$resid, resid_dpo) + labs(title = &amp;quot;Double Poisson&amp;quot;)

color_scheme_set(&amp;quot;green&amp;quot;)
gpo_resid_density &amp;lt;- ppc_dens_overlay(dat$resid, resid_gpo) + labs(title = &amp;quot;Generalized Poisson&amp;quot;)

Poisson_resid_density / negbin_resid_density / dpo_resid_density / gpo_resid_density &amp;amp;
  theme_minimal() &amp;amp; 
  xlim(c(-3.5, 3.5))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:4321/post/Generalized-Poisson-in-Stan_files/figure-html/ppd-residuals-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It’s quite a bit clearer from these plots that the DPO and GPO models are closer to replicating the distribution of the data than the Poisson model.
The negative binomial model is not obviously mis-specified either.&lt;/p&gt;
&lt;p&gt;A notable difference between the negative binomial versus the DPO and GPO distributions is in the form of the mean-variance relationship.
For the negative binomial, the variance increases with the square of the mean, whereas for the DPO and GPO, the variance increases in constant proportion to the mean.
The residual posterior density plots above don’t really capture these mean-variance relationships in an obvious way.
I took one more crack at a posterior predictive check to get at this.
Below is a figure showing the loess smooth of the squared residuals versus &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; based on the posterior predictive distributions versus the real data. I couldn’t find an easy way to do this with the &lt;code&gt;bayesplot&lt;/code&gt; functions I’ve used above, so I had to bang it out in regular &lt;code&gt;ggplot&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X_grid &amp;lt;- seq(min(dat$X), max(dat$X), length.out = 200)

smooth_square_resid &amp;lt;- function(r, x_dat, X_pred = X_grid) {
  loess_fit &amp;lt;- loess(I(r^2) ~ x_dat)
  predict(loess_fit, newdata = data.frame(x_dat = X_pred))
}

dat$sm &amp;lt;- smooth_square_resid(r = dat$resid, x_dat = dat$X, X_pred = dat$X)

smooth_square_resid_ppcs &amp;lt;- function(R, x_dat, X_pred = X_grid) {
  smooth_list &amp;lt;- apply(R, 1, smooth_square_resid, x_dat = dat$X, X_pred = X_pred, simplify = FALSE)
  tibble(
    group = 1:length(smooth_list),
    X = rep(list(X_pred), length(smooth_list)),
    sm = smooth_list
  )
}

smooth_MV &amp;lt;- 
  list(
    Poisson = resid_Poisson, 
    `Negative binomial` = resid_negbin,
    `Double Poisson` = resid_dpo,
    `Generalized Poisson` = resid_gpo
  ) %&amp;gt;%
  map_dfr(smooth_square_resid_ppcs, x_dat = dat$X, .id = &amp;quot;distribution&amp;quot;) %&amp;gt;%
  unnest(X, sm) %&amp;gt;%
  mutate(
    distribution = factor(distribution, levels = c(&amp;quot;Poisson&amp;quot;, &amp;quot;Negative binomial&amp;quot;,&amp;quot;Double Poisson&amp;quot;, &amp;quot;Generalized Poisson&amp;quot;))
  )

ggplot(smooth_MV, aes(X, sm, group = group, color = distribution)) + 
  geom_line(alpha = 0.4) + 
  geom_line(data = dat, aes(X, sm, group = NULL), color = &amp;quot;black&amp;quot;, linewidth = 1.25) + 
  scale_color_manual(values = c(&amp;quot;grey&amp;quot;,&amp;quot;purple&amp;quot;,&amp;quot;lightblue&amp;quot;,&amp;quot;lightgreen&amp;quot;)) + 
  scale_y_continuous(limits = c(0, 9), breaks = seq(0,8,2), expand = expansion(0,0)) + 
  facet_wrap(~ distribution, ncol = 1) + 
  theme_minimal() + 
  theme(legend.position = &amp;quot;none&amp;quot;) + 
  labs(y = &amp;quot;Loess smooth of squared residuals&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:4321/post/Generalized-Poisson-in-Stan_files/figure-html/ppd-residual-smooth-1.png&#34; width=&#34;576&#34; /&gt;
Aha! Here we can clearly see that the negative binomial model generates residuals that have more curvature to the mean-variance relationship, and so don’t really fit with the observed data. The double Poisson and generalized Poisson both generate residuals that match the observed mean-variance relationship decently well.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;colophon&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Colophon&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.2.2 (2022-10-31 ucrt)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19045)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United States.utf8 
## [2] LC_CTYPE=English_United States.utf8   
## [3] LC_MONETARY=English_United States.utf8
## [4] LC_NUMERIC=C                          
## [5] LC_TIME=English_United States.utf8    
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] loo_2.5.1           bayesplot_1.9.0     brms_2.18.0        
##  [4] Rcpp_1.0.10         rstan_2.26.23       StanHeaders_2.26.27
##  [7] gamlss.dist_6.0-3   MASS_7.3-57         patchwork_1.1.3    
## [10] lubridate_1.9.2     forcats_1.0.0       stringr_1.5.0      
## [13] dplyr_1.1.2         purrr_1.0.2         readr_2.1.4        
## [16] tidyr_1.3.0         tibble_3.2.1        ggplot2_3.4.3      
## [19] tidyverse_2.0.0    
## 
## loaded via a namespace (and not attached):
##   [1] TH.data_1.1-2        colorspace_2.1-0     RcppEigen_0.3.3.9.2 
##   [4] ellipsis_0.3.2       ggridges_0.5.3       estimability_1.3    
##   [7] markdown_1.7         QuickJSR_1.0.5       base64enc_0.1-3     
##  [10] rstudioapi_0.15.0    farver_2.1.1         DT_0.29             
##  [13] fansi_1.0.4          mvtnorm_1.1-3        splines_4.2.2       
##  [16] bridgesampling_1.1-2 codetools_0.2-18     cachem_1.0.6        
##  [19] knitr_1.40           shinythemes_1.2.0    jsonlite_1.8.4      
##  [22] shiny_1.7.4          compiler_4.2.2       emmeans_1.7.3       
##  [25] backports_1.4.1      Matrix_1.6-3         fastmap_1.1.0       
##  [28] cli_3.6.1            later_1.3.0          htmltools_0.5.4     
##  [31] prettyunits_1.1.1    tools_4.2.2          igraph_1.3.5        
##  [34] coda_0.19-4          gtable_0.3.4         glue_1.6.2          
##  [37] reshape2_1.4.4       posterior_1.3.1      jquerylib_0.1.4     
##  [40] vctrs_0.6.3          nlme_3.1-157         blogdown_1.10       
##  [43] crosstalk_1.2.0      tensorA_0.36.2       xfun_0.40           
##  [46] ps_1.6.0             timechange_0.2.0     mime_0.12           
##  [49] miniUI_0.1.1.1       lifecycle_1.0.3      gtools_3.9.3        
##  [52] zoo_1.8-10           scales_1.2.1         colourpicker_1.1.1  
##  [55] hms_1.1.3            promises_1.2.0.1     Brobdingnag_1.2-9   
##  [58] parallel_4.2.2       sandwich_3.0-1       inline_0.3.19       
##  [61] shinystan_2.6.0      yaml_2.3.5           gridExtra_2.3       
##  [64] sass_0.4.5           stringi_1.7.12       highr_0.9           
##  [67] dygraphs_1.1.1.6     checkmate_2.1.0      pkgbuild_1.3.1      
##  [70] rlang_1.1.1          pkgconfig_2.0.3      matrixStats_0.62.0  
##  [73] BH_1.78.0-0          distributional_0.3.1 evaluate_0.18       
##  [76] lattice_0.20-45      labeling_0.4.3       rstantools_2.2.0    
##  [79] htmlwidgets_1.6.2    processx_3.7.0       tidyselect_1.2.0    
##  [82] plyr_1.8.8           magrittr_2.0.3       bookdown_0.26       
##  [85] R6_2.5.1             generics_0.1.3       multcomp_1.4-23     
##  [88] mgcv_1.8-41          pillar_1.9.0         withr_2.5.0         
##  [91] xts_0.12.1           survival_3.4-0       abind_1.4-5         
##  [94] crayon_1.5.2         utf8_1.2.3           tzdb_0.3.0          
##  [97] rmarkdown_2.18       grid_4.2.2           callr_3.7.2         
## [100] threejs_0.3.3        digest_0.6.30        xtable_1.8-4        
## [103] httpuv_1.6.8         RcppParallel_5.1.5   stats4_4.2.2        
## [106] munsell_0.5.0        bslib_0.4.2          shinyjs_2.1.0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Implementing Efron&#39;s double Poisson distribution in Stan</title>
      <link>http://localhost:4321/double-poisson-in-stan/</link>
      <pubDate>Fri, 15 Sep 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/double-poisson-in-stan/</guid>
      <description>


&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\def\Pr{{\text{Pr}}}
\def\E{{\text{E}}}
\def\Var{{\text{Var}}}
\def\Cov{{\text{Cov}}}
\def\bm{\mathbf}
\def\bs{\boldsymbol}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For a project I am working on, we are using &lt;a href=&#34;https://mc-stan.org/&#34;&gt;Stan&lt;/a&gt; to fit generalized random effects location-scale models to a bunch of count data. We’re interested in using the double-Poisson distribution, as described by &lt;a href=&#34;https://doi.org/10.2307/2289002&#34;&gt;Efron (1986)&lt;/a&gt;.
This is an interesting distribution because it admits for both over- and under-dispersion relative to the Poisson distribution, whereas most of the conventional alternatives such as the negative binomial distribution or Poisson-normal mixture distribution allow only for over-dispersion.
The double-Poisson distribution is not implemented in Stan, so we’ve had to write our own distribution function. That’s fine and not particularly difficult. What’s a bit more of a challenge is writing Stan functions to generate random samples from the double-Poisson, so that we can generate posterior predictive checks.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; In this post, I’ll walk through the implementation of the custom distribution functions needed to use the double-Poisson in Stan.
The &lt;a href=&#34;https://cran.r-project.org/package=gamlss.dist&#34;&gt;&lt;code&gt;gamlss.dist&lt;/code&gt; package&lt;/a&gt; provides a full set of distributional functions for the double-Poisson distribution, including a sampler. Thus, I can validate my Stan functions against the functions from &lt;code&gt;gamlss.dist&lt;/code&gt;.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(patchwork)   # composing figures
library(gamlss.dist) # DPO distribution functions
library(rstan)       # Stan interface to R
library(brms)        # fitting generalized linear models
library(bayesplot)   # Examine fitted models
library(loo)         # Model fit measures&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;the-double-poisson&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The double-Poisson&lt;/h2&gt;
&lt;p&gt;The double-Poisson distribution is a discrete distribution for non-negative counts, with support &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{S}_X = \{0, 1, 2, 3, ...\}\)&lt;/span&gt;.
The mean-variance relationship of the double-Poisson is approximately constant; for &lt;span class=&#34;math inline&#34;&gt;\(X \sim DPO(\mu, \phi)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\text{E}(X) \approx \mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}(X) \approx \mu / \phi\)&lt;/span&gt;, so that the double-Poisson distribution approximately satisfies the assumptions of a quasi-Poisson generalized linear model (although not quite exactly so).&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://doi.org/10.2307/2289002&#34;&gt;Efron (1986)&lt;/a&gt; gives the following expression for the density of the double-Poisson distribution with mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and inverse-disperson &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[
f(x | \mu, \phi) = \frac{\phi^{1/2} e^{-\phi \mu}}{c(\mu,\phi)} \left(\frac{e^{-x} x^x}{x!}\right) \left(\frac{e \mu}{x}\right)^{\phi x},
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(c(\mu,\phi)\)&lt;/span&gt; is a scaling constant to ensure that the density sums to one, which is closely approximated by
&lt;span class=&#34;math display&#34;&gt;\[
c(\mu, \phi) \approx 1 + \frac{1 - \phi}{12 \mu \phi}\left(1 + \frac{1}{\mu \phi}\right).
\]&lt;/span&gt;
We then have
&lt;span class=&#34;math display&#34;&gt;\[
\ln f(x | \mu, \phi) = \frac{1}{2} \ln \phi - \phi \mu - \ln c(\mu, \phi) + x (\phi + \phi \ln \mu - 1) + (1 - \phi) x \ln(x) - \ln \left(x!\right),
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(0 \times \ln (0)\)&lt;/span&gt; is evaluated as 0.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;log-of-the-probability-mass-function&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Log of the probability mass function&lt;/h1&gt;
&lt;p&gt;For purposes of using this distribution in Stan, it’s sufficient to provide the log of the probability mass function up to a constant—there’s no need to normalize it to sum to one. Thus, we can ignore the &lt;span class=&#34;math inline&#34;&gt;\(c(\mu, \phi)\)&lt;/span&gt; term above. Here’s a Stan function implementing the lpmf:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stancode_lpmf &amp;lt;- &amp;quot;
real dpo_lpmf(int X, real mu, real phi) {
  real ans;
  real A = inv(2) * log(phi) - phi * mu;
  if (X == 0)
    ans = A;
  else
    ans = A + X * (phi * (1 + log(mu)) - 1) - lgamma(X + 1) + (1 - phi) * X * log(X);
  return ans;
}
&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To check that this is accurate, I’ll compare the Stan function to the corresponding function from &lt;code&gt;gamlss.dist&lt;/code&gt; for a couple of different parameter values and for &lt;span class=&#34;math inline&#34;&gt;\(x = 0,...,100\)&lt;/span&gt;. If my function is accurate, the calculated log-probabilities should differ by a constant value for each set of parameters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;writeLines(paste(&amp;quot;functions {&amp;quot;, stancode_lpmf, &amp;quot;}&amp;quot;, sep = &amp;quot;\n&amp;quot;), &amp;quot;DPO-lpmf.stan&amp;quot;)
expose_stan_functions(&amp;quot;DPO-lpmf.stan&amp;quot;)

test_lpmf &amp;lt;- 
  expand_grid(
    mu = c(2, 5, 10, 20),
    phi = c(0.1, 0.2, 0.5, 1, 2, 5, 10),
    X = 0:100
  ) %&amp;gt;%
  mutate(
    gamlss_lpmf = dDPO(x = X, mu = mu, sigma = 1 / phi, log = TRUE),
    my_lpmf = pmap_dbl(.l = list(X = X, mu = mu, phi = phi), .f = dpo_lpmf),
    diff = my_lpmf - gamlss_lpmf
  )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(test_lpmf, aes(factor(phi), diff, color = factor(phi))) + 
  geom_boxplot() + 
  facet_wrap( ~ mu, labeller = &amp;quot;label_both&amp;quot;, ncol = 2) + 
  theme_minimal() + 
  labs(x = &amp;quot;phi&amp;quot;) + 
  theme(legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:4321/post/Double-Poisson-in-Stan_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Checks out. Onward!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cumulative-distribution-function&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Cumulative distribution function&lt;/h1&gt;
&lt;p&gt;I’ll next implement a function to evaluate the cumulative distriution function over a range of values. This is an expensive calculation, but it can be improved a little bit by noting the relationship between sequential values of the probability mass function. Letting &lt;span class=&#34;math inline&#34;&gt;\(d = \exp \left(\phi + \phi \ln \mu - 1 \right)\)&lt;/span&gt;, observe that
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(0 | \mu, \phi) &amp;amp;= \frac{\phi^{1/2} e^{-\phi \mu}}{c(\mu,\phi)} \\
f(1 | \mu, \phi) &amp;amp;= f(0 | \mu, \phi) \times d \\
f(x | \mu, \phi) &amp;amp;= f(x - 1 | \mu, \phi) \times d \times \frac{\exp\left[(1 - \phi)(x - 1)\left(\ln(x) - \ln(x - 1)\right) \right]}{x^\phi}
\end{aligned}
\]&lt;/span&gt;
where the last expression holds for &lt;span class=&#34;math inline&#34;&gt;\(x \geq 2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The function below computes the cumulative distribution function over the range &lt;span class=&#34;math inline&#34;&gt;\(x = 0,...,m\)&lt;/span&gt; as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Compute &lt;span class=&#34;math inline&#34;&gt;\(f(x | \mu, \phi)\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(x = 0,1,2,...\)&lt;/span&gt;, without the scaling constant &lt;span class=&#34;math inline&#34;&gt;\(c(\mu, \phi)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Take &lt;span class=&#34;math inline&#34;&gt;\(F(0 | \mu, \phi) = f(0 | \mu, \phi)\)&lt;/span&gt; and accumulate &lt;span class=&#34;math inline&#34;&gt;\(F(x | \mu, \phi) = F(x - 1 | \mu, \phi) + f(x | \mu, \phi)\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(x = 0,...,m\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Check if &lt;span class=&#34;math inline&#34;&gt;\(f(x | \mu, \phi) / F(x | \mu, \phi)\)&lt;/span&gt; is small (less than &lt;span class=&#34;math inline&#34;&gt;\(10^{-8}\)&lt;/span&gt;), in which case accumulation stops at the value &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;The normalized cumulative distribution function will then be &lt;span class=&#34;math inline&#34;&gt;\(F(x | \mu, \phi) / F(n | \mu, \phi)\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stancode_cdf &amp;lt;- &amp;quot; 
vector dpo_cdf(real mu, real phi, int maxval) {
  real d = exp(phi * (1 + log(mu)) - 1);
  real prob;
  int n = maxval + 1;
  vector[n] cdf;
  cdf[1] = sqrt(phi) * exp(-mu * phi);
  prob = cdf[1] * d;
  cdf[2] = cdf[1] + prob;
  for (i in 2:maxval) {
    prob = prob * d * exp((1 - phi) * (i - 1) * (log(i) - log(i - 1))) / (i^phi);
    cdf[i + 1] = cdf[i] + prob;
    if (prob / cdf[i + 1] &amp;lt; 1e-8) {
      n = i + 1;
      break;
    }
  }
  return cdf / cdf[n];
}
&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To check that this is accurate, I’ll again compare the Stan function to the corresponding function from &lt;code&gt;gamlss.dist&lt;/code&gt;. If my function is accurate, the computed cdf values should be proportional to the cdf calculated from &lt;code&gt;gamlss.dist::pDPO()&lt;/code&gt; and the ratio should be very close to 1.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;writeLines(paste(&amp;quot;functions {&amp;quot;, stancode_cdf, &amp;quot;}&amp;quot;, sep = &amp;quot;\n&amp;quot;), &amp;quot;DPO-cdf.stan&amp;quot;)
expose_stan_functions(&amp;quot;DPO-cdf.stan&amp;quot;)

test_cdf &amp;lt;- 
  expand_grid(
    mu = c(2, 5, 10, 20),
    phi = c(0.1, 0.2, 0.5, 1, 2, 5, 10),
  ) %&amp;gt;%
  mutate(
    maxval = 20 * mu / pmin(1, phi),
    my_cdf = pmap(.l = list(mu = mu, phi = phi, maxval = maxval), .f = dpo_cdf)
  ) %&amp;gt;%
  unnest(my_cdf) %&amp;gt;%
  filter(!is.nan(my_cdf)) %&amp;gt;%
  group_by(mu, phi) %&amp;gt;%
  mutate(
    q = row_number() - 1L,
    gamlss_cdf = pDPO(q = q, mu = mu, sigma = 1 / phi),
    ratio = my_cdf / gamlss_cdf
  )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(test_cdf, aes(factor(phi), ratio, color = factor(phi))) + 
  geom_boxplot() + 
  facet_wrap( ~ mu, labeller = &amp;quot;label_both&amp;quot;, ncol = 2) + 
  theme_minimal() + 
  labs(x = &amp;quot;phi&amp;quot;) + 
  ylim(1 + c(-1e-6, 1e-6)) + 
  theme(legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:4321/post/Double-Poisson-in-Stan_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Still on track here (although you might wonder—would I be sharing this post if I couldn’t get the function working?).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;quantile-function-and-sampler&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Quantile function and sampler&lt;/h1&gt;
&lt;p&gt;The main other thing we need is a function for generating random samples from the double-Poisson. The &lt;code&gt;gamlss.dist&lt;/code&gt; package has the function &lt;code&gt;rDPO()&lt;/code&gt; for this purpose. It’s implemented using the standard inversion method, by calculating quantiles of the double-Poisson corresponding to a random sample from a uniform distribution. Just for funzies, I’ll implement the same approach using Stan.&lt;/p&gt;
&lt;p&gt;The function below calculates quantiles by finding the minimum value of &lt;span class=&#34;math inline&#34;&gt;\(q \geq 0\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(F(q + 1 | \mu, \phi) \geq p\)&lt;/span&gt; for a specified probability &lt;span class=&#34;math inline&#34;&gt;\(p \in [0, 1]\)&lt;/span&gt;. It is vectorized over &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; and solves for &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; by starting with the smallest &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; and continuing through the largest value.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stancode_quantile &amp;lt;- &amp;quot; 
vector dpo_cdf(real mu, real phi, int maxval) {
  real d = exp(phi * (1 + log(mu)) - 1);
  real prob;
  int n = maxval + 1;
  vector[n] cdf;
  cdf[1] = sqrt(phi) * exp(-mu * phi);
  prob = cdf[1] * d;
  cdf[2] = cdf[1] + prob;
  for (i in 2:maxval) {
    prob = prob * d * exp((1 - phi) * (i - 1) * (log(i) - log(i - 1))) / (i^phi);
    cdf[i + 1] = cdf[i] + prob;
    if (prob / cdf[i + 1] &amp;lt; 1e-8) {
      n = i + 1;
      break;
    }
  }
  return cdf / cdf[n];
}
array[] int dpo_quantiles(vector p, real mu, real phi, int maxval) {
  int N = rows(p);
  array[N] int qs;
  array[N] int indices = sort_indices_asc(p);
  vector[maxval + 1] cdf_vec = dpo_cdf(mu, phi, maxval);
  int j = 0;
  for (i in indices) {
    while (cdf_vec[j + 1] &amp;lt; p[i]) {
      j += 1;
    }
    qs[i] = j;
  }
  return qs;
}
&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If my quantile function is accurate, it should match the value computed from &lt;code&gt;gamlss.dist::qDPO()&lt;/code&gt; exactly.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;writeLines(paste(&amp;quot;functions {&amp;quot;, stancode_quantile, &amp;quot;}&amp;quot;, sep = &amp;quot;\n&amp;quot;), &amp;quot;DPO-quantile.stan&amp;quot;)
expose_stan_functions(&amp;quot;DPO-quantile.stan&amp;quot;)

test_quantile &amp;lt;- 
  expand_grid(
    mu = c(2, 5, 10, 20),
    phi = c(0.1, 0.2, 0.5, 1, 2, 5, 10),
  ) %&amp;gt;%
  mutate(
    maxval = 20 * mu / pmin(1, phi),
    p = map(1:n(), ~ runif(100)),
    my_q = pmap(.l = list(p = p, mu = mu, phi = phi, maxval = maxval), .f = dpo_quantiles),
    gamlss_q = pmap(.l = list(p = p, mu = mu, sigma = 1 / phi), .f = qDPO)
  ) %&amp;gt;%
  unnest(c(p, my_q, gamlss_q)) %&amp;gt;%
  mutate(
    diff = my_q - gamlss_q
  )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(test_quantile, aes(factor(phi), diff, color = factor(phi))) + 
  geom_boxplot() + 
  facet_wrap( ~ mu, labeller = &amp;quot;label_both&amp;quot;, ncol = 2) + 
  theme_minimal() + 
  labs(x = &amp;quot;phi&amp;quot;) + 
  theme(legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:4321/post/Double-Poisson-in-Stan_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Phew, still got it!&lt;/p&gt;
&lt;p&gt;The last piece of the puzzle is to write a sampler by generating random points from a uniform distribution, then computing the double-Poisson quantiles of these random points. I will implement this two ways: first with an argument for the number of random variates to generate and then, more simply, to generate a single random variate.&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stancode_qr &amp;lt;- &amp;quot;
real dpo_lpmf(int X, real mu, real phi) {
  real ans;
  real A = inv(2) * log(phi) - phi * mu;
  if (X == 0)
    ans = A;
  else
    ans = A + X * (phi * (1 + log(mu)) - 1) - lgamma(X + 1) + (1 - phi) * X * log(X);
  return ans;
}
vector dpo_cdf(real mu, real phi, int maxval) {
  real d = exp(phi * (1 + log(mu)) - 1);
  real prob;
  int n = maxval + 1;
  vector[n] cdf;
  cdf[1] = sqrt(phi) * exp(-mu * phi);
  prob = cdf[1] * d;
  cdf[2] = cdf[1] + prob;
  for (i in 2:maxval) {
    prob = prob * d * exp((1 - phi) * (i - 1) * (log(i) - log(i - 1))) / (i^phi);
    cdf[i + 1] = cdf[i] + prob;
    if (prob / cdf[i + 1] &amp;lt; 1e-8) {
      n = i + 1;
      break;
    }
  }
  return cdf / cdf[n];
}
array[] int dpo_quantiles(vector p, real mu, real phi, int maxval) {
  int N = rows(p);
  array[N] int qs;
  array[N] int indices = sort_indices_asc(p);
  vector[maxval + 1] cdf_vec = dpo_cdf(mu, phi, maxval);
  int j = 0;
  for (i in indices) {
    while (cdf_vec[j + 1] &amp;lt; p[i]) {
      j += 1;
    }
    qs[i] = j;
  }
  return qs;
}
array[] int dpo_sample_rng(int n, real mu, real phi, int maxval) {
  vector[n] p;
  for (i in 1:n) {
    p[i] = uniform_rng(0,1);
  }
  array[n] int x = dpo_quantiles(p, mu, phi, maxval);
  return x;
}
int dpo_quantile(real p, real mu, real phi, int maxval) {
  vector[maxval + 1] cdf_vec = dpo_cdf(mu, phi, maxval);
  int q = 0;
  while (cdf_vec[q + 1] &amp;lt; p) {
      q += 1;
    }
  return q;
}
int dpo_rng(real mu, real phi, int maxval) {
  real p = uniform_rng(0,1);
  int x = dpo_quantile(p, mu, phi, maxval);
  return x;
}
&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To check this function, I’ll generate some large samples from the double-Poisson with a few different parameter sets. If the sampler is working properly, the empirical cumulative distribution should line up closely with the cumulative distribution computed using &lt;code&gt;gamlss.dist::pDPO()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;writeLines(paste(&amp;quot;functions {&amp;quot;, stancode_qr, &amp;quot;}&amp;quot;, sep = &amp;quot;\n&amp;quot;), &amp;quot;DPO-rng.stan&amp;quot;)
expose_stan_functions(&amp;quot;DPO-rng.stan&amp;quot;)

test_rng &amp;lt;- 
  expand_grid(
    mu = c(2, 5, 10, 20),
    phi = c(0.1, 0.2, 0.5, 1, 2, 5, 10),
  ) %&amp;gt;%
  mutate(
    x = pmap(.l = list(n = 10000, mu = mu, phi = phi, maxval = 5000), .f = dpo_sample_rng),
    tb = map(x, ~ as.data.frame(table(.x)))
  ) %&amp;gt;%
  dplyr::select(-x) %&amp;gt;%
  group_by(mu, phi) %&amp;gt;%
  unnest(tb) %&amp;gt;%
  mutate(
    .x = as.integer(levels(.x))[.x],
    Freq_cum = cumsum(Freq) / 10000,
    gamlss_F = pDPO(q = .x, mu = mu, sigma = 1 / phi)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(test_rng, aes(gamlss_F, Freq_cum, color = factor(phi))) + 
  geom_abline(slope = 1, color = &amp;quot;blue&amp;quot;, linetype = &amp;quot;dashed&amp;quot;) + 
  geom_point() + geom_line() +  
  facet_grid(phi ~ mu, labeller = &amp;quot;label_both&amp;quot;) + 
  theme_minimal() + 
  labs(x = &amp;quot;Theoretical cdf (gamlss.dist)&amp;quot;, y = &amp;quot;Empirical cdf (my function)&amp;quot;) + 
  theme(legend.position = &amp;quot;none&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:4321/post/Double-Poisson-in-Stan_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looks pretty good, no?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;using-the-custom-distribution-functions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Using the custom distribution functions&lt;/h1&gt;
&lt;p&gt;To finish out my tests of these functions, let me demonstrate their use in an actual estimation problem. I’ll generate data based on a simple generalized linear model with a single predictor &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, where the outcome &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; follows a double-Poisson distribution conditional on &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. The data-generating process is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
X &amp;amp;\sim N(0, 1) \\
Y|X &amp;amp;\sim DPO(\mu(X), \phi) \\
\log \mu(X) &amp;amp;= 2 + 0.3 \times X
\end{aligned}
\]&lt;/span&gt;
To make things interesting, I’ll set the dispersion parameter to &lt;span class=&#34;math inline&#34;&gt;\(1 / \phi = 0.6\)&lt;/span&gt; so that the outcome is &lt;em&gt;under&lt;/em&gt;-dispersed relative to the Poisson.&lt;/p&gt;
&lt;p&gt;The following code generates a large sample from the data-generating process. To keep things R-centric, I use &lt;code&gt;gamlss.dist::rDPO&lt;/code&gt; to generate the outcome.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(20230913)
N &amp;lt;- 600
X &amp;lt;- rnorm(N)
mu &amp;lt;- exp(2 + 0.3 * X)
phi_inv &amp;lt;- 0.6
Y &amp;lt;- rDPO(N, mu = mu, sigma = phi_inv)
dat &amp;lt;- data.frame(X = X, Y = Y)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s what the sample looks like, along with a smoothed regression estimated using a basic cubic spline:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(dat, aes(X, Y)) + 
  geom_point(alpha = 0.1) + 
  geom_smooth(method = &amp;#39;gam&amp;#39;, formula = y ~ s(x, bs = &amp;quot;cs&amp;quot;)) + 
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:4321/post/Double-Poisson-in-Stan_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;comparison-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Comparison models&lt;/h2&gt;
&lt;p&gt;Before using the custom distribution, I’ll fit a couple of out-of-the-box models that are useful points of comparison.
Surely the simplest, quickest, and dirtiest way to estimate such a regression is with a generalized linear model, using the “quasi-Poisson” family to allow for non-unit dispersion. In R:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;quasi_fit &amp;lt;- glm(Y ~ X, family = quasipoisson(link = &amp;quot;log&amp;quot;), data = dat)
summary(quasi_fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = Y ~ X, family = quasipoisson(link = &amp;quot;log&amp;quot;), data = dat)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -2.77671  -0.58205  -0.03293   0.49158   2.52711  
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  1.98784    0.01219  163.03   &amp;lt;2e-16 ***
## X            0.29276    0.01178   24.85   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for quasipoisson family taken to be 0.6324771)
## 
##     Null deviance: 777.74  on 599  degrees of freedom
## Residual deviance: 384.90  on 598  degrees of freedom
## AIC: NA
## 
## Number of Fisher Scoring iterations: 4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This approach recovers the data-generating parameters quite well, with a dispersion estimate of 0.632 compared to the true dispersion parameter of 0.6.&lt;/p&gt;
&lt;p&gt;Now let me fit the same generalized linear model but assuming that the outcome follows a true Poisson distribution (with unit dispersion). I’ll fit the model in a Bayesian framework with the &lt;code&gt;brms&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Poisson_fit &amp;lt;- 
  brm(
    Y ~ X, family = poisson(link = &amp;quot;log&amp;quot;),
    data = dat, 
    warmup = 500, 
    iter = 1500, 
    chains = 4, 
    cores = 4,
    seed = 20230913
  )

summary(Poisson_fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: poisson 
##   Links: mu = log 
## Formula: Y ~ X 
##    Data: dat (Number of observations: 600) 
##   Draws: 4 chains, each with iter = 1500; warmup = 500; thin = 1;
##          total post-warmup draws = 4000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     1.99      0.02     1.96     2.02 1.00     2733     2582
## X             0.29      0.01     0.26     0.32 1.00     2705     2485
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This specification recovers the intercept and slope parameters well too, but doesn’t provide any estimate of dispersion.&lt;/p&gt;
&lt;p&gt;As an alternative, I’ll also fit the model using the negative binomial distribution, which is a generalization of the Poisson that allows for over-dispersion (but not under-dispersion):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;negbin_fit &amp;lt;- 
  brm(
    Y ~ X, family = negbinomial(link = &amp;quot;log&amp;quot;),
    data = dat, 
    warmup = 500, 
    iter = 1500, 
    chains = 4, 
    cores = 4,
    seed = 20230913
  )

summary(negbin_fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: negbinomial 
##   Links: mu = log; shape = identity 
## Formula: Y ~ X 
##    Data: dat (Number of observations: 600) 
##   Draws: 4 chains, each with iter = 1500; warmup = 500; thin = 1;
##          total post-warmup draws = 4000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     1.99      0.02     1.96     2.02 1.00     3571     3057
## X             0.29      0.01     0.26     0.32 1.00     3696     3141
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## shape   313.78    130.37   136.26   622.52 1.00     3132     3122
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;brms&lt;/code&gt; package implements the negative binomial using the rate parameterization, so the &lt;code&gt;shape&lt;/code&gt; parameter corresponds to the inverse dispersion. Thus, a large shape parameter (as in the above fit) implies dispersion that is very close to one (i.e., close to the Poisson).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;double-poisson-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Double-Poisson model&lt;/h2&gt;
&lt;p&gt;Now I’ll fit the same model as previously but using my custom-built double-Poisson distribution. Following &lt;a href=&#34;https://cran.r-project.org/web/packages/brms/vignettes/brms_customfamilies.html&#34;&gt;Paul Buerkner’s vignette&lt;/a&gt; on using custom distributions in &lt;code&gt;brms&lt;/code&gt;, I’ll first specify the custom family object for the double-Poisson:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;double_Poisson &amp;lt;- custom_family(
  &amp;quot;dpo&amp;quot;, dpars = c(&amp;quot;mu&amp;quot;,&amp;quot;phi&amp;quot;),
  links = c(&amp;quot;log&amp;quot;,&amp;quot;log&amp;quot;),
  lb = c(0, 0), ub = c(NA, NA),
  type = &amp;quot;int&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I set the defaults to use a log-link for the mean (just as with the Poisson and negative binomial families) and a log-link for the inverse-dispersion.
Next, I’ll create an object to add the custom stan code from above into the code created by &lt;code&gt;brm&lt;/code&gt; for fitting the model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;double_Poisson_stanvars &amp;lt;- stanvar(scode = stancode_qr, block = &amp;quot;functions&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I’ll also need to specify a prior to use for the &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; parameter of the double-Poisson distribution:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;phi_prior &amp;lt;- prior(exponential(1), class = &amp;quot;phi&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now I’m ready to fit the model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;DPO_fit &amp;lt;- 
  brm(
    Y ~ X, family = double_Poisson,
    prior = phi_prior,
    stanvars = double_Poisson_stanvars,
    data = dat, 
    warmup = 500, 
    iter = 1500, 
    chains = 4, 
    cores = 4,
    seed = 20230913
  )

summary(DPO_fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: dpo 
##   Links: mu = log; phi = identity 
## Formula: Y ~ X 
##    Data: dat (Number of observations: 600) 
##   Draws: 4 chains, each with iter = 1500; warmup = 500; thin = 1;
##          total post-warmup draws = 4000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     1.99      0.01     1.96     2.01 1.00     3468     3162
## X             0.29      0.01     0.27     0.32 1.00     3745     2786
## 
## Family Specific Parameters: 
##     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## phi     1.55      0.09     1.38     1.73 1.00     3814     2780
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The regression coefficient estimates are basically identical to those from the Poisson and negative-binomial models, estimated with slightly better precision than with the Poisson or negative binomial families. However, we get a posterior for &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; that corresponds to &lt;em&gt;under&lt;/em&gt;-dispersion. Here’s the posterior for the dispersion (i.e., &lt;span class=&#34;math inline&#34;&gt;\(1 / \phi\)&lt;/span&gt;):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mcmc_areas(DPO_fit, pars = &amp;quot;phi&amp;quot;, transformations = \(x) 1 / x) + 
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:4321/post/Double-Poisson-in-Stan_files/figure-html/DPO-dispersion-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-comparison&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model comparison&lt;/h2&gt;
&lt;p&gt;I’d like to get a sense of how much better the double-Poisson model does with capturing the real data-generating process compared to the simple Poisson model or the negative binomial model. There’s a wide range of diagnostics that can inform such comparisons. I’ll consider the leave-one-out information criteria (LOOIC) and also look at some posterior predictive checks.&lt;/p&gt;
&lt;p&gt;To calculate LOOIC for the double-Poisson model, I first need to provide a &lt;code&gt;log_lik&lt;/code&gt; function that &lt;code&gt;brms&lt;/code&gt; can use&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;. Here’s code, using the Stan function from above:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;expose_functions(DPO_fit, vectorize = TRUE)
log_lik_dpo &amp;lt;- function(i, prep) {
  mu &amp;lt;- brms::get_dpar(prep, &amp;quot;mu&amp;quot;, i = i)
  phi &amp;lt;- brms::get_dpar(prep, &amp;quot;phi&amp;quot;, i = i)
  y &amp;lt;- prep$data$Y[i]
  dpo_lpmf(y, mu, phi)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I can then compute LOOIC for all three models:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;loo(DPO_fit, Poisson_fit, negbin_fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Output of model &amp;#39;DPO_fit&amp;#39;:
## 
## Computed from 4000 by 600 log-likelihood matrix
## 
##          Estimate   SE
## elpd_loo  -1305.7 16.9
## p_loo         2.9  0.2
## looic      2611.5 33.8
## ------
## Monte Carlo SE of elpd_loo is 0.0.
## 
## All Pareto k estimates are good (k &amp;lt; 0.5).
## See help(&amp;#39;pareto-k-diagnostic&amp;#39;) for details.
## 
## Output of model &amp;#39;Poisson_fit&amp;#39;:
## 
## Computed from 4000 by 600 log-likelihood matrix
## 
##          Estimate   SE
## elpd_loo  -1330.0 11.3
## p_loo         1.3  0.1
## looic      2660.1 22.6
## ------
## Monte Carlo SE of elpd_loo is 0.0.
## 
## All Pareto k estimates are good (k &amp;lt; 0.5).
## See help(&amp;#39;pareto-k-diagnostic&amp;#39;) for details.
## 
## Output of model &amp;#39;negbin_fit&amp;#39;:
## 
## Computed from 4000 by 600 log-likelihood matrix
## 
##          Estimate   SE
## elpd_loo  -1333.2 11.1
## p_loo         1.3  0.1
## looic      2666.3 22.2
## ------
## Monte Carlo SE of elpd_loo is 0.0.
## 
## All Pareto k estimates are good (k &amp;lt; 0.5).
## See help(&amp;#39;pareto-k-diagnostic&amp;#39;) for details.
## 
## Model comparisons:
##             elpd_diff se_diff
## DPO_fit       0.0       0.0  
## Poisson_fit -24.3       6.1  
## negbin_fit  -27.4       6.3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By these measures, the double-Poisson model has substantially better fit than either of the other models.&lt;/p&gt;
&lt;p&gt;To do posterior predictive checks, I need to provide a &lt;code&gt;posterior_predict&lt;/code&gt; function that &lt;code&gt;brms&lt;/code&gt; can use. I’ll again do an implementation that uses my custom &lt;code&gt;dpo_rng()&lt;/code&gt; from Stan.&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_predict_dpo &amp;lt;- function(i, prep, maxval = NULL, ...) {
  mu &amp;lt;- brms::get_dpar(prep, &amp;quot;mu&amp;quot;, i = i)
  phi &amp;lt;- brms::get_dpar(prep, &amp;quot;phi&amp;quot;, i = i)
  if (is.null(maxval)) maxval &amp;lt;- 20 * mu / min(phi, 1)
  dpo_rng(mu, phi, maxval = maxval)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Functions in hand, I can now compute posterior predictions for the double-Poisson model and make pretty pictures of them, along with corresponding plots for the Poisson and negative-binomial models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Yrep_Poisson &amp;lt;- posterior_predict(Poisson_fit, draws = 400) 
color_scheme_set(&amp;quot;blue&amp;quot;)
Poisson_root &amp;lt;- ppc_rootogram(dat$Y, Yrep_Poisson, style = &amp;quot;hanging&amp;quot;) + labs(title = &amp;quot;Poisson&amp;quot;)

Yrep_negbin &amp;lt;- posterior_predict(negbin_fit, draws = 400)
color_scheme_set(&amp;quot;green&amp;quot;)
negbin_root &amp;lt;- ppc_rootogram(dat$Y, Yrep_negbin, style = &amp;quot;hanging&amp;quot;) + labs(title = &amp;quot;Negative-binomial&amp;quot;)

Yrep_dpo &amp;lt;- posterior_predict(DPO_fit, draws = 400)
color_scheme_set(&amp;quot;purple&amp;quot;)
dpo_root &amp;lt;- ppc_rootogram(dat$Y, Yrep_dpo, style = &amp;quot;hanging&amp;quot;) + labs(title = &amp;quot;Double-Poisson&amp;quot;)

dpo_root / Poisson_root / negbin_root &amp;amp;
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:4321/post/Double-Poisson-in-Stan_files/figure-html/ppd-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The differences in predicted frequencies are not that obvious from these plots. The main notable difference is that the Poisson and negative-binomial distributions predict more small counts (in the range of 0 to 3) than are observed, whereas the double-Poisson does better at matching the observed frequency in this range.&lt;/p&gt;
&lt;p&gt;I think the lack of glaring differences in the above plots happens because I’m just looking at the marginal distribution of the outcome, and the (explained) variation due to the predictor dampens the degree of under-dispersion. To see this, I’ll create some plots that are grouped by quintiles of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat$g &amp;lt;- cut(dat$X, breaks = quantile(dat$X, seq(0,1,0.2)), include.lowest = TRUE)

color_scheme_set(&amp;quot;blue&amp;quot;)
Poisson_bars &amp;lt;- ppc_bars_grouped(
  dat$Y, Yrep_Poisson, dat$g, 
  prob = 0.5, 
  facet_args = list(ncol = 5)
) + 
  labs(title = &amp;quot;Poisson&amp;quot;)

color_scheme_set(&amp;quot;green&amp;quot;)
negbin_bars &amp;lt;- ppc_bars_grouped(
  dat$Y, Yrep_negbin, dat$g, 
  prob = 0.5, 
  facet_args = list(ncol = 5)
) + 
  labs(title = &amp;quot;Negative-binomial&amp;quot;)

color_scheme_set(&amp;quot;purple&amp;quot;)
dpo_bars &amp;lt;- ppc_bars_grouped(
  dat$Y, Yrep_dpo, dat$g, 
  prob = 0.5, 
  facet_args = list(ncol = 5)
) + 
  labs(title = &amp;quot;Double-Poisson&amp;quot;)

dpo_bars / Poisson_bars / negbin_bars &amp;amp;
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:4321/post/Double-Poisson-in-Stan_files/figure-html/ppd-grouped-1.png&#34; width=&#34;1152&#34; /&gt;
Still kind of subtle, I suppose, but you can see more clearly that the double-Poisson does a better job than the other distributions at matching the modes (peaks) of the empirical distribution in each of these subgroups.&lt;/p&gt;
&lt;p&gt;One last approach is to look directly at the degree of dispersion in the posterior predictive distributions relative to the actual data. I’ll calculate this dispersion by re-fitting the quick-and-dirty quasi-poisson model in each sample:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dispersion_coef &amp;lt;- function(y) {
  quasi_fit &amp;lt;- glm(y ~ dat$X, family = quasipoisson(link = &amp;quot;log&amp;quot;))
  sum(residuals(quasi_fit, type = &amp;quot;pearson&amp;quot;)^2) / quasi_fit$df.residual
}

color_scheme_set(&amp;quot;blue&amp;quot;)
Poisson_disp &amp;lt;- ppc_stat(dat$Y, Yrep_Poisson, stat = dispersion_coef, binwidth = 0.02) + 
  labs(title = &amp;quot;Poisson&amp;quot;)

color_scheme_set(&amp;quot;green&amp;quot;)
negbin_disp &amp;lt;- ppc_stat(dat$Y, Yrep_negbin, stat = dispersion_coef, binwidth = 0.02) + 
  labs(title = &amp;quot;Negative-binomial&amp;quot;)

color_scheme_set(&amp;quot;purple&amp;quot;)
dpo_disp &amp;lt;- ppc_stat(dat$Y, Yrep_dpo, stat = dispersion_coef, binwidth = 0.02) + 
  labs(title = &amp;quot;Double-Poisson&amp;quot;)

dpo_disp / Poisson_disp / negbin_disp &amp;amp;
  theme_minimal() &amp;amp; 
  xlim(c(0.45, 1.3))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:4321/post/Double-Poisson-in-Stan_files/figure-html/ppc-dispersion-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From this, we can clearly see that the Poisson and negative binomial model generate data with approximately unit dispersion, which doesn’t match at all with the degree of dispersion in the observed data.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;kudos&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Kudos&lt;/h1&gt;
&lt;p&gt;So there you have it. It’s really quite feasible to build models with custom distributions. Efron (1986) also describes a double-binomial distribution (as an approximation to the “quasi-binomial” family of generalized linear models), which you could play with implementing for yourself, dear reader, if you are in the mood.
Major kudos to &lt;a href=&#34;https://paul-buerkner.github.io/&#34;&gt;Paul Buerkner&lt;/a&gt; for &lt;a href=&#34;https://paul-buerkner.github.io/brms/&#34;&gt;&lt;code&gt;brms&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://jgabry.github.io/&#34;&gt;Jonah Gabry&lt;/a&gt; and collaborators for &lt;a href=&#34;https://mc-stan.org/bayesplot/&#34;&gt;&lt;code&gt;bayesplot&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&#34;https://mc-stan.org/about/team/&#34;&gt;the incredible team of folks&lt;/a&gt; developing &lt;a href=&#34;https://mc-stan.org/&#34;&gt;&lt;code&gt;Stan&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;colophon&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Colophon&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.2.2 (2022-10-31 ucrt)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19045)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United States.utf8 
## [2] LC_CTYPE=English_United States.utf8   
## [3] LC_MONETARY=English_United States.utf8
## [4] LC_NUMERIC=C                          
## [5] LC_TIME=English_United States.utf8    
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] loo_2.5.1           bayesplot_1.9.0     brms_2.18.0        
##  [4] Rcpp_1.0.10         rstan_2.26.23       StanHeaders_2.26.27
##  [7] gamlss.dist_6.0-3   MASS_7.3-57         patchwork_1.1.1    
## [10] forcats_0.5.1       stringr_1.5.0       dplyr_1.1.2        
## [13] purrr_1.0.2         readr_2.1.2         tidyr_1.3.0        
## [16] tibble_3.2.1        ggplot2_3.4.0       tidyverse_1.3.1    
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.4.2         backports_1.4.1      RcppEigen_0.3.3.9.2 
##   [4] plyr_1.8.8           igraph_1.3.5         splines_4.2.2       
##   [7] crosstalk_1.2.0      TH.data_1.1-2        rstantools_2.2.0    
##  [10] inline_0.3.19        digest_0.6.30        htmltools_0.5.4     
##  [13] fansi_1.0.4          magrittr_2.0.3       BH_1.78.0-0         
##  [16] checkmate_2.1.0      tzdb_0.3.0           modelr_0.1.8        
##  [19] RcppParallel_5.1.5   matrixStats_0.62.0   xts_0.12.1          
##  [22] sandwich_3.0-1       timechange_0.2.0     prettyunits_1.1.1   
##  [25] colorspace_2.1-0     rvest_1.0.2          haven_2.5.0         
##  [28] xfun_0.34            callr_3.7.2          crayon_1.5.2        
##  [31] jsonlite_1.8.4       survival_3.4-0       zoo_1.8-10          
##  [34] glue_1.6.2           gtable_0.3.1         emmeans_1.7.3       
##  [37] distributional_0.3.1 pkgbuild_1.3.1       abind_1.4-5         
##  [40] scales_1.2.1         mvtnorm_1.1-3        DBI_1.1.2           
##  [43] miniUI_0.1.1.1       xtable_1.8-4         stats4_4.2.2        
##  [46] DT_0.23              htmlwidgets_1.6.2    httr_1.4.3          
##  [49] threejs_0.3.3        posterior_1.3.1      ellipsis_0.3.2      
##  [52] pkgconfig_2.0.3      farver_2.1.1         sass_0.4.5          
##  [55] dbplyr_2.1.1         utf8_1.2.3           tidyselect_1.2.0    
##  [58] labeling_0.4.2       rlang_1.1.1          reshape2_1.4.4      
##  [61] later_1.3.0          munsell_0.5.0        cellranger_1.1.0    
##  [64] tools_4.2.2          cachem_1.0.6         cli_3.6.1           
##  [67] generics_0.1.3       broom_0.8.0          ggridges_0.5.3      
##  [70] evaluate_0.18        fastmap_1.1.0        yaml_2.3.5          
##  [73] processx_3.7.0       knitr_1.40           fs_1.6.1            
##  [76] nlme_3.1-157         mime_0.12            xml2_1.3.3          
##  [79] compiler_4.2.2       shinythemes_1.2.0    rstudioapi_0.13     
##  [82] reprex_2.0.1         bslib_0.4.2          stringi_1.7.12      
##  [85] highr_0.9            ps_1.6.0             blogdown_1.10       
##  [88] Brobdingnag_1.2-9    lattice_0.20-45      Matrix_1.5-1        
##  [91] markdown_1.7         shinyjs_2.1.0        tensorA_0.36.2      
##  [94] vctrs_0.6.3          pillar_1.9.0         lifecycle_1.0.3     
##  [97] jquerylib_0.1.4      bridgesampling_1.1-2 estimability_1.3    
## [100] httpuv_1.6.8         QuickJSR_1.0.5       R6_2.5.1            
## [103] bookdown_0.26        promises_1.2.0.1     gridExtra_2.3       
## [106] codetools_0.2-18     colourpicker_1.1.1   gtools_3.9.3        
## [109] assertthat_0.2.1     withr_2.5.0          shinystan_2.6.0     
## [112] multcomp_1.4-23      mgcv_1.8-41          parallel_4.2.2      
## [115] hms_1.1.3            grid_4.2.2           coda_0.19-4         
## [118] rmarkdown_2.18       shiny_1.7.4          lubridate_1.9.2     
## [121] base64enc_0.1-3      dygraphs_1.1.1.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;To be clear up front, what I present is more complicated than really necessary because of these existing R functions to simulate values from the double-Poisson—we can just use the functions from &lt;code&gt;gamlss.dist&lt;/code&gt; for purposes of posterior predictive checks (about which more below).
I’m trying to work in Stan to the maximum extent possible solely as an excuse to learn more about the language, which I haven’t used much up until today.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;I should also note that the &lt;a href=&#34;http://www.bamlss.org/index.html&#34;&gt;&lt;code&gt;bamlss&lt;/code&gt; package&lt;/a&gt; provides similar functionality and can be combined with &lt;code&gt;gamlss.dist&lt;/code&gt; to accomplish basically the same thing as I’m going to do here.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;The simpler version is what’s needed for generating posterior predictive checks, the fancy version is just to show off how clever I am.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;Rather than exposing and calling the Stan function, one could just re-implement the log likelihood in R. (Probably the easier way in practice, but again I’m trying to learn me some Stan here…)&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;Of course, I could have saved a bunch of trouble by just using &lt;code&gt;gamlss.dist::rDPO()&lt;/code&gt; instead.&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Variance component estimates in meta-analysis with mis-specified sampling correlation</title>
      <link>http://localhost:4321/variance-components-with-misspecified-correlation/</link>
      <pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/variance-components-with-misspecified-correlation/</guid>
      <description>


&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\def\Pr{{\text{Pr}}}
\def\E{{\text{E}}}
\def\Var{{\text{Var}}}
\def\Cov{{\text{Cov}}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In a recent paper with Beth Tipton, we proposed &lt;a href=&#34;http://localhost:4321/publication/rve-meta-analysis-expanding-the-range/&#34;&gt;new working models&lt;/a&gt; for meta-analyses involving dependent effect sizes. The central idea of our approach is to use a working model that captures the main features of the effect size data, such as by allowing for both between- and within-study heterogeneity in the true effect sizes (rather than only between-study heterogeneity). Doing so will lead to more precise estimates of overall average effects or, in models that include predictor variables, more precise estimates of meta-regression coefficients. Further, one can combine this working model with robust variance estimation methods to provide protection against the possibility that some of the model’s assumptions could be mis-specified.&lt;/p&gt;
&lt;p&gt;In order to estimate these new working models, the analyst must first make some assumption about the degree of correlation between effect size estimates that come from the same sample. In typical applications, it can be difficult to obtain good empirical information about the correlation between effect size estimates, and so it is common to impose some simplifying assumptions and use rough guesses about the degree of correlation. There’s a sense that this might not matter much—particularly because robust variance estimation should protect the inferences if the assumptions about the correlation are wrong. However, I’m still curious about the extent to which these assumptions about the correlation structure matter for anything.&lt;/p&gt;
&lt;p&gt;There’s a few reasons to wonder about how much the correlation matters. One is that the analyst might actually care about the variance component estimates from the working model, if they’re substantively interested in the extent of heterogeneity or if they’re trying to make predictions about the distribution of effect sizes that could be expected in a new study. Compared to earlier working models, the variance component estimates of the models that we proposed in the paper seem to be relatively more sensitive to the assumed correlation. Second, one alternative analytic strategy that’s been proposed (and applied) for meta-analysis of dependent effect sizes is to use a multi-level meta-analysis (MLMA) model. The MLMA is a special case of the correlated-and-hierarchical effects model that we described in the paper, the main difference being that MLMA &lt;em&gt;ignores&lt;/em&gt; any correlations between effect size estimates (at the level of the sampling errors), or equivalently, assumes that the correlations are all zero. Thus, MLMA is one specific way that this correlation assumption might be mis-specified. There’s some simulation evidence that inferences based on MLMA may be robust (even without using robust variance estimation), but it’s not clear how general this robustness property might be.&lt;/p&gt;
&lt;p&gt;In this post, I’m going to look at the implications of using a mis-specified assumption about the sampling correlation for the variance components in the correlated-and-hierarchical effects working model. As in &lt;a href=&#34;http://localhost:4321/weighting-in-multivariate-meta-analysis/&#34;&gt;my previous post on weights in multivariate meta-analysis&lt;/a&gt;, I’m going to mostly limit consideration to the simple (but important!) case of an intercept-only model, without any further predictors of effect size, to see what can be learned about how the variance components can go wrong.&lt;/p&gt;
&lt;div id=&#34;the-correlated-and-hierarchical-effects-che-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The correlated-and-hierarchical effects (CHE) model&lt;/h1&gt;
&lt;p&gt;Consider a meta-analytic dataset with effect size estimates &lt;span class=&#34;math inline&#34;&gt;\(T_{ij}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(i = 1,...,k_j\)&lt;/span&gt; indexes effect size estimates within study &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; indexes studies, for &lt;span class=&#34;math inline&#34;&gt;\(j = 1,...,J\)&lt;/span&gt;. Say that effect size estimate &lt;span class=&#34;math inline&#34;&gt;\(T_{ij}\)&lt;/span&gt; has sampling variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_{ij}\)&lt;/span&gt;, and there is some sampling correlation between effect sizes &lt;span class=&#34;math inline&#34;&gt;\(h\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; within study &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;, denoted &lt;span class=&#34;math inline&#34;&gt;\(\phi_{hij}\)&lt;/span&gt;.
The correlated-and-hierarchical effects (or CHE) model describes the distribution of effect sizes using random effects to capture between-study heterogeneity (as in the basic random effects model) and within-study heterogeneity in true effect sizes. In hierarchical notation, the model is
&lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
T_{ij} &amp;amp;= \theta_j + \nu_{ij} + e_{ij} \\
\theta_j &amp;amp;= \mu + \eta_j
\end{align}
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\Var(e_{ij}) = \sigma^2_{ij}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\Var(\nu_{ij}) = \omega^2\)&lt;/span&gt; is the within-study variance, and &lt;span class=&#34;math inline&#34;&gt;\(\Var(\eta_j) = \tau^2\)&lt;/span&gt; is the between-study variance.
To simplify things, let us also assume that the effect size estimates from a given study &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; all have equal sampling variance, so &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2_{1j} = \sigma^2_{2j} = \cdots = \sigma^2_{k_jj} = \sigma^2_j\)&lt;/span&gt;, and that there is a common correlation between any pair of effect size estimates from the same study, so &lt;span class=&#34;math inline&#34;&gt;\(\Cov(e_{hj}, e_{ij}) = \phi \sigma^2_j\)&lt;/span&gt; for some correlation &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Typically, the analyst would estimate this working model using restricted maximum likelihood (REML) estimation to obtain estimates of the variance components &lt;span class=&#34;math inline&#34;&gt;\(\tau^2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\omega^2\)&lt;/span&gt;, after specifying a value of &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;. With an adequately large sample of studies, the REML estimators should be close-to-unbiased and accurate. But what if the assumed correlation is wrong? Let’s suppose that the analyst estimates (via REML) the CHE working model but uses the assumption that there is a common correlation between effect size estimates of &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;, which is not necessarily equal to the true correlation &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;. What are the consequences for estimating &lt;span class=&#34;math inline&#34;&gt;\(\tau^2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\omega^2\)&lt;/span&gt;?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;mis-specified-reml&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Mis-specified REML&lt;/h1&gt;
&lt;p&gt;To figure out what’s going on here, we need to know something about how REML estimators behave under mis-specified models. For starters, I’ll work with a more general case than the CHE model described above. Suppose that we have a vector of multi-variate normal outcomes &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{T}_j\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(j = 1,...,J\)&lt;/span&gt;, explained by a set of covariates &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}_j\)&lt;/span&gt;, and with true variance-covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\Phi_j\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[
\mathbf{T}_j \ \sim \ N\left( \mathbf{X}_j \beta, \boldsymbol\Phi_j \right)
\]&lt;/span&gt;
However, suppose that we posit a variance structure &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\Omega_j(\boldsymbol\theta)\)&lt;/span&gt;, which is a function of a &lt;span class=&#34;math inline&#34;&gt;\(v\)&lt;/span&gt;-dimensional variance component parameter &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\theta\)&lt;/span&gt;, and where &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\Phi_j\)&lt;/span&gt; is not necessarily conformable to &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\Omega_j(\boldsymbol\theta)\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{T}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}\)&lt;/span&gt; denote the full vector of outcomes and the full (stacked) predictor matrix for &lt;span class=&#34;math inline&#34;&gt;\(j = 1,...,J\)&lt;/span&gt;, and let &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\Phi\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\Omega\)&lt;/span&gt; denote the corresponding block-diagonal variance-covariance matrices.&lt;/p&gt;
&lt;p&gt;We estimate &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\theta\)&lt;/span&gt; by REML, which maximizes the log likelihood
&lt;span class=&#34;math display&#34;&gt;\[
2 l_R(\boldsymbol\theta) = c -\log \left|\boldsymbol\Omega_j(\boldsymbol\theta)\right| - \log \left|\mathbf{X}&amp;#39; \boldsymbol\Omega^{-1}_j(\boldsymbol\theta) \mathbf{X}\right| - \mathbf{T}&amp;#39;\mathbf{Q}(\boldsymbol\theta)\mathbf{T},
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Q}(\boldsymbol\theta) = \boldsymbol\Omega^{-1}_j(\boldsymbol\theta) - \boldsymbol\Omega^{-1}_j(\boldsymbol\theta) \mathbf{X} \left(\mathbf{X}&amp;#39; \boldsymbol\Omega^{-1}_j(\boldsymbol\theta) \mathbf{X}\right)^{-1} \mathbf{X}&amp;#39;\boldsymbol\Omega^{-1}_j(\boldsymbol\theta)\)&lt;/span&gt;. Equivalently, the REML estimators solve the score equations
&lt;span class=&#34;math display&#34;&gt;\[
\frac{\partial l_R(\boldsymbol\theta)}{\partial \theta_q} = 0, \qquad \text{for} \qquad q = 1,...,v.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Under mis-specification, the REML estimators converge (as &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; increases) to the values that minimize the Kullback-Liebler divergence between the posited model and the true data-generating process. For the restricted likelihood, the Kullback-Liebler divergence is given by
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\mathcal{KL}(\theta, \theta_0) &amp;amp;= \E\left[l_R(\boldsymbol\Phi) - l_R(\boldsymbol\theta)\right] \\
&amp;amp;= c + \log \left| \boldsymbol\Omega(\boldsymbol\theta) \right| + \log \left| \mathbf{X}&amp;#39;\boldsymbol\Omega^{-1}(\boldsymbol\theta) \mathbf{X} \right| + \text{tr}\left(\mathbf{Q}(\boldsymbol\theta) \boldsymbol\Phi\right),
\end{aligned}
\]&lt;/span&gt;
where the expectation in the first line is taken with respect to the true data-generating process and where &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; (in the second line) is a constant that does not depend on &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\theta\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;back-to-che&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Back to CHE&lt;/h1&gt;
&lt;p&gt;Let me now jump back to the special case of the CHE model for a meta-analysis with no predictors. Let &lt;span class=&#34;math inline&#34;&gt;\(\tau_*^2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\omega_*^2\)&lt;/span&gt; denote the variance components in the true data-generating process. Let &lt;span class=&#34;math inline&#34;&gt;\(\tilde\tau^2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\tilde\omega^2\)&lt;/span&gt; denote the asymptotic limits of the REML estimators under the mis-specified model. In this model, &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}_j = \mathbf{1}_j\)&lt;/span&gt;,
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\boldsymbol\Phi_j &amp;amp;= \left(\tau_*^2 + \phi \sigma_j^2\right) \mathbf{1}_j \mathbf{1}_j&amp;#39; + \left(\omega_*^2 + (1 - \phi) \sigma_j^2\right) \mathbf{I}_j \\
\boldsymbol\Omega_j &amp;amp;= \left(\tilde\tau^2 + \rho \sigma_j^2\right) \mathbf{1}_j \mathbf{1}_j&amp;#39; + \left(\tilde\omega^2 + (1 - \rho) \sigma_j^2\right) \mathbf{I}_j \\
\boldsymbol\Omega_j^{-1} &amp;amp;= \frac{1}{\tilde\omega^2 + (1 - \rho)\sigma_j^2}\left[\mathbf{I}_j - \frac{\tilde\tau^2 + \rho \sigma_j^2}{k_j \tilde\tau^2 + k_j \rho \sigma_j^2 + \tilde\omega^2 + (1 - \rho)\sigma_j^2} \mathbf{1}_j \mathbf{1}_j&amp;#39; \right].
\end{aligned}
\]&lt;/span&gt;
Let &lt;span class=&#34;math inline&#34;&gt;\(\displaystyle{\tilde{w}_j = \frac{k_j}{k_j \tilde\tau^2 + k_j \rho \sigma_j^2 + \tilde\omega^2 + (1 - \rho)\sigma_j^2}}\)&lt;/span&gt; denote the weight assigned to study &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; under the mis-specified model, with the total weight denoted as &lt;span class=&#34;math inline&#34;&gt;\(\displaystyle{\tilde{W} = \sum_{j=1}^J \tilde{w}_j}\)&lt;/span&gt;. Similarly, let &lt;span class=&#34;math inline&#34;&gt;\(\displaystyle{w^*_j = \frac{k_j}{k_j \tau_*^2 + k_j \phi \sigma_j^2 + \omega_*^2 + (1 - \phi)\sigma_j^2}}\)&lt;/span&gt; denote the weight that &lt;em&gt;should&lt;/em&gt; be assigned to study &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; under the true model. Then we have that
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\text{tr}\left(\mathbf{Q} \boldsymbol\Phi\right) &amp;amp;= \text{tr}\left(\boldsymbol\Omega^{-1} \boldsymbol\Phi\right) - \text{tr}\left[\left(\mathbf{1}&amp;#39;\boldsymbol\Omega^{-1} \mathbf{1}\right)^{-1} \mathbf{1}&amp;#39;\boldsymbol\Omega^{-1} \boldsymbol\Phi \boldsymbol\Omega^{-1} \mathbf{1}\right] \\
&amp;amp;= \sum_{j=1}^J \text{tr}\left(\boldsymbol\Omega^{-1}_j \boldsymbol\Phi_j\right) - \frac{1}{\tilde{W}}\sum_{j=1}^J \mathbf{1}_j&amp;#39;\boldsymbol\Omega^{-1}_j \boldsymbol\Phi_j \boldsymbol\Omega_j^{-1} \mathbf{1}_j \\
&amp;amp;= \sum_{j=1}^J \frac{k_j}{\tilde\omega^2 + (1 - \rho)\sigma_j^2}\left[\tau_*^2 + \omega_*^2 + \sigma_j^2 - \left(\tilde\tau^2 + \rho \sigma_j^2\right) \frac{\tilde{w}_j}{w^*_j}\right] - \frac{1}{\tilde{W}}\sum_{j=1}^J \frac{\tilde{w}_j^2}{w^*_j} \\
&amp;amp;= \sum_{j=1}^J (k_j - 1) \left(\frac{\omega_*^2 + (1 - \phi)\sigma_j^2}{\tilde\omega^2 + (1 - \rho)\sigma_j^2}\right) + \sum_{j=1}^J \frac{\tilde{w}_j}{w_j^*} - \frac{1}{\tilde{W}}\sum_{j=1}^J \frac{\tilde{w}_j^2}{w^*_j},
\end{aligned}
\]&lt;/span&gt;
and
&lt;span class=&#34;math display&#34;&gt;\[
\log \left| \boldsymbol\Omega \right| = \sum_{j=1}^J\log \left| \boldsymbol\Omega_j \right| = \sum_{j=1}^J\left[ \left(k_j - 1\right) \log\left(\tilde\omega^2 + (1 - \rho)\sigma_j^2\right) - \log \left(\frac{\tilde{w}_j}{k_j}\right)\right]
\]&lt;/span&gt;
and
&lt;span class=&#34;math display&#34;&gt;\[
\log \left|\mathbf{1}&amp;#39;\boldsymbol\Omega^{-1} \mathbf{1}\right| = \log \left(\tilde{W}\right),
\]&lt;/span&gt;
It follows that the REML estimators converge to the values &lt;span class=&#34;math inline&#34;&gt;\(\tilde\tau^2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\tilde\omega^2\)&lt;/span&gt; that minimize
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\mathcal{KL}\left(\tilde\tau^2, \tilde\omega^2, \rho, \tau_*^2, \omega_*^2, \phi\right) &amp;amp;= \sum_{j=1}^J (k_j - 1) \left(\frac{\omega_*^2 + (1 - \phi)\sigma_j^2}{\tilde\omega^2 + (1 - \rho)\sigma_j^2}\right) + \sum_{j=1}^J \frac{\tilde{w}_j}{w_j^*} - \frac{1}{\tilde{W}}\sum_{j=1}^J \frac{\tilde{w}_j^2}{w^*_j} \\
&amp;amp; \qquad \qquad + \sum_{j=1}^J\left(k_j - 1\right) \log\left(\tilde\omega^2 + (1 - \rho)\sigma_j^2\right) - \sum_{j=1}^J \log \left(\frac{\tilde{w}_j}{k_j}\right) + \log(\tilde{W})
\end{aligned}
\]&lt;/span&gt;
This is a complicated non-linear objective function, but it can be minimized numerically using standard techniques.&lt;/p&gt;
&lt;p&gt;Here are some heatmaps of the function for &lt;span class=&#34;math inline&#34;&gt;\(\tau = 0.2\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\omega = 0.1\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\phi = 0.4\)&lt;/span&gt;, and some simulated values for &lt;span class=&#34;math inline&#34;&gt;\(k_j\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_j^2\)&lt;/span&gt;, for three different assumed correlations:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
set.seed(20211124)

CHE_KL &amp;lt;- function(to, tau, omega, phi, rho, k_j, sigmasq_j) {
  
  trs_j &amp;lt;- to[1]^2 + rho * sigmasq_j
  ors_j &amp;lt;- to[2]^2 + (1 - rho) * sigmasq_j
  w_j &amp;lt;- k_j / (k_j * trs_j + ors_j)
  W &amp;lt;- sum(w_j)
  
  tausq_ps_j &amp;lt;- tau^2 + phi * sigmasq_j
  omegasq_ps_j &amp;lt;- omega^2 + (1 - phi) * sigmasq_j
  wj_star &amp;lt;- k_j / (k_j * tausq_ps_j + omegasq_ps_j)
  
  A1 &amp;lt;- sum((k_j - 1) * omegasq_ps_j / ors_j)
  A2 &amp;lt;- sum(w_j / wj_star)
  A3 &amp;lt;- sum(w_j^2 / wj_star) / W
  B &amp;lt;- sum((k_j - 1) * log(ors_j) - log(w_j / k_j))
  C &amp;lt;- log(W)
  
  A1 + A2 - A3 + B + C
  
}

tau &amp;lt;- 0.2
omega &amp;lt;- 0.1
phi &amp;lt;- 0.4
J &amp;lt;- 20
k_j &amp;lt;- 1 + rpois(J, 5)
sigmasq_j &amp;lt;- 4 / pmax(rgamma(J, 3, scale = 30), 20)


KL_dat &amp;lt;- 
  cross_df(list(t = seq(0,0.4,0.01),
                o = seq(0,0.2,0.005),
                rho = c(0, 0.4, 0.8))) %&amp;gt;%
  mutate(
    to = map2(.x = t, .y = o, ~ c(.x, .y)),
    KL = map2_dbl(.x = to, .y = rho, .f = CHE_KL, 
                  tau = tau, omega = omega,
                  phi = phi, k_j = k_j, sigmasq_j = sigmasq_j),
    rho = paste(&amp;quot;rho ==&amp;quot;, rho)
  ) %&amp;gt;%
  group_by(rho)

KL_min &amp;lt;- 
  KL_dat %&amp;gt;%
  filter(KL == min(KL))

KL_dat %&amp;gt;%
  mutate(KL = -pmin(0.25, (KL - min(KL)) / (max(KL) - min(KL)))) %&amp;gt;%
ggplot() + 
  facet_wrap(~ rho, scales = &amp;quot;free&amp;quot;, labeller = &amp;quot;label_parsed&amp;quot;) + 
  geom_contour_filled(aes(x = t, y = o, z = KL), bins = 30) + 
  geom_point(x = tau, y = omega, color = &amp;quot;white&amp;quot;, size = 2) + 
  geom_point(data = KL_min, aes(x = t, y = o), color = &amp;quot;red&amp;quot;, size = 2) + 
  theme_minimal() + 
  labs(x = expression(tau), y = expression(omega)) + 
  scale_x_continuous(expand = c(0,0)) + 
  scale_y_continuous(expand = c(0,0)) + 
  theme(legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:4321/post/Variance-component-estimation-in-misspecified-CHE_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The white points correspond to the true parameter values, while the red points correspond with the values that minimized the K-L divergence. In the middle plot, where &lt;span class=&#34;math inline&#34;&gt;\(\rho = 0.4\)&lt;/span&gt; corresponds to the true sampling correlation, the function is minimized at the true values of &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\omega\)&lt;/span&gt;. In the left-hand plot, assuming &lt;span class=&#34;math inline&#34;&gt;\(\rho = 0.0\)&lt;/span&gt; leads to an upwardly biased value of &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; and a downwardly biased value of &lt;span class=&#34;math inline&#34;&gt;\(\omega\)&lt;/span&gt;. In the right-hand plot, assuming &lt;span class=&#34;math inline&#34;&gt;\(\rho = 0.8\)&lt;/span&gt; leads to a smaller value of &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; and a larger value of &lt;span class=&#34;math inline&#34;&gt;\(\omega\)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;completely-balanced-designs&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Completely balanced designs&lt;/h2&gt;
&lt;p&gt;Things simplify considerably in the special case that the sample of studies is completely balanced, such that &lt;span class=&#34;math inline&#34;&gt;\(k_1 = k_2 = \cdots = k_J\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_1^2 = \sigma_2^2 = \cdots = \sigma_J^2\)&lt;/span&gt;. In such a design, the log-likelihood depends on &lt;span class=&#34;math inline&#34;&gt;\(\tau^2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\omega^2\)&lt;/span&gt; only through the quantities &lt;span class=&#34;math inline&#34;&gt;\(a = \tau^2 + \rho \sigma^2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(b = \omega^2 + (1 - \rho) \sigma^2\)&lt;/span&gt;. It follows that
&lt;span class=&#34;math display&#34;&gt;\[
l_R\left(\tau^2, \omega^2, \phi\right) = l_R\left(\tilde\tau^2, \tilde\omega^2, \rho\right)
\]&lt;/span&gt;
so long as
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\tau^2 + \phi \sigma^2 &amp;amp;= \tilde\tau^2 + \rho \sigma^2 \\
\omega^2 + (1 - \phi)\sigma^2 &amp;amp;= \tilde\omega^2 + (1 - \rho) \sigma^2.
\end{aligned}
\]&lt;/span&gt;
If we assume that &lt;span class=&#34;math inline&#34;&gt;\((\rho - \phi)\sigma^2 &amp;lt; \tau^2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\((\phi - \rho)\sigma^2 &amp;lt; \omega^2\)&lt;/span&gt;, then we can set
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\tilde\tau^2 &amp;amp;= \tau^2 - \left(\rho - \phi\right) \sigma^2 \\
\tilde\omega^2 &amp;amp;= \omega^2 + \left(\rho - \phi\right) \sigma^2
\end{aligned}
\]&lt;/span&gt;
and achieve the exact same likelihood.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; Because the Kullback-Liebler divergence is minimized at the log likelihood of the true parameter values, setting &lt;span class=&#34;math inline&#34;&gt;\(\tilde\tau^2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\tilde\omega^2\)&lt;/span&gt; equal to the above quantities will also minimize the K-L divergence.&lt;/p&gt;
&lt;p&gt;The relationships here are fairly intuitive, I think. When &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt; is an over-estimate of the true correlation &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;, then the between-study variance will be under-estimated and the within-study variance will be over-estimated, each to an extent that depends on a) the difference between &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; and b) the size of the (average) sampling variance. When &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt; is an under-estimate of the true correlation &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;, then the between-study variance will be over-estimated and the within-study variance will be under-estimated, each to an extent that depends on the same components. It’s also rather intriguing to see that the total variance (the sum of &lt;span class=&#34;math inline&#34;&gt;\(\tau^2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\omega^2\)&lt;/span&gt;) is totally invariant to &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt; and will be preserved no matter what assumption we make regarding the sample correlation.&lt;/p&gt;
&lt;p&gt;In practice, of course, it’s pretty unlikely to have a meta-analytic dataset that is completely balanced. Still, the formulas for this completely balanced case might nonetheless be useful as heuristics for the direction of the biases in the parameter estimates—perhaps even as rough guides for the magnitude of bias that could be expected.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;finding-tildetau2-and-tildeomega2-in-imbalanced-designs&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Finding &lt;span class=&#34;math inline&#34;&gt;\(\tilde\tau^2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\tilde\omega^2\)&lt;/span&gt; in imbalanced designs&lt;/h2&gt;
&lt;p&gt;In imbalanced designs, we can find &lt;span class=&#34;math inline&#34;&gt;\(\tilde\tau^2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\tilde\omega^2\)&lt;/span&gt; by direct minimization of &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{KL}\)&lt;/span&gt;, given design information &lt;span class=&#34;math inline&#34;&gt;\(k_1,...,k_J\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_1^2,...,\sigma_J^2\)&lt;/span&gt;; true parameter values &lt;span class=&#34;math inline&#34;&gt;\(\tau^2\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\omega^2\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;; and assumed correlation &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;. The plot below depicts how &lt;span class=&#34;math inline&#34;&gt;\(\tilde\tau\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\tilde\omega\)&lt;/span&gt;, and the total SD &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{\tilde\tau^2 + \tilde\omega^2}\)&lt;/span&gt; change as a function of the assumed correlation &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;, for various levels of true correlation &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;, when the design is imbalanced. As previously, I use &lt;span class=&#34;math inline&#34;&gt;\(\tau = 0.2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\omega = 0.1\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;find_tau_omega &amp;lt;- function(tau, omega, phi, rho, k_j, sigmasq_j) {

  res &amp;lt;- optim(par = c(tau + 0.001, omega + 0.001), fn = CHE_KL, 
                tau = tau, omega = omega, phi = phi, rho = rho,
                k_j = k_j, sigmasq_j = sigmasq_j,
                lower = c(0,0), method = &amp;quot;L-BFGS-B&amp;quot;)

  data.frame(tau_tilde = res$par[1], omega_tilde = res$par[2])
}

sigmasq_bar &amp;lt;- mean(sigmasq_j)

opt_params &amp;lt;- 
  cross_df(list(tau = tau,
                omega = omega,
                phi = seq(0.2,0.8,0.2),
                rho = seq(0,0.95,0.05))) %&amp;gt;%
  mutate(
    res = pmap(., .f = find_tau_omega, k_j = k_j, sigmasq_j = sigmasq_j),
  ) %&amp;gt;%
  unnest(res) %&amp;gt;%
  mutate(
    total_tilde = sqrt(tau_tilde^2 + omega_tilde^2),
    tau_pred = sqrt(pmax(0,tau^2 + (phi - rho) * sigmasq_bar)),
    omega_pred = sqrt(pmax(0, omega^2 - (phi - rho) * sigmasq_bar)),
    total_pred = sqrt(tau_pred^2+ omega_pred^2),
    phi_lab = paste(&amp;quot;phi ==&amp;quot;, phi)
  )

opt_params %&amp;gt;% 
  pivot_longer(c(ends_with(&amp;quot;_tilde&amp;quot;), ends_with(&amp;quot;_pred&amp;quot;)),
               names_to = &amp;quot;q&amp;quot;, values_to = &amp;quot;p&amp;quot;) %&amp;gt;%
  separate(q, into = c(&amp;quot;param&amp;quot;,&amp;quot;type&amp;quot;)) %&amp;gt;%
  mutate(
    type = recode(type, tilde = &amp;quot;exact&amp;quot;, pred = &amp;quot;balanced&amp;quot;),
    type = factor(type, levels = c(&amp;quot;exact&amp;quot;,&amp;quot;balanced&amp;quot;)),
    param = factor(param, levels = c(&amp;quot;tau&amp;quot;,&amp;quot;omega&amp;quot;,&amp;quot;total&amp;quot;),
                   labels = c(&amp;quot;tau&amp;quot;,&amp;quot;omega&amp;quot;,&amp;quot;sqrt(tau^2 + omega^2)&amp;quot;))
  ) %&amp;gt;%
  ggplot(aes(rho, p, color = type, linetype = type)) + 
  geom_hline(yintercept = 0) + 
  geom_line() + 
  scale_color_brewer(type = &amp;quot;qual&amp;quot;, palette = 2) + 
  facet_grid(param ~ phi_lab, labeller = &amp;quot;label_parsed&amp;quot;) + 
  theme_minimal() + 
  labs(x = expression(rho), y = &amp;quot;Parameter&amp;quot;, color = &amp;quot;&amp;quot;, linetype = &amp;quot;&amp;quot;) +
  theme(legend.position = &amp;quot;bottom&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:4321/post/Variance-component-estimation-in-misspecified-CHE_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The top row of the figure shows &lt;span class=&#34;math inline&#34;&gt;\(\tilde\tau\)&lt;/span&gt;, the middle row shows &lt;span class=&#34;math inline&#34;&gt;\(\tilde\omega\)&lt;/span&gt;, and the bottom row shows the total SD, for varying levels of assumed correlation &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;. The solid green lines represent the values that actually minimize the KL divergence. The dashed orange lines correspond to the minimizing values assuming complete balance (and using the average value of the &lt;span class=&#34;math inline&#34;&gt;\(\sigma_j^2\)&lt;/span&gt;’s to evaluate the bias). The “balanced” approximations are fairly close—close enough to use as heuristics, at least—although they’re not perfect. In particular, the balanced approximation becomes discrepant from the real minimizing values when &lt;span class=&#34;math inline&#34;&gt;\(\tilde\tau\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(\tilde\omega\)&lt;/span&gt; gets closer to zero. It’s also notable that the total variance is nearly constant (except when one or the other variance component is zero) and the balanced approximation is quite close to the real minimizing values.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;implications&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Implications&lt;/h1&gt;
&lt;p&gt;This post was mostly just to satisfy my own curiosity about how variance components behave in the MLMA and, more broadly, under mis-specified correlated-and-hierarchical effects meta-analysis models. I don’t think the bias formulas have much practical utility because, if you’re concerned about bias due to mis-specified sampling correlations, the first thing to do is try and develop better assumptions about the sampling correlation structure. Still, I think this analysis might be helpful for purposes of gauging how far off from the true your variance component estimates might be. In further work along these lines, it might be useful to examine the consequences of the biased variance component estimates for the efficiency of overall average effect size estimates based on mis-specified CHE models and the accuracy of model-based standard errors and confidence intervals under mis-specification. It would also be important to verify that these approximations provide accurate predictions for the bias of variance component estimates in realistic meta-analytic data (especially with a small or moderate number of studies).&lt;/p&gt;
&lt;p&gt;Another implication of this investigation is that &lt;em&gt;imbalance&lt;/em&gt; in the data structure seems to matter. When all studies have an equal number of effect sizes and are equally precise, then everything is simpler and more robust to mistaken assumptions about sampling correlation. Variance component estimation matters more for meta-analytic data in which some studies are more precise or contribute more effect size estimates than others. Therefore, further investigations—including simulation studies—of methods for handling dependent effect sizes really need to examine conditions with imbalanced data in order to draw defensible, generalizable conclusions about the robustness or utility of particular methods.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Consequently, &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; is not identifiable (in the statistical sense) in the completely balanced design.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Implications of mean-variance relationships for standardized mean differences</title>
      <link>http://localhost:4321/mean-variance-relationships-and-smds/</link>
      <pubDate>Tue, 02 Nov 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/mean-variance-relationships-and-smds/</guid>
      <description>


&lt;p&gt;I spend more time than I probably should discussing meta-analysis problems on the &lt;a href=&#34;https://stat.ethz.ch/mailman/listinfo/r-sig-meta-analysis&#34;&gt;R-SIG-meta-analysis listserv&lt;/a&gt;. The questions that folks pose there are often quite interesting—especially when they’re motivated by issues that they’re wrestling with while trying to complete meta-analysis projects in their diverse fields. For those interested in meta-analytic methodology, I think perusing the mailing list is a good way to get a bit of ground sense about problems that come up in practice and places where there is a need for new methodological work, or at least further methodological guidance.&lt;/p&gt;
&lt;p&gt;Recently, a &lt;a href=&#34;https://stat.ethz.ch/pipermail/r-sig-meta-analysis/2021-September/003318.html&#34;&gt;question came up&lt;/a&gt; on the listserv about whether it was reasonable to use the standardized mean difference metric for synthesizing studies where the outcomes are measured as proportions. Luke Martinez wrote:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I’m doing a meta-analysis where the papers report only “mean” and “sd” of some form of proportion and/or “mean” and “sd” of corresponding raw frequencies. (For context, the papers ask students to read, find, and correct the wrong words in a text.) … My question is given that all these studies only report “mean” and “sd”, can I simply use a SMD effect size?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I think this is an interesting question because, while the &lt;a href=&#34;https://stat.ethz.ch/pipermail/r-sig-meta-analysis/2021-September/003320.html&#34;&gt;SMD could work perfectly fine&lt;/a&gt; as an effect size metric for proportions, there are also other alternatives that could be considered, such as odds ratios or response ratios or raw differences in proportions. Further, there are some situations where the SMD has disadvantages for synthesizing contrasts between proportions. Thus, it’s a situation where one has to make a choice about the effect size metric, and where the most common metric (the SMD) might not be the right answer. As &lt;a href=&#34;https://stat.ethz.ch/pipermail/r-sig-meta-analysis/2021-October/003331.html&#34;&gt;I wrote in reply&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I would suggest that you could also consider other effect measures besides the SMD. For example, the response ratio is also a scale-free metric that could work with the proportion outcomes that you’ve described, and would also be appropriate for raw frequency counts as long as the total number possible is the same for the groups being compared within a given study.&lt;/p&gt;
&lt;p&gt;Whether the response ratio would be more appropriate than the SMD is hard to gauge. One would need to know more about how the proportions were assessed and how the assessment procedures varied from study to study. For instance, did some studies use passages with many possible errors to be corrected while other studies used passages with just a few errors? Did the difficulty of the passages differ from study to study? Were there very low or very high mean proportions in any studies? Does there seem to be a relationship between the means and the variances of the proportions of a given group?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In a &lt;a href=&#34;https://stat.ethz.ch/pipermail/r-sig-meta-analysis/2021-October/003361.html&#34;&gt;follow-up&lt;/a&gt;, I elaborated on some potential problems with using the SMD:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Variation in the number of possible errors (and perhaps also in the length of the time provided for the test?) suggests that the measures from different studies may have varying degrees of reliability. Varying reliability introduces heterogeneity in the SMD (because the denominator is inflated or shrunk by the degree of reliability).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;A relationship between the M and SD of the proportions for a given group suggests that the distribution of the individual-level outcomes might also exhibit mean-variance relationships. (I say “suggests” rather than implies because there’s an ecological inference here, i.e., assuming something about individual-level variation on the basis of group-level variation.) If this supposition is reasonable, then that introduces a further potential source of heterogeneity in the SMDs (study-to-study variation in the M for the reference group influences the SD of the reference group, thereby inflating or shrinking the SMDs).&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;And I suggested a possible work-flow for examining the choice of effect size metric:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Here’s how I might proceed if I were conducting
this analysis:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Calculate &lt;em&gt;both&lt;/em&gt; SMDs and log-transformed response ratios for the full set of studies.&lt;/li&gt;
&lt;li&gt;Examine the distribution of effect size estimates for each metric (using histograms or funnel plots). If one of the distributions is skewed or has extreme outliers, take that as an indication that the metric might not be appropriate.&lt;/li&gt;
&lt;li&gt;Fit meta-analytic models to summarize the distribution of effect sizes in each metric, using a model that appropriately describes the dependence structure of the estimates. Calculate I-squared statistics, give preference to the metric with lower I-squared.&lt;/li&gt;
&lt;li&gt;If (2) and (3) don’t lead to a clearly preferable metric, then choose between SMD and RR based on whichever will make the synthesis results easier to explain to people.&lt;/li&gt;
&lt;li&gt;(Optional/extra credit) Whichever metric you choose, repeat your main analyses using the other metric and stuff all those results in supplementary materials, to satisfy any inveterate statistical curmudgeons who might review/read your synthesis.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;(When I referred to “inveterate statistical curmudgeons”, I mostly had myself in mind.)&lt;/p&gt;
&lt;p&gt;In this post, I want to provide a bit more detail regarding why I think mean-variance relationships in raw data can signal that the standardized mean differences might be less useful as an effect size metric compared to alternatives. The concern is actually broader than meta-analyses of outcomes measured as proportions, so I’ll start with a different case and then return to a situation similar to the one described in the original question.&lt;/p&gt;
&lt;div id=&#34;mean-variance-relationships-can-induce-heterogeneity&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Mean-variance relationships can induce heterogeneity&lt;/h2&gt;
&lt;p&gt;The standardized mean difference parameter for a given study can be defined as:
&lt;span class=&#34;math display&#34;&gt;\[
\delta_i = \frac{\mu_{Bi} - \mu_{Ai}}{\sigma_{Ai}},
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\mu_{Ai}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mu_{Bi}\)&lt;/span&gt; are the (population) mean outcomes in group &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; and group &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; of study &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{Ai}\)&lt;/span&gt; is the (population) standard deviation in group &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; of study &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;
The ideal case for using the SMD metric is when the outcomes in different studies are linearly equatable, so that the outcome scale in one study can be directly translated into the outcome scale of another study. However, if outcomes exhibit mean-variance relationships, linearly equatability seems rather implausible, and we might expect that SMDs will display heterogeneity across studies as a result.&lt;/p&gt;
&lt;p&gt;Let me lay out an example of a situation where the outcomes exhibit mean-variance relationships and where, as a consequence, the SMD metric becomes heterogeneous. Suppose that we have &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; studies, each involving a two-group comparison, with groups of equal size. In study &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, the outcomes in group &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; follow a poisson distribution with mean &lt;span class=&#34;math inline&#34;&gt;\(\mu_{Ai}\)&lt;/span&gt;, so that the variance of the outcomes in group &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is also &lt;span class=&#34;math inline&#34;&gt;\(\mu_{Ai}\)&lt;/span&gt;, for &lt;span class=&#34;math inline&#34;&gt;\(i = 1,...,k\)&lt;/span&gt;. The outcomes in group &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; follow a poisson distribution with mean &lt;span class=&#34;math inline&#34;&gt;\(\mu_{Bi}\)&lt;/span&gt;, so the variance is also &lt;span class=&#34;math inline&#34;&gt;\(\mu_{Bi}\)&lt;/span&gt;. Now, suppose that there is a fixed, proportional relationship between &lt;span class=&#34;math inline&#34;&gt;\(\mu_{Bi}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mu_{Ai}\)&lt;/span&gt;,
so that &lt;span class=&#34;math inline&#34;&gt;\(\mu_{Bi} = \lambda \mu_{Ai}\)&lt;/span&gt; for some &lt;span class=&#34;math inline&#34;&gt;\(\lambda &amp;gt; 0\)&lt;/span&gt;. In other words, the treatment contrast is &lt;em&gt;constant&lt;/em&gt; on the scale of the response ratio.
However, the means in group &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; vary from study to study. To make things concrete, let’s assume that the means in group &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; follow a gamma distribution with shape parameter &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; and rate parameter &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[
\mu_{Ai} \sim \Gamma(\alpha, \beta).
\]&lt;/span&gt;
What does this model imply about the distribution of standardized mean differences across this set of studies?&lt;/p&gt;
&lt;p&gt;Under this model, the SMD parameter for study &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; is:
&lt;span class=&#34;math display&#34;&gt;\[
\delta_i = \frac{\mu_{Bi} - \mu_{Ai}}{\sqrt{\mu_{Ai}}} = (\lambda - 1) \times \sqrt{\mu_{Ai}}.
\]&lt;/span&gt;
The first term in the above expression is a constant that only
depend on the size of the response ratio, but the second term is random because we have assumed that the group &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; means vary from study to study. It will therefore create heterogeneity in the SMD parameters—the greater the variance of the &lt;span class=&#34;math inline&#34;&gt;\(\mu_{Ai}\)&lt;/span&gt;’s, the greater the heterogeneity in &lt;span class=&#34;math inline&#34;&gt;\(\delta_i\)&lt;/span&gt;. Specifically, under the above assumptions, the effect size parameters follow a &lt;a href=&#34;https://en.wikipedia.org/wiki/Nakagami_distribution&#34;&gt;Nakagami distribution&lt;/a&gt;:
&lt;span class=&#34;math display&#34;&gt;\[
\delta_i \sim \text{Nakagami}\left(m = \alpha, \Omega = \frac{(\lambda - 1)^2 \alpha}{\beta}\right)
\]&lt;/span&gt;
Thus, even though we have a model where there is an underlying fixed relationship between &lt;span class=&#34;math inline&#34;&gt;\(\mu_{Ai}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mu_{Bi}\)&lt;/span&gt;, using the SMD metric for synthesis will lead to a situation with heterogeneous effects (even if all of the studies had large sample sizes and so effect sizes in individual studies are precisely estimated).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;an-example-with-proportions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;An example with proportions&lt;/h2&gt;
&lt;p&gt;This sort of behavior is not restricted to the poisson-gamma model I sketched above. The key features of that example are a) the assumption that the outcomes have a strong mean-variance relationship and b) the assumption that the &lt;span class=&#34;math inline&#34;&gt;\(\mu_{Ai}\)&lt;/span&gt;’s are heterogeneous across studies. If both of these hold, then the resulting SMDs will also be heterogeneous. I’ll now describe a similar model, but where the outcomes within each study are proportions.&lt;/p&gt;
&lt;p&gt;As before, suppose that we have &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; studies, each involving a two-group comparison, with groups of equal size. In study &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, the outcomes in group &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; follow a binomial distribution with mean proportion &lt;span class=&#34;math inline&#34;&gt;\(\pi_{Ai}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(T_i\)&lt;/span&gt; trials, so that the variance of the outcomes in group &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(\pi_{Ai}\left(1 - \pi_{Ai}\right) T_i\)&lt;/span&gt;, for &lt;span class=&#34;math inline&#34;&gt;\(i = 1,...,k\)&lt;/span&gt;. The outcomes in group &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; also follow a binomial distribution, this one with mean proportion &lt;span class=&#34;math inline&#34;&gt;\(\pi_{Bi}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(T_i\)&lt;/span&gt; trials, so the variance is &lt;span class=&#34;math inline&#34;&gt;\(\pi_{Bi}\left(1 - \pi_{Bi}\right) T_i\)&lt;/span&gt;. Next, to induce variation in the group-&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; means, let’s assume that the mean proportions follow a beta distribution:
&lt;span class=&#34;math display&#34;&gt;\[
\pi_{Ai} \sim \text{Beta}(\alpha, \beta).
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Finally, suppose that &lt;span class=&#34;math inline&#34;&gt;\(\pi_{Bi} = \lambda_i \pi_{Ai}\)&lt;/span&gt; for some &lt;span class=&#34;math inline&#34;&gt;\(\lambda_i &amp;gt; 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Under these assumptions, the SMD parameter for study &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; is:
&lt;span class=&#34;math display&#34;&gt;\[
\delta_i = \frac{\pi_{Bi}T_i - \pi_{Ai} T_i}{\sqrt{\pi_{Ai} (1 - \pi_{Ai}) T_i}} = (\lambda_i - 1) \times \sqrt{T_i} \times \sqrt{\frac{\pi_{Ai}}{1 - \pi_{Ai}}}.
\]&lt;/span&gt;
From the above expression, it can be seen that there are three potential sources of variation in &lt;span class=&#34;math inline&#34;&gt;\(\delta_i\)&lt;/span&gt;: variation in the study-specific response ratio &lt;span class=&#34;math inline&#34;&gt;\(\lambda_i\)&lt;/span&gt;, variation in the group-&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; proportions &lt;span class=&#34;math inline&#34;&gt;\(\pi_{Ai}\)&lt;/span&gt;, and variation in the number of trials &lt;span class=&#34;math inline&#34;&gt;\(T_i\)&lt;/span&gt;. The total heterogeneity in &lt;span class=&#34;math inline&#34;&gt;\(\delta_i\)&lt;/span&gt; will depend on all three, as well as on the co-variation between &lt;span class=&#34;math inline&#34;&gt;\(\lambda_i\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\pi_{Ai}\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(T_i\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;To make this concrete, let me simulate some meta-analytic data that follows the above model. To do so, I’ll need to make some additional distributional assumptions&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;that &lt;span class=&#34;math inline&#34;&gt;\(\lambda_i\)&lt;/span&gt; is log-normally distributed such that &lt;span class=&#34;math inline&#34;&gt;\(\ln \lambda_i \sim N(\ln \Lambda, \tau^2)\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;that the number of trials is uniformly distributed on the integers between &lt;span class=&#34;math inline&#34;&gt;\(t_{min}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(t_{max}\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;that &lt;span class=&#34;math inline&#34;&gt;\(N_i\)&lt;/span&gt;, the number of observations per group in study &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, is uniformly distributed on the integers between &lt;span class=&#34;math inline&#34;&gt;\(n_{min}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(n_{max}\)&lt;/span&gt;; and&lt;/li&gt;
&lt;li&gt;that &lt;span class=&#34;math inline&#34;&gt;\(\pi_{Ai}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\lambda_i\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(T_i\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(N_i\)&lt;/span&gt; are mutually independent.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Here’s a function that generates study-specific parameter values and sample proportions:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sim_binom_summary &amp;lt;- function(pi_i, T_i, n_i) {
  y &amp;lt;- rbinom(n_i, size = T_i, prob = pi_i) / T_i
  data.frame(M = mean(y), SD = sd(y))
}

sim_props &amp;lt;- function(
  k, # number of studies
  alpha, beta, # parameters of pi_Ai distribution,
  Lambda, tau, # parameters of lambda_i distribution
  t_min, t_max, # parameters of T_i distribution
  n_min, n_max # parameters of the sample size distribution
) {
  
  # simulate parameters
  pi_Ai &amp;lt;- rbeta(k, shape1 = alpha, shape2 = beta)
  lambda_i &amp;lt;- exp(rnorm(k, mean = log(Lambda), sd = tau))
  pi_Bi &amp;lt;- lambda_i * pi_Ai
  T_i &amp;lt;- sample(t_min:t_max, size = k, replace = TRUE)
  delta_i &amp;lt;- (pi_Bi - pi_Ai) * T_i / sqrt(pi_Ai * (1 - pi_Ai) * T_i)
  n_i &amp;lt;- sample(n_min:n_max, size = k, replace = TRUE)
  
  # simulate data
  stats_A &amp;lt;- purrr::pmap_dfr(list(pi_i = pi_Ai, T_i = T_i, n_i = n_i),
                             sim_binom_summary) 
                             
  stats_B &amp;lt;- purrr::pmap_dfr(list(pi_i = pi_Bi, T_i = T_i, n_i = n_i),
                             sim_binom_summary)
  
  # compile
  res &amp;lt;- data.frame(
    pi_Ai = pi_Ai, pi_Bi = pi_Bi, 
    lambda_i = lambda_i, T_i = T_i, 
    delta_i = delta_i, n_i = n_i,
    mA = stats_A$M, sdA = stats_A$SD,
    mB = stats_B$M, sdB = stats_B$SD
  )

  # effect size calculations
  res &amp;lt;- metafor::escalc(
    data = res, measure = &amp;quot;ROM&amp;quot;, var.names = c(&amp;quot;lRR&amp;quot;, &amp;quot;V_lRR&amp;quot;),
    m1i = mB, m2i = mA, 
    sd1i = sdB, sd2i = sdA,
    n1i = n_i, n2i = n_i
  )
  res &amp;lt;- metafor::escalc(
    data = res, measure = &amp;quot;SMD&amp;quot;, var.names = c(&amp;quot;d&amp;quot;, &amp;quot;V_d&amp;quot;),
    m1i = mB, m2i = mA, 
    sd1i = sdB, sd2i = sdA,
    n1i = n_i, n2i = n_i
  )
  
  res
}

set.seed(20211024)
dat &amp;lt;- sim_props(k = 60, alpha = 12, beta = 4, 
                 Lambda = 0.7, tau = .05,
                 t_min = 5, t_max = 18,
                 n_min = 10, n_max = 40)

head(dat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
##       pi_Ai     pi_Bi  lambda_i T_i    delta_i n_i        mA       sdA 
## 1 0.7584480 0.5836965 0.7695933  11 -1.3540950  24 0.7500000 0.1080650 
## 2 0.7359047 0.4950740 0.6727420  16 -2.1851474  24 0.7786458 0.1222235 
## 3 0.7132014 0.4773027 0.6692398  12 -1.8068471  10 0.7333333 0.1097134 
## 4 0.6223653 0.4627406 0.7435193   9 -0.9877857  30 0.6666667 0.1399386 
## 5 0.5916619 0.4205407 0.7107787   6 -0.8527716  28 0.5833333 0.2103299 
## 6 0.7266748 0.5014601 0.6900751   9 -1.5160305  35 0.7619048 0.1209466 
##          mB       sdB     lRR  V_lRR       d    V_d 
## 1 0.6174242 0.1285066 -0.1945 0.0027 -1.0983 0.0959 
## 2 0.5260417 0.1275776 -0.3922 0.0035 -1.9888 0.1245 
## 3 0.3583333 0.1622089 -0.7161 0.0227 -2.5934 0.3681 
## 4 0.4555556 0.1943213 -0.3808 0.0075 -1.2306 0.0793 
## 5 0.4583333 0.2060055 -0.2412 0.0119 -0.5921 0.0746 
## 6 0.4920635 0.1793349 -0.4372 0.0045 -1.7447 0.0789&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the specified parameter values, there is only a small amount of true heterogeneity in the log of the response ratios (the blue density). Of course, there is further heterogeneity in the log response ratio estimates (the green density) due to sampling error:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
ggplot(dat) + 
  geom_density(aes(log(lambda_i), ..scaled..), fill = &amp;quot;blue&amp;quot;, alpha = 0.5) + 
  geom_density(aes(lRR, ..scaled..), fill = &amp;quot;green&amp;quot;, alpha = 0.2) + 
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:4321/post/Mean-variance-relationships-and-SMDs_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;576&#34; /&gt;
A random effects meta-analysis confirms that there is only a modest degree of true heterogeneity in the log response ratios:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(metafor)
rma(yi = lRR, vi = V_lRR, data = dat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Random-Effects Model (k = 60; tau^2 estimator: REML)
## 
## tau^2 (estimated amount of total heterogeneity): 0.0028 (SE = 0.0013)
## tau (square root of estimated tau^2 value):      0.0529
## I^2 (total heterogeneity / total variability):   42.01%
## H^2 (total variability / sampling variability):  1.72
## 
## Test for Heterogeneity:
## Q(df = 59) = 100.6304, p-val = 0.0006
## 
## Model Results:
## 
## estimate      se      zval    pval    ci.lb    ci.ub      
##  -0.3498  0.0111  -31.5751  &amp;lt;.0001  -0.3715  -0.3281  *** 
## 
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Contrast this with what we get from using the standardized mean difference metric. The distributions of true effect sizes (blue) and of effect size estimates (light purple) have large spread as well as strong left skew:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
ggplot(dat) + 
  geom_density(aes(delta_i, ..scaled..), fill = &amp;quot;blue&amp;quot;, alpha = 0.2) + 
  geom_density(aes(d, ..scaled..), fill = &amp;quot;purple&amp;quot;, alpha = 0.5) + 
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:4321/post/Mean-variance-relationships-and-SMDs_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;576&#34; /&gt;
A random effects meta-analysis of the standardized mean differences shows a greater degree of true heterogeneity, both in terms of the estimated &lt;span class=&#34;math inline&#34;&gt;\(\tau\)&lt;/span&gt; and in &lt;span class=&#34;math inline&#34;&gt;\(I^2\)&lt;/span&gt;, or the proportion of total variance in the effect size estimates that is attributable to true heterogeneity:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rma(yi = d, vi = V_d, data = dat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Random-Effects Model (k = 60; tau^2 estimator: REML)
## 
## tau^2 (estimated amount of total heterogeneity): 0.2838 (SE = 0.0743)
## tau (square root of estimated tau^2 value):      0.5327
## I^2 (total heterogeneity / total variability):   72.61%
## H^2 (total variability / sampling variability):  3.65
## 
## Test for Heterogeneity:
## Q(df = 59) = 203.0513, p-val &amp;lt; .0001
## 
## Model Results:
## 
## estimate      se      zval    pval    ci.lb    ci.ub      
##  -1.5967  0.0824  -19.3771  &amp;lt;.0001  -1.7582  -1.4352  *** 
## 
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;diagnostics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Diagnostics&lt;/h2&gt;
&lt;p&gt;The code above more-or-less implements the workflow I suggested for deciding between the standardized mean difference or response ratio metric (for proportions, we could also add further comparisons with log odds ratios and with raw differences in proportions). But is there further diagnostic information in the data that could provide a better sense of what is going on? I think there are a few things that might be helpful to consider.&lt;/p&gt;
&lt;p&gt;First, the issues I’m concerned with here will arise when there are mean-variance relationships in the outcomes. To get at that, we can simply plot the means and SDs of each group. In the code below, I re-structure the data so that there is one row per group per study. I then plot the SD versus the mean of each group:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(tidyr)

long_summary_stats &amp;lt;- 
  dat %&amp;gt;%
  select(n_i, T_i, mA, sdA, mB, sdB) %&amp;gt;%
  pivot_longer(cols = c(mA, sdA, mB, sdB), 
               names_to = c(&amp;quot;.value&amp;quot;,&amp;quot;group&amp;quot;),
               names_pattern = &amp;quot;(m|sd)(A|B)&amp;quot;)

ggplot(long_summary_stats,
       aes(m, sd, color = group)) + 
  geom_point() + 
  geom_smooth(se = FALSE) + 
  scale_x_continuous(limits = c(0, 1), expand = c(0,0)) + 
  expand_limits(y = 0) + 
  theme_minimal() + 
  theme(legend.position = c(0.1, 0.9))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:4321/post/Mean-variance-relationships-and-SMDs_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;480&#34; /&gt;
The plot above does suggest a mean-variance relationship, though it’s a bit messy. We can do better by using the scaled SD, after adjusting for the degree of spread that we would expect given &lt;span class=&#34;math inline&#34;&gt;\(T_i\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;long_summary_stats %&amp;gt;%
  mutate(
    sd_scaled = sd * sqrt(T_i)
  ) %&amp;gt;%
  ggplot(aes(m, sd_scaled, color = group)) + 
  geom_point() + 
  geom_smooth(se = FALSE) + 
  geom_function(fun = function(x) sqrt(x * (1 - x)),
                color = &amp;quot;black&amp;quot;) + 
  scale_x_continuous(limits = c(0, 1), expand = c(0,0)) + 
  expand_limits(y = 0) + 
  theme_minimal() + 
  theme(legend.position = c(0.1, 0.9))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:4321/post/Mean-variance-relationships-and-SMDs_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;480&#34; /&gt;
From the above, it does appear that there could be a relationship between the scaled SD and the mean. The black curve indicates the theoretical mean-variance relationship that would be expected under the binomial distribution, and indeed the empirical relationship appears to be quite similar. This suggests that mean-variance relationships might be at play (a correct supposition, since of course we know the true data-generating process here).&lt;/p&gt;
&lt;p&gt;Second, since the outcomes in each group are all proportions, we can simply plot the mean in group &lt;span class=&#34;math inline&#34;&gt;\(B\)&lt;/span&gt; versus the mean in group &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(dat, aes(mA, mB)) + 
  geom_point() + 
  geom_smooth(se = FALSE, color = &amp;quot;green&amp;quot;) + 
  geom_smooth(method = &amp;quot;lm&amp;quot;, formula = y ~ x) + 
  coord_cartesian(xlim = c(0,1), ylim = c(0,1), expand = FALSE) + 
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:4321/post/Mean-variance-relationships-and-SMDs_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;480&#34; /&gt;
This plot shows that there is a strong linear relationship between the two means, with a best-fit line that might go through the origin. This suggests that the response ratio might be an appropriate metric (although the difference in proportions might also be appropriate here, since a line with unit slope would probably fit quite well).&lt;/p&gt;
&lt;p&gt;Third (and most speculatively/hand-wavily), I think exploratory moderator analysis can be useful here, but interpreted in a non-typical way. Under the model I’ve sketched, we would expect that the standardized mean difference estimates should be systematically associated with the group-&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; means, as well as with the number of trials used to assess outcomes. The scatter-plots below show that this is indeed the case (the right-hand plot shows &lt;span class=&#34;math inline&#34;&gt;\(d_i\)&lt;/span&gt; versus &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{T_i}\)&lt;/span&gt;).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(patchwork)
mA_d_plot &amp;lt;- 
  ggplot(dat, aes(mA, d)) + 
  geom_point() + 
  geom_smooth(se = FALSE, color = &amp;quot;green&amp;quot;) + 
  geom_smooth(method = &amp;quot;lm&amp;quot;) + 
  scale_x_continuous(limits = c(0, 1), expand = c(0,0)) + 
  theme_minimal()

Ti_d_plot &amp;lt;- 
  ggplot(dat, aes(sqrt(T_i), d)) + 
  geom_point() + 
  geom_smooth(se = FALSE, color = &amp;quot;green&amp;quot;) + 
  geom_smooth(method = &amp;quot;lm&amp;quot;) + 
  theme_minimal()

mA_d_plot + Ti_d_plot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:4321/post/Mean-variance-relationships-and-SMDs_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This impression is also born out by a meta-regression that includes the group-&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; means and &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{T_i}\)&lt;/span&gt; as moderators:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rma(d ~ mA + sqrt(T_i), vi = V_d, data = dat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Mixed-Effects Model (k = 60; tau^2 estimator: REML)
## 
## tau^2 (estimated amount of residual heterogeneity):     0.0238 (SE = 0.0238)
## tau (square root of estimated tau^2 value):             0.1544
## I^2 (residual heterogeneity / unaccounted variability): 18.12%
## H^2 (unaccounted variability / sampling variability):   1.22
## R^2 (amount of heterogeneity accounted for):            91.60%
## 
## Test for Residual Heterogeneity:
## QE(df = 57) = 68.9706, p-val = 0.1330
## 
## Test of Moderators (coefficients 2:3):
## QM(df = 2) = 110.9125, p-val &amp;lt; .0001
## 
## Model Results:
## 
##            estimate      se     zval    pval    ci.lb    ci.ub      
## intrcpt      2.5225  0.3964   6.3632  &amp;lt;.0001   1.7455   3.2995  *** 
## mA          -2.8326  0.4336  -6.5321  &amp;lt;.0001  -3.6825  -1.9827  *** 
## sqrt(T_i)   -0.6109  0.0756  -8.0855  &amp;lt;.0001  -0.7590  -0.4628  *** 
## 
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here are the same plots as above, but using the log of the response ratio as the effect size metric:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mA_lRR_plot &amp;lt;- 
  ggplot(dat, aes(mA, lRR)) + 
  geom_point() + 
  geom_smooth(se = FALSE, color = &amp;quot;green&amp;quot;) + 
  geom_smooth(method = &amp;quot;lm&amp;quot;) + 
  scale_x_continuous(limits = c(0, 1), expand = c(0,0)) + 
  theme_minimal()

Ti_lRR_plot &amp;lt;- 
  ggplot(dat, aes(sqrt(T_i), lRR)) + 
  geom_point() + 
  geom_smooth(se = FALSE, color = &amp;quot;green&amp;quot;) + 
  geom_smooth(method = &amp;quot;lm&amp;quot;) + 
  theme_minimal()

mA_lRR_plot + Ti_lRR_plot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:4321/post/Mean-variance-relationships-and-SMDs_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;100%&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the left-hand plot, there does not appear to be any relationship between the effect size estimates and the group-&lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; means. In the right-hand plot, there does seem to be a mild relationship between the effect size estimates and &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{T_i}\)&lt;/span&gt;, which is a bit surprising, although the strength of the relationship is much weaker than what we saw with the standardized mean differences. Meta-regression analysis supports these interpretations:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rma(lRR ~  mA + sqrt(T_i), vi = V_lRR, data = dat)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Mixed-Effects Model (k = 60; tau^2 estimator: REML)
## 
## tau^2 (estimated amount of residual heterogeneity):     0.0019 (SE = 0.0011)
## tau (square root of estimated tau^2 value):             0.0439
## I^2 (residual heterogeneity / unaccounted variability): 32.87%
## H^2 (unaccounted variability / sampling variability):   1.49
## R^2 (amount of heterogeneity accounted for):            31.30%
## 
## Test for Residual Heterogeneity:
## QE(df = 57) = 84.4977, p-val = 0.0105
## 
## Test of Moderators (coefficients 2:3):
## QM(df = 2) = 10.6344, p-val = 0.0049
## 
## Model Results:
## 
##            estimate      se     zval    pval    ci.lb    ci.ub     
## intrcpt     -0.2362  0.0950  -2.4864  0.0129  -0.4224  -0.0500   * 
## mA           0.1061  0.0948   1.1196  0.2629  -0.0796   0.2918     
## sqrt(T_i)   -0.0553  0.0179  -3.0852  0.0020  -0.0904  -0.0202  ** 
## 
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now, you might think that a meta-analyst should get excited about the standardized mean difference results, since they’ve uncovered two systematic predictors of effect size magnitude. However, both of these factors are purely operational, arbitrary features of the (simulated) study designs, rather than theoretically or substantively interesting features of the studies. Considered in this light, the finding that they each moderate the magnitude of the standardized mean differences is, more than anything else, &lt;em&gt;annoying&lt;/em&gt;. If we wanted to examine other more theoretically interesting moderators, we’d have to do so in a way that accounts for these methodological predictors. At minimum, that would mean including them all in a meta-regression (leading to a model with 3+ predictors). Further, we would have to worry about whether the functional form of the regression is reasonable. Simply adding the theoretical moderator to the model amounts to assuming that it predicts effect size magnitude in a linear, additive fashion, but what if that’s not the right model? Since we know the true data-generating process here, we can see that the linear, additive model &lt;em&gt;would not&lt;/em&gt; be correct. But in practice, when we don’t know the true process, this would be much murkier.&lt;/p&gt;
&lt;p&gt;The general principle that I’m suggesting here is that effect sizes should ideally be on a metric that is &lt;em&gt;independent&lt;/em&gt; of arbitrary methodological factors because this should &lt;em&gt;reduce&lt;/em&gt; overall heterogeneity and &lt;em&gt;simplify&lt;/em&gt; the model, making it easier to detect real relations of interest. If one has a choice between several different effect size metrics, then a metric that shows clear associations with methodological factors should be discounted in favor of metrics that do not show such associations or show them only weakly. How to fully operationalize this sort of decision (as one would need to when writing a protocol for a meta-analysis, for example), I’m not yet sure about. It seems like a useful avenue for further methodological work.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Yes, there are other ways to define the SMD. Yes, usually we use the standard deviation pooled across both groups. I’m going to use the standard deviation in group &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; alone because it simplifies some of the mathy bits. Please feel free to work through the case with a pooled SD for yourself.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;One of the vexing things about simulations is that you often end up needing to specify a bunch of assumptions about auxiliary quantities, beyond those of the model you’re actually interested in investigating.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Standardized mean differences in single-group, repeated measures designs</title>
      <link>http://localhost:4321/smds-in-single-group/</link>
      <pubDate>Wed, 06 Oct 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/smds-in-single-group/</guid>
      <description>
&lt;script src=&#34;http://localhost:4321/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;I received a question from a colleague about computing variances and covariances for standardized mean difference effect sizes from a design involving a single group, measured repeatedly over time. Deriving these quantities is a little exercise in normal distribution theory, which I find kind of relaxing sometimes (hey, we all have our coping mechanisms!).&lt;/p&gt;
&lt;div id=&#34;the-set-up&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The set-up&lt;/h1&gt;
&lt;p&gt;Consider a study in which a single group of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; participants was measured at each of &lt;span class=&#34;math inline&#34;&gt;\(T + 1\)&lt;/span&gt; time-points, indexed as &lt;span class=&#34;math inline&#34;&gt;\(t = 0,...,T\)&lt;/span&gt;. At the first time-point, there has not yet been any exposure to an intervention. At the second and subsequent time-points, there is some degree of exposure, and so we are interested in describing change between time point &lt;span class=&#34;math inline&#34;&gt;\(t &amp;gt; 0\)&lt;/span&gt; and time-point 0. For each time-point, we have a sample mean &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}_t\)&lt;/span&gt; and a sample standard deviation &lt;span class=&#34;math inline&#34;&gt;\(s_{t}\)&lt;/span&gt;. For now, assume that there is complete response. Let &lt;span class=&#34;math inline&#34;&gt;\(\mu_t\)&lt;/span&gt; denote the population mean and &lt;span class=&#34;math inline&#34;&gt;\(\sigma_t\)&lt;/span&gt; denote the population standard deviation, both at time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(\rho_{st}\)&lt;/span&gt; denote the correlation between outcomes measured at time &lt;span class=&#34;math inline&#34;&gt;\(s\)&lt;/span&gt; and time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;, for &lt;span class=&#34;math inline&#34;&gt;\(s,t = 0,..,T\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\rho_{tt} = 1\)&lt;/span&gt;. We might also have sample correlations for each time point, denoted &lt;span class=&#34;math inline&#34;&gt;\(r_{st}\)&lt;/span&gt;. We calculate a standardized mean difference for each time-point &lt;span class=&#34;math inline&#34;&gt;\(t &amp;gt; 0\)&lt;/span&gt; by taking
&lt;span class=&#34;math display&#34;&gt;\[
d_t = \frac{\bar{y}_t - \bar{y}_0}{s_P},
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(s_p\)&lt;/span&gt; is the sample standard deviation pooled across all time-points:
&lt;span class=&#34;math display&#34;&gt;\[
s_P^2 = \frac{1}{T+1}\sum_{t=0}^T s_t^2.
\]&lt;/span&gt;
The question is then, what is &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}(d_t)\)&lt;/span&gt; and what is &lt;span class=&#34;math inline&#34;&gt;\(\text{Cov}(d_s, d_t)\)&lt;/span&gt;, for &lt;span class=&#34;math inline&#34;&gt;\(s,t = 1,...,T\)&lt;/span&gt;?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-results&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The results&lt;/h1&gt;
&lt;p&gt;Define the &lt;em&gt;unstandardized&lt;/em&gt; mean difference between time-point &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; and time-point 0 as &lt;span class=&#34;math inline&#34;&gt;\(D_t = \bar{y}_t - \bar{y}_0\)&lt;/span&gt;. Then, from the algebra of variances and covariances, we have
&lt;span class=&#34;math display&#34;&gt;\[
\text{Var}(D_t) = \frac{1}{n}\left(\sigma_0^2 + \sigma_t^2 - 2 \rho_{t0} \sigma_0 \sigma_t\right)
\]&lt;/span&gt;
and
&lt;span class=&#34;math display&#34;&gt;\[\text{Cov}(D_s, D_t) = \frac{1}{n}\left[\sigma_0^2 + \rho_{st} \sigma_s \sigma_t - \sigma_0 \left(\rho_{s0} \sigma_s + \rho_{t0} \sigma_t\right) \right].\]&lt;/span&gt;
From a &lt;a href=&#34;http://localhost:4321/distribution-of-sample-variances/&#34;&gt;previous post&lt;/a&gt; about the distribution of sample variances, we have that
&lt;span class=&#34;math display&#34;&gt;\[
\text{Cov}(s_s^2, s_t^2) = \frac{2 \left(\rho_{st} \sigma_s \sigma_t\right)^2}{n - 1}.
\]&lt;/span&gt;
Consequently,
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\text{Var}(s_P^2) &amp;amp;= \frac{1}{(T + 1)^2} \sum_{s=0}^T \sum_{t=0}^T \text{Cov}(s_s^2, s_t^2) \\
&amp;amp;= \frac{2}{(n-1)(T + 1)^2} \sum_{s=0}^T \sum_{t=0}^T \left(\rho_{st} \sigma_s \sigma_t\right)^2.
\end{aligned}
\]&lt;/span&gt;
Let &lt;span class=&#34;math inline&#34;&gt;\(\sigma_P^2 = \frac{1}{T+1}\sum_{t=0}^T \sigma_t^2\)&lt;/span&gt; denote the average population variance across all &lt;span class=&#34;math inline&#34;&gt;\(T + 1\)&lt;/span&gt; time-points, and let &lt;span class=&#34;math inline&#34;&gt;\(\delta_t\)&lt;/span&gt; denote the standardized mean difference parameter at time &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;. Then, following the &lt;a href=&#34;http://localhost:4321/multivariate-delta-method/&#34;&gt;multivariate delta method&lt;/a&gt;,
&lt;span class=&#34;math display&#34;&gt;\[
\text{Var}(d_t) \approx \frac{\text{Var}(D_t)}{\sigma_P^2} + \frac{\delta_t^2}{2 \nu} \qquad \text{and} \qquad \text{Cov}(d_s, d_t) \approx \frac{\text{Cov}(D_s, D_t)}{\sigma_P^2} + \frac{\delta_s \delta_t}{2 \nu},
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\displaystyle{\nu = \frac{2 \sigma_P^4}{\text{Var}(s_P^2)}}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Without imposing further assumptions, and assuming that we have access to the sample correlations between time-points, a feasible estimator of the sampling variance of &lt;span class=&#34;math inline&#34;&gt;\(d_t\)&lt;/span&gt; is
&lt;span class=&#34;math display&#34;&gt;\[
V_t = \frac{s_0^2 + s_t^2 - 2 r_{t0} s_0 s_t}{n s_P^2} + \frac{d_t^2}{2 \hat\nu},
\]&lt;/span&gt;
where
&lt;span class=&#34;math display&#34;&gt;\[
\hat\nu = \frac{(n-1) s_p^4}{\frac{1}{(T + 1)^2}\sum_{s=0}^T \sum_{t=0}^T r_{st}^2 s_s^2 s_t^2}.
\]&lt;/span&gt;
Similarly, a feasible estimator for the covariance between &lt;span class=&#34;math inline&#34;&gt;\(d_s\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(d_t\)&lt;/span&gt; is
&lt;span class=&#34;math display&#34;&gt;\[
C_{st} = \frac{s_0^2 + r_{st} s_s s_t - s_0 \left(r_{s0} s_s + r_{t0} s_t\right)}{n s_P^2} + \frac{d_s d_t}{2 \hat\nu}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In some cases, it might be reasonable to use further assumptions about distributional structure in order to simplify these approximations. In particular, suppose we assume that the population variances are constant across time-points, &lt;span class=&#34;math inline&#34;&gt;\(\sigma_0 = \sigma_1 = \cdots = \sigma_T\)&lt;/span&gt;. In this case, the variances and covariances no longer depend on the scale of the outcome, and we have
&lt;span class=&#34;math display&#34;&gt;\[
\hat\nu = \frac{(n - 1)(T + 1)}{T R + 1}, \qquad \text{where} \qquad R = \frac{2}{T (T + 1)}\sum_{s=0}^{T-1} \sum_{t=s+1}^T r_{st}^2
\]&lt;/span&gt;
(here, &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; is the average of the squared correlations between pairs of distinct time-points). Since &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt; will always be less than 1, &lt;span class=&#34;math inline&#34;&gt;\(\hat\nu\)&lt;/span&gt; will always be larger than &lt;span class=&#34;math inline&#34;&gt;\(n - 1\)&lt;/span&gt;. If sample correlations aren’t reported or available, it would seem fairly reasonable to use &lt;span class=&#34;math inline&#34;&gt;\(\hat\nu = n - 1\)&lt;/span&gt;, or to make a rough assumption about the average squared correlation &lt;span class=&#34;math inline&#34;&gt;\(R\)&lt;/span&gt;. With the approximate degrees of freedom &lt;span class=&#34;math inline&#34;&gt;\(\hat\nu\)&lt;/span&gt;, the variances and covariances are then given by
&lt;span class=&#34;math display&#34;&gt;\[
V_t = \frac{2(1 - r_{t0})}{n} + \frac{d_t^2}{2 \hat\nu} \qquad \text{and} \qquad C_{st} = \frac{1 + r_{st} - r_{s0} - r_{t0}}{n} + \frac{d_s d_t}{2 \hat\nu}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;extension&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Extension&lt;/h1&gt;
&lt;p&gt;In some contexts, one might encounter a design that uses &lt;em&gt;over-lapping&lt;/em&gt; but &lt;em&gt;not identical&lt;/em&gt; samples at each time-point. For instance, in a rotating panel survey, each participant is measured repeatedly for some small number of time-points &lt;span class=&#34;math inline&#34;&gt;\(p &amp;lt; T + 1\)&lt;/span&gt; (say &lt;span class=&#34;math inline&#34;&gt;\(p = 2\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(p = 3\)&lt;/span&gt;), and new participants are added to the sample with each new time-point. The simple repeated measures set-up that I described in this post is an imperfect approximation for such designs. In dealing with such a design, suppose that one knew the total number of observations at each time-point, denoted &lt;span class=&#34;math inline&#34;&gt;\(n_t\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(t = 0,...,T\)&lt;/span&gt;, as well as the number of observations that were common across any pair of time-points, denoted as &lt;span class=&#34;math inline&#34;&gt;\(n_{st}\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(s,t = 0,...,T\)&lt;/span&gt;. Further suppose that the drop-outs and additions are ignorable (missing completely at random), so that any subset of participants defined by a pattern of response or non-response is still representative of the full population. I leave it as an exercise for the reader (a relaxing and fun one!) to derive &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}(d_t)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\text{Cov}(d_s, d_t)\)&lt;/span&gt; under such a model.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Finding the distribution of significant effect sizes</title>
      <link>http://localhost:4321/number-of-significant-effects/</link>
      <pubDate>Tue, 27 Apr 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/number-of-significant-effects/</guid>
      <description>
&lt;script src=&#34;http://localhost:4321/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;In basic meta-analysis, where each study contributes just a single effect size estimate, there has been a lot of work devoted to developing models for selective reporting. Most of these models formulate the selection process as a function of the statistical significance of the effect size estimate; some also allow for the possibility that the precision of the study’s effect influences the probability of selection (i.e., bigger studies are more likely to be reported, regardless of statistical significance).&lt;/p&gt;
&lt;p&gt;A problem that I’ve been mulling recently is how to think about selective reporting in meta-analyses that include some studies with &lt;em&gt;multiple&lt;/em&gt; effect size estimates. This setting is quite a bit more complicated than basic meta-analysis because there are several different ways that selective reporting could happen. It could be that each effect size estimate is selected (or censored) individually, on the basis of its statistical significance. However, it seems just as plausible that the pattern of statistical significance across the full set of results could influence whether &lt;em&gt;any&lt;/em&gt; of the results get selected.&lt;/p&gt;
&lt;p&gt;In pondering this stuff, I’m trying to find ways to simplify the space of possibilities or formulate stylized (or “toy”) problems that are more tractable. Here is one such problem. I’ll write it as a question such as you might find in a problem set from a course on statistical distribution theory.&lt;/p&gt;
&lt;div id=&#34;the-general-problem&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The general problem&lt;/h1&gt;
&lt;p&gt;Consider a study that assesses some effect size across &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; different outcomes. Let &lt;span class=&#34;math inline&#34;&gt;\(T_i\)&lt;/span&gt; denote the effect size estimate for outcome &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, let &lt;span class=&#34;math inline&#34;&gt;\(V_i\)&lt;/span&gt; denote the sampling variance of the effect size estimate for outcome &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, and let &lt;span class=&#34;math inline&#34;&gt;\(\theta_i\)&lt;/span&gt; denote the true effect size parameter for corresponding to outcome &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;. Assume that
&lt;span class=&#34;math display&#34;&gt;\[T_i \sim N(\theta_i, V_i),\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(V_i\)&lt;/span&gt; is known. Define &lt;span class=&#34;math inline&#34;&gt;\(A_i\)&lt;/span&gt; as an indicator that is equal to one if &lt;span class=&#34;math inline&#34;&gt;\(T_i\)&lt;/span&gt; is statistically significant at level &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; based on a one-sided test, and otherwise equal to zero. (Equivalently, let &lt;span class=&#34;math inline&#34;&gt;\(A_i\)&lt;/span&gt; be equal to one if the effect is statistically significant at level &lt;span class=&#34;math inline&#34;&gt;\(2 \alpha\)&lt;/span&gt; and in the theoretically expected direction.) Formally,
&lt;span class=&#34;math display&#34;&gt;\[A_i = I\left(\frac{T_i}{\sqrt{V_i}} &amp;gt; q_\alpha \right)\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(q_\alpha = \Phi^{-1}(1 - \alpha)\)&lt;/span&gt; is the critical value from a standard normal distribution (e.g., &lt;span class=&#34;math inline&#34;&gt;\(q_{.05} = 1.645\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(q_{.025} = 1.96\)&lt;/span&gt;). Let &lt;span class=&#34;math inline&#34;&gt;\(N_A = \sum_{i=1}^m A_i\)&lt;/span&gt; denote the total number of statistically significant effect sizes in the study. Our general interest is in the distribution of &lt;span class=&#34;math inline&#34;&gt;\(N_A\)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;compound-symmetry&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Compound symmetry&lt;/h2&gt;
&lt;p&gt;In general, the distribution of &lt;span class=&#34;math inline&#34;&gt;\(N_A\)&lt;/span&gt; will depend on the joint distribution of &lt;span class=&#34;math inline&#34;&gt;\((T_1,...,T_m)\)&lt;/span&gt;, so we will need to make some further assumptions regarding that joint distribution in order to make progress here. One simplifying assumption that seems worth considering is that the effect size estimates follow a compound symmetric distribution. Specifically, assume that all of the effect size estimates have equal sampling variance, &lt;span class=&#34;math inline&#34;&gt;\(V_1 = V_2 = \cdots = V_m = V\)&lt;/span&gt;, and that there is a constant correlation between every pair of effect size estimates:
&lt;span class=&#34;math display&#34;&gt;\[\text{Cov}(T_h, T_i) = \rho V\]&lt;/span&gt;
for some correlation &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;. Further, assume that the true effect sizes vary based on a compound symmetric distribution where
&lt;span class=&#34;math display&#34;&gt;\[
\theta_i \sim N(\mu, \omega^2).
\]&lt;/span&gt;
All of this implies that the joint distribution of the effect size estimates is compound symmetric:
&lt;span class=&#34;math display&#34;&gt;\[
\left(\begin{array}{c} T_1 \\ T_2 \\ \vdots \\ T_m \end{array}\right) \sim N\left[ \mu \mathbf{1}_m, \ \left(\omega^2 + \rho V\right)\mathbf{J}_m + (1 - \rho) V \mathbf{I}_m \right],
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{1}_m\)&lt;/span&gt; is an &lt;span class=&#34;math inline&#34;&gt;\(m \times 1\)&lt;/span&gt; vector of 1’s, &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{J}_m\)&lt;/span&gt; is an &lt;span class=&#34;math inline&#34;&gt;\(m \times m\)&lt;/span&gt; matrix of 1’s, and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{I}_m\)&lt;/span&gt; is an &lt;span class=&#34;math inline&#34;&gt;\(m \times m\)&lt;/span&gt; identity matrix.&lt;/p&gt;
&lt;p&gt;Given the above assumptions, What is the distribution of &lt;span class=&#34;math inline&#34;&gt;\(N_A\)&lt;/span&gt;?&lt;/p&gt;
&lt;p&gt;Please write to me if you’d like to discuss the theory or implications of this problem.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Simulating correlated standardized mean differences for meta-analysis</title>
      <link>http://localhost:4321/simulating-correlated-smds/</link>
      <pubDate>Mon, 30 Sep 2019 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/simulating-correlated-smds/</guid>
      <description>


&lt;p&gt;As I’ve discussed in &lt;a href=&#34;http://localhost:4321/Sometimes-aggregating-effect-sizes-is-fine&#34;&gt;previous posts&lt;/a&gt;, meta-analyses in psychology, education, and other areas often include studies that contribute multiple, statistically dependent effect size estimates.
I’m interested in methods for meta-analyzing and meta-regressing effect sizes from data structures like this, and studying this sort of thing often entails conducting Monte Carlo simulations.
Monte Carlo simulations involve generating artificial data—in this case, a set of studies, each of which has one or more dependent effect size estimates—that follows a certain distributional model, applying different analytic methods to the artificial data, and then repeating the process a bunch of times.
Because we know the true parameters that govern the data-generating process, we can evaluate the performance of the analytic methods in terms of bias, accuracy, hypothesis test calibration and power, confidence interval coverage, and the like.&lt;/p&gt;
&lt;p&gt;In this post, I’ll discuss two alternative methods to simulate meta-analytic datasets that include studies with multiple, dependent effect size estimates: simulating individual participant-level data or simulating summary statistics. I’ll focus on the case of the standardized mean difference (SMD) because it is so common in meta-analyses of intervention studies. For simplicity, I’ll assume that the effect sizes all come from simple, two-group comparisons (without any covariate adjustment or anything like that) and that the individual observations are multi-variate normally distributed within each group. Our goal will be to simulate a set of &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; studies, where study &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; is based on measuring &lt;span class=&#34;math inline&#34;&gt;\(J_k\)&lt;/span&gt; outcomes on a sample of &lt;span class=&#34;math inline&#34;&gt;\(N_k\)&lt;/span&gt; participants, all for &lt;span class=&#34;math inline&#34;&gt;\(k = 1,...,K\)&lt;/span&gt;.
Let &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\delta_k = (\delta_{1k} \cdots \delta_{J_k k})&amp;#39;\)&lt;/span&gt; be the &lt;span class=&#34;math inline&#34;&gt;\(J_k \times 1\)&lt;/span&gt; vector of true standardized mean differences for study &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;.
I’ll assume that we know these true effect size parameters for all &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; studies, so that I can avoid committing to any particular form of random effects model.&lt;/p&gt;
&lt;div id=&#34;simulating-individual-participant-level-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Simulating individual participant-level data&lt;/h1&gt;
&lt;p&gt;The most direct way to simulate this sort of effect size data is to generate outcome data for every artificial participant in every artificial study. Let &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Y}_{ik}^T\)&lt;/span&gt; be the &lt;span class=&#34;math inline&#34;&gt;\(J_k \times 1\)&lt;/span&gt; vector of outcomes for treatment group participant &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; in study &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, and let &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Y}_{ik}^C\)&lt;/span&gt; be the &lt;span class=&#34;math inline&#34;&gt;\(J_k \times 1\)&lt;/span&gt; vector outcomes for control group participant &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; in study &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, for &lt;span class=&#34;math inline&#34;&gt;\(i=1,...,N_k / 2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(k = 1,...,K\)&lt;/span&gt;. Assuming multi-variate normality of the outcomes, we can generate these outcome vectors as
&lt;span class=&#34;math display&#34;&gt;\[
\mathbf{Y}_{ik}^T \sim N\left(\boldsymbol\delta_k, \boldsymbol\Psi_k\right) \qquad \text{and}\qquad \mathbf{Y}_{ik}^C \sim N\left(\mathbf{0}, \boldsymbol\Psi_k\right),
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\Psi_k\)&lt;/span&gt; is the population correlation matrix of the outcomes in study &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;.
Note that I am setting the mean outcomes of the control group participants to zero and also specifying that the outcomes all have unit variance within each group.
After simulating data based on these distributions, the effect size estimates for each outcome can be calculated directly, following standard formulas.&lt;/p&gt;
&lt;p&gt;Here’s what this approach looks like in code.
It is helpful to simplify things by focusing on simulating just a single study with multiple, correlated effect sizes.
Focusing first on just the input parameters, a function might look like the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r_SMDs_raw &amp;lt;- function(delta, J, N, Psi) {
  # stuff
  return(ES_data)  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the above function skeleton, &lt;code&gt;delta&lt;/code&gt; would be the true effect size parameter &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\delta_k\)&lt;/span&gt;, &lt;code&gt;J&lt;/code&gt; would be the number of effect sizes to generate &lt;span class=&#34;math inline&#34;&gt;\((J_k)\)&lt;/span&gt;, &lt;code&gt;N&lt;/code&gt; is the total number of participants &lt;span class=&#34;math inline&#34;&gt;\((N_k)\)&lt;/span&gt;, and &lt;code&gt;Psi&lt;/code&gt; is a matrix of correlations between the outcomes &lt;span class=&#34;math inline&#34;&gt;\((\Psi_k)\)&lt;/span&gt;.
From these parameters, we’ll generate raw data, calculate effect size estimates and standard errors, and return the results in a little dataset.&lt;/p&gt;
&lt;p&gt;To make the function a little bit easier to use, I’m going overload the &lt;code&gt;Psi&lt;/code&gt; argument so that it can be a single number, indicating a common correlation between the outcomes. Thus, instead of having to feed in a &lt;span class=&#34;math inline&#34;&gt;\(J_k \times J_k\)&lt;/span&gt; matrix, you can specify a single correlation &lt;span class=&#34;math inline&#34;&gt;\(r_k\)&lt;/span&gt;, and the function will assume that all of the outcomes are equicorrelated. In code, the logic is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;if (!is.matrix(Psi)) Psi &amp;lt;- Psi + diag(1 - Psi, nrow = J)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s the function with the innards:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r_SMDs_raw &amp;lt;- function(delta, J, N, Psi) {

  require(mvtnorm) # for simulating multi-variate normal data
  
  # create Psi matrix assuming equicorrelation
  if (!is.matrix(Psi)) Psi &amp;lt;- Psi + diag(1 - Psi, nrow = J)
  
  # generate control group summary statistics
  Y_C &amp;lt;- rmvnorm(n = N / 2, mean = rep(0, J), sigma = Psi)
  ybar_C &amp;lt;- colMeans(Y_C)
  sd_C &amp;lt;- apply(Y_C, 2, sd)
  
  # generate treatment group summary statistics
  delta &amp;lt;- rep(delta, length.out = J)
  Y_T &amp;lt;- rmvnorm(n = N / 2, mean = delta, sigma = Psi)
  ybar_T &amp;lt;- colMeans(Y_T)
  sd_T &amp;lt;- apply(Y_T, 2, sd)

  # calculate Cohen&amp;#39;s d
  sd_pool &amp;lt;- sqrt((sd_C^2 + sd_T^2) / 2)
  ES &amp;lt;- (ybar_T - ybar_C) / sd_pool
  
  # calculate SE of d
  SE &amp;lt;- sqrt(4 / N + ES^2 / (2 * (N - 2)))

  data.frame(ES = ES, SE = SE, N = N)

}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In action:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;delta &amp;lt;- rnorm(4, mean = 0.2, sd = 0.1)
r_SMDs_raw(delta = delta, J = 4, N = 40, Psi = 0.6)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: mvtnorm&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            ES        SE  N
## 1 -0.19106514 0.3169863 40
## 2  0.18427227 0.3169334 40
## 3  0.25646209 0.3175932 40
## 4  0.00210429 0.3162279 40&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or if you’d rather specify the full &lt;span class=&#34;math inline&#34;&gt;\(\Psi_k\)&lt;/span&gt; matrix yourself:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Psi_k &amp;lt;- 0.6 + diag(0.4, nrow = 4)
Psi_k&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3] [,4]
## [1,]  1.0  0.6  0.6  0.6
## [2,]  0.6  1.0  0.6  0.6
## [3,]  0.6  0.6  1.0  0.6
## [4,]  0.6  0.6  0.6  1.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r_SMDs_raw(delta = delta, J = 4, N = 40, Psi = Psi_k)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           ES        SE  N
## 1 -0.1597097 0.3167580 40
## 2 -0.1717717 0.3168410 40
## 3 -0.4369032 0.3201744 40
## 4  0.0657410 0.3163177 40&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;exercises&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exercises&lt;/h2&gt;
&lt;p&gt;The function above is serviceable but quite basic. I can think of several additional features that one might like to have for use in research simulations, but I’m feeling both cheeky and lazy at the moment, so I’ll leave them for you, dear reader. Here are some suggested exercises:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Add an argument to the function, &lt;code&gt;Hedges_g = TRUE&lt;/code&gt;, which controls where the simulated effect size is Hedges’ &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt; or Cohen’s &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;. If it is Hedges’ g, make sure that the standard error is corrected too.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Add an argument to the function, &lt;code&gt;p_val = TRUE&lt;/code&gt;, which allows the user to control whether or not to return &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-values from the test of mean differences for each outcome. Note that the p-values should be for a test of the &lt;em&gt;raw&lt;/em&gt; mean differences between groups, rather than a test of the effect size &lt;span class=&#34;math inline&#34;&gt;\(\delta_{jk} = 0\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Add an argument to the function, &lt;code&gt;corr_mat = FALSE&lt;/code&gt;, which controls whether the function returns just the simulated effect sizes and SEs or both the simulated effect sizes and the full sampling variance-covariance matrix of the effect sizes. See &lt;a href=&#34;http://localhost:4321/correlations-between-SMDs&#34;&gt;here&lt;/a&gt; for the relevant formulas.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;simulating-summary-statistics&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Simulating summary statistics&lt;/h1&gt;
&lt;p&gt;Another approach to simulating SMDs is to sample from the distribution of the &lt;em&gt;summary statistics&lt;/em&gt; used in calculating the effect size. This approach should simplify the code, at the cost of having to use a bit of distribution theory. Let &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\bar{y}}_{Tk}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\bar{y}}_{Ck}\)&lt;/span&gt; be the &lt;span class=&#34;math inline&#34;&gt;\(J_k \times 1\)&lt;/span&gt; vectors of sample means for the treatment and control groups, respectively. Let &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{S}_k\)&lt;/span&gt; be the &lt;span class=&#34;math inline&#34;&gt;\(J_k \times J_k\)&lt;/span&gt; sample covariance matrix of the outcomes, pooled across the treatment and control groups. Again assuming multi-variate normality, and following the same notation as above:
&lt;span class=&#34;math display&#34;&gt;\[
\mathbf{\bar{y}}_{Ck} \sim N\left(\mathbf{0}, \frac{2}{N_k} \boldsymbol\Psi_k\right), \qquad \mathbf{\bar{y}}_{Tk} \sim N\left(\boldsymbol\delta_k, \frac{2}{N_k} \boldsymbol\Psi_k\right),
\]&lt;/span&gt;
and
&lt;span class=&#34;math display&#34;&gt;\[
\left(\mathbf{\bar{y}}_{Tk} - \mathbf{\bar{y}}_{Ck}\right) \sim N\left(\boldsymbol\delta_k, \frac{4}{N_k} \boldsymbol\Psi_k\right).
\]&lt;/span&gt;
This shows how we could directly simulate the numerator of the standardized mean difference.&lt;/p&gt;
&lt;p&gt;A &lt;a href=&#34;http://localhost:4321/distribution-of-sample-variances&#34;&gt;further bit of distribution theory&lt;/a&gt; says that the pooled sample covariance matrix follows a multiple of a &lt;a href=&#34;https://en.wikipedia.org/wiki/Wishart_distribution&#34;&gt;Wishart distribution&lt;/a&gt; with &lt;span class=&#34;math inline&#34;&gt;\(N_k - 2\)&lt;/span&gt; degrees of freedom and scale matrix &lt;span class=&#34;math inline&#34;&gt;\(\Psi_k\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[
(N_k - 2) \mathbf{S}_k \sim Wishart\left(N_k - 2, \Psi_k \right).
\]&lt;/span&gt;
Thus, to simulate the denominators of the SMD estimates, we can simulate a single Wishart matrix, pull out the diagonal entries, divide by &lt;span class=&#34;math inline&#34;&gt;\(N_k - 2\)&lt;/span&gt;, and take the square root. In all, we draw a single &lt;span class=&#34;math inline&#34;&gt;\(J_k \times 1\)&lt;/span&gt; observation from a multi-variate normal distribution and a single &lt;span class=&#34;math inline&#34;&gt;\(J_k \times J_k\)&lt;/span&gt; observation from a Wishart distribution. In contrast, the raw data approach requires simulating &lt;span class=&#34;math inline&#34;&gt;\(N_k\)&lt;/span&gt; observations from a multi-variate normal distribution, then calculating &lt;span class=&#34;math inline&#34;&gt;\(4 J_k\)&lt;/span&gt; summary statistics (M and SD for each group on each outcome).&lt;/p&gt;
&lt;div id=&#34;exercises-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exercises&lt;/h2&gt;
&lt;p&gt;Once again, I’ll leave it to you, dear reader, to do the fun programming bits:&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Create a modified version of the function &lt;code&gt;r_SMDs_raw&lt;/code&gt; that simulates summary statistics instead of raw data (Call it &lt;code&gt;r_SMDs_stats&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;Use the &lt;code&gt;microbenchmark&lt;/code&gt; package (or your preferred benchmarking tool) to compare the computational efficiency of both versions of the function.&lt;/li&gt;
&lt;li&gt;Check your work! Verify that both versions of the function generate the same distributions if the same parameters are used as input.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;which-approach-is-better&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Which approach is better?&lt;/h1&gt;
&lt;p&gt;Like many things in research, there’s no clearly superior method here. The advantage of the summary statistics approach is computational efficiency. It should generally be faster than the raw data approach, and if you need to generate 10,000 meta-analysis each with 80 studies in them, the computational savings might add up. On the other hand, computational efficiency isn’t everything.&lt;/p&gt;
&lt;p&gt;I see two potential advantages of the raw data approach. First is interpretability: simulating raw data is likely easier to understand. It feels tangible and familiar, harkening back to those bygone days we spent learning ANOVA, whereas the summary statistics approach requires a bit of distribution theory to follow (bookmark this blog post!). Second is extensibility: it is relatively straightforward to extend the approach to use other distributional models for the raw dat (perhaps you want to look at outcomes that follow a &lt;a href=&#34;https://en.wikipedia.org/wiki/Multivariate_t-distribution&#34;&gt;multi-variate &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; distribution&lt;/a&gt;?) or more complicated estimators of the SMD (difference-in-differences? covariate-adjusted? cluster-randomized trial?). To use the summary statistics approach in more complicated scenarios, you’d have to work out the sampling distributions for yourself, or locate the right reference.&lt;/p&gt;
&lt;p&gt;Of course, there’s also no need to choose between these two approaches. As I’m trying to hint at in Exercise 6, it’s actually useful to write both. Then, you can use the (potentially slower) raw data version to verify that the summary statistics version is correct.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simulating-full-meta-analyses&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Simulating full meta-analyses&lt;/h1&gt;
&lt;p&gt;So far we’ve got a data-generating function that simulates a single study’s worth of effect size estimates. To study meta-analytic methods, we’ll need to build out the function to simulate multiple studies. To do so, I think it’s useful to use the technique of &lt;a href=&#34;https://r4ds.had.co.nz/iteration.html&#34;&gt;mapping&lt;/a&gt;, as implemented in the &lt;code&gt;purrr&lt;/code&gt; package’s &lt;code&gt;map_*&lt;/code&gt; functions. The idea here is to first generate a “menu” of study-specific parameters for each of &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; studies, then apply the &lt;code&gt;r_SMDs&lt;/code&gt; function to each parameter set.&lt;/p&gt;
&lt;p&gt;Let’s consider how to do this for a simple random effects model, where the true effect size parameter is constant within each study (i.e., &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\delta_k = (\delta_k \cdots \delta_k)&amp;#39;\)&lt;/span&gt;), and in a model without covariates. We’ll need to generate a true effect for each study, along with a sample size, an outcome dimension, and a correlation between outcomes. For the true effects, I’ll assume that
&lt;span class=&#34;math display&#34;&gt;\[
\delta_k \sim N(\mu, \tau^2),
\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[
J_k \sim 2 + Poisson(3),
\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[
N_k \sim 20 + 2 \times Poisson(10),
\]&lt;/span&gt;
and
&lt;span class=&#34;math display&#34;&gt;\[
r_k \sim Beta\left(\rho \nu, (1 - \rho)\nu\right),
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\rho = \text{E}(r_k)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\nu &amp;gt; 0\)&lt;/span&gt; controls the variability of &lt;span class=&#34;math inline&#34;&gt;\(r_k\)&lt;/span&gt; across studies, with smaller &lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt; corresponding to more variable correlations.
Specifically, &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}(r_k) = \rho (1 - \rho) / (1 + \nu)\)&lt;/span&gt;.
These distributions are just made up, without any particular justification.&lt;/p&gt;
&lt;p&gt;Here’s what these distributional models look like in R code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;K &amp;lt;- 6
mu &amp;lt;- 0.2
tau &amp;lt;- 0.05
J_mean &amp;lt;- 5
N_mean &amp;lt;- 45
rho &amp;lt;- 0.6
nu &amp;lt;- 39

study_data &amp;lt;- 
  data.frame(
    delta = rnorm(K, mean = mu, sd = tau),
    J = 2 + rpois(K, J_mean - 2),
    N = 20 + 2 * rpois(K, (N_mean - 20) / 2),
    Psi = rbeta(K, rho * nu, (1 - rho) * nu)
  )

study_data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       delta J  N       Psi
## 1 0.1749657 6 56 0.6670410
## 2 0.1371771 4 52 0.7952095
## 3 0.1430044 2 46 0.5551301
## 4 0.1953675 6 46 0.5339670
## 5 0.1653242 4 42 0.5623903
## 6 0.1419457 7 40 0.6615825&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we have the “menu” of study-level characteristics, it’s just a matter of mapping the parameters to the data-generating function. One way to do this is with &lt;code&gt;pmap_df&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(purrr)
meta_data &amp;lt;- pmap_df(study_data, r_SMDs_raw, .id = &amp;quot;study&amp;quot;)
meta_data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    study           ES        SE  N
## 1      1  0.427048814 0.2704019 56
## 2      1  0.206502285 0.2679989 56
## 3      1  0.270244756 0.2685234 56
## 4      1  0.423149362 0.2703451 56
## 5      1  0.525878094 0.2720096 56
## 6      1  0.746186579 0.2767383 56
## 7      2 -0.005809721 0.2773507 52
## 8      2 -0.082222645 0.2774719 52
## 9      2  0.114670949 0.2775871 52
## 10     2 -0.001432641 0.2773501 52
## 11     3 -0.031231291 0.2949027 46
## 12     3  0.302264458 0.2966391 46
## 13     4  0.085338908 0.2950242 46
## 14     4 -0.062511255 0.2949592 46
## 15     4 -0.040178730 0.2949150 46
## 16     4 -0.082519741 0.2950151 46
## 17     4  0.207953122 0.2957160 46
## 18     4 -0.005713721 0.2948845 46
## 19     5  0.293666394 0.3103483 42
## 20     5  0.258312309 0.3099551 42
## 21     5  0.362126706 0.3112512 42
## 22     5  0.177656049 0.3092452 42
## 23     6 -0.115158991 0.3165035 40
## 24     6  0.094349350 0.3164129 40
## 25     6 -0.052996601 0.3162862 40
## 26     6 -0.042766762 0.3162658 40
## 27     6 -0.314584445 0.3182800 40
## 28     6  0.078519103 0.3163560 40
## 29     6 -0.103034241 0.3164486 40&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(meta_data$study)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 1 2 3 4 5 6 
## 6 4 2 6 4 7&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Putting it all together into a function, we have&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r_meta &amp;lt;- function(K, mu, tau, J_mean, N_mean, rho, nu) {
  require(purrr)
  
  study_data &amp;lt;- 
    data.frame(
      delta = rnorm(K, mean = mu, sd = tau),
      J = 2 + rpois(K, J_mean - 2),
      N = 20 + 2 * rpois(K, (N_mean - 20) / 2),
      Psi = rbeta(K, rho * nu, (1 - rho) * nu)
    )
  
  pmap_df(study_data, r_SMDs_raw, .id = &amp;quot;study&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;exercises-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exercises&lt;/h2&gt;
&lt;ol start=&#34;7&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Modify &lt;code&gt;r_meta&lt;/code&gt; so that it uses &lt;code&gt;r_SMDs_stats&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Add options to &lt;code&gt;r_meta&lt;/code&gt; for &lt;code&gt;Hedges_g&lt;/code&gt;, &lt;code&gt;p_val = TRUE&lt;/code&gt;, and &lt;code&gt;corr_mat = FALSE&lt;/code&gt; and ensure that these get passed along to the &lt;code&gt;r_SMDs&lt;/code&gt; function.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;One way to check that the &lt;code&gt;r_meta&lt;/code&gt; function is working properly is to generate a very large meta-analytic dataset, then to verify that the generated distributions align with expectations. Here’s a very large meta-analytic dataset:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;meta_data &amp;lt;- 
  r_meta(100000, mu = 0.2, tau = 0.05, 
         J_mean = 5, N_mean = 40, 
         rho = 0.6, nu = 39)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Compare the distribution of the simulated dataset against what you would expect to get based on the input parameters.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Modify the &lt;code&gt;r_meta&lt;/code&gt; function so that &lt;span class=&#34;math inline&#34;&gt;\(J_k\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(N_k\)&lt;/span&gt; are correlated, according to
&lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
J_k &amp;amp;\sim 2 + Poisson(\mu_J - 2) \\
N_k &amp;amp;\sim 20 + 2 \times Poisson\left(\frac{1}{2}(\mu_N - 20) + \alpha (J_k - \mu_J) \right)
\end{align}
\]&lt;/span&gt;
for user-specified values of &lt;span class=&#34;math inline&#34;&gt;\(\mu_J\)&lt;/span&gt; (the average number of outcomes per study), &lt;span class=&#34;math inline&#34;&gt;\(\mu_N\)&lt;/span&gt; (the average total sample size per study), and &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;, which controls the degree of dependence between &lt;span class=&#34;math inline&#34;&gt;\(J_k\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(N_k\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;a-challenge&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A challenge&lt;/h2&gt;
&lt;p&gt;The meta-analytic model that we’re using here is quite simple—simplistic, even—and for some simulation studies, something more complex might be needed. For example, we might need to generate data from a model that includes within-study random effects, as in:
&lt;span class=&#34;math display&#34;&gt;\[
\delta_{jk} = \mu + u_k + v_{jk}, \quad \text{where}\quad u_k \sim N(0, \tau^2), \quad v_{jk} \sim N(0, \omega^2).
\]&lt;/span&gt;
Even more complex would be to simulate from a multi-level meta-regression model
&lt;span class=&#34;math display&#34;&gt;\[
\delta_{jk} = \mathbf{x}_{jk} \boldsymbol\beta + u_k + v_{jk}, \quad \text{where}\quad u_k \sim N(0, \tau^2), \quad v_{jk} \sim N(0, \omega^2),
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{x}_{jk}\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(1 \times p\)&lt;/span&gt; row-vector of covariates describing outcome &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; in study &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\beta\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(p \times 1\)&lt;/span&gt; vector of meta-regression coefficients. In past work, I’ve done this by writing a data-generating function that takes a fixed design matrix &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X} = \left(\mathbf{x}_{11}&amp;#39; \cdots \mathbf{x}_{J_K K}&amp;#39;\right)&amp;#39;\)&lt;/span&gt; as an input argument, along with &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\beta\)&lt;/span&gt;. The design matrix would also include an identifier for each unique study. There are surely better (simpler, easier to follow) ways to implement the multi-level meta-regression model. I’ll once again leave it to you to work out an approach.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Sampling variance of Pearson r in a two-level design</title>
      <link>http://localhost:4321/variance-of-r-in-two-level-design/</link>
      <pubDate>Thu, 19 Apr 2018 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/variance-of-r-in-two-level-design/</guid>
      <description>


&lt;p&gt;Consider Pearson’s correlation coefficient, &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt;, calculated from two variables &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; with population correlation &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;. If one calculates &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; from a simple random sample of &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; observations, then its sampling variance will be approximately&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Var}(r) \approx \frac{1}{N}\left(1 - \rho^2\right)^2.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;But what if the observations are drawn from a multi-stage sample? If one uses the raw correlation between the observations (ignoring the multi-level structure), then the &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; will actually be a weighted average of within-cluster and between-cluster correlations (see Snijders &amp;amp; Bosker, 2012). Intuitively, I would expect that the sampling variance of the between-cluster correlation will be a function of the number of clusters (regardless of the number of observations per cluster), so the variance of &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; from a multi-stage sample would not necessarily be the same as that from a simple random sample. What is the sampling variance of &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; in this design?&lt;/p&gt;
&lt;p&gt;Let me be more precise here by formalizing the sampling process. Suppose that we have a sample with &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; clusters, &lt;span class=&#34;math inline&#34;&gt;\(n_j\)&lt;/span&gt; observations in cluster &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;, and total sample size &lt;span class=&#34;math inline&#34;&gt;\(N = \sum_{j=1}^m n_j\)&lt;/span&gt;. Assume that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
X_{ij} &amp;amp;= \mu_x + v^x_j + e^x_{ij} \\
Y_{ij} &amp;amp;= \mu_y + v^y_j + e^y_{ij},
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;for &lt;span class=&#34;math inline&#34;&gt;\(i=1,...,n_j\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(j=1,...,m\)&lt;/span&gt;, where&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\left[\begin{array}{c} v^x_j \\ v^y_j \end{array}\right] &amp;amp;\sim N\left(\left[\begin{array}{c}0 \\ 0 \end{array}\right], \left[\begin{array}{cc}\omega_x^2 &amp;amp; \phi \omega_x \omega_y \\ \phi \omega_x \omega_y &amp;amp; \omega_y^2\end{array}\right]\right) \\ 
\left[\begin{array}{c} e^x_{ij} \\ e^y_{ij} \end{array}\right] &amp;amp;\sim N\left(\left[\begin{array}{c}0 \\ 0 \end{array}\right], \left[\begin{array}{cc}\sigma_x^2 &amp;amp; \rho \sigma_x \sigma_y \\ \rho \sigma_x \sigma_y &amp;amp; \sigma_y^2\end{array}\right]\right)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and the error terms are mutually independent unless otherwise noted. The raw Pearson’s &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; is calculated using the total sums of squares and cross-products:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
r = \frac{SS_{xy}}{\sqrt{SS_{xx} SS_{yy}}},
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
SS_{xx} &amp;amp;= \sum_{j=1}^m \sum_{i=1}^{n_j} \left(X_{ij} - \bar{\bar{x}}\right)^2, \qquad \bar{\bar{x}} = \frac{1}{N} \sum_{j=1}^m \sum_{i=1}^{n_j} X_{ij} \\
SS_{xy} &amp;amp;= \sum_{j=1}^m \sum_{i=1}^{n_j} \left(Y_{ij} - \bar{\bar{y}}\right)^2, \qquad \bar{\bar{y}} = \frac{1}{N} \sum_{j=1}^m \sum_{i=1}^{n_j} Y_{ij} \\
SS_{xy} &amp;amp;= \sum_{j=1}^m \sum_{i=1}^{n_j} \left(X_{ij} - \bar{\bar{x}}\right) \left(Y_{ij} - \bar{\bar{y}}\right).
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;div id=&#34;common-correlation-and-icc&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Common correlation and ICC&lt;/h3&gt;
&lt;p&gt;The distribution of the total correlation seems to be pretty complicated. So far, I’ve been able to obtain the variance of &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; for a special case that makes some further, fairly restrictive assumptions. Specifically, assume that the correlation is constant across the two levels, so that &lt;span class=&#34;math inline&#34;&gt;\(\phi = \rho\)&lt;/span&gt;, and that the intra-class correlation of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is the same as that of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(k = \omega_x^2 / \sigma_x^2 = \omega_y^2 / \sigma_y^2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\psi = k / (k + 1) = \omega_x^2 / (\omega_x^2 + \sigma_x^2)\)&lt;/span&gt;. Then&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Var}(r) \approx \frac{(1 - \rho^2)^2}{\tilde{N}},
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\tilde{N} = \frac{N[g_1 k + 1]^2}{g_2 k^2 + 2 g_1 k + 1} \approx \frac{N}{1 + (g_2 - g_1^2)\psi^2},
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with &lt;span class=&#34;math inline&#34;&gt;\(\displaystyle{g_1 = 1 - \frac{1}{N^2}\sum_{j=1}^m n_j^2}\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\displaystyle{g_2 = \frac{1}{N}\sum_{j=1}^m n_j^2 - \frac{2}{N^2}\sum_{j=1}^m n_j^3 + \frac{1}{N^3} \left(\sum_{j=1}^m n_j^2 \right)^2}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;If the clusters are all of equal size &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;, then&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\tilde{N} = \frac{nm[k(m - 1) / m + 1]^2}{k^2 n (m - 1)/m + 2 k (m - 1) / m + 1} \approx \frac{N}{1 + (n - 1) \psi^2},
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The right-hand expression is a further approximation that will be very close to right so long as &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; is not too too small.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;z-transformation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Z-transformation&lt;/h3&gt;
&lt;p&gt;Under the (restrictive) assumptions of common correlation and equal ICCs, Fisher’s z transformation is variance-stabilizing (as it is under simple random sampling), so it seems reasonable to use&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Var}\left(z(r)\right) \approx \frac{1}{\tilde{N} - 3}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;design-effect&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Design effect&lt;/h3&gt;
&lt;p&gt;The design effect (&lt;span class=&#34;math inline&#34;&gt;\(DEF\)&lt;/span&gt;) is the ratio of the actual sampling variance of &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; to the sampling variance in a simple random sample of the same size. For the special case that I’ve described,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
DEF = \frac{N}{\tilde{N}} = 1 + (g_2 - g_1^2) \psi^2,
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;or with equal cluster-sizes, &lt;span class=&#34;math inline&#34;&gt;\(DEF = 1 + (n - 1)\psi^2\)&lt;/span&gt;. These expressions make it clear that the design effect for the correlation is &lt;em&gt;not&lt;/em&gt; equivalent to the well-known design effect for means or mean differences in cluster-randomized designs, which is &lt;span class=&#34;math inline&#34;&gt;\(1 + (n - 1)\psi\)&lt;/span&gt;. We need to take the &lt;em&gt;square&lt;/em&gt; of the ICC here, which will make the design effect for &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; &lt;em&gt;smaller&lt;/em&gt; than the design effect for a mean (or difference in means) based on the same sample.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;other-special-cases&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Other special cases&lt;/h3&gt;
&lt;p&gt;There are some further special cases that are not to hard to work out and could be useful as rough approximations at least. One is if the within-cluster correlation is zero &lt;span class=&#34;math inline&#34;&gt;\((\rho = 0)\)&lt;/span&gt; and we’re interested in the between-cluster correlation &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;. Then the total correlation can be corrected for what is essentially measurement error using formulas from &lt;a href=&#34;https://www.amazon.com/Methods-Meta-Analysis-Correcting-Research-Findings/dp/141290479X&#34;&gt;Hunter and Schmidt (2004)&lt;/a&gt;. A further specialization is if &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is a cluster-level measure, so that &lt;span class=&#34;math inline&#34;&gt;\(\sigma_x^2 = 0\)&lt;/span&gt;. I’ll consider these in a later post, perhaps.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The multivariate delta method</title>
      <link>http://localhost:4321/multivariate-delta-method/</link>
      <pubDate>Wed, 11 Apr 2018 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/multivariate-delta-method/</guid>
      <description>


&lt;p&gt;The delta method is surely one of the most useful techniques in classical statistical theory. It’s perhaps a bit odd to put it this way, but I would say that the delta method is something like the precursor to the bootstrap, in terms of its utility and broad range of applications—both are “first-line” tools for solving statistical problems. There are many good references on the delta-method, ranging from &lt;a href=&#34;https://en.wikipedia.org/wiki/Delta_method&#34;&gt;the Wikipedia page&lt;/a&gt; to a short introduction in &lt;em&gt;The American Statistician&lt;/em&gt; (&lt;a href=&#34;https://doi.org/10.1080%2F00031305.1992.10475842&#34;&gt;Oehlert, 1992&lt;/a&gt;). Many statistical theory textbooks also include a longer or shorter discussion of the method (e.g., Stuart &amp;amp; Ord, 1996; Casella &amp;amp; Berger, 2002).&lt;/p&gt;
&lt;p&gt;I use the delta method all the time in my work, especially to derive approximations to the sampling variance of some estimator (or covariance between two estimators). Here I’ll give one formulation of the multivariate delta method that I find particularly useful for this purpose. (This is nothing at all original. I’m only posting it on the off chance that others might find my crib notes helpful—and by “others” I mostly mean myself in six months…)&lt;/p&gt;
&lt;div id=&#34;multi-variate-delta-method-covariances&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Multi-variate delta method covariances&lt;/h3&gt;
&lt;p&gt;Suppose that we have a &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-dimensional vector of statistics &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{T} = \left(T_1,...,T_p \right)\)&lt;/span&gt; that converge in distribution to the parameter vector &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\theta = \left(\theta_1,...,\theta_p\right)\)&lt;/span&gt; and have asymptotic covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\Sigma / n\)&lt;/span&gt;, i.e.,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\sqrt{n} \left(\mathbf{T} - \boldsymbol\theta\right) \stackrel{D}{\rightarrow} N\left( \mathbf{0}, \boldsymbol\Sigma \right).
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now consider two functions &lt;span class=&#34;math inline&#34;&gt;\(f\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt;, both of which take vectors as inputs, return scalar quantities, and don’t have funky (discontinuous) derivatives. The asymptotic covariance between &lt;span class=&#34;math inline&#34;&gt;\(f(\mathbf{T})\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(g(\mathbf{T})\)&lt;/span&gt; is then approximately&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Cov} \left(f(\mathbf{T}), g(\mathbf{T}) \right) \approx \frac{1}{n} \sum_{j=1}^p \sum_{k=1}^p  \frac{\partial f}{ \partial \theta_j}\frac{\partial g}{ \partial \theta_k}\sigma_{jk}, 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{jk}\)&lt;/span&gt; is the entry in row &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; and column &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; of the matrix &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\Sigma\)&lt;/span&gt;. If the entries of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{T}\)&lt;/span&gt; are asymptotically uncorrelated , then this simplifies to&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Cov} \left(f(\mathbf{T}), g(\mathbf{T}) \right) \approx \frac{1}{n} \sum_{j=1}^p \frac{\partial f}{ \partial \theta_j}\frac{\partial g}{ \partial \theta_j} \sigma_{jj}. 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If we are interested in the variance of a single statistic, then the above formulas simplify further to&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Var} \left(f(\mathbf{T})\right) \approx \frac{1}{n} \sum_{j=1}^p \sum_{k=1}^p  \frac{\partial f}{ \partial \theta_j}\frac{\partial f}{ \partial \theta_k}\sigma_{jk} 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Var} \left(f(\mathbf{T}) \right) \approx \frac{1}{n}\sum_{j=1}^p \left(\frac{\partial f}{ \partial \theta_j}\right)^2 \sigma_{jj}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;in the case of uncorrelated &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{T}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Finally, if we are dealing with a univariate transformation &lt;span class=&#34;math inline&#34;&gt;\(f(\theta)\)&lt;/span&gt;, then of course the above simplifies even further to&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Var}\left(f(T)\right) = \left(\frac{\partial f}{\partial \theta}\right)^2 \text{Var}(T)
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;pearsons-r&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Pearson’s &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;These formulas are useful for all sorts of things. For example, they can be used to derive the sampling variance of Pearson’s correlation coefficient. Suppose we have a simple random sample of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; observations from a multivariate normal distribution with mean 0 and variance-covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\Phi = \left[\begin{array}{cc}\phi_{xx} &amp;amp; \phi_{xy} \\ \phi_{xy} &amp;amp; \phi_{yy} \end{array}\right]\)&lt;/span&gt;. Pearson’s correlation is calculated as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
r = \frac{s_{xy}}{\sqrt{s_{xx} s_{yy}}},
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(s_{xx}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(s_{yy}\)&lt;/span&gt; are sample variances and &lt;span class=&#34;math inline&#34;&gt;\(s_{xy}\)&lt;/span&gt; is the sample covariance. These sample variances and covariances are unbiased estimates of &lt;span class=&#34;math inline&#34;&gt;\(\phi_{xx}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\phi_{yy}\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\phi_{xy}\)&lt;/span&gt;, respectively. So in terms of the above notation, we have &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{T} = \left(s_{xx}, s_{yy}, s_{xy}\right)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\theta = \left(\phi_{xx}, \phi_{yy}, \phi_{xy}\right)\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\rho = \phi_{xy} / \sqrt{\phi_{xx} \phi_{yy}}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;From &lt;a href=&#34;http://localhost:4321/distribution-of-sample-variances&#34;&gt;a previous post&lt;/a&gt;, we can work out the variance-covariance matrix of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{T}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Var}\left(\sqrt{n - 1} \left[\begin{array}{c} s_{xx} \\ s_{yy} \\ s_{xy}\end{array}\right]\right) = \boldsymbol\Sigma = \left[\begin{array}{ccc} 2 \phi_{xx}^2 &amp;amp; &amp;amp; \\ 2 \phi_{xy}^2 &amp;amp; 2 \phi_{yy}^2 &amp;amp; \\ 2 \phi_{xy} \phi_{xx} &amp;amp; 2 \phi_{xy} \phi_{yy} &amp;amp; \phi_{xy}^2 + \phi_{xx} \phi_{yy}\end{array}\right].
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The last piece is to find the derivatives of &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; with respect to &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{T}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\frac{\partial r}{\partial \phi_{xy}} &amp;amp;= \phi_{xx}^{-1/2} \phi_{yy}^{-1/2} \\
\frac{\partial r}{\partial \phi_{xx}} &amp;amp;= -\frac{1}{2} \phi_{xy} \phi_{xx}^{-3/2} \phi_{yy}^{-1/2} \\
\frac{\partial r}{\partial \phi_{yy}} &amp;amp;= -\frac{1}{2} \phi_{xy} \phi_{xx}^{-1/2} \phi_{yy}^{-3/2}
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Putting the pieces together, we have&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
(n - 1) \text{Var}(r) &amp;amp;\approx \sigma_{11} \left(\frac{\partial r}{\partial \phi_{xy}}\right)^2 + \sigma_{22} \left(\frac{\partial r}{ \partial \phi_{xx}}\right)^2 + \sigma_{33} \left(\frac{\partial r}{ \partial \phi_{yy}}\right)^2 \\
&amp;amp; \qquad \qquad + 2 \sigma_{12} \frac{\partial r}{\partial \phi_{xy}}\frac{\partial r}{\partial \phi_{xx}} + 2 \sigma_{13} \frac{\partial r}{\partial \phi_{xy}}\frac{\partial r}{\partial \phi_{yy}}+ 2 \sigma_{23} \frac{\partial r}{\partial \phi_{xx}}\frac{\partial r}{\partial \phi_{yy}} \\
&amp;amp;= \frac{\phi_{xy}^2 + \phi_{xx} \phi_{yy}}{\phi_{xx} \phi_{yy}} + \frac{\phi_{xy}^2\phi_{xx}^2}{2 \phi_{xx}^3 \phi_{yy}} + \frac{\phi_{xy}^2\phi_{yy}^2}{2 \phi_{xx} \phi_{yy}^3} \\
&amp;amp; \qquad \qquad - \frac{2\phi_{xy} \phi_{xx}}{\phi_{xx}^2 \phi_{yy}} - \frac{2\phi_{xy} \phi_{yy}}{\phi_{xx} \phi_{yy}^2} + \frac{\phi_{xy}^4}{\phi_{xx}^2 \phi_{yy}^2} \\
&amp;amp;= 1 - 2\frac{\phi_{xy}^2}{\phi_{xx} \phi_{yy}} + \frac{\phi_{xy}^4}{\phi_{xx}^2 \phi_{yy}^2} \\
&amp;amp;= \left(1 - \rho^2\right)^2.
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;fishers-z-transformation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fisher’s &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;-transformation&lt;/h3&gt;
&lt;p&gt;Meta-analysts will be very familiar with Fisher’s &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt;-transformation of &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt;, given by &lt;span class=&#34;math inline&#34;&gt;\(z(\rho) = \frac{1}{2} \log\left(\frac{1 + \rho}{1 - \rho}\right)\)&lt;/span&gt;.
Fisher’s &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; is the variance-stabilizing (and also normalizing) transformation of &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt;, meaning that &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}\left(z(r)\right)\)&lt;/span&gt; is approximately a constant function of sample size, not depending on the degree of correlation &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;. We can see this using another application of the delta method:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\frac{\partial z}{\partial \rho} = \frac{1}{1 - \rho^2}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Thus,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Var}\left(z(r)\right) \approx \frac{1}{(1 - \rho^2)^2} \times \text{Var}(r) = \frac{1}{n - 1}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The variance of &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; is usually given as &lt;span class=&#34;math inline&#34;&gt;\(1 / (n - 3)\)&lt;/span&gt;, which is even closer to exact. Here we’ve obtained the variance of &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; using two applications of the delta-method. Because of &lt;a href=&#34;https://en.wikipedia.org/wiki/Chain_rule&#34;&gt;the chain rule&lt;/a&gt;, we’d have ended up with the same result if we’d gone straight from the sample variances and covariances, using the multivariate delta method and the derivatives of &lt;span class=&#34;math inline&#34;&gt;\(z\)&lt;/span&gt; with respect to &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\theta\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;covariances-between-correlations&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Covariances between correlations&lt;/h3&gt;
&lt;p&gt;These same techniques can be used to work out expressions for the covariances between correlations estimated on the same sample. For instance, suppose you’ve measured four variables, &lt;span class=&#34;math inline&#34;&gt;\(W\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(Z\)&lt;/span&gt;, on a simple random sample of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; observations. What is &lt;span class=&#34;math inline&#34;&gt;\(\text{Cov}(r_{xy}, r_{xz})\)&lt;/span&gt;? What is &lt;span class=&#34;math inline&#34;&gt;\(\text{Cov}(r_{wx}, r_{yz})\)&lt;/span&gt;? I’ll leave the derivations for you to work out. See &lt;a href=&#34;http://dx.doi.org/10.1037//0033-2909.87.2.245&#34;&gt;Steiger (1980)&lt;/a&gt; for solutions.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>2SLS standard errors and the delta-method</title>
      <link>http://localhost:4321/delta-method-and-2sls-ses/</link>
      <pubDate>Sat, 07 Oct 2017 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/delta-method-and-2sls-ses/</guid>
      <description>


&lt;p&gt;I just covered instrumental variables in my course on causal inference, and so I have two-stage least squares (2SLS) estimation on the brain. In this post I’ll share something I realized in the course of prepping for class: that standard errors from 2SLS estimation are equivalent to delta method standard errors based on the Wald IV estimator. (I’m no econometrician, so this had never occurred to me before. Perhaps it will be interesting to other non-econometrician readers. And perhaps the econometricians can point me to the relevant page in Wooldridge or Angrist and Pischke or whomever that explains this better than I have.)&lt;/p&gt;
&lt;p&gt;Let’s consider a system with an outcome &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt;, a focal treatment &lt;span class=&#34;math inline&#34;&gt;\(t_i\)&lt;/span&gt; identified by a single instrument &lt;span class=&#34;math inline&#34;&gt;\(z_i\)&lt;/span&gt;, along with a row-vector of exogenous covariates &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{x}_i\)&lt;/span&gt;, all for &lt;span class=&#34;math inline&#34;&gt;\(i = 1,...,n\)&lt;/span&gt;. The usual estimating equations are:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
y_i &amp;amp;= \mathbf{x}_i \delta_0 + t_i \delta_1 + e_i \\
t_i &amp;amp;= \mathbf{x}_i \alpha_0 + z_i \alpha_1 + u_i.
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;With a single-instrument, the 2SLS estimator of &lt;span class=&#34;math inline&#34;&gt;\(\delta_1\)&lt;/span&gt; is exactly equivalent to the Wald estimator&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat\delta_1 = \frac{\hat\beta_1}{\hat\alpha_1},
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\hat\alpha_1\)&lt;/span&gt; is the OLS estimator from the first-stage regression of &lt;span class=&#34;math inline&#34;&gt;\(t_i\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{x}_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(z_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat\beta_1\)&lt;/span&gt; is the OLS estimator from the regression&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
y_i = \mathbf{x}_i \beta_0 + z_i \beta_1 + v_i.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The delta-method approximation for &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}(\hat\delta_1)\)&lt;/span&gt; is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Var}\left(\hat\delta_1\right) \approx \frac{1}{\alpha_1^2}\left[ \text{Var}\left(\hat\beta_1\right) + \delta_1^2 \text{Var}\left(\hat\alpha_1\right) - 2 \delta_1 \text{Cov}\left(\hat\beta_1, \hat\alpha_1\right) \right]. 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Substituting the estimators in place of parameters, and using heteroskedasticity-consistent (HC0, to be precise) estimators for &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}\left(\hat\beta_1\right)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}\left(\hat\alpha_1\right)\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\text{Cov}\left(\hat\beta_1, \hat\alpha_1\right)\)&lt;/span&gt;, it turns out the feasible delta-method variance estimator is &lt;em&gt;exactly&lt;/em&gt; equivalent to the HC0 variance estimator from 2SLS.&lt;/p&gt;
&lt;div id=&#34;connecting-delta-method-and-2sls&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Connecting delta-method and 2SLS&lt;/h3&gt;
&lt;p&gt;To demonstrate this claim, let’s first partial out the covariates, taking &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\ddot{y}} = \left[\mathbf{I} - \mathbf{X}\left(\mathbf{X}&amp;#39;\mathbf{X}\right)^{-1}\mathbf{X}&amp;#39;\right]\mathbf{y}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\ddot{t}} = \left[\mathbf{I} - \mathbf{X}\left(\mathbf{X}&amp;#39;\mathbf{X}\right)^{-1}\mathbf{X}&amp;#39;\right]\mathbf{t}\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\ddot{z}} = \left[\mathbf{I} - \mathbf{X}\left(\mathbf{X}&amp;#39;\mathbf{X}\right)^{-1}\mathbf{X}&amp;#39;\right]\mathbf{z}\)&lt;/span&gt;. The OLS estimators of &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\alpha_1\)&lt;/span&gt; are then&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat\beta_1 = \left(\mathbf{\ddot{z}}&amp;#39;\mathbf{\ddot{z}}\right)^{-1}\mathbf{\ddot{z}}&amp;#39;\mathbf{\ddot{y}}, \qquad \text{and} \qquad \hat\alpha_1 = \left(\mathbf{\ddot{z}}&amp;#39;\mathbf{\ddot{z}}\right)^{-1}\mathbf{\ddot{z}}&amp;#39;\mathbf{\ddot{t}}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The HC0 variance and covariance estimators for these coefficients have the usual sandwich form:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
V^{\beta_1} &amp;amp;= \left(\mathbf{\ddot{z}}&amp;#39;\mathbf{\ddot{z}}\right)^{-1}\left(\sum_{i=1}^n \ddot{z}_i^2 \ddot{v}_i^2\right) \left(\mathbf{\ddot{z}}&amp;#39;\mathbf{\ddot{z}}\right)^{-1} \\
V^{\alpha_1} &amp;amp;= \left(\mathbf{\ddot{z}}&amp;#39;\mathbf{\ddot{z}}\right)^{-1}\left(\sum_{i=1}^n \ddot{z}_i^2 \ddot{u}_i^2\right) \left(\mathbf{\ddot{z}}&amp;#39;\mathbf{\ddot{z}}\right)^{-1} \\
V^{\alpha_1\beta_1} &amp;amp;= \left(\mathbf{\ddot{z}}&amp;#39;\mathbf{\ddot{z}}\right)^{-1}\left(\sum_{i=1}^n \ddot{z}_i^2 \ddot{u}_i \ddot{v}_i\right) \left(\mathbf{\ddot{z}}&amp;#39;\mathbf{\ddot{z}}\right)^{-1},
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\ddot{v}_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\ddot{u}_i\)&lt;/span&gt; are the residuals from the regressions of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\ddot{y}}\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\ddot{z}}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\ddot{t}}\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\ddot{z}}\)&lt;/span&gt;, respectively. Combining all these terms, the delta-method variance estimator is then&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
V^{\delta_1} = \frac{1}{\hat\alpha_1^2}\left(\mathbf{\ddot{z}}&amp;#39;\mathbf{\ddot{z}}\right)^{-1}\left[\sum_{i=1}^n \ddot{z}_i^2\left(\ddot{v}_i^2 + \hat\delta_1^2 \ddot{u}_i^2 - 2 \hat\delta_1\ddot{u}_i \ddot{v}_i\right)\right] \left(\mathbf{\ddot{z}}&amp;#39;\mathbf{\ddot{z}}\right)^{-1}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Remember this formula because we’ll return to it shortly.&lt;/p&gt;
&lt;p&gt;Now consider the 2SLS estimator. To calculate this, we begin by taking the fitted values from the regression of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\ddot{t}}\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\ddot{z}}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\mathbf{\tilde{t}} = \mathbf{\ddot{z}}\left(\mathbf{\ddot{z}}&amp;#39;\mathbf{\ddot{z}}\right)^{-1}\mathbf{\ddot{z}}&amp;#39;\mathbf{\ddot{t}} = \mathbf{\ddot{z}} \hat\alpha_1.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We then regress &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\ddot{y}}\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\tilde{t}}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\hat\delta_1 = \left(\mathbf{\tilde{t}}&amp;#39;\mathbf{\tilde{t}}\right)^{-1} \mathbf{\tilde{t}}&amp;#39; \mathbf{\ddot{y}}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The HC0 variance estimator corresponding to the 2SLS estimator is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
V^{2SLS} = \left(\mathbf{\tilde{t}}&amp;#39;\mathbf{\tilde{t}}\right)^{-1} \left(\sum_{i=1}^n \tilde{t}_i^2 \tilde{e}_i^2 \right) \left(\mathbf{\tilde{t}}&amp;#39;\mathbf{\tilde{t}}\right)^{-1},
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\tilde{e}_i = \ddot{y}_i - \ddot{t}_i \hat\delta_1\)&lt;/span&gt;. Note that these residuals are calculated based on &lt;span class=&#34;math inline&#34;&gt;\(\ddot{t}_i\)&lt;/span&gt;, the &lt;em&gt;full&lt;/em&gt; treatment variable, not the fitted values &lt;span class=&#34;math inline&#34;&gt;\(\tilde{t}_i\)&lt;/span&gt;. The full treatment variable can be expressed as &lt;span class=&#34;math inline&#34;&gt;\(\ddot{t}_i = \tilde{t}_i + \ddot{u}_i\)&lt;/span&gt;, by which it follows that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\tilde{e}_i = \ddot{y}_i - \tilde{t}_i \hat\delta_1 - \ddot{u}_i \hat\delta_1.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;But &lt;span class=&#34;math inline&#34;&gt;\(\tilde{t}_i \hat\delta_1 = \ddot{z}_i \hat\alpha_1 \hat\delta_1 = \ddot{z}_i \hat\beta_1\)&lt;/span&gt;, and so&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\tilde{e}_i = \ddot{y}_i - \ddot{z}_i \hat\beta_1 - \ddot{u}_i \hat\delta_1 = \ddot{v}_i - \ddot{u}_i \hat\delta_1.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The 2SLS variance estimator is therefore&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
V^{2SLS} &amp;amp;= \left(\mathbf{\tilde{t}}&amp;#39;\mathbf{\tilde{t}}\right)^{-1} \left(\sum_{i=1}^n \tilde{t}_i^2 \tilde{e}_i^2 \right) \left(\mathbf{\tilde{t}}&amp;#39;\mathbf{\tilde{t}}\right)^{-1} \\
&amp;amp;= \left(\hat\alpha_1^2 \mathbf{\ddot{z}}&amp;#39;\mathbf{\ddot{z}}\right)^{-1} \left(\sum_{i=1}^n \hat\alpha_1^2 \ddot{z}_i^2 \tilde{e}_i^2 \right) \left(\hat\alpha_1^2 \mathbf{\ddot{z}}&amp;#39;\mathbf{\ddot{z}}\right)^{-1} \\
&amp;amp;= \frac{1}{\hat\alpha_1^2}\left(\mathbf{\ddot{z}}&amp;#39;\mathbf{\ddot{z}}\right)^{-1} \left(\sum_{i=1}^n \ddot{z}_i^2 \tilde{e}_i^2 \right) \left(\mathbf{\ddot{z}}&amp;#39;\mathbf{\ddot{z}}\right)^{-1} \\
&amp;amp;= \frac{1}{\hat\alpha_1^2}\left(\mathbf{\ddot{z}}&amp;#39;\mathbf{\ddot{z}}\right)^{-1} \left[\sum_{i=1}^n \ddot{z}_i^2 \left(\ddot{v}_i - \ddot{u}_i \hat\delta_1\right)^2 \right] \left(\mathbf{\ddot{z}}&amp;#39;\mathbf{\ddot{z}}\right)^{-1},
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which agrees with &lt;span class=&#34;math inline&#34;&gt;\(V^{\delta_1}\)&lt;/span&gt; as given above.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;so-what&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;So what?&lt;/h3&gt;
&lt;p&gt;If you’ve continued reading this far…I’m slightly amazed…but if you have, you may be wondering why it’s worth knowing about this relationship. The equivalence between the 2SLS variance estimator and the delta method interests me for a couple of reasons.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;First is that I had always taken the 2SLS variance estimator as being conditional on &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{t}\)&lt;/span&gt;–that is, not accounting for random variation in the treatment assignment. The delta-method form of the variance makes it crystal clear that this isn’t the case—the variance &lt;em&gt;does&lt;/em&gt; include terms for &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}(\hat\alpha_1)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\text{Cov}(\hat\beta_1, \hat\alpha_1)\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;On the other hand, there’s perhaps a sense that equivalence with the 2SLS variance estimator (the more familiar form) validates the delta method variance estimator—that is, we wouldn’t be doing something fundamentally different by using the delta method variance with a Wald estimator. For instance, we might want to estimate &lt;span class=&#34;math inline&#34;&gt;\(\alpha_1\)&lt;/span&gt; and/or &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; by some other means (e.g., by estimating &lt;span class=&#34;math inline&#34;&gt;\(\alpha_1\)&lt;/span&gt; as a marginal effect from a logistic regression or estimating &lt;span class=&#34;math inline&#34;&gt;\(\beta_1\)&lt;/span&gt; with a multi-level model). It would make good sense in this instance to use the Wald estimator &lt;span class=&#34;math inline&#34;&gt;\(\hat\beta_1 / \hat\alpha_1\)&lt;/span&gt; and to estimate its variance using the delta method form.&lt;/li&gt;
&lt;li&gt;One last reason I’m interested in this is that writing out the variance estimators will likely help in understanding how to approach small-sample corrections to &lt;span class=&#34;math inline&#34;&gt;\(V^{2SLS}\)&lt;/span&gt;. But I’ll save that for another day.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Alternative formulas for the standardized mean difference</title>
      <link>http://localhost:4321/alternative-formulas-for-the-smd/</link>
      <pubDate>Fri, 03 Jun 2016 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/alternative-formulas-for-the-smd/</guid>
      <description>


&lt;p&gt;The standardized mean difference (SMD) is surely one of the best known and most widely used effect size metrics used in meta-analysis. In generic terms, the SMD parameter is defined as the difference in population means between two groups (often this difference represents the effect of some intervention), scaled by the population standard deviation of the outcome metric. Estimates of the SMD can be obtained from a wide variety of experimental designs, ranging from simple, completely randomized designs, to repeated measures designs, to cluster-randomized trials.&lt;/p&gt;
&lt;p&gt;There’s some nuance involved in figuring out how to calculate estimates of the SMD from each design, mostly to do with exactly what sort of standard deviation to use in the denominator of the effect size. I’ll leave that discussion for another day. Here, I’d like to look at the question of how to estimate the sampling variance of the SMD. An estimate of the sampling variance is needed in order to meta-analyze a collection of effect sizes, and so getting the variance calculations right is an important (and sometimes time consuming) part of any meta-analysis project. However, the standard textbook treatments of effect size calculations cover this question only for a limited number of simple cases. I’d like to suggest a different, more general way of thinking about it, which provides a way to estimate the SMD and its variance in some non-standard cases (and also leads to slight differences from conventional formulas for the standard ones). All of this will be old hat for seasoned synthesists, but I hope it might be useful for students and researchers just getting started with meta-analysis.&lt;/p&gt;
&lt;p&gt;To start, let me review (regurgitate?) the standard presentation.&lt;/p&gt;
&lt;div id=&#34;smd-from-a-simple-independent-groups-design&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;SMD from a simple, independent groups design&lt;/h3&gt;
&lt;p&gt;Textbook presentations of the SMD estimator almost always start by introducing the estimator in the context of a &lt;strong&gt;simple, independent groups design&lt;/strong&gt;. Call the groups T and C, the sample sizes &lt;span class=&#34;math inline&#34;&gt;\(n_T\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(n_C\)&lt;/span&gt;, the sample means &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}_T\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}_C\)&lt;/span&gt;, and the sample variances &lt;span class=&#34;math inline&#34;&gt;\(s_T^2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(s_C^2\)&lt;/span&gt;. A basic moment estimator of the SMD is then&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
d = \frac{\bar{y}_T - \bar{y}_C}{s_p}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(s_p^2 = \frac{\left(n_T - 1\right)s_T^2 + \left(n_C - 1\right) s_C^2}{n_T + n_C - 2}\)&lt;/span&gt; is a pooled estimator of the population variance. The standard estimator for the sampling variance of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
V_d = \frac{n_T + n_C}{n_T n_C} + \frac{d^2}{2\left(n_T + n_C - 2\right)},
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;or some slight variant thereof. This estimator is based on a delta-method approximation for the asymptotic variance of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;It is well known that &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; has a small sample bias that depends on sample sizes. Letting&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
J(x) = 1 - \frac{3}{4x - 1},
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;the bias-corrected estimator is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
g = J\left(n_T + n_C - 2\right) \times d,
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and is often referred to as Hedges’ &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt; because it was proposed in &lt;a href=&#34;http://doi.org/10.3102/10769986006002107&#34;&gt;Hedges (1981)&lt;/a&gt;. Some meta-analysts use &lt;span class=&#34;math inline&#34;&gt;\(V_d\)&lt;/span&gt;, but with &lt;span class=&#34;math inline&#34;&gt;\(d^2\)&lt;/span&gt; replaced by &lt;span class=&#34;math inline&#34;&gt;\(g^2\)&lt;/span&gt;, as an estimator of the large-sample variance of &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt;; others use&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
V_g = J^2\left(n_T + n_C - 2\right) \left(\frac{n_T + n_C}{n_T n_C} + \frac{g^2}{2\left(n_T + n_C - 2\right)}\right).
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://doi.org/10.3102/1076998606298034&#34;&gt;Viechtbauer (2007)&lt;/a&gt; provides further details on variance estimation and confidence intervals for the SMD in this case.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-general-formula-for-g-and-its-sampling-variance&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A general formula for &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt; and its sampling variance&lt;/h3&gt;
&lt;p&gt;The above formulas are certainly useful, but in practice meta-analyses often include studies that use other, more complex designs.
Good textbook presentations also cover computation of &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt; and its variance for some other cases (e.g., Borenstein, 2009, also covers one-group pre/post designs and analysis of covariance). Less careful presentations only cover the simple, independent groups design and thus may inadvertently leave the impression that the variance estimator &lt;span class=&#34;math inline&#34;&gt;\(V_d\)&lt;/span&gt; given above applies in general. With other types of studies, &lt;span class=&#34;math inline&#34;&gt;\(V_d\)&lt;/span&gt; can be a wildly biased estimator of the actual sampling variance of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;, because it is derived under the assumption that the numerator of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; is estimated as the difference in means of two simple random samples. In some designs (e.g., ANCOVA designs, randomized block designs, repeated measures designs), the treatment effect estimate will be much more precise than this; in other designs (e.g., cluster-randomized trials), it will be less precise.&lt;/p&gt;
&lt;p&gt;Here’s what I think is a more useful way to think about the sampling variance of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;. Let’s suppose that we have an unbiased estimator for the difference in means that goes into the numerator of the SMD. Call this estimator &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, its sampling variance &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}(b)\)&lt;/span&gt;, and its standard error &lt;span class=&#34;math inline&#34;&gt;\(se_{b}\)&lt;/span&gt;. Also suppose that we have an unbiased (or reasonably close-to-unbiased) estimator of the population variance of the outcome, the square root of which goes into the denominator of the SMD. Call this estimator &lt;span class=&#34;math inline&#34;&gt;\(S^2\)&lt;/span&gt;, with expectation &lt;span class=&#34;math inline&#34;&gt;\(\text{E}\left(S^2\right) = \sigma^2\)&lt;/span&gt; and sampling variance &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}(S^2)\)&lt;/span&gt;. Finally, suppose that &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(S^2\)&lt;/span&gt; are independent (which will often be a pretty reasonable assumption). A delta-method approximation for the sampling variance of &lt;span class=&#34;math inline&#34;&gt;\(d = b / S\)&lt;/span&gt; is then&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Var}\left(d\right) \approx \frac{\text{Var}(b)}{\sigma^2} + \frac{\delta^2}{2 \nu},
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\nu = 2 \left[\text{E}\left(S^2\right)\right]^2 / \text{Var}\left(S^2\right)\)&lt;/span&gt;. Plugging in sample estimates of the relevant parameters provides a reasonable estimator for the sampling variance of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
V_d = \left(\frac{se_b}{S}\right)^2 + \frac{d^2}{2 \nu}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This estimator has two parts. The first part involves &lt;span class=&#34;math inline&#34;&gt;\(se_b / S\)&lt;/span&gt;, which is just the standard error of &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt;, but re-scaled into standard deviation units; this part captures the variability in &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; from its numerator. This scaled standard error can be calculated directly if an article reports &lt;span class=&#34;math inline&#34;&gt;\(se_b\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The second part of &lt;span class=&#34;math inline&#34;&gt;\(V_d\)&lt;/span&gt; is &lt;span class=&#34;math inline&#34;&gt;\(d^2 / (2 \nu)\)&lt;/span&gt;, which captures the variability in &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; due to its denominator. More precise estimates of &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; will have larger degrees of freedom, so that the second part will be smaller. For some designs, the degrees of freedom &lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt; depend only on sample sizes, and thus can be calculated exactly. For some other designs, &lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt; must be estimated.&lt;/p&gt;
&lt;p&gt;The same degrees of freedom can also be used in the small-sample correction for the bias of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;, as given by&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
g = J(\nu) \times d.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This small-sample correction is based on a Satterthwaite-type approximation to the distribution of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Here’s another way to express the variance estimator for &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
V_d = d^2 \left(\frac{1}{t^2} + \frac{1}{2 \nu}\right),
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; is the test statistic corresponding to the hypothesis test for no difference between groups. I’ve never seen that formula in print before, but it could be convenient if an article reports the &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; statistic (or &lt;span class=&#34;math inline&#34;&gt;\(F = t^2\)&lt;/span&gt; statistic).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;non-standard-estimators-of-d&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Non-standard estimators of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;The advantage of this formulation of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(V_d\)&lt;/span&gt; is that it can be applied in quite a wide variety of circumstances, including cases that aren’t usually covered in textbook treatments. Rather than having to use separate formulas for every combination of design and analytic approach under the sun, the same formulas apply throughout. What changes are the components of the formulas: the scaled standard error &lt;span class=&#34;math inline&#34;&gt;\(se_b / S\)&lt;/span&gt; and the degrees of freedom &lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt;. The general formulation also makes it easier to swap in different estimates of &lt;span class=&#34;math inline&#34;&gt;\(b\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;—i.e., if you estimate the numerator a different way but keep the denominator the same, you’ll need a new scaled standard error but can still use the same degrees of freedom. A bunch of examples:&lt;/p&gt;
&lt;div id=&#34;independent-groups-with-different-variances&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Independent groups with different variances&lt;/h4&gt;
&lt;p&gt;Suppose that we’re looking at two independent groups but do not want to assume that their variances are the same. In this case, it would make sense to standardize the difference in means by the control group standard deviation (without pooling), so that &lt;span class=&#34;math inline&#34;&gt;\(d = \left(\bar{y}_T - \bar{y}_C\right) / s_C\)&lt;/span&gt;. Since &lt;span class=&#34;math inline&#34;&gt;\(s_C^2\)&lt;/span&gt; has &lt;span class=&#34;math inline&#34;&gt;\(\nu = n_C - 1\)&lt;/span&gt; degrees of freedom, the small-sample bias correction will then need to be &lt;span class=&#34;math inline&#34;&gt;\(J(n_C - 1)\)&lt;/span&gt;. The scaled standard error will be&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\frac{se_b}{s_C} = \sqrt{\frac{s_T^2}{s_C^2 n_T} + \frac{1}{n_C}}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is then everything that we need to calculate &lt;span class=&#34;math inline&#34;&gt;\(V_d\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(V_g\)&lt;/span&gt;, etc.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;multiple-independent-groups&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Multiple independent groups&lt;/h4&gt;
&lt;p&gt;Suppose that the study involves &lt;span class=&#34;math inline&#34;&gt;\(K - 1\)&lt;/span&gt; treatment groups, 1 control group, and &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; total participants. If the meta-analysis will include SMDs comparing &lt;em&gt;each&lt;/em&gt; treatment group to the control group, it would make sense to pool the sample variance across all &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; groups rather than just the pair of groups, so that a common estimate of scale is used across all the effect sizes. The pooled standard deviation is then calculated as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
s_p^2 = \frac{1}{N - K} \sum_{k=0}^K (n_k - 1) s_k^2.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For a comparison between treatment group &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; and the control group, we would then use&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
d = \frac{\bar{y}_k - \bar{y}_C}{s_p}, \qquad \nu = N - K, \qquad \frac{se_b}{s_p} = \sqrt{\frac{1}{n_C} + \frac{1}{n_k}},
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(n_k\)&lt;/span&gt; is the sample size for treatment group &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; (cf. Gleser &amp;amp; Olkin, 2009).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;single-group-pre-test-post-test-design&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Single group, pre-test post-test design&lt;/h4&gt;
&lt;p&gt;Suppose that a study involves taking pre-test and post-test measurements on a single group of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; participants. Borenstein (2009) recommends calculating the standardized mean difference for this study as the difference in means between the post-test and pre-test, scaled by the pooled (across pre- and post-test measurements) standard deviation. With obvious notation:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
d = \frac{\bar{y}_{post} - \bar{y}_{pre}}{s_p}, \qquad \text{where} \qquad s_p^2 = \frac{1}{2}\left(s_{pre}^2 + s_{post}^2\right).
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In this design,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\frac{se_b}{s_p} = \sqrt{\frac{2(1 - r)}{n}},
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; is the sample correlation between the pre- and post-tests. The remaining question is what to use for &lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt;. Borenstein (2009) uses &lt;span class=&#34;math inline&#34;&gt;\(\nu = n - 1\)&lt;/span&gt;. My previous post &lt;a href=&#34;http://localhost:4321/distribution-of-sample-variances/&#34;&gt;on the sampling covariance of sample variances&lt;/a&gt; gave the result that &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}(s_p^2) = \sigma^4 (1 + \rho^2) / (n - 1)\)&lt;/span&gt;, which would instead suggest using&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\nu = \frac{2 (n - 1)}{1 + r^2}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This formula will tend to give slightly larger degrees of freedom, but probably won’t be that discrepant from Borenstein’s approach except in quite small samples. It would be interesting to investigate which approach is better in small samples (i.e., leading to less biased estimates of the SMD and more accurate estimates of sampling variance, and by how much), although its possible than neither is all that good because the variance estimator itself is based on a large-sample approximation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;two-group-pre-test-post-test-design-ancova-estimation&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Two group, pre-test post-test design: ANCOVA estimation&lt;/h4&gt;
&lt;p&gt;Suppose that a study involves taking pre-test and post-test measurements on two groups of participants, with sample sizes &lt;span class=&#34;math inline&#34;&gt;\(n_T\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(n_C\)&lt;/span&gt; respectively. One way to analyze this design is via ANCOVA using the pre-test measure as the covariate, so that the treatment effect estimate is the difference in adjusted post-test means. In this design, the scaled standard error will be approximately&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\frac{se_b}{S} = \sqrt{ \frac{(n_C + n_T)(1 - r^2)}{n_C n_T} },
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(r\)&lt;/span&gt; is the pooled, within-group sample correlation between the pre-test and the post-test measures (this approximation assumes that the pre-test SMD between groups is relatively small). Alternately, if &lt;span class=&#34;math inline&#34;&gt;\(se_b\)&lt;/span&gt; is provided then the scaled standard error could be calculated directly.&lt;/p&gt;
&lt;p&gt;Borenstein (2009) suggests calculating &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; as the difference in adjusted means, scaled by the pooled sample variances on the post-test measures. The post-test pooled sample variance will have the same degrees of freedom as in the two-sample t-test case: &lt;span class=&#34;math inline&#34;&gt;\(\nu = n_C + n_T - 2\)&lt;/span&gt;. (Borenstein instead uses &lt;span class=&#34;math inline&#34;&gt;\(\nu = n_C + n_T - 2 - q\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; is the number of covariates in the analysis, but this won’t usually make much difference unless the total sample size is quite small.)&lt;/p&gt;
&lt;p&gt;Scaling by the pooled post-test sample variance isn’t the only reasonable way to estimate the SMD though. If the covariate is a true pre-test, then why not scale by the pooled pre-test sample variance instead? To do so, you would need to calculate &lt;span class=&#34;math inline&#34;&gt;\(se_b / S\)&lt;/span&gt; directly and use &lt;span class=&#34;math inline&#34;&gt;\(\nu = n_C + n_T - 2\)&lt;/span&gt;. If it is reasonable to assume that the pre- and post-test population variances are equal, then another alternative would be to pool across the pre-test &lt;em&gt;and&lt;/em&gt; post-test sample variances in each group. Using this approach, you would again need to calculate &lt;span class=&#34;math inline&#34;&gt;\(se_b / S\)&lt;/span&gt; directly and then use &lt;span class=&#34;math inline&#34;&gt;\(\nu = 2(n_C + n_T - 2) / (1 + r^2)\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;two-group-pre-test-post-test-design-repeated-measures-estimation&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Two group, pre-test post-test design: repeated measures estimation&lt;/h4&gt;
&lt;p&gt;Another way to analyze the data from the same type of study design is to use repeated measures ANOVA. I’ve recently encountered a number of studies that use this approach (here’s a recent example from &lt;a href=&#34;http://dx.doi.org/10.1371/journal.pone.0154075&#34;&gt;a highly publicized study in PLOS ONE&lt;/a&gt;—see Table 2). The studies I’ve seen typically report the sample means and variances in each group and at each time point, from which the difference in change scores can be calculated. Let &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}_{gt}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(s_{gt}^2\)&lt;/span&gt; denote the sample mean and sample variance in group &lt;span class=&#34;math inline&#34;&gt;\(g = T, C\)&lt;/span&gt; at time &lt;span class=&#34;math inline&#34;&gt;\(t = 0, 1\)&lt;/span&gt;. The numerator of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; would then be calculated as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
b = \left(\bar{y}_{T1} - \bar{y}_{T0}\right) - \left(\bar{y}_{C1} - \bar{y}_{C0}\right),
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;which has sampling variance &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}(b) = 2(1 - \rho)\sigma^2\left(n_C + n_T \right) / (n_C n_T)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt; is the correlation between the pre-test and the post-test measures. Thus, the scaled standard error is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\frac{se_b}{S} = \sqrt{\frac{2(1 - r)(n_C + n_T)}{n_C n_T}}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;As with ANCOVA, there are several potential options for calculating the denominator of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Using the pooled sample variances on the post-test measures, with &lt;span class=&#34;math inline&#34;&gt;\(\nu = n_C + n_T - 2\)&lt;/span&gt;;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Using the pooled sample variances on the pre-test measures, with &lt;span class=&#34;math inline&#34;&gt;\(\nu = n_C + n_T - 2\)&lt;/span&gt;; or&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Using the pooled sample variances at both time points and in both groups, i.e.,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
  S^2 = \frac{(n_C - 1)(s_{C0}^2 + s_{C1}^2) + (n_T - 1)(s_{T0}^2 + s_{T1}^2)}{2(n_C + n_T - 2)},
  \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with &lt;span class=&#34;math inline&#34;&gt;\(\nu = 2(n_C + n_T - 2) / (1 + r^2)\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The range of approaches to scaling is the same as for ANCOVA. This makes sense because both analyses are based on data from the same study design, so the parameter of interest should be the same (i.e., the target parameter should not change based on the analytic method). Note that all of these approaches are a bit different than the effect size estimator proposed by &lt;a href=&#34;http://doi.org/10.1037//1082-989X.7.1.105&#34;&gt;Morris and DeShon (2002)&lt;/a&gt; for the two-group, pre-post design; their approach does not fit into my framework because it involves taking a difference between standardized effect sizes (and therefore involves two separate estimates of scale, rather than just one).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;randomized-trial-with-longitudinal-follow-up&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Randomized trial with longitudinal follow-up&lt;/h4&gt;
&lt;p&gt;Many independent-groups designs—especially randomized trials in field settings—involve repeated, longitudinal follow-up assessments. An increasingly common approach to analysis of such data is through hierarchical linear models, which can be used to account for the dependence structure among measurements taken on the same individual. In this setting, &lt;a href=&#34;http://doi.org/10.1037/a0014699&#34;&gt;Feingold (2009)&lt;/a&gt; proposes that the SMD be calculated as the model-based estimate of the treatment effect at the final follow-up time, scaled by the within-groups variance of the outcome at that time point. Let &lt;span class=&#34;math inline&#34;&gt;\(\hat\beta_1\)&lt;/span&gt; denote the estimated difference in slopes (change per unit time) between groups in a linear growth model, &lt;span class=&#34;math inline&#34;&gt;\(F\)&lt;/span&gt; denote the duration of the study, and &lt;span class=&#34;math inline&#34;&gt;\(s_{pF}^2\)&lt;/span&gt; denote the pooled sample variance of the outcome at the final time point. For this model, Feingold (2009) proposes to calculate the standardized mean difference as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
d = \frac{F \hat\beta_1}{s_{pF}}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In a later paper, &lt;a href=&#34;http://doi.org/10.1037/a0037721&#34;&gt;Feingold (2015)&lt;/a&gt; proposes that the sampling variance of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; be estimated as &lt;span class=&#34;math inline&#34;&gt;\(F \times se_{\hat\beta_1} / s_{pF}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(se_{\hat\beta_1}\)&lt;/span&gt; is the standard error of the estimated slope. My framework suggests that a better estimate of the sampling variance, which accounts for the uncertainty of the scale estimate, would be to use&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
V_d = \left(\frac{F \times se_{\hat\beta_1}}{s_{pF}}\right)^2 + \frac{d^2}{2 \nu},
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with &lt;span class=&#34;math inline&#34;&gt;\(\nu = n_T + n_C - 2\)&lt;/span&gt;. The same &lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt; could be used to bias-correct the effect size estimate.&lt;/p&gt;
&lt;p&gt;If estimates of the variance components of the HLM are reported, one could use them to construct a model-based estimate of the scale parameter in the denominator of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;. I explored this approach in a paper that uses HLM to model single-case designs, which are a certain type of longitudinal experiment that typically involve a very small number of participants (&lt;a href=&#34;http://doi.org/10.3102/1076998614547577&#34;&gt;Pustejovsky, Hedges, &amp;amp; Shadish, 2014&lt;/a&gt;). Estimates of the scale parameter can usually be written as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
S_{model}^2 = \mathbf{r}&amp;#39;\boldsymbol\omega,
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\omega\)&lt;/span&gt; is a vector of all the variance components in the model and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{r}\)&lt;/span&gt; is a vector of weights that depend on the model specification and length of follow-up. This estimate of scale will usually be more precise than &lt;span class=&#34;math inline&#34;&gt;\(s_{pF}^2\)&lt;/span&gt; because it makes use of all of the data (and modeling assumptions). However, it can be challenging to determine appropriate degrees of freedom for &lt;span class=&#34;math inline&#34;&gt;\(S_{model}^2\)&lt;/span&gt;. For single-case designs, I used estimates of &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}(\boldsymbol\omega)\)&lt;/span&gt; based on the inverse of the expected information matrix—call the estimate &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{V}_{\boldsymbol\omega}\)&lt;/span&gt;—in which case&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\nu = \frac{2 S_{model}^4}{\mathbf{r}&amp;#39; \mathbf{V}_{\boldsymbol\omega} \mathbf{r}}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;However, most published articles will not provide estimates of the sampling variances of the variance components—in fact, a lot of software for estimating HLMs does not even provide these. It would be useful to work out some reasonable approximations for the degrees of freedom in these models—approximations that can be calculated based on the information that’s typically available—and to investigate the extent to which there’s any practical benefit to using &lt;span class=&#34;math inline&#34;&gt;\(S_{model}^2\)&lt;/span&gt; over &lt;span class=&#34;math inline&#34;&gt;\(s_{pF}^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cluster-randomized-trials&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Cluster-randomized trials&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;http://doi.org/10.3102/1076998606298043&#34;&gt;Hedges (2007)&lt;/a&gt; addresses estimation of standardized mean differences for cluster-randomized trials, in which the units of measurement are nested within higher-level clusters that comprise the units of randomization. Such designs involve two variance components (within- and between-cluster variance), and thus there are three potential approaches to scaling the treatment effect: standardize by the total variance (i.e., the sum of the within- and between-cluster components), standardize by the within-cluster variance, or standardize by the between-cluster variance. Furthermore, some of the effect sizes can be estimated in several different ways, each with a different sampling variance. &lt;a href=&#34;http://doi.org/10.3102/1076998606298043&#34;&gt;Hedges (2007)&lt;/a&gt; gives sampling variance estimates for each estimator of each effect size, but they all follow the same general formula as given above. (The appendix of the article actually gives the same formula as above, but using a more abstract formulation.)&lt;/p&gt;
&lt;p&gt;For example, suppose the target SMD parameter uses the total variance and that we have data from a two-level, two-arm cluster randomized trial with &lt;span class=&#34;math inline&#34;&gt;\(M\)&lt;/span&gt; clusters, &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; observations per cluster, and total sample sizes in each arm of &lt;span class=&#34;math inline&#34;&gt;\(N_T\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(N_C\)&lt;/span&gt;, respectively. Let &lt;span class=&#34;math inline&#34;&gt;\(\tau^2\)&lt;/span&gt; be the between-cluster variance, &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; be the within-cluster variance, and &lt;span class=&#34;math inline&#34;&gt;\(\rho = \tau^2 / (\tau^2 + \sigma^2)\)&lt;/span&gt;. The target parameter is &lt;span class=&#34;math inline&#34;&gt;\(\delta = \left(\mu_T - \mu_C\right) / \left(\tau^2 + \sigma^2\right)\)&lt;/span&gt;. The article assumes that the treatment effect will be estimated by the difference in grand means, &lt;span class=&#34;math inline&#34;&gt;\(\bar{\bar{y}}_T - \bar{\bar{y}}_C\)&lt;/span&gt;. Letting &lt;span class=&#34;math inline&#34;&gt;\(S_B^2\)&lt;/span&gt; be the pooled sample variance of the cluster means within each arm and &lt;span class=&#34;math inline&#34;&gt;\(S_W^2\)&lt;/span&gt; be the pooled within-cluster sample variance, the total variance is estimated as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
S_{total}^2 = S_B^2 + \frac{n - 1}{n} S_W^2.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;An estimate of the SMD is then&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
d = \left(\bar{\bar{y}}_T - \bar{\bar{y}}_C \right) / \sqrt{S_{total}^2}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The scaled standard error of &lt;span class=&#34;math inline&#34;&gt;\(\bar{\bar{y}}_T - \bar{\bar{y}}_C\)&lt;/span&gt; is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
se_b = \sqrt{\left(\frac{N_C + N_T}{N_C N_T}\right)\left[1 + (n - 1)\rho\right]}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The appendix of the article demonstrates that &lt;span class=&#34;math inline&#34;&gt;\(\text{E}\left(S_{total}^2\right) = \tau^2 + \sigma^2\)&lt;/span&gt; and&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Var}\left( S_{total}^2 \right) = \frac{2}{n^2}\left(\frac{(n \tau^2 + \sigma^2)^2}{M - 2} + \frac{(n - 1)^2 \sigma^4}{N_C + N_T - M}\right),
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;by which it follows that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\nu = \frac{n^2 M (M - 2)}{M[(n - 1)\rho + 1]^2 + (M - 2)(n - 1)(1 - \rho)^2}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Substituting &lt;span class=&#34;math inline&#34;&gt;\(se_b / S_{total}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt; into the formula for &lt;span class=&#34;math inline&#34;&gt;\(V_d\)&lt;/span&gt; gives the same as Expression (14) in the article.&lt;/p&gt;
&lt;p&gt;A limitation of &lt;a href=&#34;http://doi.org/10.3102/1076998606298043&#34;&gt;Hedges (2007)&lt;/a&gt; is that it only covers the case where the treatment effect is estimated by the difference in grand means (although it does cover the case of unequal cluster sizes, which gets quite messy). In practice, every cluster-randomized trial I’ve ever seen uses baseline covariates to adjust the mean difference (often based on a hierarchical linear model) and improve the precision of the treatment effect estimate. The SMD estimate should also be based on this covariate-adjustment estimate, scaled by the total variance &lt;em&gt;without adjusting for the covariate&lt;/em&gt;. An advantage of the general formulation given above is that its clear how to estimate the sampling variance of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;. I would guess that it will often be possible to calculate the scaled standard error directly, given the standard error of the covariate-adjusted treatment effect estimate. And since &lt;span class=&#34;math inline&#34;&gt;\(S_{total}\)&lt;/span&gt; would be estimated just as before, its degrees of freedom remain the same.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://doi.org/10.3102/1076998610376617&#34;&gt;Hedges (2011)&lt;/a&gt; discusses estimation of SMDs in three-level cluster-randomized trials—an even more complicated case. However, the general approach is the same; all that’s needed are the scaled standard error and the degrees of freedom &lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt; of whatever combination of variance components go into the denominator of the effect size. In both the two-level and three-level cases, the degrees of freedom get quite complicated in unbalanced samples and are probably not calculable from the information that is usually provided in an article. Hedges (2007, 2011) comments on a couple of cases where more tractable approximations can be used, although it seems like there might be room for further investigation here.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;closing-thoughts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Closing thoughts&lt;/h3&gt;
&lt;p&gt;I think this framework is useful in that it unifies a large number of cases that have been treated separately, and can also be applied (more-or-less immediately) to &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; estimators that haven’t been widely considered before, such as the &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; that involves scaling by the pooled pre-and-post, treatment-and-control sample variance. I hope it also illustrates that, while the point estimator &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; can be applied across a large number of study designs, the sampling variance of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; depends on the details of the design and estimation methods. The same is true for other families of effect sizes as well. For example, in other work I’ve demonstrated that the sampling variance of the correlation coefficient depends on the design from which the correlations are estimated (&lt;a href=&#34;http://doi.org/10.1037/a0033788&#34;&gt;Pustejovsky, 2014&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;If you have read this far, I’d love to get your feedback about whether you think this is a useful way to organize the calculations of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; estimators. Is this helpful? Or nothing you didn’t already know? Or still more complicated than it should be? Leave a comment!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;p&gt;Borenstein, M. (2009). Effect sizes for continuous data. In H. M. Cooper, L. V Hedges, &amp;amp; J. C. Valentine (Eds.), The Handbook of Research Synthesis and Meta-Analysis (pp. 221–236). New York, NY: Russell Sage Foundation.&lt;/p&gt;
&lt;p&gt;Feingold, A. (2009). Effect sizes for growth-modeling analysis for controlled clinical trials in the same metric as for classical analysis. Psychological Methods, 14(1), 43–53. &lt;a href=&#34;doi:10.1037/a0014699&#34; class=&#34;uri&#34;&gt;doi:10.1037/a0014699&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Feingold, A. (2015). Confidence interval estimation for standardized effect sizes in multilevel and latent growth modeling. Journal of Consulting and Clinical Psychology, 83(1), 157–168. &lt;a href=&#34;doi:10.1037/a0037721&#34; class=&#34;uri&#34;&gt;doi:10.1037/a0037721&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Gleser, L. J., &amp;amp; Olkin, I. (2009). Stochastically dependent effect sizes. In H. Cooper, L. V. Hedges, &amp;amp; J. C. Valentine (Eds.), The Handbook of Research Synthesis and Meta-Analysis (2nd ed., pp. 357–376). New York, NY: Russell Sage Foundation.&lt;/p&gt;
&lt;p&gt;Hedges, L. V. (2007). Effect sizes in cluster-randomized designs. Journal of Educational and Behavioral Statistics, 32(4), 341–370. &lt;a href=&#34;doi:10.3102/1076998606298043&#34; class=&#34;uri&#34;&gt;doi:10.3102/1076998606298043&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Hedges, L. V. (2011). Effect sizes in three-level cluster-randomized experiments. Journal of Educational and Behavioral Statistics, 36(3), 346–380. &lt;a href=&#34;doi:10.3102/1076998610376617&#34; class=&#34;uri&#34;&gt;doi:10.3102/1076998610376617&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Morris, S. B., &amp;amp; DeShon, R. P. (2002). Combining effect size estimates in meta-analysis with repeated measures and independent-groups designs. Psychological Methods, 7(1), 105–125. &lt;a href=&#34;doi:10.1037//1082-989X.7.1.105&#34; class=&#34;uri&#34;&gt;doi:10.1037//1082-989X.7.1.105&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Pustejovsky, J. E. (2014). Converting from d to r to z when the design uses extreme groups, dichotomization, or experimental control. Psychological Methods, 19(1), 92–112. &lt;a href=&#34;doi:10.1037/a0033788&#34; class=&#34;uri&#34;&gt;doi:10.1037/a0033788&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Pustejovsky, J. E., Hedges, L. V, &amp;amp; Shadish, W. R. (2014). Design-comparable effect sizes in multiple baseline designs: A general modeling framework. Journal of Educational and Behavioral Statistics, 39(5), 368–393. &lt;a href=&#34;doi:10.3102/1076998614547577&#34; class=&#34;uri&#34;&gt;doi:10.3102/1076998614547577&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Viechtbauer, W. (2007). Approximate confidence intervals for standardized effect sizes in the two-independent and two-dependent samples design. Journal of Educational and Behavioral Statistics, 32(1), 39–60. &lt;a href=&#34;doi:10.3102/1076998606298034&#34; class=&#34;uri&#34;&gt;doi:10.3102/1076998606298034&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The sampling distribution of sample variances</title>
      <link>http://localhost:4321/distribution-of-sample-variances/</link>
      <pubDate>Mon, 25 Apr 2016 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/distribution-of-sample-variances/</guid>
      <description>


&lt;p&gt;A colleague and her students asked me the other day whether I knew of a citation that gives the covariance between the sample variances of two outcomes from a common sample. This sort of question comes up in meta-analysis problems occasionally. I didn’t know of a convenient reference that directly answers the question, but I was able to suggest some references that would help (listed below). While the students work on deriving it, I’ll provide the answer here so that they can check their work.&lt;/p&gt;
&lt;p&gt;Suppose that we have a sample of &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; observations &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{y}_1,...,\mathbf{y}_n\)&lt;/span&gt; from a &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-dimensional multivariate normal distribution with mean &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\mu\)&lt;/span&gt; and covariance &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\Sigma = \left[\sigma_{jk}\right]_{j,k=1,...,p}\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\bar{y}}\)&lt;/span&gt; denote the (multivariate) sample mean, with entries &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}_1,...,\bar{y}_p\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{S}\)&lt;/span&gt; denote the sample covariance matrix, with entries &lt;span class=&#34;math inline&#34;&gt;\(\left[s_{jk}\right]_{j,k=1,...,p}\)&lt;/span&gt; where&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
s_{jk} = \frac{1}{n - 1}\sum_{i=1}^n (y_{ij} - \bar{y}_j)(y_{ik} - \bar{y}_k).
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Then &lt;span class=&#34;math inline&#34;&gt;\((n - 1)\mathbf{S}\)&lt;/span&gt; follows a Wishart distribution with &lt;span class=&#34;math inline&#34;&gt;\(n - 1\)&lt;/span&gt; degrees of freedom and scale matrix &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\Sigma\)&lt;/span&gt; (Searle, 2006, p. 352; Muirhead, 1982, p. 86; or any textbook on multivariate analysis).&lt;/p&gt;
&lt;p&gt;The sampling covariance between two sample covariances, say &lt;span class=&#34;math inline&#34;&gt;\(s_{jk}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(s_{lm}\)&lt;/span&gt;, can then be derived from the properties of the Wishart distribution. Expressions for this are available in Searle (2006) or Muirhead (1982). The former is a bit hard to parse because it uses the &lt;span class=&#34;math inline&#34;&gt;\(\text{vec}\)&lt;/span&gt; and Kronecker product operators; Muirhead (1982, p. 90) gives the following simple expression:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Cov}\left(s_{jk}, s_{lm}\right) = \frac{\sigma_{jl}\sigma_{km} + \sigma_{jm}\sigma_{kl}}{n - 1}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For sample variances, this reduces to&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Cov}\left(s_j^2, s_l^2\right) = \frac{2\sigma_{jl}^2}{n - 1}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The formula also reduces to the well-known result that the sampling variance of the sample variance is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Var}\left(s_j^2\right) = \frac{2 \sigma_{jj}^2}{n - 1}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;One application of this bit of distribution theory is to find the sampling variance of an average of sample variances. Suppose that we have a bivariate normal distribution where both measures have the same variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma_{11} = \sigma_{22} = \sigma^2\)&lt;/span&gt; and correlation &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;. One estimate of this common variance is to take the simple average of the sample variances, &lt;span class=&#34;math inline&#34;&gt;\(s_{\bullet}^2 = \left(s_1^2 + s_2^2\right) / 2\)&lt;/span&gt;. Then using the above:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{aligned}
\text{Var}\left(s_{\bullet}^2\right) &amp;amp;= \frac{1}{4}\left[\text{Var}\left(s_1^2\right) + \text{Var}\left(s_2^2\right) + 2\text{Cov}\left(s_1^2, s_2^2\right) \right] \\
&amp;amp;= \frac{\sigma^4 \left(1 + \rho^2\right)}{n - 1}.
\end{aligned}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To see that this is correct, consider the extreme cases. If the two measures are perfectly correlated, then averaging the sample variances has no benefit because &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}\left(s_{\bullet}^2\right) = \text{Var}\left(s_1^2\right) = \text{Var}\left(s_2^2\right)\)&lt;/span&gt;. If they are exactly uncorrelated, then averaging the sample variances is equivalent to pooling the sample variance from two independent samples.&lt;/p&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;p&gt;Muirhead, R. J. (1982). Aspects of Multivariate Statistical Theory. New York, NY: John Wiley &amp;amp; Sons.&lt;/p&gt;
&lt;p&gt;Searle, S. R. (2006). Matrix Algebra Useful for Statistics. Hoboken, NJ: John Wiley &amp;amp; Sons.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Standard errors and confidence intervals for NAP</title>
      <link>http://localhost:4321/nap-ses-and-cis/</link>
      <pubDate>Sun, 28 Feb 2016 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/nap-ses-and-cis/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;http://doi.org/10.1016/j.beth.2008.10.006&#34;&gt;Parker and Vannest (2009)&lt;/a&gt; proposed non-overlap of all pairs (NAP) as an effect size index for use in single-case research. NAP is defined in terms of all pair-wise comparisons between the data points in two different phases for a given case (i.e., a treatment phase versus a baseline phase). For an outcome that is desirable to increase, NAP is the proportion of all such pair-wise comparisons where the treatment phase observation exceeds the baseline phase observation, with pairs that are exactly tied getting a weight of 1/2. NAP belongs to the family of non-overlap measures, which also includes the percentage of non-overlapping data, the improvement rate difference, and several other indices. It is exactly equivalent to &lt;a href=&#34;http://doi.org/10.2307/1165329&#34;&gt;Vargha and Delaney’s (2000)&lt;/a&gt; modified Common Language Effect Size and has been proposed as an effect size index in other contexts too (e.g., &lt;a href=&#34;http://doi.org/10.1002/sim.2256&#34;&gt;Acion, Peterson, Temple, &amp;amp; Arndt, 2006&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The developers of NAP have created a &lt;a href=&#34;http://singlecaseresearch.org/calculators&#34;&gt;web-based tool&lt;/a&gt; for calculating it (as well as several other non-overlap indices), and I have the impression that the tool is fairly widely used. For example, &lt;a href=&#34;http://doi.org/10.1007%2Fs10864-013-9189-x&#34;&gt;Roth, Gillis, and DiGennaro Reed (2014)&lt;/a&gt; and &lt;a href=&#34;http://doi.org/10.1007/s10803-015-2373-1&#34;&gt;Whalon, Conroy, Martinez, and Welch (2015)&lt;/a&gt; both used NAP in their meta-analyses of single-case research, and both noted that they used &lt;a href=&#34;http://www.singlecaseresearch.org/calculators/nap&#34;&gt;singlecaseresearch.org&lt;/a&gt; for calculating the effect size measure. Given that the web tool is being used, it is worth scrutinizing the methods behind the calculations it reports. As of this writing, the standard error and confidence intervals reported along with the NAP statistic are incorrect, and should not be used. After introducing a bit of notation, I’ll explain why the existing methods are deficient. I’ll also suggest some methods for calculating standard errors and confidence intervals that are potentially more accurate.&lt;/p&gt;
&lt;div id=&#34;preliminaries&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Preliminaries&lt;/h2&gt;
&lt;p&gt;Suppose that we have data from the baseline phase and treatment phase for a single case. Let &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; denote the number of baseline observations and &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; denote the number of treatment phase observations. Let &lt;span class=&#34;math inline&#34;&gt;\(y^A_1,...,y^A_m\)&lt;/span&gt; denote the baseline phase data and &lt;span class=&#34;math inline&#34;&gt;\(y^B_1,...,y^B_n\)&lt;/span&gt; denote the treatment phase data. Then NAP is calculated as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{NAP} = \frac{1}{m n} \sum_{i=1}^m \sum_{j=1}^n \left[I\left(y^B_j &amp;gt; y^A_i\right) + 0.5 I\left(y^B_j = y^A_i\right)\right]
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;What is NAP an estimate of? The parameter of interest is the probability that a randomly selected treatment phase observation will exceed a randomly selected baseline phase observation (again, with an adjustment for ties):&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\theta = \text{Pr}(Y^B &amp;gt; Y^A) + 0.5 \text{Pr}(Y^B = Y^A).
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Vargha and Delaney call &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; the &lt;em&gt;measure of stochastic superiority&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;NAP is very closely related to another non-overlap index called Tau (&lt;a href=&#34;http://doi.org/10.1016/j.beth.2010.08.006&#34;&gt;Parker, Vannest, Davis, &amp;amp; Sauber, 2011&lt;/a&gt;). Tau is nothing more than a linear re-scaling of NAP to the range of [-1, 1]:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Tau} = \frac{S}{m n} = 2 \times \text{NAP} - 1,
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
S = \sum_{i=1}^m \sum_{j=1}^n \left[I\left(y^B_j &amp;gt; y^A_i\right) - I\left(y^B_j &amp;lt; y^A_i\right)\right].
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt; is Kendall’s S statistic, which is closely related to the Mann-Whitney &lt;span class=&#34;math inline&#34;&gt;\(U\)&lt;/span&gt; test.&lt;/p&gt;
&lt;p&gt;Here is an R function for calculating NAP:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;NAP &amp;lt;- function(yA, yB) {
  m &amp;lt;- length(yA)
  n &amp;lt;- length(yB)
  U &amp;lt;- sum(sapply(yA, function(i) sapply(yB, function(j) (j &amp;gt; i) + 0.5 * (j == i))))
  U / (m * n)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Using the data from the worked example in &lt;a href=&#34;http://doi.org/10.1016/j.beth.2008.10.006&#34;&gt;Parker and Vannest (2009)&lt;/a&gt;, the function result agrees with their reported NAP of 0.96:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;yA &amp;lt;- c(4, 3, 4, 3, 4, 7, 5, 2, 3, 2)
yB &amp;lt;- c(5, 9, 7, 9, 7, 5, 9, 11, 11, 10, 9)
NAP(yA, yB)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9636364&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;standard-errors&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Standard errors&lt;/h2&gt;
&lt;p&gt;The webtool at &lt;a href=&#34;http://www.singlecaseresearch.org/calculators/nap&#34;&gt;singlecaseresearch.org&lt;/a&gt; reports a standard error for NAP (it is labelled as “SDnap”), which from what I can tell is based on the formula&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{SE}_{\text{Tau}} = \sqrt{\frac{m + n + 1}{3 m n}}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This formula appears to actually be the standard error for Tau, rather than for NAP. Since &lt;span class=&#34;math inline&#34;&gt;\(\text{NAP} = \left(\text{Tau} + 1\right) / 2\)&lt;/span&gt;, the standard error for NAP should be half as large:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{SE}_{null} = \sqrt{\frac{m + n + 1}{12 m n}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;(cf. &lt;a href=&#34;http://dx.doi.org/10.1037/1082-989X.6.2.135&#34;&gt;Grissom &amp;amp; Kim, 2001, p. 141&lt;/a&gt;). However, even the latter formula is not always correct. It is valid only when the observations are all mutually independent and when the treatment phase data are drawn from the same distribution as the baseline phase data—that is, when the treatment has no effect on the outcome. I’ve therefore denoted it as &lt;span class=&#34;math inline&#34;&gt;\(\text{SE}_{null}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;other-standard-error-estimators&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Other standard error estimators&lt;/h3&gt;
&lt;p&gt;Because an equivalent effect size measure is used in other contexts like clinical medicine, there has actually been a fair bit of research into better approaches for assessing the uncertainty in NAP. &lt;a href=&#34;http://dx.doi.org/10.1148/radiology.143.1.7063747&#34;&gt;Hanley and McNeil (1982)&lt;/a&gt; proposed an estimator for the sampling variance of NAP that is designed for continuous outcome measures, where exact ties are impossible. Modifying it slightly (and in entirely ad hoc fashion) to account for ties, let&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
Q_1 &amp;amp;= \frac{1}{m n^2}\sum_{i=1}^m \left[\sum_{j=1}^n I\left(y^B_j &amp;gt; y^A_i\right) + 0.5 I\left(y^B_j = y^A_i\right)\right]^2 \\
Q_2 &amp;amp;= \frac{1}{m^2 n}\sum_{j=1}^n \left[\sum_{i=1}^m I\left(y^B_j &amp;gt; y^A_i\right) + 0.5 I\left(y^B_j = y^A_i\right)\right]^2.
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Then the Hanley-McNeil variance estimator is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
V_{HM} = \frac{1}{mn} \left[\text{NAP}\left(1 - \text{NAP}\right) + (n - 1)\left(Q_1 - \text{NAP}^2\right) + (m - 1)\left(Q_2 - \text{NAP}^2\right)\right],
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with &lt;span class=&#34;math inline&#34;&gt;\(\text{SE}_{HM} = \sqrt{V_{HM}}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The same authors also propose a different estimator, which is based on the assumption that the outcome data are exponentially distributed. Even though this is a strong and often inappropriate assumption, there is evidence that this estimator works even for other, non-exponential distributions. &lt;a href=&#34;http://dx.doi.org/10.1002/sim.2324&#34;&gt;Newcombe (2006)&lt;/a&gt; suggested a further modification of their estimator, and I’ll describe his version. Let &lt;span class=&#34;math inline&#34;&gt;\(h = (m + n) / 2 - 1\)&lt;/span&gt;. Then&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
V_{New} = \frac{h}{mn} \text{NAP}\left(1 - \text{NAP}\right)\left[\frac{1}{h} + \frac{1 - \text{NAP}}{2 - \text{NAP}} + \frac{\text{NAP}}{1 + \text{NAP}}\right],
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with &lt;span class=&#34;math inline&#34;&gt;\(\text{SE}_{New} = \sqrt{V_{New}}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Here are R functions to calculate each of these variance estimators.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;V_HM &amp;lt;- function(yA, yB) {
  m &amp;lt;- length(yA)
  n &amp;lt;- length(yB)
  U &amp;lt;- sapply(yB, function(j) (j &amp;gt; yA) + 0.5 * (j == yA))
  t &amp;lt;- sum(U) / (m * n)
  Q1 &amp;lt;- sum(rowSums(U)^2) / (m * n^2)
  Q2 &amp;lt;- sum(colSums(U)^2) / (m^2 * n)
  (t * (1 - t) + (n - 1) * (Q1 - t^2) + (m - 1) * (Q2 - t^2)) / (m * n)
}

V_New &amp;lt;- function(yA, yB) {
  m &amp;lt;- length(yA)
  n &amp;lt;- length(yB)
  t &amp;lt;- NAP(yA, yB)
  h &amp;lt;- (m + n) / 2 - 1
  t * (1 - t) * (1 + h * (1 - t) / (2 - t) + h * t / (1 + t)) / (m * n)
}

sqrt(V_HM(yA, yB))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.03483351&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sqrt(V_New(yA, yB))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.04370206&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For the worked example dataset from Parker and Vannest, the Newcombe estimator yields a standard error that is about 25% larger than the Hanley-McNeil estimator. Both of these are substantially smaller than the null standard error, which in this example is &lt;span class=&#34;math inline&#34;&gt;\(\text{SE}_{null} = 0.129\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-small-simulation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A small simulation&lt;/h3&gt;
&lt;p&gt;Simulation methods can be used to examine how well these various standard error formulas estimate the actual sampling variation of NAP. For simplicity, I’ll simulate normally distributed data where&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y^A \sim N(0, 1) \qquad \text{and} \qquad Y^B \sim N\left(\sqrt{2}\Phi^{-1}(\theta), 1\right)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;for varying values of the effect size estimand (&lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;) and a couple of different sample sizes.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sample_NAP &amp;lt;- function(delta, m, n, iterations) {
  NAPs &amp;lt;- replicate(iterations, {
    yA &amp;lt;- rnorm(m)
    yB &amp;lt;- rnorm(n, mean = delta)
    c(NAP = NAP(yA, yB), V_HM = V_HM(yA, yB), V_New = V_New(yA, yB))
  })
  data.frame(sd = sd(NAPs[&amp;quot;NAP&amp;quot;,]), 
             SE_HM = sqrt(mean(NAPs[&amp;quot;V_HM&amp;quot;,])), 
             SE_New = sqrt(mean(NAPs[&amp;quot;V_New&amp;quot;,])))
}

library(dplyr)
library(tidyr)
theta &amp;lt;- seq(0.5, 0.95, 0.05)
m &amp;lt;- c(5, 10, 15, 20, 30)
n &amp;lt;- c(5, 10, 15, 20, 30)

expand.grid(theta = theta, m = m, n = n) %&amp;gt;%
  group_by(theta, m, n) %&amp;gt;% 
  mutate(delta = sqrt(2) * qnorm(theta)) -&amp;gt;
  params 

params %&amp;gt;%
  do(sample_NAP(.$delta, .$m, .$n, iterations = 2000)) %&amp;gt;%
  mutate(se_null = sqrt((m + n + 1) / (12 * m * n))) %&amp;gt;%
  gather(&amp;quot;sd&amp;quot;,&amp;quot;val&amp;quot;, sd, SE_HM, SE_New, se_null) -&amp;gt;
  NAP_sim&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
ggplot(NAP_sim, aes(theta, val, color = sd)) + 
  facet_grid(n ~ m, labeller = &amp;quot;label_both&amp;quot;) + 
  geom_line() + 
  theme_bw() + theme(legend.position = &amp;quot;bottom&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:4321/post/NAP-SEs-and-CIs_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the above figure, the actual sampling standard deviation of NAP (in red) and the value of &lt;span class=&#34;math inline&#34;&gt;\(\text{SE}_{null}\)&lt;/span&gt; (in purple) are plotted against the true value of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, with separate plots for various combinations of &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;. The expected value of the standard errors &lt;span class=&#34;math inline&#34;&gt;\(\text{SE}_{HM}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\text{SE}_{New}\)&lt;/span&gt; (actually the square root of the expectation of the variance estimators) are depicted in green and blue, respectively. The value of &lt;span class=&#34;math inline&#34;&gt;\(\text{SE}_{null}\)&lt;/span&gt; agrees with the actual standard error when &lt;span class=&#34;math inline&#34;&gt;\(\delta = 0\)&lt;/span&gt;, but the two diverge when there is a positive treatment effect. It appears that &lt;span class=&#34;math inline&#34;&gt;\(\text{SE}_{HM}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\text{SE}_{New}\)&lt;/span&gt; both under-estimate the actual standard error when &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt; is equal to 5, and over-estimate for the largest values of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;. However, both of these estimators offer a marked improvement over &lt;span class=&#34;math inline&#34;&gt;\(\text{SE}_{null}\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;confidence-intervals&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Confidence intervals&lt;/h2&gt;
&lt;p&gt;The webtool at &lt;a href=&#34;http://www.singlecaseresearch.org/calculators/nap&#34;&gt;singlecaseresearch.org&lt;/a&gt; also reports 85% and 90% confidence intervals for NAP. These confidence intervals appear to have the same two problems as the standard errors. First, they are constructed as CIs for Tau rather than for NAP. For the &lt;span class=&#34;math inline&#34;&gt;\(100\% \times (1 - \alpha)\)&lt;/span&gt; CI, let &lt;span class=&#34;math inline&#34;&gt;\(z_{\alpha / 2}\)&lt;/span&gt; be the appropriate critical value from a standard normal distribution. The CIs reported by the webtool are given by&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Tau} \pm \text{SE}_{\text{Tau}} \times z_{\alpha / 2}. 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is probably just an oversight in the programming, which could be corrected by instead using&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{NAP} \pm \text{SE}_{null} \times z_{\alpha / 2}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In parallel with the standard error formulas, I’ll call this formula the null confidence interval. Funnily enough, the upper bound of the null CI is the same as the upper bound of the Tau CI. However, the lower bound is going to be quite a bit larger than the lower bound for the Tau CI, so that the null CI will be much narrower.&lt;/p&gt;
&lt;p&gt;The second problem is that even the null CI has poor coverage properties because it is based on &lt;span class=&#34;math inline&#34;&gt;\(\text{SE}_{null}\)&lt;/span&gt;, which can drastically over-estimate the standard error of NAP for non-null values.&lt;/p&gt;
&lt;div id=&#34;other-confidence-intervals&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Other confidence intervals&lt;/h3&gt;
&lt;p&gt;As I noted above, there has been a fair amount of previous research into how to construct CIs for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, the parameter estimated by NAP. As is often the case with these sorts of problems, there are many different methods available, scattered across the literature. Fortunately, there are two (at least) fairly comprehensive simulation studies that compare the performance of various methods under a wide range of conditions. &lt;a href=&#34;http://dx.doi.org/10.1002/sim.2324&#34;&gt;Newcombe (2006)&lt;/a&gt; examined a range of methods based on inverting Wald-type test statistics (which give CIs of the form &lt;span class=&#34;math inline&#34;&gt;\(\text{estimate} \pm \text{SE} \times z_{\alpha / 2}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\text{SE}\)&lt;/span&gt; is some standard error estimate) and score-based methods (in which the standard error is estimated using the candidate parameter value). Based on an extensive simulation, he suggested a score-based method in which the end-points of the CI are defined the values of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; that satisfy:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
(\text{NAP} - \theta)^2 = \frac{z^2_{\alpha / 2} h \theta (1 - \theta)}{mn}\left[\frac{1}{h} + \frac{1 - \theta}{2 - \theta} + \frac{\theta}{1 + \theta}\right],
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(h = (m + n) / 2 - 1\)&lt;/span&gt;. This equation is a fourth-degree polynomial in &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, easily solved using a numerical root-finding algorithm.&lt;/p&gt;
&lt;p&gt;In a different simulation study, &lt;a href=&#34;http://dx.doi.org/10.1080/00273171.2012.658329&#34;&gt;Ruscio and Mullen (2012)&lt;/a&gt; examined the performance of a selection of different confidence intervals for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, including several methods not considered by Newcombe. Among the methods that they examined, they find that the bias-corrected, accelerated (BCa) bootstrap CI performs particularly well (and seems to outperform the score-based CI recommended by Newcombe).&lt;/p&gt;
&lt;p&gt;Neither &lt;a href=&#34;http://dx.doi.org/10.1002/sim.2324&#34;&gt;Newcombe (2006)&lt;/a&gt; nor &lt;a href=&#34;http://dx.doi.org/10.1080/00273171.2012.658329&#34;&gt;Ruscio and Mullen (2012)&lt;/a&gt; considered constructing a confidence interval by directly pivoting the Mann-Whitney U test (the same technique used to construct confidence intervals for the Hodges-Lehmann estimator of location shift), although it seems to me that this would be possible and potentially an attractive approach in the context of SCDs. The main caveat is that such a CI would require stronger distributional assumptions than those studied in the simulations, such as that the distributions of &lt;span class=&#34;math inline&#34;&gt;\(Y^A\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(Y^B\)&lt;/span&gt; differ by an additive (or multiplicative) constant. In any case, it seems like it would be worth exploring this approach too.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;another-small-simulation&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Another small simulation&lt;/h3&gt;
&lt;p&gt;Here is an R function for calculating several different CIs for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, including the null CI, Wald-type CIs based on &lt;span class=&#34;math inline&#34;&gt;\(V_{HM}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(V_{New}\)&lt;/span&gt;, and the score-type CI recommended by &lt;a href=&#34;http://dx.doi.org/10.1002/sim.2324&#34;&gt;Newcombe (2006)&lt;/a&gt;. I haven’t programmed the BCa bootstrap because it would take a bit more thought to figure out how to simulate it efficiently.&lt;/p&gt;
&lt;p&gt;The following code simulates the coverage rates of nominal 90% CIs based on each of these methods, following the same simulation set-up as above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;NAP_CIs &amp;lt;- function(yA, yB, alpha = .05) {
  m &amp;lt;- length(yA)
  n &amp;lt;- length(yB)
  U &amp;lt;- sapply(yB, function(j) (j &amp;gt; yA) + 0.5 * (j == yA))
  t &amp;lt;- sum(U) / (m * n)
  
  # variance estimators
  V_null &amp;lt;- (m + n + 1) / (12 * m * n)
  
  Q1 &amp;lt;- sum(rowSums(U)^2) / (m * n^2)
  Q2 &amp;lt;- sum(colSums(U)^2) / (m^2 * n)
  V_HM &amp;lt;- (t * (1 - t) + (n - 1) * (Q1 - t^2) + (m - 1) * (Q2 - t^2)) / (m * n)
  
  h &amp;lt;- (m + n) / 2 - 1
  V_New &amp;lt;- t * (1 - t) * (1 + h * (1 - t) / (2 - t) + h * t / (1 + t)) / (m * n)
  
  # Wald-type confidence intervals
  z &amp;lt;- qnorm(1 - alpha / 2)
  SEs &amp;lt;- sqrt(c(null = V_null, HM = V_HM, Newcombe = V_New))
  Wald_lower &amp;lt;- t - z * SEs
  Wald_upper &amp;lt;- t + z * SEs
  
  # score-type confidence interval
  f &amp;lt;- function(x) m * n * (t - x)^2 * (2 - x) * (1 + x) - 
    z^2 * x * (1 - x) * (2 + h + (1 + 2 * h) * x * (1 - x))
  score_lower &amp;lt;- if (t &amp;gt; 0) uniroot(f, c(0, t))$root else 0
  score_upper &amp;lt;- if (t &amp;lt; 1) uniroot(f, c(t, 1))$root else 1
  list(NAP = t, 
       CI = data.frame(lower = c(Wald_lower, score = score_lower), 
                       upper = c(Wald_upper, score = score_upper)))
}

NAP_CIs(yA, yB)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $NAP
## [1] 0.9636364
## 
## $CI
##              lower     upper
## null     0.7106061 1.2166666
## HM       0.8953639 1.0319088
## Newcombe 0.8779819 1.0492908
## score    0.7499741 0.9950729&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sample_CIs &amp;lt;- function(delta, m, n, alpha = .05, iterations) {
  NAPs &amp;lt;- replicate(iterations, {
    yA &amp;lt;- rnorm(m)
    yB &amp;lt;- rnorm(n, mean = delta)
    NAP_CIs(yA, yB, alpha = alpha)
  }, simplify = FALSE)
  theta &amp;lt;- mean(sapply(NAPs, function(x) x$NAP))
  coverage &amp;lt;- rowMeans(sapply(NAPs, function(x) (x$CI$lower &amp;lt; theta) &amp;amp; (theta &amp;lt; x$CI$upper)))
  data.frame(CI = rownames(NAPs[[1]]$CI), coverage = coverage)
}

params %&amp;gt;% 
  do(sample_CIs(delta = .$delta, m = .$m, n = .$n, alpha = .10, iterations = 5000)) -&amp;gt;
  NAP_CI_sim&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(NAP_CI_sim, aes(theta, coverage, color = CI)) + 
  facet_grid(n ~ m, labeller = &amp;quot;label_both&amp;quot;, scales = &amp;quot;free_y&amp;quot;) + 
  geom_line() + 
  labs(y = &amp;quot;SE&amp;quot;) + 
  geom_hline(yintercept=.90, linetype=&amp;quot;dashed&amp;quot;) +
  theme_bw() + theme(legend.position = &amp;quot;bottom&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:4321/post/NAP-SEs-and-CIs_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;960&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The figure above plots the coverage rates of several different confidence intervals for &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;: the naive CI (in blue), the HM Wald CI (red), the Newcombe Wald CI (green), and the Newcombe score CI (purple). The dashed horizontal line is the nominal coverage rate of 90%. It can be seen that the null CI has the correct coverage only when &lt;span class=&#34;math inline&#34;&gt;\(\theta \leq .6\)&lt;/span&gt;; for larger values of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;, its coverage becomes too conservative (tending towards 100%). The Wald-type CIs have below-nominal coverage rates, which improve as the sample size in each phase increases but remain too liberal even at the largest sample size considered. Finally, Newcombe’s score CI maintains close-to-nominal coverage over a wider range of &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; values. Although these CIs have below-nominal coverage for the smallest sample sizes, they generally have good coverage for &lt;span class=&#34;math inline&#34;&gt;\(\theta &amp;lt; .9\)&lt;/span&gt; and when the sample size in each phase is 10 or more. It is also notable that their coverage rates appear to become more accurate as the sample size in a given group increases, even if the sample size in the other group is fairly small and remains constant.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;caveats&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Caveats&lt;/h2&gt;
&lt;p&gt;My aim in this post was to highlight the problems with how &lt;a href=&#34;http://www.singlecaseresearch.org/calculators/nap&#34;&gt;singlecaseresearch.org&lt;/a&gt; calculates standard errors and CIs for the NAP statistic. Some of these issues could easily be resolved by correcting the relevant formulas so that they are appropriate for NAP rather than Tau. However, even with these corrections, better approaches exist for calculating standard errors and CIs. I’ve highlighted some promising ones above, which seem worthy of further investigation. But I should also emphasize that these methods do come with some important caveats too.&lt;/p&gt;
&lt;p&gt;First, all of the methods I’ve discussed are premised on having mutually independent observations. In the presence of serial correlation, I would anticipate that any of these standard errors will be too small and any of the confidence intervals will be too narrow. (This could readily be verified through simulation, although I have not done so here.)&lt;/p&gt;
&lt;p&gt;Second, my small simulations are based on the assumption of normally distributed, homoskedastic observations in each phase, which is not a particularly good model for the types of outcome measures commonly used in single case research. In some of my other work, I’ve developed statistical models for data collected by systematic direct observation of behavior, which is the most prevalent type of outcome data in single-case research. Before recommending any particular method, the performance of the standard error formulas (e.g., the Hanley-McNeil and Newcombe estimators) and CI methods (such as Newcombe’s score CI) should be examined under more realistic models for behavioral observation data.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Correlations between standardized mean differences</title>
      <link>http://localhost:4321/correlations-between-smds/</link>
      <pubDate>Thu, 17 Sep 2015 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/correlations-between-smds/</guid>
      <description>


&lt;p&gt;Several students and colleagues have asked me recently about an issue that comes up in multivariate meta-analysis when some of the studies include multiple treatment groups and multiple outcome measures. In this situation, one might want to include effect size estimates for each treatment group and each outcome measure. In order to do so in fully multivariate meta-analysis, estimates of the covariances among all of these efffect sizes are needed. The covariance among effect sizes arises for several reasons:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;For a single outcome measure, effect sizes based on different treatment groups compared to a common control group will be correlated because the same control group data is used to calculate both effect sizes;&lt;/li&gt;
&lt;li&gt;Effect sizes based on a single treatment group and a single control group, but for different outcome measures, will be correlated because the outcomes are measured on the same set of units (in both the treatment group and the control group).&lt;/li&gt;
&lt;li&gt;Effect sizes based on different treatment groups and for different outcome measures will be correlated because the outcomes are measured on the same set of units in the control group (though not in the treatment group).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For standardized mean difference (SMD) measures of effect size, formulas for the covariance are readily available for the first two cases (see e.g., Gleser &amp;amp; Olkin, 2009), but not for the third case. Below I review the formulas for the covariance between SMDs in the first two cases and provide a formula for the third case.&lt;/p&gt;
&lt;div id=&#34;notation-and-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Notation and Model&lt;/h1&gt;
&lt;p&gt;Suppose that the experiment has a control group that includes &lt;span class=&#34;math inline&#34;&gt;\(n_0\)&lt;/span&gt; units and &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt; treatment groups that include &lt;span class=&#34;math inline&#34;&gt;\(n_1,...,n_T\)&lt;/span&gt; units, respectively. Also suppose that &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; outcome measures are made on each unit in each group. The formulas below assume that the data follow a one-way MANOVA model. Let &lt;span class=&#34;math inline&#34;&gt;\(y_{ijt}\)&lt;/span&gt; denote the score for unit &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; on outcome &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; in group &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;. Then I assume that&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y_{ijt} = \mu_{jt} + \epsilon_{ijt},\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where the errors are multi-variate normally distributed with mean zero, variance that can differ across outcome but not across treatment group, and correlation that is constant across treatment groups, i.e. &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}\left(\epsilon_{ijt}\right) = \sigma^2_j\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\text{Cov}\left(\epsilon_{ijt}, \epsilon_{ikt} \right) = \rho_{jk}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Denote the mean score on outcome &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; in group &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}_{jt}\)&lt;/span&gt; and the standard deviation of the scores on outcome &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; in group &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(s_{jt}\)&lt;/span&gt;, both for &lt;span class=&#34;math inline&#34;&gt;\(j = 1,...,J\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(t = 0,...,T\)&lt;/span&gt; (with &lt;span class=&#34;math inline&#34;&gt;\(t = 0\)&lt;/span&gt; corresponding to the control group). Also required are estimates of the correlations among outcome measures 1 through &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt;, after partialling out differences between treatment groups. Let &lt;span class=&#34;math inline&#34;&gt;\(r_{jk}\)&lt;/span&gt; denote the partial correlation between measure &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; and measure &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, for &lt;span class=&#34;math inline&#34;&gt;\(j = 1,...,J - 1\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(k = j + 1,...,J\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;With multiple treatment groups, one might wonder how best to compute the standard deviation for purposes of scaling the treatment effect estimates. In their discussion of SMDs from multiple treatment studies, Gleser and Olkin (2009) assume (though they don’t actually state outright) that the standard deviation will be pooled across all &lt;span class=&#34;math inline&#34;&gt;\(T + 1\)&lt;/span&gt; groups. The pooled standard deviation for outcome &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; is calculated as the square root of the pooled variance,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[s_{jP}^2 = \frac{1}{N - T - 1} \sum_{t=0}^T (n_t - 1)s_{jt}^2,\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(N = \sum_{t=0}^T n_t\)&lt;/span&gt;. The standardized mean difference for treatment &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; on outcome &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; is then estimated as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[d_{jt} = \frac{\bar{y}_{jt} - \bar{y}_{j0}}{s_{jP}}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;for &lt;span class=&#34;math inline&#34;&gt;\(j = 1,...,J\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(t = 1,...,T\)&lt;/span&gt;. The conventional estimate of the large-sample variance of &lt;span class=&#34;math inline&#34;&gt;\(d_{jt}\)&lt;/span&gt; is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{Var}(d_{jt}) \approx \frac{1}{n_0} + \frac{1}{n_t} + \frac{d_{jt}^2}{2 (N - T - 1)}.\]&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;covariances&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Covariances&lt;/h1&gt;
&lt;p&gt;For SMDs based on a common outcome measure and a common control group, but different treatment groups, the large-sample covariance between the effect size estimates can be estimated as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{Cov}(d_{jt},d_{ju}) \approx \frac{1}{n_0} + \frac{d_{jt} d_{ju}}{2 (N - T - 1)}.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The above differs slightly from Gleser and Olkin (2009, Formula 19.19) because it uses the degrees of freedom &lt;span class=&#34;math inline&#34;&gt;\(N - T - 1\)&lt;/span&gt; in the denominator of the second term, rather than the total sample size. If the total sample size is larger relative to the number of treatment groups, the discrepancy should be minor.&lt;/p&gt;
&lt;p&gt;SMDs based on a single treatment group but for different outcome measures follow a structure that is essentially equivalent to what Gleser and Olkin (2009) call a “multiple-endpoint” study. The large-sample covariance between the effect size estimates can be estimated as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{Cov}(d_{jt},d_{kt}) \approx r_{jk} \left(\frac{1}{n_0} + \frac{1}{n_t}\right) + \frac{r_{jk}^2 d_{jt} d_{kt}}{2 (N - T - 1)}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;(cf. Gleser &amp;amp; Olkin, 2009, Formula 19.19). Note that if the degrees of freedom are large relative to &lt;span class=&#34;math inline&#34;&gt;\(d_{jt}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(d_{kt}\)&lt;/span&gt;, then the correlation between the effect sizes will be approximately equal to &lt;span class=&#34;math inline&#34;&gt;\(\text{Cor}(d_{jt},d_{kt}) \approx r_{jk}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Finally, the large-sample covariance between SMDs based on different treatment groups and different outcome measures can be estimated as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{Cov}(d_{jt},d_{ku}) \approx \frac{r_{jk}}{n_0} + \frac{r_{jk}^2 d_{jt} d_{ku}}{2 (N - T - 1)}.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is similar to the previous formula, but does not include the term corresponding to the covariance between different outcome measures in a common treatment group.&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(r_{jj} = 1\)&lt;/span&gt; is used for the correlation of an outcome measure with itself, all of the above formulas (including the variance of &lt;span class=&#34;math inline&#34;&gt;\(d_{jt}\)&lt;/span&gt;) can be expressed compactly as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\text{Cov}(d_{jt},d_{ku}) \approx r_{jk} \left(\frac{1}{n_0} + \frac{I(t = u)}{n_t}\right) + \frac{r_{jk}^2 d_{jt} d_{ku}}{2 (N - T - 1)},\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(I(A)\)&lt;/span&gt; is equal to one if &lt;span class=&#34;math inline&#34;&gt;\(A\)&lt;/span&gt; is true and equal to zero otherwise.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Gleser, L. J., &amp;amp; Olkin, I. (2009). Stochastically dependent effect sizes. In H. Cooper, L. V. Hedges, &amp;amp; J. C. Valentine (Eds.), The Handbook of Research Synthesis and Meta-Analysis (2nd ed., pp. 357-376). New York, NY: Russell Sage Foundation.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
