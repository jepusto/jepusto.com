<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>weighting | James E. Pustejovsky</title>
    <link>/tags/weighting/</link>
      <atom:link href="/tags/weighting/index.xml" rel="self" type="application/rss+xml" />
    <description>weighting</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2021</copyright><lastBuildDate>Tue, 09 Jun 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>weighting</title>
      <link>/tags/weighting/</link>
    </image>
    
    <item>
      <title>Weighting in multivariate meta-analysis</title>
      <link>/weighting-in-multivariate-meta-analysis/</link>
      <pubDate>Tue, 09 Jun 2020 00:00:00 +0000</pubDate>
      <guid>/weighting-in-multivariate-meta-analysis/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;One common question about multivariate/multi-level meta-analysis is how such models assign weight to individual effect size estimates. When a version of the question came up recently on the &lt;a href=&#34;https://stat.ethz.ch/pipermail/r-sig-meta-analysis/2020-June/002149.html&#34;&gt;R-sig-meta-analysis listserv&lt;/a&gt;, Dr. Wolfgang Viechtbauer offered a &lt;a href=&#34;http://www.metafor-project.org/doku.php/tips:weights_in_rma.mv_models&#34;&gt;whole blog post&lt;/a&gt; in reply, demonstrating how weights work in simpler fixed effect and random effects meta-analysis and then how things get more complicated in multivariate models. I started thumb-typing my own reply as well, but then decided it would be better to write up a post so that I could use a bit of math notation (and to give my thumbs a break). So, in this post I’ll try to add some further intuition on how weights work in certain multivariate meta-analysis models. Most of the discussion will apply to models that include multiple level of random effects, but no predictors. I’ll also comment briefly on meta-regression models with only study-level predictor variables, and finally give some pointers to work on more complicated models.&lt;/p&gt;
&lt;div id=&#34;a-little-background&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A little background&lt;/h2&gt;
&lt;p&gt;It’s helpful to start by looking briefly at the basic fixed effect and random effects models, assuming that we’ve got a set of studies that each contribute a single effect size estimate so everything’s independent. Letting &lt;span class=&#34;math inline&#34;&gt;\(T_j\)&lt;/span&gt; be the effect size from study &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;, with sampling variance &lt;span class=&#34;math inline&#34;&gt;\(V_j\)&lt;/span&gt;, both for &lt;span class=&#34;math inline&#34;&gt;\(j = 1,...,k\)&lt;/span&gt;, the basic random effects model is:
&lt;span class=&#34;math display&#34;&gt;\[
T_j = \mu + \eta_j + e_j
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is the overall average effect size, &lt;span class=&#34;math inline&#34;&gt;\(v_j\)&lt;/span&gt; is a random effect with variance &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}(\eta_j) = \tau^2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e_j\)&lt;/span&gt; is a sampling error with known variance &lt;span class=&#34;math inline&#34;&gt;\(V_j\)&lt;/span&gt;. The first step in estimating this model is to estimate &lt;span class=&#34;math inline&#34;&gt;\(\tau^2\)&lt;/span&gt;. There’s lots of methods for doing so, but let’s not worry about those details—just pick one and call the estimate &lt;span class=&#34;math inline&#34;&gt;\(\hat\tau^2\)&lt;/span&gt;. Then, to estimate &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;, we take a weighted average of the effect size estimates:
&lt;span class=&#34;math display&#34;&gt;\[
\hat\mu = \frac{1}{W} \sum_{j=1}^k w_j T_j, \qquad \text{where} \quad W = \sum_{j=1}^k w_j.
\]&lt;/span&gt;
The weights used in the weighted average are chosen to make the overall estimate as precise as possible (i.e., having the smallest possible sampling variance or standard error). Mathematically, the best possible weights are &lt;strong&gt;&lt;em&gt;inverse variance&lt;/em&gt;&lt;/strong&gt; weights, that is, setting the weight for each effect size estimate proportional to the inverse of how much variance there is in each estimate. With inverse variance weights, larger studies with more precise effect size estimates will tend to get more weight and smaller, noisier studies will tend to get less weight.&lt;/p&gt;
&lt;p&gt;In the basic random effects model, the weights for each study are proportional to
&lt;span class=&#34;math display&#34;&gt;\[
w_j = \frac{1}{\hat\tau^2 + V_j},
\]&lt;/span&gt;
for &lt;span class=&#34;math inline&#34;&gt;\(j = 1,...,k\)&lt;/span&gt;. The denominator term here includes both the (estimated) between-study heterogeneity and the sampling variance because both terms contribute to how noisy the effect size estimate is. In the fixed effect model, we ignore between-study heterogeneity so the weights are inversely proportional to the sampling variances, with &lt;span class=&#34;math inline&#34;&gt;\(w_j = 1 / V_j\)&lt;/span&gt;. In the random effects model, larger between-study heterogeneity will make the weights closer to equal, while smaller between-study heterogeneity will lead to weights that tend to emphasize larger studies with more precise estimates. In the remainder, I’ll show that there are some similar dynamics at work in a more complicated, multivariate meta-analysis model.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-multivariate-meta-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A multivariate meta-analysis&lt;/h2&gt;
&lt;p&gt;Now let’s consider the case where some or all studies in our synthesis contribute more than one effect size estimate. Say that we have effect sizes &lt;span class=&#34;math inline&#34;&gt;\(T_{ij}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(i = 1,...,n_j\)&lt;/span&gt; indexes effect size estimates within study &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; indexes studies, for &lt;span class=&#34;math inline&#34;&gt;\(j = 1,...,k\)&lt;/span&gt;. Say that effect size estimate &lt;span class=&#34;math inline&#34;&gt;\(T_{ij}\)&lt;/span&gt; has sampling variance &lt;span class=&#34;math inline&#34;&gt;\(V_{ij}\)&lt;/span&gt;, and there is some sampling correlation between effect sizes &lt;span class=&#34;math inline&#34;&gt;\(h\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; within study &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;, denoted &lt;span class=&#34;math inline&#34;&gt;\(r_{hij}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;There are many models that a meta-analyst might consider for this data structure. A fairly common one would be a model that includes random effects not only for between-study heterogeneity (as in the basic random effects model) but also random effects capturing within-study heterogeneity in true effect sizes. Let me write this model heirarchically, as
&lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
T_{ij} &amp;amp;= \theta_j + \nu_{ij} + e_{ij} \\
\theta_j &amp;amp;= \mu + \eta_j
\end{align}
\]&lt;/span&gt;
In the first line of the model, &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt; denotes the average effect size parameter for study &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\nu_{ij}\)&lt;/span&gt; captures within-study heterogeneity in the true effect size parameters and &lt;span class=&#34;math inline&#34;&gt;\(e_{ij}\)&lt;/span&gt; is a sampling error. Above, I’ve assumed that we know the structure of the sampling errors, so &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}(e_{ij}) = V_{ij}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\text{Cov}(e_{hj}, e_{ij}) = r_{hij} \sqrt{V_{hj} V_{ij}}\)&lt;/span&gt;. Let’s also denote the within-study variance as &lt;span class=&#34;math inline&#34;&gt;\(\omega^2\)&lt;/span&gt;, so &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}(\nu_{ij}) = \omega^2\)&lt;/span&gt;.
In the second line of the model, &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is still the overall average effect size across all studies and effect sizes within studies and &lt;span class=&#34;math inline&#34;&gt;\(\eta_j\)&lt;/span&gt; is a between-study error, with &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}(\eta_j) = \tau^2\)&lt;/span&gt;, capturing the degree of heterogeneity in the &lt;em&gt;average&lt;/em&gt; effect sizes (the &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt;’s) across studies.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;One thing to note about this model is that it treats all of the effect sizes as coming from a population with a common mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;. Some statisticians might object to calling it a multivariate model because we’re not distinguishing averages for different dimensions (or variates) of the effect sizes. To this I say: whatev’s, donkey! I’m calling it multivariate because you have to use the &lt;code&gt;rma.mv()&lt;/code&gt; function from the &lt;code&gt;metafor&lt;/code&gt; package to estimate it. I will acknowledge, though, that there will often be reason to use more complicated models, for example by replacing the overall average &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; with some meta-regression &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{x}_{ij} \boldsymbol\beta\)&lt;/span&gt;. That’s a discussion for another day. For now, we’re only going to consider the model with an overall average effect size parameter &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;. The question is, &lt;strong&gt;&lt;em&gt;how do the individual effect size estimates &lt;span class=&#34;math inline&#34;&gt;\(T_{ij}\)&lt;/span&gt; contribute to the estimate of this overall average effect?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;equally-precise-effect-size-estimates&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Equally precise effect size estimates&lt;/h2&gt;
&lt;p&gt;To make some headway, it is helpful to first consider an even more specific model where, within a given study, all effect size estimates are equally precise and equally correlated. In particular, let’s assume that for each study &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;, the sampling variances are all equal, with &lt;span class=&#34;math inline&#34;&gt;\(V_{ij} = V_j\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(i = 1,...,n_j\)&lt;/span&gt;, and the correlations between the sampling errors are also all equal, with &lt;span class=&#34;math inline&#34;&gt;\(r_{hij} = r_j\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(h,i = 1,...,n_j\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;These assumptions might not be all that far-fetched. Within a given study, if the effect size estimates are for different measures of a common construct, it’s not unlikely that they would all be based on similar sample sizes (+/- a bit of item non-response). It might be a bit less likely if the effect size estimates are for treatment effects from different follow-up times (since drop-out/non-response tends to increase over time) or different treatment groups compared to a common control group—but still perhaps not entirely unreasonable. Further, it’s rather &lt;em&gt;uncommon&lt;/em&gt; to have good information about the correlations between effect size estimates from a given study (because primary studies don’t often report all of the information needed to calculate these correlations). In practice, meta-analysts might need to simply &lt;a href=&#34;/imputing-covariance-matrices-for-multi-variate-meta-analysis/&#34;&gt;make a rough guess about the correlations&lt;/a&gt; and then use robust variance estimation and/or sensitivity analysis to check themselves. And if we’re just ball-parking, then we’ll probably assume a single correlation for all of the studies.&lt;/p&gt;
&lt;p&gt;The handy thing about this particular scenario is that, because all of the effect size estimates within a study are equally precise and equally correlated, the most efficient way to estimate an average effect for a given study is to &lt;strong&gt;&lt;em&gt;just take the simple average&lt;/em&gt;&lt;/strong&gt; (and, intuitively, this seems like the only sensible thing to do). To be precise, consider how we would estimate &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt; for a given study &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;. The most precise possible estimate is simply
&lt;span class=&#34;math display&#34;&gt;\[
\hat\theta_j = \frac{1}{n_j} \sum_{i=1}^{n_j} T_{ij}.
\]&lt;/span&gt;
And we could do the same for each of the other studies, &lt;span class=&#34;math inline&#34;&gt;\(j = 1,...,k\)&lt;/span&gt;.
It turns out that the estimate of the overall average effect size is a weighted average of these study-specific average effect sizes:
&lt;span class=&#34;math display&#34;&gt;\[
\hat\mu = \frac{1}{W} \sum_{j=1}^k w_j \hat\theta_j,
\]&lt;/span&gt;
for some weights &lt;span class=&#34;math inline&#34;&gt;\(w_1,...,w_k\)&lt;/span&gt;. But what are these weights? Just like in the basic random effects model, they are inverse-variance weights. It’s just that the variance is a little bit more complicated.&lt;/p&gt;
&lt;p&gt;Consider how precise each of the study-specific estimates are, relative to the true effects in their respective studies. Conditional on the true effect &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt;,
&lt;span class=&#34;math display&#34;&gt;\[
\text{Var}(\hat\theta_j | \theta_j) = \frac{1}{n_j}\left(\omega^2 + (n_j - 1) r_j V_j + V_j\right).
\]&lt;/span&gt;
Without conditioning on &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt;, the variance of the &lt;span class=&#34;math inline&#34;&gt;\(\hat\theta_j\)&lt;/span&gt; estimates also includes a term for variation in the true study-specific average effect sizes, becoming
&lt;span class=&#34;math display&#34;&gt;\[
\text{Var}(\hat\theta_j) = \tau^2 + \frac{1}{n_j}\left(\omega^2 + (n_j - 1) r_j V_j + V_j\right).
\]&lt;/span&gt;
The weights used in estimating &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; are the inverse of this quantity:
&lt;span class=&#34;math display&#34;&gt;\[
w_j = \frac{1}{\tau^2 + \frac{1}{n_j}\left(\omega^2 + (n_j - 1) r_j V_j + V_j\right)}.
\]&lt;/span&gt;
Within a study, each individual effect size gets an &lt;span class=&#34;math inline&#34;&gt;\(n_j^{th}\)&lt;/span&gt; of this study-level weight. We can therefore write the overall average as
&lt;span class=&#34;math display&#34;&gt;\[
\hat\mu = \frac{1}{W} \sum_{j=1}^k \sum_{i=1}^{n_j} w_{ij} T_{ij},
\]&lt;/span&gt;
where
&lt;span class=&#34;math display&#34;&gt;\[
w_{ij} = \frac{1}{n_j \tau^2 + \omega^2 + (n_j - 1) r_j V_j + V_j}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;There are several things worth noting about this expression for the weights. First, suppose that there is little between-study or within-study heterogeneity, so &lt;span class=&#34;math inline&#34;&gt;\(\tau^2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\omega^2\)&lt;/span&gt; are both close to zero. Then the weights are driven by the number of effect sizes within the study (&lt;span class=&#34;math inline&#34;&gt;\(n_j\)&lt;/span&gt;), the sampling variance of those effect sizes (&lt;span class=&#34;math inline&#34;&gt;\(V_j\)&lt;/span&gt;) and their correlation &lt;span class=&#34;math inline&#34;&gt;\(r_j\)&lt;/span&gt;. If &lt;span class=&#34;math inline&#34;&gt;\(r_j\)&lt;/span&gt; is near one, then averaging together a bunch of highly correlated estimates doesn’t improve precision much, relative to just using one of the effect sizes. The study-specific average effect estimate will therefore have variance close to &lt;span class=&#34;math inline&#34;&gt;\(V_j\)&lt;/span&gt; (i.e., the variance of a single effect size estimate). If &lt;span class=&#34;math inline&#34;&gt;\(r_j\)&lt;/span&gt; is below one, then averaging yields a more precise estimate than any of the individual effect sizes, and averaging together more effect sizes will yield a more precise estimate at the study level. If the assumed correlations are reasonably accurate, the weights used in the multi-variate meta-analysis will appropriately take into account the number of effect sizes within each study and the precision of those effect sizes.&lt;/p&gt;
&lt;p&gt;Second, now suppose that there is no between-study heterogeneity (&lt;span class=&#34;math inline&#34;&gt;\(\tau^2 = 0\)&lt;/span&gt;) but there is positive within-study heterogeneity. Larger degrees of within-study heterogeneity will tend to equalize the weights &lt;em&gt;at the effect size level&lt;/em&gt;, regardless of how effect size estimates are nested within studies. When there is within-study heterogeneity, averaging together a bunch of estimates will yield a more precise estimate of study-specific average effects. Therefore, when &lt;span class=&#34;math inline&#34;&gt;\(\omega^2\)&lt;/span&gt; is larger, studies with more effect sizes will tend to get a relatively larger share of the weight.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Third and finally, between-study heterogeneity will tend to equalize the weights at the study level, so that the overall average is pulled closer to a simple average of the study-specific average effects. This works very much like in basic random effects meta-analysis, where increased heterogeneity will lead to weights that are closer to equal and an average effect size estimate that is closer to a simple average.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-computational-example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A computational example&lt;/h2&gt;
&lt;p&gt;I think it’s useful to verify algebraic results like the ones I’ve given above by checking that you can reproduce them with real data. I’ll use the &lt;code&gt;corrdat&lt;/code&gt; dataset from the &lt;code&gt;robumeta&lt;/code&gt; package for illustration. The dataset has one duplicated row in it (I have no idea why!), which I’ll remove before analyzing further.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)

data(corrdat, package = &amp;quot;robumeta&amp;quot;)

corrdat &amp;lt;- 
  corrdat %&amp;gt;%
  distinct(studyid, esid, .keep_all = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This dataset included a total of 171 effect size estimates from 39 unique studies. For each study, between 1 and 18 eligible effect size estimates were reported. Here is a histogram depicting the number of studies by the number of reported effect size estimates:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/Weighting-in-multivariate-meta-analysis_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Here is the plot of the variances of each effect size versus the study IDs:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/Weighting-in-multivariate-meta-analysis_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For most of the studies, the effect sizes have very similar sampling variances. One exception is study 9, where two of the effect sizes have variances of under 0.20 and the other two effect sizes have variances in excess of 0.35. Another exception is study 30, which has one effect size with much larger variance than the others.&lt;/p&gt;
&lt;p&gt;Just for sake of illustration, I’m going to &lt;em&gt;enforce&lt;/em&gt; my assumption that effect sizes have equal variances within each study by recomputing the sampling variances as the &lt;em&gt;average&lt;/em&gt; sampling variance within each study. I will then impute a sampling variance-covariance matrix for the effect sizes, assuming a correlation of 0.7 for effects from the same study:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(clubSandwich)

corrdat &amp;lt;- 
  corrdat %&amp;gt;%
  group_by(studyid) %&amp;gt;%
  mutate(V_bar = mean(var)) %&amp;gt;%
  ungroup()

V_mat &amp;lt;- impute_covariance_matrix(vi = corrdat$V_bar, 
                                  cluster = corrdat$studyid,
                                  r = 0.7)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With this variance-covariance matrix, I can then estimate the multivariate meta-analysis model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(metafor)

MVMA_fit &amp;lt;- rma.mv(yi = effectsize, V = V_mat, 
                   random = ~ 1 | studyid / esid,
                   data = corrdat)

summary(MVMA_fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Multivariate Meta-Analysis Model (k = 171; method: REML)
## 
##   logLik  Deviance       AIC       BIC      AICc 
## -94.7852  189.5703  195.5703  204.9777  195.7149   
## 
## Variance Components:
## 
##             estim    sqrt  nlvls  fixed        factor 
## sigma^2.1  0.0466  0.2159     39     no       studyid 
## sigma^2.2  0.1098  0.3314    171     no  studyid/esid 
## 
## Test for Heterogeneity:
## Q(df = 170) = 1141.4235, p-val &amp;lt; .0001
## 
## Model Results:
## 
## estimate      se    zval    pval   ci.lb   ci.ub 
##   0.2263  0.0589  3.8413  0.0001  0.1108  0.3417  *** 
## 
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Based on this model, between-study heterogeneity is estimated as &lt;span class=&#34;math inline&#34;&gt;\(\hat\tau = 0.216\)&lt;/span&gt; and within-study heterogeneity is estimated as &lt;span class=&#34;math inline&#34;&gt;\(\hat\omega = 0.331\)&lt;/span&gt;, both of which are quite high. The overall average effect size estimate is 0.226, with a standard error of 0.059.&lt;/p&gt;
&lt;p&gt;I’ll first get the weights used in &lt;code&gt;rma.mv&lt;/code&gt; to compute the overall average. The weights are represented as an &lt;span class=&#34;math inline&#34;&gt;\(N \times N\)&lt;/span&gt; matrix. Taking the row or column sums, then rescaling by the total, gives the weight assigned to each effect size estimate:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;W_mat &amp;lt;- weights(MVMA_fit, type = &amp;quot;matrix&amp;quot;)
corrdat$w_ij_metafor &amp;lt;- colSums(W_mat) / sum(W_mat)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To verify that the formulas above are correct, I’ll use them to directly compute weights:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r &amp;lt;- 0.7
tau_sq &amp;lt;- MVMA_fit$sigma2[1]
omega_sq &amp;lt;- MVMA_fit$sigma2[2]

corrdat_weights &amp;lt;- 
  corrdat %&amp;gt;%
  group_by(studyid) %&amp;gt;%
  mutate(
    n_j = n(),
    w_ij = 1 / (n_j * tau_sq + omega_sq + (n_j - 1) * r * V_bar + V_bar)
  ) %&amp;gt;%
  ungroup() %&amp;gt;%
  mutate(
    w_ij = w_ij / sum(w_ij)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The weights I computed are perfectly correlated with the weights used &lt;code&gt;rma.mv&lt;/code&gt;, as can be seen in the plot below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(corrdat_weights, aes(w_ij, w_ij_metafor)) + 
  geom_point() + 
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Weighting-in-multivariate-meta-analysis_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;384&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If we remove the within-study random effect term from the model, the weights will be equivalent to setting &lt;span class=&#34;math inline&#34;&gt;\(\omega^2\)&lt;/span&gt; to zero, but with a different estimate of &lt;span class=&#34;math inline&#34;&gt;\(\tau^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;MVMA_no_omega &amp;lt;- rma.mv(yi = effectsize, V = V_mat, 
                        random = ~ 1 | studyid,
                        data = corrdat)
MVMA_no_omega&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Multivariate Meta-Analysis Model (k = 171; method: REML)
## 
## Variance Components:
## 
##             estim    sqrt  nlvls  fixed   factor 
## sigma^2    0.0951  0.3084     39     no  studyid 
## 
## Test for Heterogeneity:
## Q(df = 170) = 1141.4235, p-val &amp;lt; .0001
## 
## Model Results:
## 
## estimate      se    zval    pval   ci.lb   ci.ub 
##   0.2235  0.0619  3.6122  0.0003  0.1022  0.3448  *** 
## 
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Re-fitting the model with &lt;code&gt;rma.mv()&lt;/code&gt; gives an between-study heterogeneity estimate of &lt;span class=&#34;math inline&#34;&gt;\(\hat\tau = 0.308\)&lt;/span&gt; and an overall average effect size estimate of &lt;span class=&#34;math inline&#34;&gt;\(\hat\mu = 0\)&lt;/span&gt;. Using this estimate, I’ll compute the weights based on the formula and then use those weights to determine the overall average effect size estimate.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tau_sq &amp;lt;- MVMA_no_omega$sigma2

corrdat_weights &amp;lt;- 
  corrdat_weights %&amp;gt;%
  mutate(
    w_ij_no_omega = 1 / (n_j * tau_sq + (n_j - 1) * r * V_bar + V_bar),
    w_ij_no_omega = w_ij_no_omega / sum(w_ij_no_omega)
  )

with(corrdat_weights, weighted.mean(effectsize, w = w_ij_no_omega))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.2235231&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This matches the output of &lt;code&gt;rma.mv()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Here is a plot showing the weights of individual effect sizes for each study. In blue are the weights under the assumption that &lt;span class=&#34;math inline&#34;&gt;\(\omega^2 = 0\)&lt;/span&gt;. In green are the weights allowing for &lt;span class=&#34;math inline&#34;&gt;\(\omega^2 &amp;gt; 0\)&lt;/span&gt;. It’s notable here that introducing the within-study heterogeneity term leads to pretty big changes in the weights for some studies. In particular, studies that have only a single effect size estimate (e.g., studys 7, 8, 22, 25, 28) lose &lt;em&gt;a lot&lt;/em&gt; of weight when &lt;span class=&#34;math inline&#34;&gt;\(\omega^2 &amp;gt; 0\)&lt;/span&gt;. That’s partially because &lt;span class=&#34;math inline&#34;&gt;\(\omega^2\)&lt;/span&gt; tends to pull weight towards studies with more effect sizes, and partially because of the change in the estimate of &lt;span class=&#34;math inline&#34;&gt;\(\tau^2\)&lt;/span&gt;, which tends to equalize the weight assigned to each study.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/Weighting-in-multivariate-meta-analysis_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Below is a plot illustrating the changes in study-level weights (i.e., aggregating the weight assigned to each study). The bar color corresponds to the number of effect size estimates in each study; light grey studies have just one effect size, while studies with more effect sizes are more intensly purple. The notable drops in weight for studies with a single effect size estimate (light grey) are visible here too. Studies with more effect sizes (e.g., studies 2, 15, 30, with dark purple bars) gain weight when we allow &lt;span class=&#34;math inline&#34;&gt;\(\omega^2\)&lt;/span&gt; to be greater than zero.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/Weighting-in-multivariate-meta-analysis_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;now-without-compound-symmetry&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Now without compound symmetry&lt;/h2&gt;
&lt;p&gt;If we remove the restrictions that effect sizes from the same study have the same sampling variance and are equi-correlated, then the weights get a little bit more complicated. However, the general intuitions carry through. Let’s now consider the model with arbitrary sampling variance &lt;span class=&#34;math inline&#34;&gt;\(V_{ij}\)&lt;/span&gt; and sampling correlations within studies &lt;span class=&#34;math inline&#34;&gt;\(r_{hij}\)&lt;/span&gt;. The most efficient estimate of the study-specific average effect is now a &lt;em&gt;weighted&lt;/em&gt; average, with weights that depend on both the variances and covariances of the effect size estimates within each study. Let
&lt;span class=&#34;math display&#34;&gt;\[
\boldsymbol{\hat\Sigma}_j = \hat\omega^2 \mathbf{I}_j + \mathbf{V}_j,
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{I}_j\)&lt;/span&gt; is an &lt;span class=&#34;math inline&#34;&gt;\(n_j \times n_j\)&lt;/span&gt; identity matrix and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{V}_j\)&lt;/span&gt; is the sampling variance-covariance matrix of the effect size estimates, with entry &lt;span class=&#34;math inline&#34;&gt;\((h,i)\)&lt;/span&gt; equal to &lt;span class=&#34;math inline&#34;&gt;\(\left[\mathbf{V}_j\right]_{h,i} = r_{hij} \sqrt{V_{hj} V_{ij}}\)&lt;/span&gt;. The estimate of the study-specific average effect size for study &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; is still a weighted average:
&lt;span class=&#34;math display&#34;&gt;\[
\hat\theta_j = \frac{\sum_{i=1}^{n_j} s_{ij} T_{ij}}{\sum_{i=1}^{n_j} s_{ij}},
\]&lt;/span&gt;
where
&lt;span class=&#34;math display&#34;&gt;\[
s_{ij} = \displaystyle{\sum_{h=1}^{n_j} \left[\boldsymbol{\hat\Sigma}^{-1}\right]_{hi}},
\]&lt;/span&gt;
and &lt;span class=&#34;math inline&#34;&gt;\(\left[\boldsymbol{\hat\Sigma}^{-1}\right]_{hi}\)&lt;/span&gt; denotes entry &lt;span class=&#34;math inline&#34;&gt;\((h,i)\)&lt;/span&gt; in the inverse of the matrix &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\hat\Sigma}\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(V^C_j\)&lt;/span&gt; denote the variance of the study-specific average effect size estimate, conditional on the true &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[
V^C_j = \text{Var}(\hat\theta_j | \theta_j) = \left(\sum_{i=1}^{n_j} s_{ij} \right)^{-1}
\]&lt;/span&gt;
The unconditional variance of &lt;span class=&#34;math inline&#34;&gt;\(\hat\theta_j\)&lt;/span&gt; is then
&lt;span class=&#34;math display&#34;&gt;\[
\text{Var}(\hat\theta_j) = \tau^2 + V^C_j.
\]&lt;/span&gt;
Because the overall average effect size estimate is (still) the inverse-variance weighted average, the weight assigned at the study level is equal to
&lt;span class=&#34;math display&#34;&gt;\[
w_j = \frac{1}{\hat\tau^2 + V^C_j}
\]&lt;/span&gt;
and the weight assigned to individual effect sizes is
&lt;span class=&#34;math display&#34;&gt;\[
w_{ij} = \frac{s_{ij} V^C_j}{\hat\tau^2 + V^C_j}.
\]&lt;/span&gt;
How do &lt;span class=&#34;math inline&#34;&gt;\(\omega^2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\tau^2\)&lt;/span&gt; affect these more general weights? The intuitions that I described earlier still mostly hold. Increasing &lt;span class=&#34;math inline&#34;&gt;\(\omega^2\)&lt;/span&gt; will tend to equalize the weights at the effect size level (i.e., equalize the &lt;span class=&#34;math inline&#34;&gt;\(s_{ij}\)&lt;/span&gt; across &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;), pulling weight towards studies with more effect size estimates. Increasing &lt;span class=&#34;math inline&#34;&gt;\(\tau^2\)&lt;/span&gt; will tend to equalize the weights at the study-level.&lt;/p&gt;
&lt;p&gt;One wrinkle with the more general form of the weights is that the effect-size level weights can sometimes be &lt;em&gt;negative&lt;/em&gt; (i.e., negative &lt;span class=&#34;math inline&#34;&gt;\(s_{ij}\)&lt;/span&gt;). This will tend to happen when the sampling variances within a study are discrepant, such as when one &lt;span class=&#34;math inline&#34;&gt;\(V_{ij}\)&lt;/span&gt; is much smaller than the others in study &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;, when the (assumed or estimated) sampling correlation is high, and when &lt;span class=&#34;math inline&#34;&gt;\(\omega^2\)&lt;/span&gt; is zero or small. This is something that warrants further investigation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-about-meta-regression&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What about meta-regression?&lt;/h2&gt;
&lt;p&gt;Some of the foregoing analysis also applies to models that include predictors. In particular, the formulas I’ve given for the weights will still hold for meta-regression models &lt;strong&gt;&lt;em&gt;that include only study-level predictors&lt;/em&gt;&lt;/strong&gt;. In other words, they work for models of the following form:
&lt;span class=&#34;math display&#34;&gt;\[
T_{ij} = \mathbf{x}_j \boldsymbol\beta + \eta_j + \nu_{ij} + e_{ij},
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{x}_j\)&lt;/span&gt; is a row-vector of one or more predictors for study &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; (including a constant intercept). Introducing these predictors will alter the variance component estimates &lt;span class=&#34;math inline&#34;&gt;\(\hat\tau^2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\hat\omega^2\)&lt;/span&gt;, but the form of the weights will remain the same as above, and the intuitions still hold. This is because, for purposes of estimating &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\beta\)&lt;/span&gt;, the model is essentially the same as a meta-regression at the study level, using the study-specific average effect size estimates as input:
&lt;span class=&#34;math display&#34;&gt;\[
\hat\theta_j = \mathbf{x}_j \boldsymbol\beta + \eta_j + \tilde{e}_j
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}(\tilde{e}_j) = \text{Var}(\hat\theta_j | \theta_j)\)&lt;/span&gt;.&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Here is an illustration with the &lt;code&gt;corrdat&lt;/code&gt; meta-analysis. In these data, the variable &lt;code&gt;college&lt;/code&gt; indicates whether the effect size comes from a college-age sample; it varies only at the study level. The variable &lt;code&gt;males&lt;/code&gt;, &lt;code&gt;binge&lt;/code&gt;, and &lt;code&gt;followup&lt;/code&gt; have some within-study variation, which I’ll by taking the average of each of these predictors at the study level:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;corrdat &amp;lt;- 
  corrdat %&amp;gt;%
  group_by(studyid) %&amp;gt;%
  mutate(
    males_M = mean(males),
    binge_M = mean(binge),
    followup_M = mean(followup)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s fit a meta-regression model using all of the study-level predictors:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;MVMR_fit &amp;lt;- rma.mv(yi = effectsize, V = V_mat,
                   mods = ~ college + males_M + binge_M + followup_M,  
                   random = ~ 1 | studyid / esid,
                   data = corrdat)

summary(MVMR_fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Multivariate Meta-Analysis Model (k = 171; method: REML)
## 
##   logLik  Deviance       AIC       BIC      AICc 
## -86.6244  173.2488  187.2488  209.0327  187.9577   
## 
## Variance Components:
## 
##             estim    sqrt  nlvls  fixed        factor 
## sigma^2.1  0.0297  0.1723     39     no       studyid 
## sigma^2.2  0.1068  0.3268    171     no  studyid/esid 
## 
## Test for Residual Heterogeneity:
## QE(df = 166) = 1083.6655, p-val &amp;lt; .0001
## 
## Test of Moderators (coefficients 2:5):
## QM(df = 4) = 13.0787, p-val = 0.0109
## 
## Model Results:
## 
##             estimate      se     zval    pval    ci.lb    ci.ub 
## intrcpt      -0.0361  0.3678  -0.0982  0.9218  -0.7571   0.6849    
## college       0.2660  0.1384   1.9215  0.0547  -0.0053   0.5373  . 
## males_M       0.0023  0.0048   0.4753  0.6346  -0.0072   0.0118    
## binge_M       0.3441  0.1570   2.1927  0.0283   0.0365   0.6518  * 
## followup_M   -0.0023  0.0011  -2.0379  0.0416  -0.0044  -0.0001  * 
## 
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you might expect, between-study heterogeneity is reduced a bit by the inclusion of these predictors.&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We can check my claim of computational equivalence by fitting the meta-regression model at the study level. Here I’ll aggregate everything up to the study level and compute the study-level weights:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tau_sq_reg &amp;lt;- MVMR_fit$sigma2[1]
omega_sq_reg &amp;lt;- MVMR_fit$sigma2[2]

corrdat_studylevel &amp;lt;- 
  corrdat %&amp;gt;%
  group_by(studyid) %&amp;gt;%
  mutate(n_j = n()) %&amp;gt;%
  summarize_at(vars(effectsize, n_j, V_bar, college, binge_M, followup_M, males_M), mean
  ) %&amp;gt;%
  mutate(
    V_cond = (omega_sq_reg + (n_j - 1) * r * V_bar + V_bar) / n_j,
    w_j = 1 / (tau_sq_reg + V_cond)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now I can fit a study-level meta-regression model. I use the &lt;code&gt;weights&lt;/code&gt; argument to ensure that the meta-regression is estimated using the &lt;span class=&#34;math inline&#34;&gt;\(w_j\)&lt;/span&gt; weights:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;MR_study_fit &amp;lt;- rma(yi = effectsize, vi = V_cond, 
                    mods = ~ college + males_M + binge_M + followup_M, 
                    weights = w_j, data = corrdat_studylevel)
summary(MR_study_fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Mixed-Effects Model (k = 39; tau^2 estimator: REML)
## 
##   logLik  deviance       AIC       BIC      AICc 
## -13.0651   26.1303   38.1303   47.2884   41.2414   
## 
## tau^2 (estimated amount of residual heterogeneity):     0.0297 (SE = 0.0264)
## tau (square root of estimated tau^2 value):             0.1723
## I^2 (residual heterogeneity / unaccounted variability): 26.89%
## H^2 (unaccounted variability / sampling variability):   1.37
## R^2 (amount of heterogeneity accounted for):            37.90%
## 
## Test for Residual Heterogeneity:
## QE(df = 34) = 46.5050, p-val = 0.0748
## 
## Test of Moderators (coefficients 2:5):
## QM(df = 4) = 13.0787, p-val = 0.0109
## 
## Model Results:
## 
##             estimate      se     zval    pval    ci.lb    ci.ub 
## intrcpt      -0.0361  0.3678  -0.0982  0.9218  -0.7571   0.6849    
## college       0.2660  0.1384   1.9215  0.0547  -0.0053   0.5373  . 
## males_M       0.0023  0.0048   0.4753  0.6346  -0.0072   0.0118    
## binge_M       0.3441  0.1570   2.1927  0.0283   0.0365   0.6518  * 
## followup_M   -0.0023  0.0011  -2.0379  0.0416  -0.0044  -0.0001  * 
## 
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The meta-regression coefficient estimates are essentially identical to those from the multi-variate meta-regression, although the between-study heterogeneity estimate differs slightly because it is based on maximizing the single-level model, conditional on an estimate of &lt;span class=&#34;math inline&#34;&gt;\(\omega^2\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;and-beyond&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;And beyond!&lt;/h1&gt;
&lt;p&gt;In true multi-variate models, the meta-regression specification would typically include indicators for each dimension of the model. More generally, we might have a model that includes predictors varying within study, encoding characteristics of the outcome measures, sub-groups, or treatment conditions corresponding to each effect size estimate. The weights in these model get substantially more complicated, not in the least because the weights &lt;em&gt;are specific to the predictors&lt;/em&gt;. For instance, in a model with four within-study predictors, a different set of weights is used in estimating the coefficients corresponding to each predictor. As Dr. &lt;a href=&#34;https://twitter.com/Richard_D_Riley&#34;&gt;Richard Riley&lt;/a&gt; noted on Twitter, relevant work on more complicated models includes &lt;a href=&#34;https://doi.org/10.1177/0962280215611702&#34;&gt;this great paper by Dan Jackson and colleagues&lt;/a&gt; and &lt;a href=&#34;https://doi.org/10.1177/0962280216688033&#34;&gt;this paper by Riley and colleagues&lt;/a&gt;. The latter paper demonstrates how multivariate models entail partial “borrowing of strength” across dimensions of the effect sizes, which is very helpful for building intuition about how these models work. I would encourage you to check out both papers if you are grappling with understanding how weights work in complex meta-regression models.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Note that this model also encompasses the multi-level meta-analysis described by &lt;a href=&#34;https://doi.org/10.1002/jrsm.35&#34;&gt;Konstantopoulos (2011)&lt;/a&gt; and &lt;a href=&#34;https://doi.org/10.3758/s13428-012-0261-6&#34;&gt;Van den Noortgate, et al. (2013)&lt;/a&gt; as a special case, with &lt;span class=&#34;math inline&#34;&gt;\(r_{hij} = 0\)&lt;/span&gt; for all &lt;span class=&#34;math inline&#34;&gt;\(h,i=1,...,n_j\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(j = 1,...,k\)&lt;/span&gt;.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Perhaps that makes sense, if you’ve carefully selected the set of effect sizes for inclusion in your meta-analysis. However, it seems to me that it could sometimes lead to perverse results. Say that all studies but one include just a single effect size estimate, each using the absolute gold standard approach to assessing the outcome, but that one study took a “kitchen sink” approach and assessed the outcome a bunch of different ways, including the gold standard plus a bunch of junky scales. Inclusion of the junky scales will lead to within-study heterogeneity, which in turn will &lt;em&gt;pull the overall average effect size towards this study—the one with all the junk!&lt;/em&gt; That seems less than ideal, and the sort of situation where it would be better to select from the study with multiple outcomes the single effect size estimate based on the outcome assessment that most closely aligns with the other studies.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;Things get even simpler if the model does not include within-study random effects, as I discussed in &lt;a href=&#34;/sometimes-aggregating-effect-sizes-is-fine/&#34;&gt;a previous post&lt;/a&gt;.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;However, this need not be the case—it’s possible that introducing between-study predictors could &lt;em&gt;increase&lt;/em&gt; the estimate of between-study heterogeneity. Yes, that’s totally counter-intuitive. Multi-level models can be weird.&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A handmade clubSandwich for multi-site trials</title>
      <link>/clustered-and-interacted/</link>
      <pubDate>Sat, 09 Mar 2019 00:00:00 +0000</pubDate>
      <guid>/clustered-and-interacted/</guid>
      <description>


&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
data(STAR, package = &amp;quot;AER&amp;quot;)

STAR_urban &amp;lt;-
  STAR %&amp;gt;%
  filter(
    # limit to urban/inner city schools
    schoolk %in% c(&amp;quot;urban&amp;quot;,&amp;quot;inner-city&amp;quot;),
    # limit to complete outcome data
    !is.na(readk), !is.na(mathk)
  ) %&amp;gt;%
  droplevels() %&amp;gt;%
  # collapse control conditions
  mutate(
    stark = fct_collapse(stark, regular = c(&amp;quot;regular&amp;quot;,&amp;quot;regular+aide&amp;quot;))
  ) %&amp;gt;%
  # calculate inverse-propensity weight
  group_by(schoolidk) %&amp;gt;%
  mutate(
    n = n(),
    nT = sum(stark==&amp;quot;small&amp;quot;),
    wt = ifelse(stark==&amp;quot;small&amp;quot;, n / nT, n / (n - nT))
  ) %&amp;gt;%
  select(schoolidk, stark, readk, mathk, wt)

STAR_summary &amp;lt;- 
  STAR_urban %&amp;gt;%
  count(schoolidk)

STAR_urban %&amp;gt;%
  group_by(schoolidk, stark) %&amp;gt;%
  summarise(
    n = n(),
    wt = sum(wt)
  ) %&amp;gt;%
  mutate(n = sum(n)) %&amp;gt;%
  spread(stark, wt)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 23 x 4
## # Groups:   schoolidk [23]
##    schoolidk     n regular small
##    &amp;lt;fct&amp;gt;     &amp;lt;int&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
##  1 2            52      52    52
##  2 9           120     120   120
##  3 10           51      51    51
##  4 14           34      34    34
##  5 15           55      55    55
##  6 16          105     105   105
##  7 18           79      79    79
##  8 19           99      99    99
##  9 22          129     129   129
## 10 26           49      49    49
## # ... with 13 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After these exclusions, the data include a total of 1810 students from 23 schools, ranging in size from 34 to 134 students.&lt;/p&gt;
&lt;p&gt;For starters, let’s get the average impacts using a seeming unrelated regression specification, with both conventional and clubSandwich standard errors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(clubSandwich)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Registered S3 method overwritten by &amp;#39;clubSandwich&amp;#39;:
##   method    from    
##   bread.mlm sandwich&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;STAR_wt &amp;lt;- lm(cbind(readk, mathk) ~ 0 + schoolidk + stark, weights = wt, data = STAR_urban)

# conventional SEs
CR0 &amp;lt;- 
  coef_test(STAR_wt, vcov = &amp;quot;CR0&amp;quot;, 
            cluster = STAR_urban$schoolidk, 
            test = &amp;quot;z&amp;quot;,
            coefs = c(&amp;quot;readk:starksmall&amp;quot;,&amp;quot;mathk:starksmall&amp;quot;))
CR0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              Coef. Estimate   SE t-stat p-val (z) Sig.
## 1 readk:starksmall     6.21 3.13   1.98    0.0473    *
## 2 mathk:starksmall    12.47 5.58   2.23    0.0254    *&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# clubSandwich SEs
CR2 &amp;lt;- 
  coef_test(STAR_wt, vcov = &amp;quot;CR2&amp;quot;, 
            cluster = STAR_urban$schoolidk, 
            coefs = c(&amp;quot;readk:starksmall&amp;quot;,&amp;quot;mathk:starksmall&amp;quot;))

CR2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              Coef. Estimate   SE t-stat d.f. p-val (Satt) Sig.
## 1 readk:starksmall     6.21 2.70    2.3   19       0.0332    *
## 2 mathk:starksmall    12.47 4.79    2.6   19       0.0174    *&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now I’ll do it “by hand”—or rather, with a bit of &lt;code&gt;dplyr&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# summary statistics by site

school_summaries &amp;lt;- 
  STAR_urban %&amp;gt;%
  group_by(schoolidk, stark) %&amp;gt;%
  summarise(
    # means by arm and site
    readk = mean(readk),
    mathk = mean(mathk),
    n_arm = n()
  ) %&amp;gt;%
  summarise(
    # impact estimates by site
    readk = diff(readk),
    mathk = diff(mathk),
    n = sum(n_arm),
    p = n_arm[stark==&amp;quot;small&amp;quot;] / n
  ) %&amp;gt;%
  mutate(
    w = n
  )

# overall impacts

school_summaries %&amp;gt;%
  gather(&amp;quot;subject&amp;quot;,&amp;quot;impact_j&amp;quot;, readk, mathk) %&amp;gt;%
  group_by(subject) %&amp;gt;%
  summarise(
    impact = weighted.mean(impact_j, w = w),
    SE_CR0 = sqrt(sum(w^2 * (impact_j - impact)^2) / sum(w)^2),
    SE_CR2 = sqrt(sum(w^2 * (impact_j - impact)^2 / (1 - w / sum(w))) / sum(w)^2),
    df_CR2 = 1 / (sum(w^2 / (sum(w) - w)^2) - 
                    2 * sum(w^3 / (sum(w) - w)^2) / sum(w) + 
                    sum(w^2 / (sum(w) - w))^2 / sum(w)^2)
  ) %&amp;gt;%
  knitr::kable(digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;subject&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;impact&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;SE_CR0&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;SE_CR2&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;df_CR2&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;mathk&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12.47&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.58&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.76&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;19.07&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;readk&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.21&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.13&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.23&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;19.07&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The CR0 and CR2 standard errors match the results from &lt;code&gt;coef_test&lt;/code&gt;, as do the Satterthwaite degrees of freedom. Note that the degrees of freedom are equal to 19 in this case, a bit less than &lt;span class=&#34;math inline&#34;&gt;\(J - 1 = 22\)&lt;/span&gt; due to variation in the weight assigned to each school.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A handmade clubSandwich for multi-site trials</title>
      <link>/handmade-clubsandwich/</link>
      <pubDate>Sat, 09 Mar 2019 00:00:00 +0000</pubDate>
      <guid>/handmade-clubsandwich/</guid>
      <description>


&lt;p&gt;I’m just back from the &lt;a href=&#34;https://sree.org/conferences/2019s&#34;&gt;Society for Research on Educational Effectiveness&lt;/a&gt; meetings, where I presented work on small-sample corrections for cluster-robust variance estimators in two-stage least squares models, which I’ve implemented in the &lt;a href=&#34;/software/clubSandwich/&#34;&gt;&lt;code&gt;clubSandwich&lt;/code&gt;&lt;/a&gt; R package. &lt;a href=&#34;/files/SREE-2019-2SLS-CRVE.html&#34;&gt;Here’s my presentation&lt;/a&gt;. So I had “clubSandwich” estimators on the brain when a colleague asked me about whether the methods were implemented in SAS.&lt;/p&gt;
&lt;p&gt;The short answer is “no.”&lt;/p&gt;
&lt;p&gt;The moderately longer answer is “not unless we can find funding to pay someone who knows how to program properly in SAS.” However, for the specific model that my colleague was interested in, it turns out that the small-sample corrections implemented in clubSandwich can be expressed in closed form, and they’re simple enough that they could easily be hand-calculated. I’ll sketch out the calculations in the remainder of this post.&lt;/p&gt;
&lt;div id=&#34;a-multi-site-trial&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A multi-site trial&lt;/h2&gt;
&lt;p&gt;Consider a multi-site trial conducted across &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; sites, which we take as a sample from a larger super-population of sites. Each site consists of &lt;span class=&#34;math inline&#34;&gt;\(n_j\)&lt;/span&gt; units, of which &lt;span class=&#34;math inline&#34;&gt;\(p_j n_j\)&lt;/span&gt; are randomized to treatment and the remainder &lt;span class=&#34;math inline&#34;&gt;\((1 - p_j) n_j\)&lt;/span&gt; are randomized to control. For each unit &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; in each site &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;, we have an outcome &lt;span class=&#34;math inline&#34;&gt;\(y_{ij}\)&lt;/span&gt; and a treatment indicator &lt;span class=&#34;math inline&#34;&gt;\(t_{ij}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;A conventional approach to estimating the overall average impact in this setting is to use a model with a treatment indicator and fixed effects for each site:
&lt;span class=&#34;math display&#34;&gt;\[
y_{ij} = \beta_j + \delta t_{ij} + e_{ij}
\]&lt;/span&gt;
and then to cluster the standard errors by site. Clustering by site makes sense here if (and only if) we’re interested in generalizing to the super-population of sites.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\hat\delta_j\)&lt;/span&gt; denote the impact estimate from site &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;, calculated as the difference in means between treated and untreated units at site &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[
\hat\delta_j = \frac{1}{n_j p_j} \left(\sum_{i=1}^{n_j} t_{ij} y_{ij}\right) - \frac{1}{n_j (1 - p_j)} \left(\sum_{i=1}^{n_j} (1 - t_{ij}) y_{ij}\right).
\]&lt;/span&gt;
for &lt;span class=&#34;math inline&#34;&gt;\(j = 1,..,J\)&lt;/span&gt;. The overall impact estimate here is a precision-weighted average of the site-specific impacts:
&lt;span class=&#34;math display&#34;&gt;\[
\hat\delta = \frac{1}{W} \sum_{j=1}^J w_j \hat\delta_j,
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(w_j = n_j p_j (1 - p_j)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(W = \sum_j w_j\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sandwich-estimators&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Sandwich estimators&lt;/h2&gt;
&lt;p&gt;The conventional clustered variance estimator (or sandwich estimator) for &lt;span class=&#34;math inline&#34;&gt;\(\hat\delta\)&lt;/span&gt; is a simple function of the (weighted) sample variance of the site-specific effects. It can be calculated directly as:
&lt;span class=&#34;math display&#34;&gt;\[
V^{CR0} = \frac{1}{W^2} \sum_{j=1}^J w_j^2 \left(\hat\delta_j - \hat\delta\right)^2.
\]&lt;/span&gt;
Under a conventional random effects model for the &lt;span class=&#34;math inline&#34;&gt;\(\delta_j\)&lt;/span&gt;s, this estimator has a downward bias in finite samples.&lt;/p&gt;
&lt;p&gt;The clubSandwich variance estimator here uses an estimator for the sample variance of site-specific effects that is unbiased under a certain working model. It is only slightly more complicated to calculate:
&lt;span class=&#34;math display&#34;&gt;\[
V^{CR2} = \frac{1}{W^2} \sum_{j=1}^J \frac{w_j^2 \left(\hat\delta_j - \hat\delta\right)^2}{1 - w_j / W}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The other difference between conventional methods and the clubSandwich approach is in the reference distribution used to calculate hypothesis tests and confidence intervals. The conventional approach uses a standard normal reference distribution (i.e., a z-test) that is asymptotically justified. The clubSandwich approach uses a &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; reference distribution, with degrees of freedom estimated using a Satterthwaite approximation. In the present context, the degrees of freedom are a little bit ugly but still not hard to calculate:
&lt;span class=&#34;math display&#34;&gt;\[
df = \left[\sum_{j=1}^J \frac{w_j^2}{(W - w_j)^2} - \frac{2}{W}\sum_{j=1}^J \frac{w_j^3}{(W - w_j)^2} + \frac{1}{W^2} \left(\sum_{j=1}^J \frac{w_j^2}{W - w_j} \right)^2 \right]^{-1}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In the special case that all sites are of the same size and use a constant treatment allocation, the weights become equal. The clubSandwich variance estimator then reduces to
&lt;span class=&#34;math display&#34;&gt;\[
V^{CR2} = \frac{S_\delta^2}{J} \qquad \text{where} \qquad S_\delta^2 = \frac{1}{J - 1}\sum_{j=1}^J \left(\hat\delta_j - \hat\delta\right)^2,
\]&lt;/span&gt;
and the degrees of freedom reduce to simply &lt;span class=&#34;math inline&#34;&gt;\(df = J - 1\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;tennessee-star&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tennessee STAR&lt;/h2&gt;
&lt;p&gt;Here is a worked example of the calculations (using R of course, because my SAS programming skills atrophied years ago). I’ll use data from the famous Tennessee STAR class size experiment, which was a multi-site trial in which students were randomized to small or regular-sized kindergarten classes within each of several dozen schools. To make the small-sample issues more pronounced, I’ll limit the sample to urban schools and look at impacts of small class-size on reading and math scores at the end of kindergarten. STAR was actually a three-arm trial—the third arm being a regular-sized class but with an additional teacher aide. For simplicity (and following convention), I’ll collapse the teacher-aide condition and the regular-sized class condition into a single arm and also limit the sample to students with complete outcome data on both tests.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;ggplot2&amp;#39; was built under R version 4.0.3&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;tibble&amp;#39; was built under R version 4.0.3&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;readr&amp;#39; was built under R version 4.0.3&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(STAR, package = &amp;quot;AER&amp;quot;)

STAR_urban &amp;lt;-
  STAR %&amp;gt;%
  filter(
    # limit to urban/inner city schools
    schoolk %in% c(&amp;quot;urban&amp;quot;,&amp;quot;inner-city&amp;quot;),
    # limit to complete outcome data
    !is.na(readk), !is.na(mathk)
  ) %&amp;gt;%
  droplevels() %&amp;gt;%
  # collapse control conditions
  mutate(stark = fct_collapse(stark, regular = c(&amp;quot;regular&amp;quot;,&amp;quot;regular+aide&amp;quot;))) %&amp;gt;%
  select(schoolidk, stark, readk, mathk)

STAR_summary &amp;lt;- 
  STAR_urban %&amp;gt;%
  count(schoolidk)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After these exclusions, the data include a total of 1810 students from 23 schools, ranging in size from 34 to 134 students.&lt;/p&gt;
&lt;p&gt;For starters, let’s get the average impacts using a seeming unrelated regression specification, with both conventional and clubSandwich standard errors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(clubSandwich)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: package &amp;#39;clubSandwich&amp;#39; was built under R version 4.0.3&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Registered S3 method overwritten by &amp;#39;clubSandwich&amp;#39;:
##   method    from    
##   bread.mlm sandwich&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;STAR_fit &amp;lt;- lm(cbind(readk, mathk) ~ 0 + schoolidk + stark, data = STAR_urban)

# conventional SEs
CR0 &amp;lt;- 
  coef_test(STAR_fit, vcov = &amp;quot;CR0&amp;quot;, 
            cluster = STAR_urban$schoolidk, 
            test = &amp;quot;z&amp;quot;,
            coefs = c(&amp;quot;readk:starksmall&amp;quot;,&amp;quot;mathk:starksmall&amp;quot;))

CR0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              Coef. Estimate   SE t-stat p-val (z) Sig.
## 1 readk:starksmall     6.16 2.73   2.25    0.0241    *
## 2 mathk:starksmall    12.13 4.79   2.53    0.0113    *&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# clubSandwich SEs
CR2 &amp;lt;- 
  coef_test(STAR_fit, vcov = &amp;quot;CR2&amp;quot;, 
            cluster = STAR_urban$schoolidk, 
            coefs = c(&amp;quot;readk:starksmall&amp;quot;,&amp;quot;mathk:starksmall&amp;quot;))

CR2&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              Coef. Estimate   SE t-stat d.f. p-val (Satt) Sig.
## 1 readk:starksmall     6.16 2.81   2.19   19       0.0409    *
## 2 mathk:starksmall    12.13 4.92   2.47   19       0.0234    *&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now I’ll do it “by hand”—or rather, with a bit of &lt;code&gt;dplyr&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# summary statistics by site

school_summaries &amp;lt;- 
  STAR_urban %&amp;gt;%
  group_by(schoolidk, stark) %&amp;gt;%
  summarise(
    # means by arm and site
    readk = mean(readk),
    mathk = mean(mathk),
    n_arm = n()
  ) %&amp;gt;%
  summarise(
    # impact estimates by site
    readk = diff(readk),
    mathk = diff(mathk),
    n = sum(n_arm),
    p = n_arm[stark==&amp;quot;small&amp;quot;] / n
  ) %&amp;gt;%
  mutate(w = n * p * (1 - p))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` regrouping output by &amp;#39;schoolidk&amp;#39; (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` ungrouping output (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# overall impacts

school_summaries %&amp;gt;%
  gather(&amp;quot;subject&amp;quot;,&amp;quot;impact_j&amp;quot;, readk, mathk) %&amp;gt;%
  group_by(subject) %&amp;gt;%
  summarise(
    impact = weighted.mean(impact_j, w = w),
    SE_CR0 = sqrt(sum(w^2 * (impact_j - impact)^2) / sum(w)^2),
    SE_CR2 = sqrt(sum(w^2 * (impact_j - impact)^2 / (1 - w / sum(w))) / sum(w)^2),
    df_CR2 = 1 / (sum(w^2 / (sum(w) - w)^2) - 
                    2 * sum(w^3 / (sum(w) - w)^2) / sum(w) + 
                    sum(w^2 / (sum(w) - w))^2 / sum(w)^2)
  ) %&amp;gt;%
  knitr::kable(digits = 2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` ungrouping output (override with `.groups` argument)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;subject&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;impact&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;SE_CR0&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;SE_CR2&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;df_CR2&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;mathk&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12.13&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.79&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.92&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;18.99&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;readk&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6.16&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.73&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.81&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;18.99&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The CR0 and CR2 standard errors match the results from &lt;code&gt;coef_test&lt;/code&gt;, as do the Satterthwaite degrees of freedom. Note that the degrees of freedom are equal to 19 in this case, a bit less than &lt;span class=&#34;math inline&#34;&gt;\(J - 1 = 22\)&lt;/span&gt; due to variation in the weight assigned to each school.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;other-weights&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Other weights&lt;/h2&gt;
&lt;p&gt;Some analysts might not like the approach of using precision-weighted average of the site-specific impacts, as I’ve examined here. Instead, one might choose to weight the site-specific effects by the site-specific sample sizes, or to use some sort of random effects weighting that allows for random heterogeneity across sites. The formulas given above for conventional and clubSandwich clustered variance estimators apply directly to other weighting schemes too. Just substitute your favorite weights in place of &lt;span class=&#34;math inline&#34;&gt;\(w_j\)&lt;/span&gt;. When doing so, the clubSandwich estimator will be exactly unbiased under the assumption that your preferred weighting scheme corresponds to inverse-variance weighting, and the Satterthwaite degrees of freedom approximation will be derived under the same model.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Effective sample size aggregation</title>
      <link>/effective-sample-size-aggregation/</link>
      <pubDate>Tue, 22 Jan 2019 00:00:00 +0000</pubDate>
      <guid>/effective-sample-size-aggregation/</guid>
      <description>


&lt;p&gt;In settings with independent observations, sample size is one way to quickly characterize the precision of an estimate. But what if your estimate is based on &lt;em&gt;weighted&lt;/em&gt; data, where each observation doesn’t necessarily contribute to equally to the estimate? Here, one useful way to gauge the precision of an estimate is the &lt;em&gt;effective sample size&lt;/em&gt; or ESS. Suppose that we have &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; independent observations &lt;span class=&#34;math inline&#34;&gt;\(Y_1,...,Y_N\)&lt;/span&gt; drawn from a population with standard deviation &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;, and that observation &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; receives weight &lt;span class=&#34;math inline&#34;&gt;\(w_i\)&lt;/span&gt;. We take the weighted sample mean
&lt;span class=&#34;math display&#34;&gt;\[
\tilde{y} = \frac{1}{W} \sum_{i=1}^N w_i Y_i, \qquad \text{where} \qquad W = \sum_{i=1}^N w_i.
\]&lt;/span&gt;
with sampling variance
&lt;span class=&#34;math display&#34;&gt;\[
\text{Var}(\tilde{y}) = \frac{\sigma^2}{W^2} \sum_{i=1}^N w_i^2.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The ESS is the number of observations from an equally weighted sample that would yield the same level of precision as the weighted sample mean. In an equally weighted sample of size &lt;span class=&#34;math inline&#34;&gt;\(\tilde{N}\)&lt;/span&gt;, the variance would be simply &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2 / \tilde{N}\)&lt;/span&gt;, and so ESS is the value of &lt;span class=&#34;math inline&#34;&gt;\(\tilde{N}\)&lt;/span&gt; that solves
&lt;span class=&#34;math display&#34;&gt;\[
\frac{\sigma^2}{\tilde{N}} = \frac{\sigma^2}{W^2} \sum_{i=1}^N w_i^2.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Re-arranging, the ESS is thus defined as
&lt;span class=&#34;math display&#34;&gt;\[
\tilde{N} = \frac{W^2}{\sum_{i=1}^N w_i^2}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The ESS is reported in several packages for propensity score weighting, including &lt;a href=&#34;https://CRAN.R-project.org/package=twang&#34;&gt;twang&lt;/a&gt; and &lt;a href=&#34;https://CRAN.R-project.org/package=optweight&#34;&gt;optweight&lt;/a&gt;. In the propensity score context, ESS is a useful measure for comparing different sets of estimated propensity weights, in that weights (or propensity score models/matching methods) that have a larger ESS will yield a more precise estimate of a treatment effect. Given two sets of weights that achieve equivalent degrees of balance, the weights with larger ESS are thus preferable. Methods introduced by &lt;a href=&#34;https://doi.org/10.1080/01621459.2015.1023805&#34;&gt;Zubizarreta (2015)&lt;/a&gt;—and implemented in the &lt;a href=&#34;https://CRAN.R-project.org/package=optweight&#34;&gt;optweight&lt;/a&gt; package—take this logic a step further by using ESS as an objective function to be minimized, subject to specified balancing constraints.&lt;/p&gt;
&lt;div id=&#34;multi-site-effective-sample-size&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Multi-site effective sample size&lt;/h1&gt;
&lt;p&gt;Two of my recent projects have involved applying propensity score weighting methods in multi-site settings, where we are interested in estimating site-specific treatment effects as well as an overall aggregate effect. It is straight-forward to calculate an ESS for each site, but how then should we aggregate the ESS across sites to characterize the precision of the overall estimate? Several times now, I have found myself having to re-derive the aggregated ESS, and so I am going to work through it here now so as to save future-me (and perhaps you, dear reader) some time.&lt;/p&gt;
&lt;p&gt;Suppose that we have &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; sites, &lt;span class=&#34;math inline&#34;&gt;\(n_j\)&lt;/span&gt; observations from site &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(j = 1,...,J\)&lt;/span&gt;, and total sample size &lt;span class=&#34;math inline&#34;&gt;\(N = \sum_{j=1}^J n_j\)&lt;/span&gt;. Observation &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; from site &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; has outcome &lt;span class=&#34;math inline&#34;&gt;\(Y_{ij}\)&lt;/span&gt; and weight &lt;span class=&#34;math inline&#34;&gt;\(w_{ij}\)&lt;/span&gt;. The site-specific weighted average at site &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; is then
&lt;span class=&#34;math display&#34;&gt;\[
\tilde{y}_j = \frac{1}{W_j} \sum_{i=1}^{n_j} w_{ij} Y_{ij}, \qquad \text{where} \qquad W_j = \sum_{i=1}^{n_j} w_{ij}
\]&lt;/span&gt;
and the overall average is
&lt;span class=&#34;math display&#34;&gt;\[
\tilde{y} = \frac{1}{N} \sum_{j=1}^J n_j \ \tilde{y}_j = \frac{1}{N} \sum_{j=1}^J \sum_{i=1}^{n_j} \frac{n_j w_{ij}}{W_j} Y_{ij}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For calculating the overall average, observation &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; from unit &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; contributes weight &lt;span class=&#34;math inline&#34;&gt;\(u_{ij} = n_j w_{ij} / W_j\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Using these unit-specific weights, the effective sample size for the overall average is
&lt;span class=&#34;math display&#34;&gt;\[
ESS = \frac{N^2}{\sum_{j=1}^J \sum_{i=1}^{n_j} u_{ij}^2}.
\]&lt;/span&gt;
We can also define a site-specific ESS for site &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[
ESS_j = \frac{W_j^2}{\sum_{i=1}^{n_j} w_{ij}^2}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Using the decomposition of the weights as &lt;span class=&#34;math inline&#34;&gt;\(u_{ij} = n_j w_{ij} / W_j\)&lt;/span&gt;, the overall ESS can be written as
&lt;span class=&#34;math display&#34;&gt;\[
ESS = \frac{N^2}{\sum_{j=1}^J n_j^2 \left(\sum_{i=1}^{n_j} w_{ij}^2 / W_j^2\right)}.
\]&lt;/span&gt;
Noting that the term in the parentheses of the denominator is equivalent to &lt;span class=&#34;math inline&#34;&gt;\(1 / ESS_j\)&lt;/span&gt;, the overall ESS can therefore be written in terms of the site-specific ESSs and sample sizes:
&lt;span class=&#34;math display&#34;&gt;\[
ESS = \frac{N^2}{\sum_{j=1}^J n_j^2 / ESS_j}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;There you go. Future me will thank me for this!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
