<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Matrix Algebra | James E. Pustejovsky</title>
    <link>http://localhost:4321/tags/matrix-algebra/</link>
      <atom:link href="http://localhost:4321/tags/matrix-algebra/index.xml" rel="self" type="application/rss+xml" />
    <description>Matrix Algebra</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2024</copyright><lastBuildDate>Mon, 07 Nov 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://localhost:4321/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Matrix Algebra</title>
      <link>http://localhost:4321/tags/matrix-algebra/</link>
    </image>
    
    <item>
      <title>Corrigendum to Pustejovsky and Tipton (2018), redux</title>
      <link>http://localhost:4321/pusto-tipton-2018-theorem-2-redux/</link>
      <pubDate>Mon, 07 Nov 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/pusto-tipton-2018-theorem-2-redux/</guid>
      <description>


&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\def\Pr{{\text{Pr}}}
\def\E{{\text{E}}}
\def\Var{{\text{Var}}}
\def\Cov{{\text{Cov}}}
\def\bm{\mathbf}
\def\bs{\boldsymbol}
\]&lt;/span&gt;
&lt;strong&gt;UPDATE, March 8, 2023: The correction to our paper has now been published at &lt;em&gt;Journal of Business and Economic Statistics&lt;/em&gt;. It is available at&lt;/strong&gt; &lt;a href=&#34;https://doi.org/10.1080/07350015.2023.2174123&#34; class=&#34;uri&#34;&gt;https://doi.org/10.1080/07350015.2023.2174123&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In my &lt;a href=&#34;http://localhost:4321/publication/rve-in-fixed-effects-models/&#34;&gt;2018 paper with Beth Tipton&lt;/a&gt;, published in the &lt;em&gt;Journal of Business and Economic Statistics&lt;/em&gt;, we considered how to do cluster-robust variance estimation in fixed effects models estimated by weighted (or unweighted) least squares. As explained in &lt;a href=&#34;http://localhost:4321/pusto-tipton-2018-theorem-2/&#34;&gt;my previous post&lt;/a&gt;, we were recently alerted that Theorem 2 in the paper is incorrect as stated. It turns out, the conditions in the original version of the theorem are too general. A more limited version of the Theorem does actually hold, but only for models estimated using ordinary (unweighted) least squares, under a working model that assumes independent, homoskedastic errors. In this post, I’ll give the revised theorem, following the notation and setup of &lt;a href=&#34;http://localhost:4321/pusto-tipton-2018-theorem-2/&#34;&gt;the previous post&lt;/a&gt; (so better read that first, or what follows won’t make much sense!).&lt;/p&gt;
&lt;div id=&#34;theorem-2-revised&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Theorem 2, revised&lt;/h3&gt;
&lt;p&gt;Consider the model
&lt;span class=&#34;math display&#34; id=&#34;eq:regression&#34;&gt;\[
\bm{y}_i = \bm{R}_i \bs\beta + \bm{S}_i \bs\gamma + \bm{T}_i \bs\mu + \bs\epsilon_i, \tag{1}
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\bm{y}_i\)&lt;/span&gt; is an &lt;span class=&#34;math inline&#34;&gt;\(n_i \times 1\)&lt;/span&gt; vector of responses for cluster &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\bm{R}_i\)&lt;/span&gt; is an &lt;span class=&#34;math inline&#34;&gt;\(n_i \times r\)&lt;/span&gt; matrix of focal predictors, &lt;span class=&#34;math inline&#34;&gt;\(\bm{S}_i\)&lt;/span&gt; is an &lt;span class=&#34;math inline&#34;&gt;\(n_i \times s\)&lt;/span&gt; matrix of additional covariates that vary across multiple clusters, and &lt;span class=&#34;math inline&#34;&gt;\(\bm{T}_i\)&lt;/span&gt; is an &lt;span class=&#34;math inline&#34;&gt;\(n_i \times t\)&lt;/span&gt; matrix encoding cluster-specific fixed effects, all for &lt;span class=&#34;math inline&#34;&gt;\(i = 1,...,m\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(\bm{U}_i = \left[ \bm{R}_i \ \bm{S}_i \right]\)&lt;/span&gt; be the set of predictors that vary across clusters and &lt;span class=&#34;math inline&#34;&gt;\(\bm{X}_i = \left[ \bm{R}_i \ \bm{S}_i \ \bm{T}_i \right]\)&lt;/span&gt; be the full set of predictors. Let &lt;span class=&#34;math inline&#34;&gt;\(\bm{\ddot{U}}_i = \left(\bm{I} - \bm{T}_i \bm{M}_{\bm{T}}\bm{T}_i&amp;#39;\right) \bm{U}_i\)&lt;/span&gt; be an absorbed version of the focal predictors and the covariates. The cluster-robust variance estimator for the coefficients of &lt;span class=&#34;math inline&#34;&gt;\(\bm{U}_i\)&lt;/span&gt; is
&lt;span class=&#34;math display&#34; id=&#34;eq:CRVE&#34;&gt;\[
\bm{V}^{CR2} = \bm{M}_{\bm{\ddot{U}}} \left(\sum_{i=1}^m \bm{\ddot{U}}_i&amp;#39; \bm{W}_i \bm{A}_i \bm{e}_i \bm{e}_i&amp;#39; \bm{A}_i \bm{W}_i \bm{\ddot{U}}_i \right) \bm{M}_{\bm{\ddot{U}}},
\tag{2}
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\bm{A}_1,...,\bm{A}_m\)&lt;/span&gt; are the CR2 adjustment matrices.&lt;/p&gt;
&lt;p&gt;If we assume a working model in which &lt;span class=&#34;math inline&#34;&gt;\(\bs\Psi_i = \sigma^2 \bm{I}_i\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(i = 1,...,m\)&lt;/span&gt; and estimate the model by ordinary least squares, then the CR2 adjustment matrices have a fairly simple form:
&lt;span class=&#34;math display&#34; id=&#34;eq:A-matrix&#34;&gt;\[
\bm{A}_i = \left(\bm{I}_i - \bm{X}_i \bm{M_X} \bm{X}_i&amp;#39;\right)^{+1/2},
\tag{3}
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(B^{+1/2}\)&lt;/span&gt; is the symmetric square root of the Moore-Penrose inverse of &lt;span class=&#34;math inline&#34;&gt;\(\bm{B}\)&lt;/span&gt;. However, this form is computationally expensive because it involves the full set of predictors, &lt;span class=&#34;math inline&#34;&gt;\(\bm{X}_i\)&lt;/span&gt;, including the cluster-specific fixed effects &lt;span class=&#34;math inline&#34;&gt;\(\bm{T}_i\)&lt;/span&gt;. If the model is estimated after absorbing the cluster-specific fixed effects, then it would be convenient to use the adjustment matrices based on the absorbed predictors only,
&lt;span class=&#34;math display&#34; id=&#34;eq:A-tilde&#34;&gt;\[
\bm{\tilde{A}}_i = \left(\bm{I}_i - \bm{\ddot{U}}_i \bm{M_\ddot{U}} \bm{\ddot{U}}_i&amp;#39;\right)^{+1/2}.
\tag{4}
\]&lt;/span&gt;
The original version of Theorem 2 asserted that &lt;span class=&#34;math inline&#34;&gt;\(\bm{A}_i = \bm{\tilde{A}}_i\)&lt;/span&gt;, which is not actually the case. However, for ordinary least squares with the independent, homoskedastic working model, we can show that &lt;span class=&#34;math inline&#34;&gt;\(\bm{A}_i \bm{\ddot{U}}_i = \bm{\tilde{A}}_i \bm{\ddot{U}}_i\)&lt;/span&gt;. Thus, it doesn’t matter whether we use &lt;span class=&#34;math inline&#34;&gt;\(\bm{A}_i\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(\bm{\tilde{A}}_i\)&lt;/span&gt; to calculate the cluster-robust variance estimator. We’ll get the same result either way, but &lt;span class=&#34;math inline&#34;&gt;\(\bm{\tilde{A}}_i\)&lt;/span&gt; is bit easier to compute.&lt;/p&gt;
&lt;p&gt;Here’s a formal statement of Theorem 2:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\bm{L}_i = \left(\bm{\ddot{U}}&amp;#39;\bm{\ddot{U}} - \bm{\ddot{U}}_i&amp;#39;\bm{\ddot{U}}_i\right)\)&lt;/span&gt; and assume that &lt;span class=&#34;math inline&#34;&gt;\(\bm{L}_1,...,\bm{L}_m\)&lt;/span&gt; have full rank &lt;span class=&#34;math inline&#34;&gt;\(r + s\)&lt;/span&gt;. If &lt;span class=&#34;math inline&#34;&gt;\(\bm{W}_i = \bm{I}_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\bs\Phi_i = \bm{I}_i\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(i = 1,...,m\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(\bm{A}_i \bm{\ddot{U}}_i = \bm{\tilde{A}}_i \bm{\ddot{U}}_i\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\bm{A}_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\tilde{\bm{A}}_i\)&lt;/span&gt; are as defined in &lt;a href=&#34;#eq:A-matrix&#34;&gt;(3)&lt;/a&gt; and &lt;a href=&#34;#eq:A-tilde&#34;&gt;(4)&lt;/a&gt;, respectively.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;proof&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Proof&lt;/h3&gt;
&lt;p&gt;We can prove this revised Theorem 2 by showing how &lt;span class=&#34;math inline&#34;&gt;\(\bm{A}_i\)&lt;/span&gt; can be constructed in terms of &lt;span class=&#34;math inline&#34;&gt;\(\bm{\tilde{A}}_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\bm{T}_i\)&lt;/span&gt;. First, because &lt;span class=&#34;math inline&#34;&gt;\(\bm{T}_i&amp;#39;\bm{T}_k = \bm{0}\)&lt;/span&gt; for any &lt;span class=&#34;math inline&#34;&gt;\(i \neq k\)&lt;/span&gt;, it follows that &lt;span class=&#34;math inline&#34;&gt;\(\bm{T}_i \bm{M_T} \bm{T}_i&amp;#39;\)&lt;/span&gt; is idempotent, i.e.,
&lt;span class=&#34;math display&#34;&gt;\[
\bm{T}_i \bm{M_T} \bm{T}_i&amp;#39; \bm{T}_i \bm{M_T} \bm{T}_i&amp;#39; = \bm{T}_i \bm{M_T} \bm{T}_i&amp;#39;.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Next, denote the thin QR decomposition of &lt;span class=&#34;math inline&#34;&gt;\(\bm{\ddot{U}}_i\)&lt;/span&gt; as &lt;span class=&#34;math inline&#34;&gt;\(\bm{Q}_i \bm{R}_i\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\bm{Q}_i\)&lt;/span&gt; is semi-orthogonal &lt;span class=&#34;math inline&#34;&gt;\((\bm{Q}_i&amp;#39;\bm{Q}_i = \bm{I})\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\bm{R}_i\)&lt;/span&gt; has the same rank as &lt;span class=&#34;math inline&#34;&gt;\(\bm{\ddot{U}}_i\)&lt;/span&gt;. Next, let &lt;span class=&#34;math inline&#34;&gt;\(\bm{\tilde{B}}_i = \bm{I}_i - \bm{\ddot{U}}_i \bm{M_\ddot{U}} \bm{\ddot{U}}_i&amp;#39;\)&lt;/span&gt; and observe that this can be written as
&lt;span class=&#34;math display&#34;&gt;\[
\tilde{\bm{B}}_i = \bm{I}_i - \bm{Q}_i \bm{Q}_i&amp;#39; + \bm{Q}_i \left(\bm{I} - \bm{R}_i \bm{M}_{\bm{\ddot{U}}} \bm{R}_i&amp;#39;\right)\bm{Q}_i&amp;#39;.
\]&lt;/span&gt;
It can then be seen that
&lt;span class=&#34;math display&#34;&gt;\[
\bm{\tilde{A}}_i = \tilde{\bm{B}}_i^{+1/2} = \bm{I}_i - \bm{Q}_i \bm{Q}_i&amp;#39; + \bm{Q}_i \left(\bm{I} - \bm{R}_i \bm{M}_{\bm{\ddot{U}}} \bm{R}_i&amp;#39;\right)^{+1/2} \bm{Q}_i&amp;#39;.
\]&lt;/span&gt;
It follows that &lt;span class=&#34;math inline&#34;&gt;\(\bm{\tilde{A}}_i \bm{T}_i = \bm{T}_i\)&lt;/span&gt; because &lt;span class=&#34;math inline&#34;&gt;\(\bm{Q}_i&amp;#39;\bm{T}_i = \bm{0}\)&lt;/span&gt;. Further, &lt;span class=&#34;math inline&#34;&gt;\(\bm{\tilde{B}}_i \bm{T}_i = \bm{T}_i\)&lt;/span&gt; as well.&lt;/p&gt;
&lt;p&gt;Now, let &lt;span class=&#34;math inline&#34;&gt;\(\bm{B}_i = \left(\bm{I}_i - \bm{X}_i \bm{M_X} \bm{X}_i&amp;#39;\right)\)&lt;/span&gt; and observe that this can be written as
&lt;span class=&#34;math display&#34;&gt;\[
\bm{B}_i = \bm{I}_i - \bm{\ddot{U}}_i \bm{M_{\ddot{U}}}\bm{\ddot{U}}_i&amp;#39; - \bm{T}_i \bm{M_T}\bm{T}_i&amp;#39; = \bm{\tilde{B}}_i - \bm{T}_i \bm{M_T}\bm{T}_i&amp;#39;
\]&lt;/span&gt;
because &lt;span class=&#34;math inline&#34;&gt;\(\bm{\ddot{U}}_i&amp;#39;\bm{T}_i = \bm{0}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We then construct the full adjustment matrix &lt;span class=&#34;math inline&#34;&gt;\(\bm{A}_i\)&lt;/span&gt; as
&lt;span class=&#34;math display&#34; id=&#34;eq:A-constructed&#34;&gt;\[
\bm{A}_i = \tilde{\bm{A}}_i - \bm{T}_i \bm{M_T}\bm{T}_i&amp;#39;.
\tag{5}
\]&lt;/span&gt;
Showing that &lt;span class=&#34;math inline&#34;&gt;\(\bm{B}_i \bm{A}_i \bm{B}_i \bm{A}_i = \bm{B}_i\)&lt;/span&gt; will suffice to verify that &lt;span class=&#34;math inline&#34;&gt;\(\bm{A}_i\)&lt;/span&gt; is the symmetric square root of the Moore-Penrose inverse of &lt;span class=&#34;math inline&#34;&gt;\(\bm{B}_i\)&lt;/span&gt;. Because &lt;span class=&#34;math inline&#34;&gt;\(\bm{T}_i \bm{M_T} \bm{T}_i&amp;#39;\)&lt;/span&gt; is idempotent, &lt;span class=&#34;math inline&#34;&gt;\(\bm{\tilde{B}}_i \bm{T}_i = \bm{T}_i\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\bm{\tilde{A}}_i \bm{T}_i = \bm{T}_i\)&lt;/span&gt;, we have
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\bm{B}_i \bm{A}_i \bm{B}_i \bm{A}_i &amp;amp;= \left(\tilde{\bm{B}}_i - \bm{T}_i \bm{M_T}\bm{T}_i&amp;#39;\right) \left(\tilde{\bm{A}}_i - \bm{T}_i \bm{M_T}\bm{T}_i&amp;#39;\right)\left(\tilde{\bm{B}}_i - \bm{T}_i \bm{M_T}\bm{T}_i&amp;#39;\right) \left(\tilde{\bm{A}}_i - \bm{T}_i \bm{M_T}\bm{T}_i&amp;#39;\right) \\
&amp;amp;= \left(\tilde{\bm{B}}_i\tilde{\bm{A}}_i - \bm{T}_i \bm{M_T}\bm{T}_i&amp;#39;\right)\left(\tilde{\bm{B}}_i\tilde{\bm{A}}_i - \bm{T}_i \bm{M_T}\bm{T}_i&amp;#39;\right) \\
&amp;amp;= \left(\tilde{\bm{B}}_i\tilde{\bm{A}}_i\tilde{\bm{B}}_i\tilde{\bm{A}}_i - \bm{T}_i \bm{M_T}\bm{T}_i&amp;#39;\right) \\
&amp;amp;= \left(\tilde{\bm{B}}_i - \bm{T}_i \bm{M_T}\bm{T}_i&amp;#39;\right) \\
&amp;amp;= \bm{B}_i.
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;From the representation of &lt;span class=&#34;math inline&#34;&gt;\(\bm{A}_i\)&lt;/span&gt; in &lt;a href=&#34;#eq:A-constructed&#34;&gt;(5)&lt;/a&gt;, it is clear that &lt;span class=&#34;math inline&#34;&gt;\(\bm{A}_i \bm{\ddot{U}}_i = \bm{\tilde{A}}_i \bm{\ddot{U}}_i - \bm{T}_i \bm{M_T} \bm{T}_i&amp;#39; \bm{\ddot{U}}_i = \bm{\tilde{A}}_i \bm{\ddot{U}}_i\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Corrigendum to Pustejovsky and Tipton (2018)</title>
      <link>http://localhost:4321/pusto-tipton-2018-theorem-2/</link>
      <pubDate>Wed, 28 Sep 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/pusto-tipton-2018-theorem-2/</guid>
      <description>


&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\def\Pr{{\text{Pr}}}
\def\E{{\text{E}}}
\def\Var{{\text{Var}}}
\def\Cov{{\text{Cov}}}
\def\bm{\mathbf}
\def\bs{\boldsymbol}
\]&lt;/span&gt;
In my &lt;a href=&#34;http://localhost:4321/publication/rve-in-fixed-effects-models/&#34;&gt;2018 paper with Beth Tipton&lt;/a&gt;, published in the &lt;em&gt;Journal of Business and Economic Statistics&lt;/em&gt;, we considered how to do cluster-robust variance estimation in fixed effects models estimated by weighted (or unweighted) least squares. A careful reader, &lt;a href=&#34;https://eeecon.uibk.ac.at/~pfaffermayr/&#34;&gt;Dr. Michael Pfaffermayr&lt;/a&gt;, recently alerted us to a problem with Theorem 2 in the paper, which concerns a computational short cut for a certain cluster-robust variance estimator in models with cluster-specific fixed effects. The theorem is incorrect as stated, and we are currently working on issuing a correction for the published version of the paper. In the interim, this post details the problem with Theorem 2. I’ll first review the CR2 variance estimator, then describe the assertion of the theorem, and then provide a numerical counter-example demonstrating that the assertion is not correct as stated.&lt;/p&gt;
&lt;div id=&#34;a-fixed-effects-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;A fixed effects model&lt;/h3&gt;
&lt;p&gt;For data that can be grouped into &lt;span class=&#34;math inline&#34;&gt;\(m\)&lt;/span&gt; clusters of observations, we considered the model
&lt;span class=&#34;math display&#34; id=&#34;eq:regression&#34;&gt;\[
\bm{y}_i = \bm{R}_i \bs\beta + \bm{S}_i \bs\gamma + \bm{T}_i \bs\mu + \bs\epsilon_i, \tag{1}
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\bm{y}_i\)&lt;/span&gt; is an &lt;span class=&#34;math inline&#34;&gt;\(n_i \times 1\)&lt;/span&gt; vector of responses for cluster &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\bm{R}_i\)&lt;/span&gt; is an &lt;span class=&#34;math inline&#34;&gt;\(n_i \times r\)&lt;/span&gt; matrix of focal predictors, &lt;span class=&#34;math inline&#34;&gt;\(\bm{S}_i\)&lt;/span&gt; is an &lt;span class=&#34;math inline&#34;&gt;\(n_i \times s\)&lt;/span&gt; matrix of additional covariates that vary across multiple clusters, and &lt;span class=&#34;math inline&#34;&gt;\(\bm{T}_i\)&lt;/span&gt; is an &lt;span class=&#34;math inline&#34;&gt;\(n_i \times t\)&lt;/span&gt; matrix encoding cluster-specific fixed effects, all for &lt;span class=&#34;math inline&#34;&gt;\(i = 1,...,m\)&lt;/span&gt;. The cluster-specific fixed effects satisfy &lt;span class=&#34;math inline&#34;&gt;\(\bm{T}_h \bm{T}_i&amp;#39; = \bm{0}\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(h \neq i\)&lt;/span&gt;. Interest centers on inference for the coefficients on the focal predictors &lt;span class=&#34;math inline&#34;&gt;\(\bs\beta\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We considered estimation of Model &lt;a href=&#34;#eq:regression&#34;&gt;(1)&lt;/a&gt; by weighted least squares (WLS), possibly under a working model for the distribution of &lt;span class=&#34;math inline&#34;&gt;\(\bs\epsilon_i\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(\bm{W}_1,...,\bm{W}_m\)&lt;/span&gt; be a set of symmetric weight matrices used for WLS estimation. Sometimes, these weight matrices may be diagonal, consisting of sampling weights for each observation. Other times, the weight matrices may involve off-diagonal terms as well. Consider a working model &lt;span class=&#34;math inline&#34;&gt;\(\Var\left(\bs\epsilon_i | \bm{R}_i, \bm{S}_i, \bm{T}_i\right) = \sigma^2 \bs\Phi_i\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\bs\Phi_i\)&lt;/span&gt; is a symmetric &lt;span class=&#34;math inline&#34;&gt;\(n_i \times n_i\)&lt;/span&gt; matrix that may be a function of a low-dimensional, estimable parameter. Based on this working model, the weight matrices might be taken as &lt;span class=&#34;math inline&#34;&gt;\(\bm{W}_i = \bs{\hat\Phi}_i^{-1}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\bs{\hat\Phi}_i\)&lt;/span&gt; is an estimate of &lt;span class=&#34;math inline&#34;&gt;\(\bs\Phi_i\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-cr2-variance-estimator&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The CR2 variance estimator&lt;/h3&gt;
&lt;p&gt;In the paper, we provide a generalization of the bias-reduced linearization estimator introduced by &lt;span class=&#34;citation&#34;&gt;McCaffrey et al. (&lt;a href=&#34;#ref-McCaffrey2001generalizations&#34; role=&#34;doc-biblioref&#34;&gt;2001&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;Bell &amp;amp; McCaffrey (&lt;a href=&#34;#ref-Bell2002bias&#34; role=&#34;doc-biblioref&#34;&gt;2002&lt;/a&gt;)&lt;/span&gt; that can be applied to Model &lt;a href=&#34;#eq:regression&#34;&gt;(1)&lt;/a&gt;. The variance estimator is effectively a generalization of the HC2 correction for heteroskedasticity-robust standard errors, but that works for models with within-cluster dependence and cluster-specific fixed effects, and so we refer to it the “CR2” estimator.&lt;/p&gt;
&lt;p&gt;In order to define the CR2 variance estimator and explain the issue with Theorem 2, I’ll need to lay down a bit more notation. Let &lt;span class=&#34;math inline&#34;&gt;\(N = \sum_{i=1}^m n_i\)&lt;/span&gt; be the total sample size. Let &lt;span class=&#34;math inline&#34;&gt;\(\bm{U}_i = \left[ \bm{R}_i \ \bm{S}_i \right]\)&lt;/span&gt; be the set of predictors that vary across clusters and &lt;span class=&#34;math inline&#34;&gt;\(\bm{X}_i = \left[ \bm{R}_i \ \bm{S}_i \ \bm{T}_i \right]\)&lt;/span&gt; be the full set of predictors. Let &lt;span class=&#34;math inline&#34;&gt;\(\bm{R}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\bm{S}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\bm{T}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\bm{U}\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\bm{X}\)&lt;/span&gt; denote the stacked versions of the cluster-specific matrices (i.e., &lt;span class=&#34;math inline&#34;&gt;\(\bm{R} = \left[\bm{R}_1&amp;#39; \ \bm{R}_2&amp;#39; \ \cdots \ \bm{R}_m&amp;#39;\right]&amp;#39;\)&lt;/span&gt;, etc.). Let &lt;span class=&#34;math inline&#34;&gt;\(\bm{W} = \bigoplus_{i=1}^m \bm{W}_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\bs\Phi = \bigoplus_{i=1}^m \bs\Phi_i\)&lt;/span&gt;. For a generic matrix &lt;span class=&#34;math inline&#34;&gt;\(\bm{Z}\)&lt;/span&gt;, let &lt;span class=&#34;math inline&#34;&gt;\(\bm{M}_{Z} = \left(\bm{Z}&amp;#39;\bm{W}\bm{Z}\right)^{-1}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\bm{H}_{\bm{Z}} = \bm{Z} \bm{M}_{\bm{Z}}\bm{Z}&amp;#39;\bm{W}\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(\bm{C}_i\)&lt;/span&gt; be the &lt;span class=&#34;math inline&#34;&gt;\(n_i \times N\)&lt;/span&gt; matrix that selects the rows of cluster &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; from the full set of observations, such that &lt;span class=&#34;math inline&#34;&gt;\(\bm{X}_i = \bm{C}_i \bm{X}\)&lt;/span&gt;. These operators provide an easy way to define absorbed versions of the predictors. Specifically, let &lt;span class=&#34;math inline&#34;&gt;\(\bm{\ddot{S}} = \left(\bm{I} - \bm{H}_{\bm{T}}\right) \bm{S}\)&lt;/span&gt; be the covariates after absorbing (i.e., partialling out) the cluster-specific effects, let &lt;span class=&#34;math inline&#34;&gt;\(\bm{\ddot{U}} = \left(\bm{I} - \bm{H}_{\bm{T}}\right) \bm{U}\)&lt;/span&gt; be an absorbed version of the focal predictors and the covariates, and let &lt;span class=&#34;math inline&#34;&gt;\(\bm{\ddot{R}} = \left(\bm{I} - \bm{H}_{\bm{\ddot{S}}}\right)\left(\bm{I} - \bm{H}_{\bm{T}}\right) \bm{R}\)&lt;/span&gt; be the focal predictors after absorbing the covariates and the cluster-specific fixed effects.&lt;/p&gt;
&lt;p&gt;With this notation established, the CR2 variance estimator has the form
&lt;span class=&#34;math display&#34;&gt;\[
\bm{V}^{CR2} = \bm{M}_{\bm{\ddot{R}}} \left(\sum_{i=1}^m \bm{\ddot{R}}_i&amp;#39; \bm{W}_i \bm{A}_i \bm{e}_i \bm{e}_i&amp;#39; \bm{A}_i \bm{W}_i \bm{\ddot{R}}_i \right) \bm{M}_{\bm{\ddot{R}}},
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\bm{\ddot{R}}_i = \bm{C}_i \bm{\ddot{R}}\)&lt;/span&gt; is the cluster-specific matrix of absorbed focal predictors, &lt;span class=&#34;math inline&#34;&gt;\(\bm{e}_i\)&lt;/span&gt; is the vector of weighted least squares residuals from cluster &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\bm{A}_1,...,\bm{A}_m\)&lt;/span&gt; are a set of adjustment matrices that correct the bias of the residual cross-products.
The adjustment matrices are calculated as follows. Let &lt;span class=&#34;math inline&#34;&gt;\(\bm{D}_i\)&lt;/span&gt; be the upper-right Cholesky factorization of &lt;span class=&#34;math inline&#34;&gt;\(\bm{\Phi}_i\)&lt;/span&gt; and define the matrices
&lt;span class=&#34;math display&#34; id=&#34;eq:B-matrix&#34;&gt;\[
\bm{B}_i = \bm{D}_i \bm{C}_i \left(\bm{I} - \bm{H}_{\bm{X}}\right) \bs\Phi \left(\bm{I} - \bm{H}_{\bm{X}}\right)&amp;#39;\bm{C}_i&amp;#39; \bm{D}_i&amp;#39;
\tag{2}
\]&lt;/span&gt;
for &lt;span class=&#34;math inline&#34;&gt;\(i = 1,...,m\)&lt;/span&gt;. The adjustment matrices are then calculated as
&lt;span class=&#34;math display&#34; id=&#34;eq:A-matrix&#34;&gt;\[
\bm{A}_i = \bm{D}_i&amp;#39; \bm{B}_i^{+1/2} \bm{D}_i,
\tag{3}
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\bm{B}_i^{+1/2}\)&lt;/span&gt; is the symmetric square root of the Moore-Penrose inverse of &lt;span class=&#34;math inline&#34;&gt;\(\bm{B}_i\)&lt;/span&gt;.
Theorem 1 in the paper shows that, if the working model &lt;span class=&#34;math inline&#34;&gt;\(\bs\Phi\)&lt;/span&gt; is correctly specified and some conditions on the rank of &lt;span class=&#34;math inline&#34;&gt;\(\bm{U}\)&lt;/span&gt; are satisfied, then the CR2 estimator is exactly unbiased for the sampling variance of the weighted least squares estimator of &lt;span class=&#34;math inline&#34;&gt;\(\bs\beta\)&lt;/span&gt;. Across multiple simulation studies, it’s been observed that the CR2 estimator also works well and outperforms alternative sandwich estimators even when the working model is not correctly specified.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;theorem-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Theorem 2&lt;/h3&gt;
&lt;p&gt;The adjustment matrices given in &lt;a href=&#34;#eq:A-matrix&#34;&gt;(3)&lt;/a&gt; can be expensive to compute directly because the &lt;span class=&#34;math inline&#34;&gt;\(\bm{B}_i\)&lt;/span&gt; matrices involve computing a “residualized” version of the &lt;span class=&#34;math inline&#34;&gt;\(N \times N\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(\bs\Phi\)&lt;/span&gt; involving the full set of predictors &lt;span class=&#34;math inline&#34;&gt;\(\bm{X}\)&lt;/span&gt;—including the cluster-specific fixed effects &lt;span class=&#34;math inline&#34;&gt;\(\bm{T}_1,...,\bm{T}_m\)&lt;/span&gt;. Theorem 2 considered whether one can take a computational short cut by omitting the cluster-specific fixed effects from the calculation of the &lt;span class=&#34;math inline&#34;&gt;\(\bm{B}_i\)&lt;/span&gt; matrices. Specifically, define the modified matrices
&lt;span class=&#34;math display&#34; id=&#34;eq:B-modified&#34;&gt;\[
\bm{\tilde{B}}_i = \bm{D}_i \bm{C}_i \left(\bm{I} - \bm{H}_{\bm{\ddot{U}}}\right) \bs\Phi \left(\bm{I} - \bm{H}_{\bm{\ddot{U}}}\right)&amp;#39;\bm{C}_i&amp;#39; \bm{D}_i&amp;#39;
\tag{4}
\]&lt;/span&gt;
and
&lt;span class=&#34;math display&#34; id=&#34;eq:A-modified&#34;&gt;\[
\bm{\tilde{A}}_i = \bm{D}_i&amp;#39; \bm{\tilde{B}}_i^{+1/2} \bm{D}_i,
\tag{5}.
\]&lt;/span&gt;
Theorem 2 claims that if the weight matrices are inverse of the working model, such that &lt;span class=&#34;math inline&#34;&gt;\(\bm{W}_i = \bs\Phi_i^{-1}\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(i = 1,...,m\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(\bm{\tilde{B}}_i^{+1/2} = \bm{B}_i^{+1/2}\)&lt;/span&gt; and hence &lt;span class=&#34;math inline&#34;&gt;\(\bm{\tilde{A}}_i = \bm{A}_i\)&lt;/span&gt;. The implication is that the cluster-specific fixed effects can be ignored when calculating the adjustment matrices. However, the claimed equivalence does not actually hold.&lt;/p&gt;
&lt;p&gt;Here is a simple numerical example that contradicts the assertion of Theorem 2. I first create a predictor matrix consisting of 4 clusters, a single focal predictor, and cluster-specific fixed effects.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(20220926)
m &amp;lt;- 4                                             # number of clusters
ni &amp;lt;- 2 + rpois(m, 3.5)                            # cluster sizes
N &amp;lt;- sum(ni)                                       # total sample size
id &amp;lt;- factor(rep(LETTERS[1:m], ni))                # cluster ID
R &amp;lt;- rnorm(N)                                      # focal predictor
dat &amp;lt;- data.frame(R, id)                           # create raw data frame
X &amp;lt;- model.matrix(~ R + id + 0, data = dat)        # full predictor matrix
Ui &amp;lt;- tapply(R, id, \(x) x - mean(x))              # absorbed version of R
U &amp;lt;- unsplit(Ui, id)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Consider a model estimated by ordinary least squares, where the assumed working model is homoskedastic and independent errors, so &lt;span class=&#34;math inline&#34;&gt;\(\bs\Phi_i = \bm{I}_i\)&lt;/span&gt;, an &lt;span class=&#34;math inline&#34;&gt;\(n_i \times n_i\)&lt;/span&gt; identity matrix (with no parameters to estimate). In this case, the adjustment matrices simplify considerably, to
&lt;span class=&#34;math display&#34;&gt;\[
\bm{A}_i = \left(\bm{I}_i - \bm{X}_i \bm{M}_{X} \bm{X}_i&amp;#39; \right)^{+1/2} \qquad \text{and} \qquad \bm{\tilde{A}}_i = \left(\bm{I}_i - \bm{\ddot{U}}_i \bm{M}_{\ddot{U}} \bm{\ddot{U}}_i&amp;#39; \right)^{+1/2}.
\]&lt;/span&gt;
I calculate these directly as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;matrix_power &amp;lt;- function(x, p) {
  eig &amp;lt;- eigen(x, symmetric = TRUE)
  val_p &amp;lt;- with(eig, ifelse(values &amp;gt; 10^-12, values^p, 0))
  with(eig, vectors %*% (val_p * t(vectors)))
}

MX &amp;lt;- solve(crossprod(X))
B &amp;lt;- 
  by(X, id, as.matrix) |&amp;gt;
  lapply(\(x) diag(nrow(x)) - x %*% MX %*% t(x))
A &amp;lt;- lapply(B, matrix_power, p = -1/2)
  
MU &amp;lt;- 1 / crossprod(U)
Btilde &amp;lt;- lapply(Ui, \(x) diag(length(x)) - x %*% MU %*% t(x))
Atilde &amp;lt;- lapply(Btilde, matrix_power, p = -1/2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here are the adjustment matrices based on the full predictor matrix &lt;span class=&#34;math inline&#34;&gt;\(\bm{X}\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(A, digits = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $A
##        [,1]   [,2]   [,3]   [,4]   [,5]
## [1,]  0.853 -0.198 -0.207 -0.191 -0.257
## [2,] -0.198  0.800 -0.200 -0.200 -0.202
## [3,] -0.207 -0.200  0.801 -0.201 -0.192
## [4,] -0.191 -0.200 -0.201  0.802 -0.210
## [5,] -0.257 -0.202 -0.192 -0.210  0.860
## 
## $B
##        [,1]   [,2]   [,3]
## [1,]  0.668 -0.338 -0.330
## [2,] -0.338  0.683 -0.345
## [3,] -0.330 -0.345  0.675
## 
## $C
##        [,1]   [,2]   [,3]   [,4]   [,5]   [,6]
## [1,]  0.873 -0.206 -0.163 -0.105 -0.233 -0.166
## [2,] -0.206  0.873 -0.171 -0.229 -0.100 -0.167
## [3,] -0.163 -0.171  0.834 -0.160 -0.173 -0.167
## [4,] -0.105 -0.229 -0.160  0.931 -0.271 -0.166
## [5,] -0.233 -0.100 -0.173 -0.271  0.946 -0.168
## [6,] -0.166 -0.167 -0.167 -0.166 -0.168  0.833
## 
## $D
##        [,1]   [,2]   [,3]
## [1,]  0.797 -0.342 -0.455
## [2,] -0.342  0.667 -0.325
## [3,] -0.455 -0.325  0.780&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Compare the above with the adjustment matrices based on the absorbed predictors only:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;print(Atilde, digits = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## $A
##          [,1]      [,2]     [,3]      [,4]     [,5]
## [1,]  1.05313  0.001860 -0.00742  0.008995 -0.05657
## [2,]  0.00186  1.000065 -0.00026  0.000315 -0.00198
## [3,] -0.00742 -0.000260  1.00104 -0.001257  0.00790
## [4,]  0.00900  0.000315 -0.00126  1.001523 -0.00958
## [5,] -0.05657 -0.001980  0.00790 -0.009576  1.06022
## 
## $B
##          [,1]     [,2]     [,3]
## [1,]  1.00139 -0.00478  0.00339
## [2,] -0.00478  1.01642 -0.01163
## [3,]  0.00339 -0.01163  1.00824
## 
## $C
##           [,1]      [,2]      [,3]      [,4]     [,5]      [,6]
## [1,]  1.039180 -0.039378  4.00e-03  0.061921 -0.06632  5.94e-04
## [2,] -0.039378  1.039577 -4.02e-03 -0.062234  0.06665 -5.97e-04
## [3,]  0.003999 -0.004019  1.00e+00  0.006320 -0.00677  6.07e-05
## [4,]  0.061921 -0.062234  6.32e-03  1.097861 -0.10481  9.39e-04
## [5,] -0.066317  0.066651 -6.77e-03 -0.104808  1.11225 -1.01e-03
## [6,]  0.000594 -0.000597  6.07e-05  0.000939 -0.00101  1.00e+00
## 
## $D
##          [,1]     [,2]    [,3]
## [1,]  1.13078 -0.00914 -0.1216
## [2,] -0.00914  1.00064  0.0085
## [3,] -0.12165  0.00850  1.1131&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The matrices differ:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all.equal(A, Atilde)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Component \&amp;quot;A\&amp;quot;: Mean relative difference: 0.6073885&amp;quot;
## [2] &amp;quot;Component \&amp;quot;B\&amp;quot;: Mean relative difference: 0.7403564&amp;quot;
## [3] &amp;quot;Component \&amp;quot;C\&amp;quot;: Mean relative difference: 0.5671847&amp;quot;
## [4] &amp;quot;Component \&amp;quot;D\&amp;quot;: Mean relative difference: 0.6682793&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thus, Theorem 2 is incorrect as stated. (I have yet to identify the mis-step in the proof as given in the supplementary materials of the paper.)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;further-thoughts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Further thoughts&lt;/h3&gt;
&lt;p&gt;For this particular model specification, it is interesting to note that &lt;span class=&#34;math inline&#34;&gt;\(\bm{\tilde{A}}_i = \bm{A}_i + \bm{T}_i \bm{M}_{\bm{T}} \bm{T}_i&amp;#39;\)&lt;/span&gt;. Because &lt;span class=&#34;math inline&#34;&gt;\(\bm{\ddot{U}}_i&amp;#39; \bm{T}_i = \bm{0}\)&lt;/span&gt;, it follows that
&lt;span class=&#34;math display&#34;&gt;\[
\bm{\ddot{U}}_i&amp;#39; \bm{\tilde{A}}_i = \bm{\ddot{U}}_i&amp;#39; \left(\bm{A}_i + \bm{T}_i \bm{M}_{\bm{T}} \bm{T}_i&amp;#39; \right) = \bm{\ddot{U}}_i&amp;#39; \bm{A}_i.
\]&lt;/span&gt;
This holds in the numerical example:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;UiAtilde &amp;lt;- mapply(\(u, a) t(u) %*% a, u = Ui, a = Atilde, SIMPLIFY = FALSE)
UiA &amp;lt;- mapply(\(u, a) t(u) %*% a, u = Ui, a = A, SIMPLIFY = FALSE)
all.equal(UiAtilde, UiA)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] TRUE&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thus, although the exact statement of Theorem 2 is incorrect, the substantive implication actually still holds. For this particular example, computing the CR2 variance estimator using the short-cut adjustment matrices &lt;span class=&#34;math inline&#34;&gt;\(\bm{\tilde{A}}_1,...,\bm{\tilde{A}}_m\)&lt;/span&gt; is equivalent to computing the CR2 variance estimator using the full model adjustment matrices &lt;span class=&#34;math inline&#34;&gt;\(\bm{A}_1,...,\bm{A}_m\)&lt;/span&gt;. However, I have not yet been able to work out the general conditions under which this equivalence holds. It may require stricter conditions than those assumed in Theorem 2.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3 unnumbered&#34;&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34; line-spacing=&#34;2&#34;&gt;
&lt;div id=&#34;ref-Bell2002bias&#34; class=&#34;csl-entry&#34;&gt;
Bell, R. M., &amp;amp; McCaffrey, D. F. (2002). &lt;span class=&#34;nocase&#34;&gt;Bias reduction in standard errors for linear regression with multi-stage samples&lt;/span&gt;. &lt;em&gt;Survey Methodology&lt;/em&gt;, &lt;em&gt;28&lt;/em&gt;(2), 169–181.
&lt;/div&gt;
&lt;div id=&#34;ref-McCaffrey2001generalizations&#34; class=&#34;csl-entry&#34;&gt;
McCaffrey, D. F., Bell, R. M., &amp;amp; Botts, C. H. (2001). &lt;span class=&#34;nocase&#34;&gt;Generalizations of biased reduced linearization&lt;/span&gt;. &lt;em&gt;Proceedings of the Annual Meeting of the American Statistical Association&lt;/em&gt;.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Inverting partitioned matrices</title>
      <link>http://localhost:4321/inverting-partitioned-matrices/</link>
      <pubDate>Wed, 20 Oct 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/inverting-partitioned-matrices/</guid>
      <description>


&lt;p&gt;There’s lots of linear algebra out there that’s quite useful for statistics, but that I never learned in school or never had cause to study in depth. In the same spirit as my &lt;a href=&#34;http://localhost:4321/Woodbury-identity/&#34;&gt;previous post on the Woodbury identity&lt;/a&gt;, I thought I would share my notes on another helpful bit of math about matrices. At some point in high school or college, you might have learned how to invert a small matrix by hand. You might recall the formula for the inverse of a two-by-two matrix:
&lt;span class=&#34;math display&#34;&gt;\[
\left[\begin{array}{cc} a &amp;amp; b \\ c &amp;amp; d\end{array}\right]^{-1} = \frac{1}{ad - bc}\left[\begin{array}{rr} d &amp;amp; -b \\ -c &amp;amp; a\end{array}\right].
\]&lt;/span&gt;
It turns out that there’s a straight-forward generalization of this formula to matrices of arbitrary size, but that are &lt;em&gt;partitioned&lt;/em&gt; into four pieces. The following is based on the presentation from some old notes by Dr. Thomas Minka, &lt;a href=&#34;https://tminka.github.io/papers/matrix/&#34;&gt;Old and New Matrix Algebra Useful for Statistics&lt;/a&gt;. The statement there is quite detailed and general. My version will be for a more specific, simple case, which I’ve found to be common and handy, and that can be presented in a fairly simple form.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{P}\)&lt;/span&gt; be a matrix of arbitrary size that is composed of four sub-matrices:
&lt;span class=&#34;math display&#34;&gt;\[
\mathbf{P} = \left[\begin{array}{cc} \mathbf{A} &amp;amp; \mathbf{B} \\ \mathbf{C} &amp;amp; \mathbf{D}\end{array}\right],
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{A}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{D}\)&lt;/span&gt; are &lt;span class=&#34;math inline&#34;&gt;\(a \times a\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(d \times d\)&lt;/span&gt; matrices, both of which are invertible, and where &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{B}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{C}\)&lt;/span&gt; are of conformable dimension.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; Let &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X} = \left(\mathbf{D} - \mathbf{C}\mathbf{A}^{-1} \mathbf{B}\right)^{-1}\)&lt;/span&gt;, a &lt;span class=&#34;math inline&#34;&gt;\(d \times d\)&lt;/span&gt; matrix. Then
&lt;span class=&#34;math display&#34;&gt;\[
\mathbf{P}^{-1} = \left[\begin{array}{cc} \mathbf{A}^{-1} + \mathbf{A}^{-1} \mathbf{B} \mathbf{X} \mathbf{C} \mathbf{A}^{-1} &amp;amp; - \mathbf{A}^{-1} \mathbf{B} \mathbf{X} \\ - \mathbf{X} \mathbf{C} \mathbf{A}^{-1} &amp;amp; \mathbf{X}\end{array}\right].
\]&lt;/span&gt;
This representation is particularly helpful if &lt;span class=&#34;math inline&#34;&gt;\(d &amp;lt; a\)&lt;/span&gt;, because in this case &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}\)&lt;/span&gt; is of lower dimension and so simpler (in a sense) than &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{A}^{-1}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Another equivalency is more helpful when &lt;span class=&#34;math inline&#34;&gt;\(d &amp;gt; a\)&lt;/span&gt;. Here, take &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{W} = \left(\mathbf{A} - \mathbf{B}\mathbf{D}^{-1} \mathbf{C}\right)^{-1}\)&lt;/span&gt;, an &lt;span class=&#34;math inline&#34;&gt;\(a \times a\)&lt;/span&gt; matrix (and so of lower dimension than &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{D}\)&lt;/span&gt;). Then
&lt;span class=&#34;math display&#34;&gt;\[
\mathbf{P}^{-1} = \left[\begin{array}{cc} \mathbf{W} &amp;amp; - \mathbf{W} \mathbf{B} \mathbf{D}^{-1} \\ - \mathbf{D}^{-1} \mathbf{C} \mathbf{W} &amp;amp; \mathbf{D}^{-1} + \mathbf{D}^{-1} \mathbf{C} \mathbf{W} \mathbf{B} \mathbf{D}^{-1}\end{array}\right].
\]&lt;/span&gt;
Of course, this is just two ways of writing the same thing. You can see this by applying &lt;a href=&#34;http://localhost:4321/Woodbury-identity/&#34;&gt;everyone’s favorite matrix identity&lt;/a&gt; to find that &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{W} = \mathbf{A}^{-1} + \mathbf{A}^{-1} \mathbf{B} \mathbf{X} \mathbf{C} \mathbf{A}^{-1}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X} = \mathbf{D}^{-1} + \mathbf{D}^{-1} \mathbf{C} \mathbf{W} \mathbf{B} \mathbf{D}^{-1}\)&lt;/span&gt;. It is an interesting little algebraic exercise to show that &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{W} \mathbf{B} \mathbf{D}^{-1} = \mathbf{A}^{-1} \mathbf{B} \mathbf{X}\)&lt;/span&gt; and that &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{D}^{-1} \mathbf{C} \mathbf{W} = \mathbf{X} \mathbf{C} \mathbf{A}^{-1}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;These representations of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{P}^{-1}\)&lt;/span&gt; are useful for a variety of statistical problems. To give just one example, they lead to a very direct proof of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Frisch%E2%80%93Waugh%E2%80%93Lovell_theorem&#34;&gt;Frisch-Waugh-Lovell theorem&lt;/a&gt;, including under more general conditions than are usually stated.&lt;/p&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;&lt;a href=&#34;https://tminka.github.io/papers/matrix/&#34;&gt;Minka’s notes&lt;/a&gt; on partitioned matrices treat a more general case, in which &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{A}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{D}\)&lt;/span&gt; need not be square matrices, nor must they be invertible.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The Woodbury identity</title>
      <link>http://localhost:4321/woodbury-identity/</link>
      <pubDate>Fri, 04 Dec 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:4321/woodbury-identity/</guid>
      <description>


&lt;p&gt;As in many parts of life, statistics is full of little bits of knowledge that are useful if you happen to know them, but which hardly anybody ever bothers to mention. You would think, if something is so useful, perhaps your professors would spend a fair bit of time explaining it to you. But maybe the stuff seems trivial, obvious, or simple to them, so they don’t bother.&lt;/p&gt;
&lt;p&gt;One example of this is Excel keyboard shortcuts. In a previous life, I was an Excel jockey so I learned all the keyboard shortcuts, such as how to move the cursor to the last cell in a continuous block of entries (&lt;code&gt;ctrl&lt;/code&gt; + an arrow key). Whenever I do this while sharing a screen in a meeting, someone is invariably astounded and wants to know what dark sorcery I’m conjuring. It’s a simple trick, but a useful one—especially if you’re working with a really large dataset with thousands of rows. But it’s also something that there’s no reason to expect anyone to figure out on their own, and that no stats or quant methods professor is going to spend class time demonstrating.&lt;/p&gt;
&lt;p&gt;Let me explain another, slightly more involved example, involving one of my favorite pieces of matrix algebra. There’s a thing called the Woodbury identity, also known as the Sherman-Morrison-Woodbury identity, that is a little life hack for inverting certain types of matrices. It has a &lt;a href=&#34;https://en.wikipedia.org/wiki/Woodbury_matrix_identity&#34;&gt;Wikipedia page&lt;/a&gt;, which I have visited many times. It is a very handy bit of math, if you happen to be a statistics student working with hierarchical models (such as meta-analytic models). I’ll give a statement of the identity, then explain a bit about the connection to hierarchical models.&lt;/p&gt;
&lt;div id=&#34;the-woodbury-identity&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The Woodbury identity&lt;/h1&gt;
&lt;p&gt;Say that you’ve got four matrices, an &lt;span class=&#34;math inline&#34;&gt;\(n \times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{A}\)&lt;/span&gt;, a &lt;span class=&#34;math inline&#34;&gt;\(k \times k\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{C}\)&lt;/span&gt;, an &lt;span class=&#34;math inline&#34;&gt;\(n \times k\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{U}\)&lt;/span&gt;, and a &lt;span class=&#34;math inline&#34;&gt;\(k \times n\)&lt;/span&gt; matrix &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{V}\)&lt;/span&gt;. Assume that &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{A}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{C}\)&lt;/span&gt; are invertible. The Woodbury identity tells you how to get the inverse of a certain combination of these matrices:
&lt;span class=&#34;math display&#34;&gt;\[
\left(\mathbf{A} + \mathbf{U} \mathbf{C} \mathbf{V}\right)^{-1} = \mathbf{A}^{-1} - \mathbf{A}^{-1} \mathbf{U} \left(\mathbf{C}^{-1} + \mathbf{V} \mathbf{A}^{-1} \mathbf{U} \right)^{-1} \mathbf{V} \mathbf{A}^{-1}.
\]&lt;/span&gt;
Admit it, you’re impressed. “Dude! Mind. Blown.” you’re probably saying to yourself right now.&lt;/p&gt;
&lt;p&gt;Or perhaps you’re still a touch skeptical that this formula is worth knowing. Let me explain the connection to hierarchical models.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;hierarchical-models&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Hierarchical models&lt;/h1&gt;
&lt;p&gt;Hierarchical linear models are a mainstay of statistical analysis in many, many areas of application, including education research, where we often deal with data collected on individuals (students, teachers) nested within larger aggregate units (like schools). In meta-analysis, these models come up if we’re dealing with samples that have more than one relevant outcome, so that we have multiple effect size estimates nested within a given sample or study.&lt;/p&gt;
&lt;p&gt;Suppose we have a hierarchical structure with &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; clusters, where cluster &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; has &lt;span class=&#34;math inline&#34;&gt;\(n_j\)&lt;/span&gt; individual observations. A quite general way of expressing a hierarchical model for such a data structure is
&lt;span class=&#34;math display&#34;&gt;\[
\mathbf{Y}_j = \mathbf{X}_j \boldsymbol\beta + \mathbf{Z}_j \boldsymbol\eta_j + \boldsymbol\epsilon_j,
\]&lt;/span&gt;
for &lt;span class=&#34;math inline&#34;&gt;\(j = 1,...,J\)&lt;/span&gt;, where, for cluster &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Y}_j\)&lt;/span&gt; is an &lt;span class=&#34;math inline&#34;&gt;\(n_j \times 1\)&lt;/span&gt; vector of outcomes,&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}_j\)&lt;/span&gt; is an &lt;span class=&#34;math inline&#34;&gt;\(n_j \times p\)&lt;/span&gt; design matrix for the fixed effects,&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\beta\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(p \times 1\)&lt;/span&gt; vector of fixed effect coefficients,&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Z}_j\)&lt;/span&gt; is an &lt;span class=&#34;math inline&#34;&gt;\(n_j \times q\)&lt;/span&gt; design matrix for the random effects,&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\eta_j\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(q \times 1\)&lt;/span&gt; vector of random effects, and&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\epsilon_j\)&lt;/span&gt; is an &lt;span class=&#34;math inline&#34;&gt;\(n_j \times 1\)&lt;/span&gt; vector of level-1 errors.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this model, we assume that the random effects have mean zero and unknown variance-covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{T}\)&lt;/span&gt;, often assumed to be an unstructured, symmetric and invertible matrix; we assume that the level-1 errors are also mean zero with variance-covariance matrix &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\Sigma_j\)&lt;/span&gt;; and we assume that &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\eta_j\)&lt;/span&gt; is independent of &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\epsilon_j\)&lt;/span&gt;. In many instances, we might assume that the entries of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{e}_j\)&lt;/span&gt; are all independent, so &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\Sigma_j\)&lt;/span&gt; will be a multiple of an identity matrix, &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\Sigma_j = \sigma^2 \mathbf{I}_j\)&lt;/span&gt;. In other instances (such as models for longitudinal data), &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\Sigma\)&lt;/span&gt; might be a patterned matrix that includes off-diagonal terms, such as an auto-regressive structure.&lt;/p&gt;
&lt;p&gt;What is the marginal variance of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Y}_j | \mathbf{X}_j\)&lt;/span&gt; in this model? In other words, if we combine the variance due to the random effects and the variance of the level-1 errors, what do we get? We get
&lt;span class=&#34;math display&#34;&gt;\[
\text{Var}\left(\mathbf{Y}_j | \mathbf{X}_j \right) = \mathbf{V}_j = \mathbf{Z}_j \mathbf{T} \mathbf{Z}_j&amp;#39; + \boldsymbol\Sigma_j,
\]&lt;/span&gt;
a matrix that, if you reverse the terms, looks like
&lt;span class=&#34;math display&#34;&gt;\[
\mathbf{V}_j = \boldsymbol\Sigma_j + \mathbf{Z}_j \mathbf{T} \mathbf{Z}_j&amp;#39;
\]&lt;/span&gt;
a simple form of the combination of matrices in the left-hand side of the Woodbury identity. Thus, the identity tells us how we can invert this matrix.&lt;/p&gt;
&lt;p&gt;But why would we care about inverting this variance-covariance matrix, you might ask? One good reason is that the fixed effect coefficients in the hierarchical model are estimated by weighted least squares, where the weight matrices are the inverse of an estimate of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{V}_j\)&lt;/span&gt;. Thus, to understand how the weights in a hierarchical model work, it’s quite useful to be able to invert &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{V}_j\)&lt;/span&gt;. Another good (related) reason is that the sampling variance of the fixed effect estimates is approximately
&lt;span class=&#34;math display&#34;&gt;\[
\text{Var}(\boldsymbol{\hat\beta}) \approx \left(\sum_{j=1}^J \mathbf{X}_j&amp;#39;\mathbf{V}_j^{-1} \mathbf{X}_j \right)^{-1}
\]&lt;/span&gt;
(it would be exact if we knew the parameters of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{V}_j\)&lt;/span&gt; with certainty). So if we want to understand the precision of &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\hat\beta}\)&lt;/span&gt; or the power of a hypothesis test involving &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol{\hat\beta}\)&lt;/span&gt;, then we we won’t be able to get very far without inverting &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{V}_j\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Directly applying the identity, we get
&lt;span class=&#34;math display&#34;&gt;\[
\mathbf{V}_j^{-1} = \boldsymbol\Sigma_j^{-1} - \boldsymbol\Sigma_j^{-1} \mathbf{Z}_j \left(\mathbf{T}^{-1} + \mathbf{Z}_j&amp;#39;\boldsymbol\Sigma_j^{-1}\mathbf{Z}_j \right)^{-1} \mathbf{Z}_j&amp;#39; \boldsymbol\Sigma_j^{-1}
\]&lt;/span&gt;
This expression looks like a bit of a mess, I’ll admit, but it can be useful. Things simplify quite a bit of &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\Sigma_j^{-1}\)&lt;/span&gt; has a form that is easy to invert (like a multiple of an identity matrix) and if the dimension of the random effects &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; is small. Under these conditions, &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\Sigma_j^{-1}\)&lt;/span&gt; is easy to work with, &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{T}^{-1}\)&lt;/span&gt; is manageable because it has small dimensions, and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Z}_j&amp;#39;\boldsymbol\Sigma_j^{-1}\mathbf{Z}_j\)&lt;/span&gt; becomes manageable because it also has small dimensions (&lt;span class=&#34;math inline&#34;&gt;\(q \times q\)&lt;/span&gt;, in both cases).&lt;/p&gt;
&lt;div id=&#34;random-intercepts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Random intercepts&lt;/h2&gt;
&lt;p&gt;As an example, consider a very simple model that includes only random intercepts, so &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Z}_j = \mathbf{1}_j\)&lt;/span&gt;, an &lt;span class=&#34;math inline&#34;&gt;\(n_j \times 1\)&lt;/span&gt; vector with every entry equal to 1, and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{T}\)&lt;/span&gt; is simply &lt;span class=&#34;math inline&#34;&gt;\(\tau^2\)&lt;/span&gt;, the variance of the random intercepts. For simplicity, let’s also assume that the level-1 errors are independent, so &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\Sigma_j = \sigma^2 \mathbf{I}_j\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\Sigma_j^{-1} = \sigma^{-2} \mathbf{I}_j\)&lt;/span&gt;. Applying the Woodbury identity,
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\mathbf{V}_j^{-1} &amp;amp;= \boldsymbol\Sigma_j^{-1} - \boldsymbol\Sigma_j^{-1} \mathbf{1}_j \left(\mathbf{T}^{-1} + \mathbf{1}_j&amp;#39;\boldsymbol\Sigma_j^{-1}\mathbf{1}_j \right)^{-1} \mathbf{1}_j&amp;#39; \boldsymbol\Sigma_j^{-1} \\
&amp;amp;= \sigma^{-2} \mathbf{I}_j - \sigma^{-4} \mathbf{1}_j \left(\tau^{-2} + \sigma^{-2} \mathbf{1}_j&amp;#39;\mathbf{1}_j \right)^{-1} \mathbf{1}_j&amp;#39; \\
&amp;amp;= \sigma^{-2} \mathbf{I}_j - \sigma^{-4} \left(\tau^{-2} + \sigma^{-2} n_j \right)^{-1} \mathbf{1}_j \mathbf{1}_j&amp;#39; \\
&amp;amp;= \sigma^{-2} \left(\mathbf{I}_j - \frac{\tau^2} {\sigma^2 + n_j \tau^2} \mathbf{1}_j \mathbf{1}_j&amp;#39;\right).
\end{aligned}
\]&lt;/span&gt;
Try checking this for yourself by carrying through the matrix algebra for &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{V}_j \mathbf{V}_j^{-1}\)&lt;/span&gt;, which should come out equal to &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{I}_j\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Now suppose that the design matrix is also quite simple, consisting of just an intercept term &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}_j = \mathbf{1}_j\)&lt;/span&gt;, so that &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\beta = \beta\)&lt;/span&gt; is simply a population mean. How precise is the estimate of the population mean from this hierarchical model? Well, the sampling variance of the estimator &lt;span class=&#34;math inline&#34;&gt;\(\hat\beta\)&lt;/span&gt; is approximately
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\text{Var}(\hat\beta) &amp;amp;\approx \left(\sum_{j=1}^J \mathbf{1}_j&amp;#39;\mathbf{V}_j^{-1} \mathbf{1}_j \right)^{-1} \\
&amp;amp;= \left(\sigma^{-2}\sum_{j=1}^J \mathbf{1}_j&amp;#39; \left(\mathbf{I}_j - \frac{\tau^2} {\sigma^2 + n_j \tau^2} \mathbf{1}_j \mathbf{1}_j&amp;#39;\right) \mathbf{1}_j \right)^{-1} \\
&amp;amp;= \left(\sigma^{-2} \sum_{j=1}^J n_j \left(1 - \frac{n_j \tau^2} {\sigma^2 + n_j \tau^2} \right)  \right)^{-1} \\ 
&amp;amp;= \left( \sigma^{-2} \sum_{j=1}^J \frac{n_j \sigma^2} {\sigma^2 + n_j \tau^2} \right)^{-1} \\ 
&amp;amp;= \left(\sum_{j=1}^J \frac{n_j} {\sigma^2 + n_j \tau^2} \right)^{-1} \\
&amp;amp;= \left(\sigma^2 + \tau^2\right) \left(\sum_{j=1}^J \frac{n_j} {1 + (n_j - 1) \rho} \right)^{-1},
\end{aligned}
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\rho = \tau^2 / (\tau^2 + \sigma^2)\)&lt;/span&gt; is the intra-class correlation. Squint at this expression for a bit and you can see how the ICC influences the varince. If &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt; is near zero, then the sampling variance will be close to &lt;span class=&#34;math inline&#34;&gt;\(\left(\sigma^2 + \tau^2\right) / N\)&lt;/span&gt;, which is what you would get if you treated every observation as independent. If &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt; is near 1, then the sampling variance ends up being nearly &lt;span class=&#34;math inline&#34;&gt;\(\left(\sigma^2 + \tau^2\right) / J\)&lt;/span&gt;, which is what you would get if you treated every cluster as a single observation. For intermediate ICCs, the sample size from cluster &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; (in the numerator of the fraction inside the summation) gets cut down to size accordingly.&lt;/p&gt;
&lt;p&gt;The estimator of the population mean is a weighted average of the outcomes. Specifically,
&lt;span class=&#34;math display&#34;&gt;\[
\hat\beta = \left(\sum_{j=1}^J \mathbf{1}_j&amp;#39;\mathbf{\hat{V}}_j^{-1} \mathbf{1}_j \right)^{-1} \sum_{j=1}^J \mathbf{1}_j&amp;#39;\mathbf{\hat{V}}_j^{-1} \mathbf{Y}_j,
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\hat{V}}_j\)&lt;/span&gt; is an estimator of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{V}_j\)&lt;/span&gt;. If you carry through the matrix algebra, you’ll find that
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
\hat\beta &amp;amp;= \left(\sum_{j=1}^J \frac{n_j} {\sigma^2 + n_j \tau^2} \right)^{-1} \sum_{j=1}^J \frac{\mathbf{1}_j&amp;#39;\mathbf{Y}_j}{\sigma^2 + n_j \tau^2} \\
&amp;amp;= \frac{1}{W} \sum_{j=1}^J \sum_{i=1}^{n_j} w_j y_{ij},
\end{aligned}
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(w_j = \frac{1}{1 + (n_j - 1) \rho}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\displaystyle{W = \sum_{j=1}^J n_j w_j}\)&lt;/span&gt;. From this, we can see that the weight of a given observation depends on the ICC and the size of the cluster. If the ICC is low, then weights will all be close to 1. For higher ICCs, observations in smaller clusters get proportionately &lt;em&gt;more&lt;/em&gt; weight than observations in larger clusters.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-meta-analysis-example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A meta-analysis example&lt;/h2&gt;
&lt;p&gt;In a &lt;a href=&#34;http://localhost:4321/weighting-in-multivariate-meta-analysis/&#34;&gt;previous post&lt;/a&gt; on multi-variate meta-analysis, I examined how weighting works in some multi-variate meta-analysis models, where you have multiple effect size estimates nested within a study. Letting &lt;span class=&#34;math inline&#34;&gt;\(T_{ij}\)&lt;/span&gt; denote effect size estimate &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; in study &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt;, for &lt;span class=&#34;math inline&#34;&gt;\(i = 1,...,n_j\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(j = 1,...,J\)&lt;/span&gt;. The first model I considered in the previous post was
&lt;span class=&#34;math display&#34;&gt;\[
T_{ij} = \mu + \eta_j + \nu_{ij} + e_{ij},
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}(\eta_j) = \tau^2\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}(\nu_{ij}) = \omega^2\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}(e_{ij}) = V_j\)&lt;/span&gt;, treated as known, and &lt;span class=&#34;math inline&#34;&gt;\(\text{cor}(e_{hj}, e_{ij}) = \rho\)&lt;/span&gt; for some specified value of &lt;span class=&#34;math inline&#34;&gt;\(\rho\)&lt;/span&gt;.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; This model makes the simplifying assumptions that the effect sizes within a given study all have the same sampling variance, &lt;span class=&#34;math inline&#34;&gt;\(V_j\)&lt;/span&gt;, and that there is a single correlation between pairs of outcomes from the same study, that is constant across all pairs of outcomes and across all studies.&lt;/p&gt;
&lt;p&gt;You can write this model in matrix form as
&lt;span class=&#34;math display&#34;&gt;\[
\mathbf{T}_j = \mu \mathbf{1}_j + \eta_j \mathbf{1}_j + \boldsymbol\nu_j + \mathbf{e}_j,
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}(\boldsymbol\nu_j) = \omega^2 \mathbf{I}_j\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}(\mathbf{e}_j) = V_j \left[\rho \mathbf{1}_j \mathbf{1}_j&amp;#39; + (1 - \rho) \mathbf{I}_j\right]\)&lt;/span&gt;. It follows that
&lt;span class=&#34;math display&#34;&gt;\[
\text{Var}(\mathbf{T}_j) = (\tau^2 + V_j\rho) \mathbf{1}_j \mathbf{1}_j&amp;#39; + [\omega^2 + V_j (1 - \rho)] \mathbf{I}_j.
\]&lt;/span&gt;
The Woodbury identity comes in handy here again, if we want to examine the weights implied by this model or the sampling variance of the overall average effect size estimator.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; I’ll leave it as an exercise to find an expression for the weight assigned to effect size &lt;span class=&#34;math inline&#34;&gt;\(T_{ij}\)&lt;/span&gt; under this model.&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; You could also try finding an expression for the variance of the overall average effect size estimator &lt;span class=&#34;math inline&#34;&gt;\(\hat\mu\)&lt;/span&gt;, based on inverse-variance weighting, when the model is correctly specified.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;another-meta-analysis-example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Another meta-analysis example&lt;/h2&gt;
&lt;p&gt;In the &lt;a href=&#34;http://localhost:4321/weighting-in-multivariate-meta-analysis/&#34;&gt;previous post&lt;/a&gt;, I also covered weighting in a bit more general model, where the sampling variances and correlations are no longer quite so constrained. As before, we have
&lt;span class=&#34;math display&#34;&gt;\[
\mathbf{T}_j = \mu \mathbf{1}_j + \eta_j \mathbf{1}_j + \boldsymbol\nu_j + \mathbf{e}_j,
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}(\eta_j) = \tau^2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}(\boldsymbol\nu_j) = \omega^2 \mathbf{I}_j\)&lt;/span&gt;. But now let &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}(\mathbf{e}_j) = \boldsymbol\Sigma_j\)&lt;/span&gt; for some arbitrary, symmetric, invertible matrix &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\Sigma_j\)&lt;/span&gt;. The marginal variance of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{T}_j\)&lt;/span&gt; is therefore
&lt;span class=&#34;math display&#34;&gt;\[
\text{Var}(\mathbf{T}_j) = \tau^2\mathbf{1}_j \mathbf{1}_j&amp;#39; + \omega^2 \mathbf{I}_j + \boldsymbol\Sigma_j.
\]&lt;/span&gt;
Let &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{S}_j = \left(\omega^2 \mathbf{I}_j + \boldsymbol\Sigma_j\right)^{-1}\)&lt;/span&gt;. Try applying the Woodbury identity to invert &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}(\mathbf{T}_j)\)&lt;/span&gt; in terms of &lt;span class=&#34;math inline&#34;&gt;\(\tau^2\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(n_j\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{S}_j\)&lt;/span&gt;. Then see if you can derive the weight assigned to effect &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; in study &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; under this model. See the previous post for the solution.&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;This model is what we call the “correlated-and-hierarchical effects model” in my paper (with Beth Tipton) on &lt;a href=&#34;http://localhost:4321/publication/rve-meta-analysis-expanding-the-range/&#34;&gt;extending working models for robust variance estimation&lt;/a&gt;.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Or squint hard at the formula for the variance of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{T}_j\)&lt;/span&gt;, and you’ll see that it has the same form as the random intercepts model in the previous example. Just replace the &lt;span class=&#34;math inline&#34;&gt;\(\tau^2\)&lt;/span&gt; in that model with &lt;span class=&#34;math inline&#34;&gt;\(\tau^2 + V_j \rho\)&lt;/span&gt; and replace the &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; in that model with &lt;span class=&#34;math inline&#34;&gt;\(\omega^2 + V_j (1 - \rho)\)&lt;/span&gt;.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;See the &lt;a href=&#34;http://localhost:4321/weighting-in-multivariate-meta-analysis/&#34;&gt;previous post&lt;/a&gt; for the answer.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;In the previous post, I expressed the weights in terms of &lt;span class=&#34;math inline&#34;&gt;\(s_{ij}\)&lt;/span&gt;, the sum of the entries in row &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; of the &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{S}_j\)&lt;/span&gt; matrix. In vector form, &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{s}_j = \left(s_{1j} \ s_{2j} \ \cdots \ s_{n_j j}\right)&amp;#39; = \mathbf{S}_j \mathbf{1}_j\)&lt;/span&gt;.&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
