<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>simulation | James E. Pustejovsky</title>
    <link>https://www.jepusto.com/tags/simulation/</link>
      <atom:link href="https://www.jepusto.com/tags/simulation/index.xml" rel="self" type="application/rss+xml" />
    <description>simulation</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2023</copyright><lastBuildDate>Wed, 06 Dec 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://www.jepusto.com/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>simulation</title>
      <link>https://www.jepusto.com/tags/simulation/</link>
    </image>
    
    <item>
      <title>Implementing Consul&#39;s generalized Poisson distribution in Stan</title>
      <link>https://www.jepusto.com/generalized-poisson-in-stan/</link>
      <pubDate>Wed, 06 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://www.jepusto.com/generalized-poisson-in-stan/</guid>
      <description>


&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\def\Pr{{\text{Pr}}}
\def\E{{\text{E}}}
\def\Var{{\text{Var}}}
\def\Cov{{\text{Cov}}}
\def\bm{\mathbf}
\def\bs{\boldsymbol}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For a project I am working on, we are using &lt;a href=&#34;https://mc-stan.org/&#34;&gt;Stan&lt;/a&gt; to fit generalized random effects location-scale models to a bunch of count data.
In &lt;a href=&#34;https://www.jepusto.com/double-poisson-in-Stan/&#34;&gt;a previous post&lt;/a&gt;, I walked through our implementation of &lt;a href=&#34;https://doi.org/10.2307/2289002&#34;&gt;Efron’s (1986)&lt;/a&gt; double-Poisson distribution, which we are interested in using because it allows for both over- and under-dispersion relative to the Poisson distribution.
Another distribution with these properties is the generalized Poisson distribution described by &lt;a href=&#34;https://doi.org/10.1080/00401706.1973.10489112&#34;&gt;Consul and Jain (1973)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this post, I’ll walk through my implementation of the GPO in Stan.
The &lt;a href=&#34;https://cran.r-project.org/package=gamlss.dist&#34;&gt;&lt;code&gt;gamlss.dist&lt;/code&gt; package&lt;/a&gt; provides a full set of distributional functions for the generalized Poisson distribution, including a sampler, but the functions are configured to only allow for over-dispersion. Since I’m interested in allowing for under-dispersion as well, I’ll need to write my own functions. As in my previous post, I can validate my Stan functions against the functions from &lt;code&gt;gamlss.dist&lt;/code&gt; (although only for over-dispersion scenarios).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(patchwork)   # composing figures
library(gamlss.dist) # DPO distribution functions
library(rstan)       # Stan interface to R
library(brms)        # fitting generalized linear models
library(bayesplot)   # Examine fitted models
library(loo)         # Model fit measures&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;the-generalized-poisson&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The generalized Poisson&lt;/h2&gt;
&lt;p&gt;Consul and Jain’s generalized Poisson distribution is a discrete distribution for non-negative counts, with support &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{S}_X = \{0, 1, 2, 3, ...\}\)&lt;/span&gt;.
The mean-variance relationship of the generalized Poisson is constant; for &lt;span class=&#34;math inline&#34;&gt;\(X \sim GPO(\mu, \phi)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\text{E}(X) = \mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}(X) = \mu / \phi\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(0 &amp;lt; \phi &amp;lt; 1\)&lt;/span&gt;; the expectation and variance are not exact but are close approximations when there is underdispersion, so &lt;span class=&#34;math inline&#34;&gt;\(\phi &amp;gt; 1\)&lt;/span&gt;. Thus, like the double-Poisson distribution, the generalized Poisson satisfies the assumptions of a quasi-Poisson generalized linear model (at least approximately).&lt;/p&gt;
&lt;p&gt;The density of the generalized Poisson distribution with mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and inverse-disperson &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; is:
&lt;span class=&#34;math display&#34;&gt;\[
f(x | \mu, \phi) = \mu \sqrt{\phi} \left( x + \sqrt{\phi}(\mu - x) \right)^{x-1} \frac{\exp \left[-\left( x + \sqrt{\phi}(\mu - x)\right)\right]}{x!}.
\]&lt;/span&gt;
We then have
&lt;span class=&#34;math display&#34;&gt;\[
\ln f(x | \mu, \phi) = \frac{1}{2} \ln \phi + \ln \mu + (x - 1) \ln \left( x + \sqrt{\phi}(\mu - x) \right) - \left( x + \sqrt{\phi}(\mu - x) \right) - \ln \left(x!\right).
\]&lt;/span&gt;
Using the GPO with under-dispersed data is a little bit more controversial (by statistical standards) than using the DPO.
This is because, for parameter values corresponding to under-dispersion, its probability mass function becomes negative for large counts. In particular, note that for values &lt;span class=&#34;math inline&#34;&gt;\(x &amp;gt; \frac{\mu\sqrt\phi}{\sqrt\phi - 1}\)&lt;/span&gt;, the quantity &lt;span class=&#34;math inline&#34;&gt;\(x + \sqrt{\phi}(\mu - x)\)&lt;/span&gt; becomes negative, and so &lt;span class=&#34;math inline&#34;&gt;\(f(x| \mu, \phi)\)&lt;/span&gt; is no longer a proper probability.
Consul suggested handling this situation by truncating the distribution at &lt;span class=&#34;math inline&#34;&gt;\(m = \left\lfloor \frac{\mu\sqrt\phi}{\sqrt\phi - 1}\right\rfloor\)&lt;/span&gt;. However, doing so makes the distribution only an approximation, such that &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; is no longer exactly the mean and &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; is no longer exactly the inverse dispersion.
For modest under-dispersion of no less than 60% of the mean, &lt;span class=&#34;math inline&#34;&gt;\(1 &amp;lt; \phi &amp;lt; 5 / 3\)&lt;/span&gt; and the truncation point is fairly extreme, with &lt;span class=&#34;math inline&#34;&gt;\(m \approx 4.4 \mu\)&lt;/span&gt;, so I’m not too worried about this issue.
We’ll see how it plays out in application, of course.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;log-of-the-probability-mass-function&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Log of the probability mass function&lt;/h1&gt;
&lt;p&gt;Here’s a Stan function implementing the lpmf, with the truncation bit:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stancode_lpmf &amp;lt;- &amp;quot;
real gpo_lpmf(int X, real mu, real phi) {
  real ans;
  real m = mu / (1 - inv(sqrt(phi)));
  real z = X + sqrt(phi) * (mu - X);
  if (phi &amp;gt; 1 &amp;amp;&amp;amp; X &amp;gt; m)
    ans = negative_infinity();
  else 
    ans = log(mu) + inv(2) * log(phi) + (X - 1) * log(z) - z - lgamma(X + 1);
  return ans;
}
&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To check that this is accurate, I’ll compare the Stan function to the corresponding function from &lt;code&gt;gamlss.dist&lt;/code&gt; for a couple of different parameter values and for &lt;span class=&#34;math inline&#34;&gt;\(x = 0,...,100\)&lt;/span&gt;. If my function is accurate, my calculated log-probabilities should be equal to the results from &lt;code&gt;gamlss.dist::dGPO&lt;/code&gt;. Note that the &lt;code&gt;gamlss.dist&lt;/code&gt; function uses a different parameterization for the dispersion, with &lt;span class=&#34;math inline&#34;&gt;\(\sigma = \frac{\phi^{-1/2} - 1}{\mu}\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;writeLines(paste(&amp;quot;functions {&amp;quot;, stancode_lpmf, &amp;quot;}&amp;quot;, sep = &amp;quot;\n&amp;quot;), &amp;quot;GPO-lpmf.stan&amp;quot;)
expose_stan_functions(&amp;quot;GPO-lpmf.stan&amp;quot;)

test_lpmf &amp;lt;- 
  expand_grid(
    mu = c(2, 5, 10, 20),
    phi = seq(0.1, 0.9, 0.1),
    X = 0:100
  ) %&amp;gt;%
  mutate(
    sigma = (phi^(-1/2) - 1) / mu,
    gamlss_lpmf = dGPO(x = X, mu = mu, sigma = sigma, log = TRUE),
    my_lpmf = pmap_dbl(.l = list(X = X, mu = mu, phi = phi), .f = gpo_lpmf),
    diff = my_lpmf - gamlss_lpmf
  )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(test_lpmf, aes(factor(phi), diff, color = factor(phi))) + 
  geom_boxplot() + 
  facet_wrap( ~ mu, labeller = &amp;quot;label_both&amp;quot;, ncol = 2) + 
  theme_minimal() + 
  labs(x = &amp;quot;phi&amp;quot;) + 
  theme(legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jepusto.com/post/Generalized-Poisson-in-Stan_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Checks out. Onward!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;quantile-function&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Quantile function&lt;/h1&gt;
&lt;p&gt;I’ll next implement the generalized Poisson quantile function, taking advantage of a simple recurrence relationship for sequential values. Note that
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(0 | \mu, \phi) &amp;amp;= \exp \left(-\mu \sqrt{\phi}\right) \\
f(x | \mu, \phi) &amp;amp;= f(x - 1 | \mu, \phi) \times \frac{\left(x + \sqrt{\phi}(\mu - x)\right)^{x - 1}}{\left(x - 1 + \sqrt{\phi}(\mu - (x - 1))\right)^{x - 2}} \times \frac{\exp(\sqrt{\phi} - 1)}{x}
\end{aligned}
\]&lt;/span&gt;
where the second expression holds for &lt;span class=&#34;math inline&#34;&gt;\(x \geq 1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The function below computes the quantile given a value &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; by computing the cumulative distribution function until &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is exceeded:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stancode_quantile &amp;lt;- &amp;quot; 
int gpo_quantile(real p, real mu, real phi) {
  int q = 0;
  real phi_sqrt = sqrt(phi);
  real mu_phi_sqrt = mu * phi_sqrt;
  real m;
  if (phi &amp;gt; 1) 
    m = mu / (1 - inv(phi_sqrt));
  else 
    m = positive_infinity();
  real lpmf = - mu_phi_sqrt;
  real cdf = exp(lpmf);
  real ln_inc;
  while (cdf &amp;lt; p &amp;amp;&amp;amp; q &amp;lt; m) {
    q += 1;
    ln_inc = (q - 1) * log(mu_phi_sqrt + q * (1 - phi_sqrt)) - (q - 2) * log(mu_phi_sqrt + (q - 1) * (1 - phi_sqrt)) + (phi_sqrt - 1) - log(q);
    lpmf += ln_inc;    
    cdf += exp(lpmf);
  }
  return q;
}
&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If my quantile function is accurate, it should match the value computed from &lt;code&gt;gamlss.dist::qDPO()&lt;/code&gt; exactly.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;writeLines(paste(&amp;quot;functions {&amp;quot;, stancode_quantile, &amp;quot;}&amp;quot;, sep = &amp;quot;\n&amp;quot;), &amp;quot;GPO-quantile.stan&amp;quot;)
expose_stan_functions(&amp;quot;GPO-quantile.stan&amp;quot;)

test_quantile &amp;lt;- 
  expand_grid(
    mu = c(2, 5, 10, 20),
    phi = seq(0.1, 0.9, 0.1),
  ) %&amp;gt;%
  mutate(
    sigma = (phi^(-1/2) - 1) / mu,
    p = map(1:n(), ~ runif(100)),
  ) %&amp;gt;%
  unnest(p) %&amp;gt;%
  mutate(
    my_q = pmap_dbl(list(p = p, mu = mu, phi = phi), .f = gpo_quantile),
    gamlss_q = qGPO(p, mu = mu, sigma = sigma),
    diff = my_q - gamlss_q
  )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(test_quantile, aes(factor(phi), diff, color = factor(phi))) + 
  geom_boxplot() + 
  facet_wrap( ~ mu, labeller = &amp;quot;label_both&amp;quot;, ncol = 2) + 
  theme_minimal() + 
  labs(x = &amp;quot;phi&amp;quot;) + 
  theme(legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jepusto.com/post/Generalized-Poisson-in-Stan_files/figure-html/check-quantile-plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;I should enter this figure in the competition for the world’s most boring statistical graphic.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sampler&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Sampler&lt;/h1&gt;
&lt;p&gt;The last thing I’ll need is a sampler, which I’ll implement by generating random points from a uniform distribution, then computing the generalized Poisson quantiles of these random points. My implementation just generates a single random variate:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stancode_qr &amp;lt;- &amp;quot;
int gpo_quantile(real p, real mu, real phi) {
  int q = 0;
  real phi_sqrt = sqrt(phi);
  real mu_phi_sqrt = mu * phi_sqrt;
  real m;
  if (phi &amp;gt; 1) 
    m = mu / (1 - inv(phi_sqrt));
  else 
    m = positive_infinity();
  real lpmf = - mu_phi_sqrt;
  real cdf = exp(lpmf);
  real ln_inc;
  while (cdf &amp;lt; p &amp;amp;&amp;amp; q &amp;lt; m) {
    q += 1;
    ln_inc = (q - 1) * log(mu_phi_sqrt + q * (1 - phi_sqrt)) - (q - 2) * log(mu_phi_sqrt + (q - 1) * (1 - phi_sqrt)) + (phi_sqrt - 1) - log(q);
    lpmf += ln_inc;    
    cdf += exp(lpmf);
  }
  return q;
}

int gpo_rng(real mu, real phi) {
  real p = uniform_rng(0,1);
  int x = gpo_quantile(p, mu, phi);
  return x;
}
&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To check this function, I’ll generate some large samples from the generalized Poisson with a few different parameter sets. If the sampler is working properly, the empirical cumulative distribution should line up closely with the cumulative distribution computed using &lt;code&gt;gamlss.dist::pGPO()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;writeLines(paste(&amp;quot;functions {&amp;quot;, stancode_qr, &amp;quot;}&amp;quot;, sep = &amp;quot;\n&amp;quot;), &amp;quot;GPO-rng.stan&amp;quot;)
expose_stan_functions(&amp;quot;GPO-rng.stan&amp;quot;)

gpo_rng_sampler &amp;lt;- function(N, mu, phi) {
  replicate(N, gpo_rng(mu = mu, phi = phi))
}

test_rng &amp;lt;- 
  expand_grid(
    mu = c(2, 5, 10, 20),
    phi = seq(0.1, 0.9, 0.1),
  ) %&amp;gt;%
  mutate(
    sigma = (phi^(-1/2) - 1) / mu,
    x = pmap(.l = list(N = 10000, mu = mu, phi = phi), .f = gpo_rng_sampler),
    tb = map(x, ~ as.data.frame(table(.x)))
  ) %&amp;gt;%
  dplyr::select(-x) %&amp;gt;%
  group_by(mu, phi) %&amp;gt;%
  unnest(tb) %&amp;gt;%
  mutate(
    .x = as.integer(levels(.x))[.x],
    Freq_cum = cumsum(Freq) / 10000,
    gamlss_F = pGPO(q = .x, mu = mu, sigma = sigma)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(test_rng, aes(gamlss_F, Freq_cum, color = factor(phi))) + 
  geom_abline(slope = 1, color = &amp;quot;blue&amp;quot;, linetype = &amp;quot;dashed&amp;quot;) + 
  geom_point() + geom_line() +  
  facet_grid(phi ~ mu, labeller = &amp;quot;label_both&amp;quot;) + 
  theme_minimal() + 
  labs(x = &amp;quot;Theoretical cdf (gamlss.dist)&amp;quot;, y = &amp;quot;Empirical cdf (my function)&amp;quot;) + 
  theme(legend.position = &amp;quot;none&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jepusto.com/post/Generalized-Poisson-in-Stan_files/figure-html/check-rng-plot-1.png&#34; width=&#34;672&#34; /&gt;
Another approach to checking the sampler is to simulate a bunch of observations and check whether the empirical mean and variance match the theoretical moments. I’ll do this as well, using some values of &lt;span class=&#34;math inline&#34;&gt;\(\phi &amp;gt; 1\)&lt;/span&gt; to test whether the sampler still works when there’s under-dispersion.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_moments &amp;lt;- 
  expand_grid(
    mu = c(5, 10, 20, 40, 60),
    phi = seq(1, 2, 0.1),
  ) %&amp;gt;%
  mutate(
    x = pmap(.l = list(N = 1e5, mu = mu, phi = phi), .f = gpo_rng_sampler),
    M = map_dbl(x, mean),
    S = map_dbl(x, sd),
    M_ratio = M / mu,
    S_ratio = S / sqrt(mu / phi)
  ) %&amp;gt;%
  dplyr::select(-x) %&amp;gt;%
  pivot_longer(ends_with(&amp;quot;_ratio&amp;quot;),names_to = &amp;quot;moment&amp;quot;,values_to = &amp;quot;ratio&amp;quot;) %&amp;gt;%
  mutate(
    moment = factor(moment, levels = c(&amp;quot;M_ratio&amp;quot;, &amp;quot;S_ratio&amp;quot;), labels = c(&amp;quot;Sample mean&amp;quot;, &amp;quot;Standard deviation&amp;quot;)),
    mu = factor(mu)
  )

ggplot(test_moments, aes(phi, ratio, color = mu)) + 
  geom_point() + geom_line() + 
  facet_wrap(~ moment) + 
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jepusto.com/post/Generalized-Poisson-in-Stan_files/figure-html/test-sample-moments-1.png&#34; width=&#34;768&#34; /&gt;
Looks like the sample moments closely match the parameter values, with deviations that look pretty much like random error. Nice!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;using-the-custom-distribution-functions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Using the custom distribution functions&lt;/h1&gt;
&lt;p&gt;To finish out my tests of these functions, I’ll try out a small simulation. Following my &lt;a href=&#34;https://www.jepusto.com/Double-Poisson-in-Stan/&#34;&gt;previous post&lt;/a&gt;, I’ll generate data based on a generalized linear model with a single predictor &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, where the outcome &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; follows a generalized Poisson distribution conditional on &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. The data-generating process is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
X &amp;amp;\sim N(0, 1) \\
Y|X &amp;amp;\sim GPO(\mu(X), \phi) \\
\log \mu(X) &amp;amp;= 2 + 0.3 \times X
\end{aligned}
\]&lt;/span&gt;
with the dispersion parameter set to &lt;span class=&#34;math inline&#34;&gt;\(\phi = 10 / 7\)&lt;/span&gt; so that the outcome is &lt;em&gt;under&lt;/em&gt;-dispersed.&lt;/p&gt;
&lt;p&gt;The following code generates a large sample from the data-generating process:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(20231204)
N &amp;lt;- 600
X &amp;lt;- rnorm(N)
mu &amp;lt;- exp(2 + 0.3 * X)
phi &amp;lt;- 10 / 7
Y &amp;lt;- map_dbl(mu, gpo_rng, phi = phi)
dat &amp;lt;- data.frame(X = X, Y = Y)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s what the sample looks like, along with a smoothed regression estimated using a basic cubic spline:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(dat, aes(X, Y)) + 
  geom_point(alpha = 0.1) + 
  geom_smooth(method = &amp;#39;gam&amp;#39;, formula = y ~ s(x, bs = &amp;quot;cs&amp;quot;)) + 
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jepusto.com/post/Generalized-Poisson-in-Stan_files/figure-html/glm-scatterplot-1.png&#34; width=&#34;576&#34; /&gt;
Here is a fit using quasi-likelihood estimation of a log-linear model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;quasi_fit &amp;lt;- glm(Y ~ X, family = quasipoisson(link = &amp;quot;log&amp;quot;), data = dat)
summary(quasi_fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = Y ~ X, family = quasipoisson(link = &amp;quot;log&amp;quot;), data = dat)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.8059  -0.5958  -0.0308   0.5275   2.8044  
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  1.99558    0.01273  156.71   &amp;lt;2e-16 ***
## X            0.30081    0.01174   25.61   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for quasipoisson family taken to be 0.6689639)
## 
##     Null deviance: 852.98  on 599  degrees of freedom
## Residual deviance: 413.94  on 598  degrees of freedom
## AIC: NA
## 
## Number of Fisher Scoring iterations: 4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This approach recovers the data-generating parameters quite well, with a dispersion estimate of 0.669 compared to the true dispersion parameter of 0.7.&lt;/p&gt;
&lt;div id=&#34;candidate-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Candidate models&lt;/h2&gt;
&lt;p&gt;Now let me fit the same generalized linear model but assuming that the outcome follow a couple of different distributions, including a true Poisson (with unit dispersion), a negative binomial, the double-Poisson distribution from the previous post, and the generalized Poisson distribution. Here goes!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Poisson_fit &amp;lt;- 
  brm(
    Y ~ X, family = poisson(link = &amp;quot;log&amp;quot;),
    data = dat, 
    warmup = 500, 
    iter = 1500, 
    chains = 4, 
    cores = 4,
    seed = 20231204
  )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;negbin_fit &amp;lt;- 
  brm(
    Y ~ X, family = negbinomial(link = &amp;quot;log&amp;quot;),
    data = dat, 
    warmup = 500, 
    iter = 1500, 
    chains = 4, 
    cores = 4,
    seed = 20231204
  )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stancode_dpo &amp;lt;- &amp;quot;
real dpo_lpmf(int X, real mu, real phi) {
  real ans;
  real A = inv(2) * log(phi) - phi * mu;
  if (X == 0)
    ans = A;
  else
    ans = A + X * (phi * (1 + log(mu)) - 1) - lgamma(X + 1) + (1 - phi) * X * log(X);
  return ans;
}
vector dpo_cdf(real mu, real phi, int maxval) {
  real d = exp(phi * (1 + log(mu)) - 1);
  real prob;
  int n = maxval + 1;
  vector[n] cdf;
  cdf[1] = sqrt(phi) * exp(-mu * phi);
  prob = cdf[1] * d;
  cdf[2] = cdf[1] + prob;
  for (i in 2:maxval) {
    prob = prob * d * exp((1 - phi) * (i - 1) * (log(i) - log(i - 1))) / (i^phi);
    cdf[i + 1] = cdf[i] + prob;
    if (prob / cdf[i + 1] &amp;lt; 1e-8) {
      n = i + 1;
      break;
    }
  }
  return cdf / cdf[n];
}
int dpo_quantile(real p, real mu, real phi, int maxval) {
  vector[maxval + 1] cdf_vec = dpo_cdf(mu, phi, maxval);
  int q = 0;
  while (cdf_vec[q + 1] &amp;lt; p) {
      q += 1;
    }
  return q;
}
int dpo_rng(real mu, real phi, int maxval) {
  real p = uniform_rng(0,1);
  int x = dpo_quantile(p, mu, phi, maxval);
  return x;
}
&amp;quot;
double_Poisson &amp;lt;- custom_family(
  &amp;quot;dpo&amp;quot;, dpars = c(&amp;quot;mu&amp;quot;,&amp;quot;phi&amp;quot;),
  links = c(&amp;quot;log&amp;quot;,&amp;quot;log&amp;quot;),
  lb = c(0, 0), ub = c(NA, NA),
  type = &amp;quot;int&amp;quot;
)

double_Poisson_stanvars &amp;lt;- stanvar(scode = stancode_dpo, block = &amp;quot;functions&amp;quot;)

phi_prior &amp;lt;- prior(exponential(1), class = &amp;quot;phi&amp;quot;)

DPO_fit &amp;lt;- 
  brm(
    Y ~ X, family = double_Poisson,
    prior = phi_prior,
    stanvars = double_Poisson_stanvars,
    data = dat, 
    warmup = 500, 
    iter = 1500, 
    chains = 4, 
    cores = 4,
    seed = 20231204
  )

expose_functions(DPO_fit, vectorize = TRUE)

log_lik_dpo &amp;lt;- function(i, prep) {
  mu &amp;lt;- brms::get_dpar(prep, &amp;quot;mu&amp;quot;, i = i)
  phi &amp;lt;- brms::get_dpar(prep, &amp;quot;phi&amp;quot;, i = i)
  y &amp;lt;- prep$data$Y[i]
  dpo_lpmf(y, mu, phi)
}

posterior_predict_dpo &amp;lt;- function(i, prep, maxval = NULL, ...) {
  mu &amp;lt;- brms::get_dpar(prep, &amp;quot;mu&amp;quot;, i = i)
  phi &amp;lt;- brms::get_dpar(prep, &amp;quot;phi&amp;quot;, i = i)
  if (is.null(maxval)) maxval &amp;lt;- 20 * mu / min(phi, 1)
  dpo_rng(mu, phi, maxval = maxval)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stancode_gpo &amp;lt;- &amp;quot;
real gpo_lpmf(int X, real mu, real phi) {
  real ans;
  real m = mu / (1 - inv(sqrt(phi)));
  real z = X + sqrt(phi) * (mu - X);
  if (phi &amp;gt; 1 &amp;amp;&amp;amp; X &amp;gt; m)
    ans = negative_infinity();
  else 
    ans = log(mu) + inv(2) * log(phi) + (X - 1) * log(z) - z - lgamma(X + 1);
  return ans;
}
int gpo_quantile(real p, real mu, real phi) {
  int q = 0;
  real phi_sqrt = sqrt(phi);
  real mu_phi_sqrt = mu * phi_sqrt;
  real m;
  if (phi &amp;gt; 1) 
    m = mu / (1 - inv(phi_sqrt));
  else 
    m = positive_infinity();
  real lpmf = - mu_phi_sqrt;
  real cdf = exp(lpmf);
  real ln_inc;
  while (cdf &amp;lt; p &amp;amp;&amp;amp; q &amp;lt; m) {
    q += 1;
    ln_inc = (q - 1) * log(mu_phi_sqrt + q * (1 - phi_sqrt)) - (q - 2) * log(mu_phi_sqrt + (q - 1) * (1 - phi_sqrt)) + (phi_sqrt - 1) - log(q);
    lpmf += ln_inc;    
    cdf += exp(lpmf);
  }
  return q;
}
int gpo_rng(real mu, real phi) {
  real p = uniform_rng(0,1);
  int x = gpo_quantile(p, mu, phi);
  return x;
}
&amp;quot;
generalized_Poisson &amp;lt;- custom_family(
  &amp;quot;gpo&amp;quot;, dpars = c(&amp;quot;mu&amp;quot;,&amp;quot;phi&amp;quot;),
  links = c(&amp;quot;log&amp;quot;,&amp;quot;log&amp;quot;),
  lb = c(0, 0), ub = c(NA, NA),
  type = &amp;quot;int&amp;quot;
)

generalized_Poisson_stanvars &amp;lt;- stanvar(scode = stancode_gpo, block = &amp;quot;functions&amp;quot;)

phi_prior &amp;lt;- prior(exponential(1), class = &amp;quot;phi&amp;quot;)

GPO_fit &amp;lt;- 
  brm(
    Y ~ X, family = generalized_Poisson,
    prior = phi_prior,
    stanvars = generalized_Poisson_stanvars,
    data = dat, 
    warmup = 500, 
    iter = 1500, 
    chains = 4, 
    cores = 4,
    seed = 20231204
  )

expose_functions(GPO_fit, vectorize = TRUE)

log_lik_gpo &amp;lt;- function(i, prep) {
  mu &amp;lt;- brms::get_dpar(prep, &amp;quot;mu&amp;quot;, i = i)
  phi &amp;lt;- brms::get_dpar(prep, &amp;quot;phi&amp;quot;, i = i)
  y &amp;lt;- prep$data$Y[i]
  gpo_lpmf(y, mu, phi)
}

posterior_predict_gpo &amp;lt;- function(i, prep, maxval = NULL, ...) {
  mu &amp;lt;- brms::get_dpar(prep, &amp;quot;mu&amp;quot;, i = i)
  phi &amp;lt;- brms::get_dpar(prep, &amp;quot;phi&amp;quot;, i = i)
  gpo_rng(mu, phi)
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;model-comparison&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model comparison&lt;/h2&gt;
&lt;p&gt;Here is a comparison of LOOIC for all of the models:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;loo_comparison &amp;lt;- loo(Poisson_fit, negbin_fit, DPO_fit, GPO_fit)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Unsurprisingly, the model based on the true data-generating process clearly fits better than the models involving other outcome distributions.&lt;/p&gt;
&lt;p&gt;Here’s the posterior for the dispersion (i.e., &lt;span class=&#34;math inline&#34;&gt;\(1 / \phi\)&lt;/span&gt;) based on the GPO and DPO models:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;color_scheme_set(&amp;quot;green&amp;quot;)
GPO_dispersion &amp;lt;- 
  mcmc_areas(GPO_fit, pars = &amp;quot;phi&amp;quot;, transformations = \(x) 1 / x) + 
  scale_x_continuous(limits = c(0.55, 0.95)) + 
  theme_minimal() + 
  ggtitle(&amp;quot;Generalized Poisson&amp;quot;)

color_scheme_set(&amp;quot;brightblue&amp;quot;)
DPO_dispersion &amp;lt;- 
  mcmc_areas(DPO_fit, pars = &amp;quot;phi&amp;quot;, transformations = \(x) 1 / x) + 
  scale_x_continuous(limits = c(0.55, 0.95)) + 
  theme_minimal() + 
  ggtitle(&amp;quot;Double Poisson&amp;quot;)

DPO_dispersion / GPO_dispersion&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jepusto.com/post/Generalized-Poisson-in-Stan_files/figure-html/dispersion-comparison-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It looks like the estimated dispersion from the double Poisson model is biased upwards (towards one) a little bit). To get a better sense of this, I’ll run some posterior predictive checks, using the quasi-likelihood dispersion as a summary statistic:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Yrep_Poisson &amp;lt;- posterior_predict(Poisson_fit, ndraws = 500) 
Yrep_negbin &amp;lt;- posterior_predict(negbin_fit, ndraws = 500)
Yrep_dpo &amp;lt;- posterior_predict(DPO_fit, ndraws = 500)
Yrep_gpo &amp;lt;- posterior_predict(GPO_fit, ndraws = 500)

dispersion_coef &amp;lt;- function(y) {
  quasi_fit &amp;lt;- glm(y ~ dat$X, family = quasipoisson(link = &amp;quot;log&amp;quot;))
  sum(residuals(quasi_fit, type = &amp;quot;pearson&amp;quot;)^2) / quasi_fit$df.residual
}

color_scheme_set(&amp;quot;blue&amp;quot;)
Poisson_disp &amp;lt;- ppc_stat(dat$Y, Yrep_Poisson, stat = dispersion_coef, binwidth = 0.02) + 
  labs(title = &amp;quot;Poisson&amp;quot;)

color_scheme_set(&amp;quot;purple&amp;quot;)
negbin_disp &amp;lt;- ppc_stat(dat$Y, Yrep_negbin, stat = dispersion_coef, binwidth = 0.02) + 
  labs(title = &amp;quot;Negative-binomial&amp;quot;)

color_scheme_set(&amp;quot;brightblue&amp;quot;)
dpo_disp &amp;lt;- ppc_stat(dat$Y, Yrep_dpo, stat = dispersion_coef, binwidth = 0.02) + 
  labs(title = &amp;quot;Double Poisson&amp;quot;)

color_scheme_set(&amp;quot;green&amp;quot;)
gpo_disp &amp;lt;- ppc_stat(dat$Y, Yrep_gpo, stat = dispersion_coef, binwidth = 0.02) + 
  labs(title = &amp;quot;Generalized Poisson&amp;quot;)

Poisson_disp / negbin_disp / dpo_disp / gpo_disp &amp;amp;
  theme_minimal() &amp;amp; 
  xlim(c(0.45, 1.3))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jepusto.com/post/Generalized-Poisson-in-Stan_files/figure-html/ppc-dispersion-1.png&#34; width=&#34;768&#34; /&gt;
Both the double Poisson and the generalized Poisson models generate data with levels of dispersion similar to the observed data. Squinting pretty hard, it looks like the double Poisson model could be a little bit biased.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;marginal-posterior-predictive-densities&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Marginal posterior predictive densities&lt;/h2&gt;
&lt;p&gt;Here’s some rootograms for the posterior predictive density of the raw outcomes:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;color_scheme_set(&amp;quot;blue&amp;quot;)
Poisson_root &amp;lt;- ppc_rootogram(dat$Y, Yrep_Poisson, style = &amp;quot;hanging&amp;quot;) + labs(title = &amp;quot;Poisson&amp;quot;)
color_scheme_set(&amp;quot;purple&amp;quot;)
negbin_root &amp;lt;- ppc_rootogram(dat$Y, Yrep_negbin, style = &amp;quot;hanging&amp;quot;) + labs(title = &amp;quot;Negative-binomial&amp;quot;)
color_scheme_set(&amp;quot;brightblue&amp;quot;)
dpo_root &amp;lt;- ppc_rootogram(dat$Y, Yrep_dpo, style = &amp;quot;hanging&amp;quot;) + labs(title = &amp;quot;Double Poisson&amp;quot;)
color_scheme_set(&amp;quot;green&amp;quot;)
gpo_root &amp;lt;- ppc_rootogram(dat$Y, Yrep_gpo, style = &amp;quot;hanging&amp;quot;) + labs(title = &amp;quot;Generalized Poisson&amp;quot;)

Poisson_root / negbin_root / dpo_root / gpo_root &amp;amp;
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jepusto.com/post/Generalized-Poisson-in-Stan_files/figure-html/ppd-1.png&#34; width=&#34;672&#34; /&gt;
You can see from these that the Poisson and negative-binomial models expect more low-frequency counts than are present in the observed data. However, the figure doesn’t really capture the degree of mis-fit that is apparent with the dispersion summary statistics. I think this is because the distribution of &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; changes so much depending on the value of the predictor &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;posterior-predictive-residual-densities&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Posterior predictive residual densities&lt;/h2&gt;
&lt;p&gt;One way to focus in on the distributional assumption is to examine the distribution of residuals rather than raw outcomes. I’ll do that here by looking the deviance residuals from the quasi-Poisson GLM model, treating the calculation of the residuals as merely as transformation of the raw data. Here are some posterior predictive density plots of these deviance residuals:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# quasi-Poisson deviance residuals
dat$resid &amp;lt;- residuals(quasi_fit)

# function to calculate quasi-Poisson deviance residuals
quasi_residuals &amp;lt;- function(y) as.numeric(residuals(glm(y ~ dat$X, family = quasipoisson(link = &amp;quot;log&amp;quot;))))

# transform posterior predictive data into residuals
R &amp;lt;- 50
resid_Poisson &amp;lt;- apply(Yrep_Poisson[1:R,], 1, quasi_residuals) |&amp;gt; t()
resid_negbin &amp;lt;- apply(Yrep_negbin[1:R,], 1, quasi_residuals) |&amp;gt; t()
resid_dpo &amp;lt;- apply(Yrep_dpo[1:R,], 1, quasi_residuals) |&amp;gt; t()
resid_gpo &amp;lt;- apply(Yrep_gpo[1:R,], 1, quasi_residuals) |&amp;gt; t()

# make density plots
color_scheme_set(&amp;quot;blue&amp;quot;)
Poisson_resid_density &amp;lt;- ppc_dens_overlay(dat$resid, resid_Poisson) + labs(title = &amp;quot;Poisson&amp;quot;)

color_scheme_set(&amp;quot;purple&amp;quot;)
negbin_resid_density &amp;lt;- ppc_dens_overlay(dat$resid, resid_negbin) + labs(title = &amp;quot;Negative-binomial&amp;quot;)

color_scheme_set(&amp;quot;brightblue&amp;quot;)
dpo_resid_density &amp;lt;- ppc_dens_overlay(dat$resid, resid_dpo) + labs(title = &amp;quot;Double Poisson&amp;quot;)

color_scheme_set(&amp;quot;green&amp;quot;)
gpo_resid_density &amp;lt;- ppc_dens_overlay(dat$resid, resid_gpo) + labs(title = &amp;quot;Generalized Poisson&amp;quot;)

Poisson_resid_density / negbin_resid_density / dpo_resid_density / gpo_resid_density &amp;amp;
  theme_minimal() &amp;amp; 
  xlim(c(-3, 3))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jepusto.com/post/Generalized-Poisson-in-Stan_files/figure-html/ppd-residuals-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;It’s quite a bit clearer from these plots that the DPO and GPO models are closer to replicating the distribution of the data than are the Poisson or negative binomial models.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;colophon&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Colophon&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.2.2 (2022-10-31 ucrt)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 22621)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United States.utf8 
## [2] LC_CTYPE=English_United States.utf8   
## [3] LC_MONETARY=English_United States.utf8
## [4] LC_NUMERIC=C                          
## [5] LC_TIME=English_United States.utf8    
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] loo_2.5.1           bayesplot_1.10.0    brms_2.18.0        
##  [4] Rcpp_1.0.10         rstan_2.26.23       StanHeaders_2.26.28
##  [7] gamlss.dist_6.0-5   MASS_7.3-58.1       patchwork_1.1.2    
## [10] forcats_0.5.2       stringr_1.5.0       dplyr_1.1.3        
## [13] purrr_1.0.2         readr_2.1.3         tidyr_1.3.0        
## [16] tibble_3.2.1        ggplot2_3.4.3       tidyverse_1.3.2    
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.4.1         backports_1.4.1      RcppEigen_0.3.3.9.2 
##   [4] plyr_1.8.7           igraph_1.3.5         splines_4.2.2       
##   [7] crosstalk_1.2.0      TH.data_1.1-2        rstantools_2.2.0    
##  [10] inline_0.3.19        digest_0.6.31        htmltools_0.5.4     
##  [13] fansi_1.0.4          BH_1.78.0-0          magrittr_2.0.3      
##  [16] checkmate_2.1.0      googlesheets4_1.0.1  tzdb_0.3.0          
##  [19] modelr_0.1.9         RcppParallel_5.1.5   matrixStats_0.63.0  
##  [22] xts_0.12.2           sandwich_3.0-2       timechange_0.2.0    
##  [25] prettyunits_1.1.1    colorspace_2.1-0     rvest_1.0.3         
##  [28] haven_2.5.1          xfun_0.40            callr_3.7.3         
##  [31] crayon_1.5.2         jsonlite_1.8.4       survival_3.4-0      
##  [34] zoo_1.8-12           glue_1.6.2           gtable_0.3.4        
##  [37] gargle_1.3.0         emmeans_1.8.2        distributional_0.3.1
##  [40] pkgbuild_1.4.0       abind_1.4-5          scales_1.2.1        
##  [43] mvtnorm_1.1-3        DBI_1.1.3            miniUI_0.1.1.1      
##  [46] xtable_1.8-4         stats4_4.2.2         DT_0.26             
##  [49] htmlwidgets_1.6.2    httr_1.4.5           threejs_0.3.3       
##  [52] posterior_1.3.1      ellipsis_0.3.2       pkgconfig_2.0.3     
##  [55] farver_2.1.1         sass_0.4.5           dbplyr_2.2.1        
##  [58] utf8_1.2.3           labeling_0.4.3       tidyselect_1.2.0    
##  [61] rlang_1.1.1          reshape2_1.4.4       later_1.3.0         
##  [64] munsell_0.5.0        cellranger_1.1.0     tools_4.2.2         
##  [67] cachem_1.0.6         cli_3.6.1            generics_0.1.3      
##  [70] ggridges_0.5.4       broom_1.0.1          evaluate_0.20       
##  [73] fastmap_1.1.0        yaml_2.3.7           processx_3.7.0      
##  [76] knitr_1.42           fs_1.6.1             nlme_3.1-160        
##  [79] mime_0.12            xml2_1.3.3           compiler_4.2.2      
##  [82] shinythemes_1.2.0    rstudioapi_0.14      reprex_2.0.2        
##  [85] bslib_0.4.2          stringi_1.7.12       highr_0.10          
##  [88] ps_1.7.1             blogdown_1.13        Brobdingnag_1.2-9   
##  [91] lattice_0.20-45      Matrix_1.6-3         markdown_1.8        
##  [94] shinyjs_2.1.0        tensorA_0.36.2       vctrs_0.6.3         
##  [97] pillar_1.9.0         lifecycle_1.0.3      jquerylib_0.1.4     
## [100] bridgesampling_1.1-2 estimability_1.4.1   httpuv_1.6.8        
## [103] QuickJSR_1.0.6       R6_2.5.1             bookdown_0.29       
## [106] promises_1.2.0.1     gridExtra_2.3        codetools_0.2-18    
## [109] colourpicker_1.2.0   gtools_3.9.4         assertthat_0.2.1    
## [112] withr_2.5.0          shinystan_2.6.0      multcomp_1.4-25     
## [115] mgcv_1.8-41          parallel_4.2.2       hms_1.1.2           
## [118] grid_4.2.2           coda_0.19-4          rmarkdown_2.20      
## [121] googledrive_2.0.0    shiny_1.7.2          lubridate_1.9.2     
## [124] base64enc_0.1-3      dygraphs_1.1.1.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Implementing Efron&#39;s double Poisson distribution in Stan</title>
      <link>https://www.jepusto.com/double-poisson-in-stan/</link>
      <pubDate>Fri, 15 Sep 2023 00:00:00 +0000</pubDate>
      <guid>https://www.jepusto.com/double-poisson-in-stan/</guid>
      <description>


&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\def\Pr{{\text{Pr}}}
\def\E{{\text{E}}}
\def\Var{{\text{Var}}}
\def\Cov{{\text{Cov}}}
\def\bm{\mathbf}
\def\bs{\boldsymbol}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For a project I am working on, we are using &lt;a href=&#34;https://mc-stan.org/&#34;&gt;Stan&lt;/a&gt; to fit generalized random effects location-scale models to a bunch of count data. We’re interested in using the double-Poisson distribution, as described by &lt;a href=&#34;https://doi.org/10.2307/2289002&#34;&gt;Efron (1986)&lt;/a&gt;.
This is an interesting distribution because it admits for both over- and under-dispersion relative to the Poisson distribution, whereas most of the conventional alternatives such as the negative binomial distribution or Poisson-normal mixture distribution allow only for over-dispersion.
The double-Poisson distribution is not implemented in Stan, so we’ve had to write our own distribution function. That’s fine and not particularly difficult. What’s a bit more of a challenge is writing Stan functions to generate random samples from the double-Poisson, so that we can generate posterior predictive checks.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; In this post, I’ll walk through the implementation of the custom distribution functions needed to use the double-Poisson in Stan.
The &lt;a href=&#34;https://cran.r-project.org/package=gamlss.dist&#34;&gt;&lt;code&gt;gamlss.dist&lt;/code&gt; package&lt;/a&gt; provides a full set of distributional functions for the double-Poisson distribution, including a sampler. Thus, I can validate my Stan functions against the functions from &lt;code&gt;gamlss.dist&lt;/code&gt;.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
library(patchwork)   # composing figures
library(gamlss.dist) # DPO distribution functions
library(rstan)       # Stan interface to R
library(brms)        # fitting generalized linear models
library(bayesplot)   # Examine fitted models
library(loo)         # Model fit measures&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;the-double-poisson&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The double-Poisson&lt;/h2&gt;
&lt;p&gt;The double-Poisson distribution is a discrete distribution for non-negative counts, with support &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{S}_X = \{0, 1, 2, 3, ...\}\)&lt;/span&gt;.
The mean-variance relationship of the double-Poisson is approximately constant; for &lt;span class=&#34;math inline&#34;&gt;\(X \sim DPO(\mu, \phi)\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\text{E}(X) \approx \mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}(X) \approx \mu / \phi\)&lt;/span&gt;, so that the double-Poisson distribution approximately satisfies the assumptions of a quasi-Poisson generalized linear model (although not quite exactly so).&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://doi.org/10.2307/2289002&#34;&gt;Efron (1986)&lt;/a&gt; gives the following expression for the density of the double-Poisson distribution with mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and inverse-disperson &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[
f(x | \mu, \phi) = \frac{\phi^{1/2} e^{-\phi \mu}}{c(\mu,\phi)} \left(\frac{e^{-x} x^x}{x!}\right) \left(\frac{e \mu}{x}\right)^{\phi x},
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(c(\mu,\phi)\)&lt;/span&gt; is a scaling constant to ensure that the density sums to one, which is closely approximated by
&lt;span class=&#34;math display&#34;&gt;\[
c(\mu, \phi) \approx 1 + \frac{1 - \phi}{12 \mu \phi}\left(1 + \frac{1}{\mu \phi}\right).
\]&lt;/span&gt;
We then have
&lt;span class=&#34;math display&#34;&gt;\[
\ln f(x | \mu, \phi) = \frac{1}{2} \ln \phi - \phi \mu - \ln c(\mu, \phi) + x (\phi + \phi \ln \mu - 1) + (1 - \phi) x \ln(x) - \ln \left(x!\right),
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(0 \times \ln (0)\)&lt;/span&gt; is evaluated as 0.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;log-of-the-probability-mass-function&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Log of the probability mass function&lt;/h1&gt;
&lt;p&gt;For purposes of using this distribution in Stan, it’s sufficient to provide the log of the probability mass function up to a constant—there’s no need to normalize it to sum to one. Thus, we can ignore the &lt;span class=&#34;math inline&#34;&gt;\(c(\mu, \phi)\)&lt;/span&gt; term above. Here’s a Stan function implementing the lpmf:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stancode_lpmf &amp;lt;- &amp;quot;
real dpo_lpmf(int X, real mu, real phi) {
  real ans;
  real A = inv(2) * log(phi) - phi * mu;
  if (X == 0)
    ans = A;
  else
    ans = A + X * (phi * (1 + log(mu)) - 1) - lgamma(X + 1) + (1 - phi) * X * log(X);
  return ans;
}
&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To check that this is accurate, I’ll compare the Stan function to the corresponding function from &lt;code&gt;gamlss.dist&lt;/code&gt; for a couple of different parameter values and for &lt;span class=&#34;math inline&#34;&gt;\(x = 0,...,100\)&lt;/span&gt;. If my function is accurate, the calculated log-probabilities should differ by a constant value for each set of parameters.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;writeLines(paste(&amp;quot;functions {&amp;quot;, stancode_lpmf, &amp;quot;}&amp;quot;, sep = &amp;quot;\n&amp;quot;), &amp;quot;DPO-lpmf.stan&amp;quot;)
expose_stan_functions(&amp;quot;DPO-lpmf.stan&amp;quot;)

test_lpmf &amp;lt;- 
  expand_grid(
    mu = c(2, 5, 10, 20),
    phi = c(0.1, 0.2, 0.5, 1, 2, 5, 10),
    X = 0:100
  ) %&amp;gt;%
  mutate(
    gamlss_lpmf = dDPO(x = X, mu = mu, sigma = 1 / phi, log = TRUE),
    my_lpmf = pmap_dbl(.l = list(X = X, mu = mu, phi = phi), .f = dpo_lpmf),
    diff = my_lpmf - gamlss_lpmf
  )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(test_lpmf, aes(factor(phi), diff, color = factor(phi))) + 
  geom_boxplot() + 
  facet_wrap( ~ mu, labeller = &amp;quot;label_both&amp;quot;, ncol = 2) + 
  theme_minimal() + 
  labs(x = &amp;quot;phi&amp;quot;) + 
  theme(legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jepusto.com/post/Double-Poisson-in-Stan_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Checks out. Onward!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;cumulative-distribution-function&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Cumulative distribution function&lt;/h1&gt;
&lt;p&gt;I’ll next implement a function to evaluate the cumulative distriution function over a range of values. This is an expensive calculation, but it can be improved a little bit by noting the relationship between sequential values of the probability mass function. Letting &lt;span class=&#34;math inline&#34;&gt;\(d = \exp \left(\phi + \phi \ln \mu - 1 \right)\)&lt;/span&gt;, observe that
&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
f(0 | \mu, \phi) &amp;amp;= \frac{\phi^{1/2} e^{-\phi \mu}}{c(\mu,\phi)} \\
f(1 | \mu, \phi) &amp;amp;= f(0 | \mu, \phi) \times d \\
f(x | \mu, \phi) &amp;amp;= f(x - 1 | \mu, \phi) \times d \times \frac{\exp\left[(1 - \phi)(x - 1)\left(\ln(x) - \ln(x - 1)\right) \right]}{x^\phi}
\end{aligned}
\]&lt;/span&gt;
where the last expression holds for &lt;span class=&#34;math inline&#34;&gt;\(x \geq 2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The function below computes the cumulative distribution function over the range &lt;span class=&#34;math inline&#34;&gt;\(x = 0,...,m\)&lt;/span&gt; as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Compute &lt;span class=&#34;math inline&#34;&gt;\(f(x | \mu, \phi)\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(x = 0,1,2,...\)&lt;/span&gt;, without the scaling constant &lt;span class=&#34;math inline&#34;&gt;\(c(\mu, \phi)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Take &lt;span class=&#34;math inline&#34;&gt;\(F(0 | \mu, \phi) = f(0 | \mu, \phi)\)&lt;/span&gt; and accumulate &lt;span class=&#34;math inline&#34;&gt;\(F(x | \mu, \phi) = F(x - 1 | \mu, \phi) + f(x | \mu, \phi)\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(x = 0,...,m\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Check if &lt;span class=&#34;math inline&#34;&gt;\(f(x | \mu, \phi) / F(x | \mu, \phi)\)&lt;/span&gt; is small (less than &lt;span class=&#34;math inline&#34;&gt;\(10^{-8}\)&lt;/span&gt;), in which case accumulation stops at the value &lt;span class=&#34;math inline&#34;&gt;\(n\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;The normalized cumulative distribution function will then be &lt;span class=&#34;math inline&#34;&gt;\(F(x | \mu, \phi) / F(n | \mu, \phi)\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stancode_cdf &amp;lt;- &amp;quot; 
vector dpo_cdf(real mu, real phi, int maxval) {
  real d = exp(phi * (1 + log(mu)) - 1);
  real prob;
  int n = maxval + 1;
  vector[n] cdf;
  cdf[1] = sqrt(phi) * exp(-mu * phi);
  prob = cdf[1] * d;
  cdf[2] = cdf[1] + prob;
  for (i in 2:maxval) {
    prob = prob * d * exp((1 - phi) * (i - 1) * (log(i) - log(i - 1))) / (i^phi);
    cdf[i + 1] = cdf[i] + prob;
    if (prob / cdf[i + 1] &amp;lt; 1e-8) {
      n = i + 1;
      break;
    }
  }
  return cdf / cdf[n];
}
&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To check that this is accurate, I’ll again compare the Stan function to the corresponding function from &lt;code&gt;gamlss.dist&lt;/code&gt;. If my function is accurate, the computed cdf values should be proportional to the cdf calculated from &lt;code&gt;gamlss.dist::pDPO()&lt;/code&gt; and the ratio should be very close to 1.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;writeLines(paste(&amp;quot;functions {&amp;quot;, stancode_cdf, &amp;quot;}&amp;quot;, sep = &amp;quot;\n&amp;quot;), &amp;quot;DPO-cdf.stan&amp;quot;)
expose_stan_functions(&amp;quot;DPO-cdf.stan&amp;quot;)

test_cdf &amp;lt;- 
  expand_grid(
    mu = c(2, 5, 10, 20),
    phi = c(0.1, 0.2, 0.5, 1, 2, 5, 10),
  ) %&amp;gt;%
  mutate(
    maxval = 20 * mu / pmin(1, phi),
    my_cdf = pmap(.l = list(mu = mu, phi = phi, maxval = maxval), .f = dpo_cdf)
  ) %&amp;gt;%
  unnest(my_cdf) %&amp;gt;%
  filter(!is.nan(my_cdf)) %&amp;gt;%
  group_by(mu, phi) %&amp;gt;%
  mutate(
    q = row_number() - 1L,
    gamlss_cdf = pDPO(q = q, mu = mu, sigma = 1 / phi),
    ratio = my_cdf / gamlss_cdf
  )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(test_cdf, aes(factor(phi), ratio, color = factor(phi))) + 
  geom_boxplot() + 
  facet_wrap( ~ mu, labeller = &amp;quot;label_both&amp;quot;, ncol = 2) + 
  theme_minimal() + 
  labs(x = &amp;quot;phi&amp;quot;) + 
  ylim(1 + c(-1e-6, 1e-6)) + 
  theme(legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jepusto.com/post/Double-Poisson-in-Stan_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Still on track here (although you might wonder—would I be sharing this post if I couldn’t get the function working?).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;quantile-function-and-sampler&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Quantile function and sampler&lt;/h1&gt;
&lt;p&gt;The main other thing we need is a function for generating random samples from the double-Poisson. The &lt;code&gt;gamlss.dist&lt;/code&gt; package has the function &lt;code&gt;rDPO()&lt;/code&gt; for this purpose. It’s implemented using the standard inversion method, by calculating quantiles of the double-Poisson corresponding to a random sample from a uniform distribution. Just for funzies, I’ll implement the same approach using Stan.&lt;/p&gt;
&lt;p&gt;The function below calculates quantiles by finding the minimum value of &lt;span class=&#34;math inline&#34;&gt;\(q \geq 0\)&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;\(F(q + 1 | \mu, \phi) \geq p\)&lt;/span&gt; for a specified probability &lt;span class=&#34;math inline&#34;&gt;\(p \in [0, 1]\)&lt;/span&gt;. It is vectorized over &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; and solves for &lt;span class=&#34;math inline&#34;&gt;\(q\)&lt;/span&gt; by starting with the smallest &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; and continuing through the largest value.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stancode_quantile &amp;lt;- &amp;quot; 
vector dpo_cdf(real mu, real phi, int maxval) {
  real d = exp(phi * (1 + log(mu)) - 1);
  real prob;
  int n = maxval + 1;
  vector[n] cdf;
  cdf[1] = sqrt(phi) * exp(-mu * phi);
  prob = cdf[1] * d;
  cdf[2] = cdf[1] + prob;
  for (i in 2:maxval) {
    prob = prob * d * exp((1 - phi) * (i - 1) * (log(i) - log(i - 1))) / (i^phi);
    cdf[i + 1] = cdf[i] + prob;
    if (prob / cdf[i + 1] &amp;lt; 1e-8) {
      n = i + 1;
      break;
    }
  }
  return cdf / cdf[n];
}
array[] int dpo_quantiles(vector p, real mu, real phi, int maxval) {
  int N = rows(p);
  array[N] int qs;
  array[N] int indices = sort_indices_asc(p);
  vector[maxval + 1] cdf_vec = dpo_cdf(mu, phi, maxval);
  int j = 0;
  for (i in indices) {
    while (cdf_vec[j + 1] &amp;lt; p[i]) {
      j += 1;
    }
    qs[i] = j;
  }
  return qs;
}
&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If my quantile function is accurate, it should match the value computed from &lt;code&gt;gamlss.dist::qDPO()&lt;/code&gt; exactly.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;writeLines(paste(&amp;quot;functions {&amp;quot;, stancode_quantile, &amp;quot;}&amp;quot;, sep = &amp;quot;\n&amp;quot;), &amp;quot;DPO-quantile.stan&amp;quot;)
expose_stan_functions(&amp;quot;DPO-quantile.stan&amp;quot;)

test_quantile &amp;lt;- 
  expand_grid(
    mu = c(2, 5, 10, 20),
    phi = c(0.1, 0.2, 0.5, 1, 2, 5, 10),
  ) %&amp;gt;%
  mutate(
    maxval = 20 * mu / pmin(1, phi),
    p = map(1:n(), ~ runif(100)),
    my_q = pmap(.l = list(p = p, mu = mu, phi = phi, maxval = maxval), .f = dpo_quantiles),
    gamlss_q = pmap(.l = list(p = p, mu = mu, sigma = 1 / phi), .f = qDPO)
  ) %&amp;gt;%
  unnest(c(p, my_q, gamlss_q)) %&amp;gt;%
  mutate(
    diff = my_q - gamlss_q
  )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(test_quantile, aes(factor(phi), diff, color = factor(phi))) + 
  geom_boxplot() + 
  facet_wrap( ~ mu, labeller = &amp;quot;label_both&amp;quot;, ncol = 2) + 
  theme_minimal() + 
  labs(x = &amp;quot;phi&amp;quot;) + 
  theme(legend.position = &amp;quot;none&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jepusto.com/post/Double-Poisson-in-Stan_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Phew, still got it!&lt;/p&gt;
&lt;p&gt;The last piece of the puzzle is to write a sampler by generating random points from a uniform distribution, then computing the double-Poisson quantiles of these random points. I will implement this two ways: first with an argument for the number of random variates to generate and then, more simply, to generate a single random variate.&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stancode_qr &amp;lt;- &amp;quot;
real dpo_lpmf(int X, real mu, real phi) {
  real ans;
  real A = inv(2) * log(phi) - phi * mu;
  if (X == 0)
    ans = A;
  else
    ans = A + X * (phi * (1 + log(mu)) - 1) - lgamma(X + 1) + (1 - phi) * X * log(X);
  return ans;
}
vector dpo_cdf(real mu, real phi, int maxval) {
  real d = exp(phi * (1 + log(mu)) - 1);
  real prob;
  int n = maxval + 1;
  vector[n] cdf;
  cdf[1] = sqrt(phi) * exp(-mu * phi);
  prob = cdf[1] * d;
  cdf[2] = cdf[1] + prob;
  for (i in 2:maxval) {
    prob = prob * d * exp((1 - phi) * (i - 1) * (log(i) - log(i - 1))) / (i^phi);
    cdf[i + 1] = cdf[i] + prob;
    if (prob / cdf[i + 1] &amp;lt; 1e-8) {
      n = i + 1;
      break;
    }
  }
  return cdf / cdf[n];
}
array[] int dpo_quantiles(vector p, real mu, real phi, int maxval) {
  int N = rows(p);
  array[N] int qs;
  array[N] int indices = sort_indices_asc(p);
  vector[maxval + 1] cdf_vec = dpo_cdf(mu, phi, maxval);
  int j = 0;
  for (i in indices) {
    while (cdf_vec[j + 1] &amp;lt; p[i]) {
      j += 1;
    }
    qs[i] = j;
  }
  return qs;
}
array[] int dpo_sample_rng(int n, real mu, real phi, int maxval) {
  vector[n] p;
  for (i in 1:n) {
    p[i] = uniform_rng(0,1);
  }
  array[n] int x = dpo_quantiles(p, mu, phi, maxval);
  return x;
}
int dpo_quantile(real p, real mu, real phi, int maxval) {
  vector[maxval + 1] cdf_vec = dpo_cdf(mu, phi, maxval);
  int q = 0;
  while (cdf_vec[q + 1] &amp;lt; p) {
      q += 1;
    }
  return q;
}
int dpo_rng(real mu, real phi, int maxval) {
  real p = uniform_rng(0,1);
  int x = dpo_quantile(p, mu, phi, maxval);
  return x;
}
&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To check this function, I’ll generate some large samples from the double-Poisson with a few different parameter sets. If the sampler is working properly, the empirical cumulative distribution should line up closely with the cumulative distribution computed using &lt;code&gt;gamlss.dist::pDPO()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;writeLines(paste(&amp;quot;functions {&amp;quot;, stancode_qr, &amp;quot;}&amp;quot;, sep = &amp;quot;\n&amp;quot;), &amp;quot;DPO-rng.stan&amp;quot;)
expose_stan_functions(&amp;quot;DPO-rng.stan&amp;quot;)

test_rng &amp;lt;- 
  expand_grid(
    mu = c(2, 5, 10, 20),
    phi = c(0.1, 0.2, 0.5, 1, 2, 5, 10),
  ) %&amp;gt;%
  mutate(
    x = pmap(.l = list(n = 10000, mu = mu, phi = phi, maxval = 5000), .f = dpo_sample_rng),
    tb = map(x, ~ as.data.frame(table(.x)))
  ) %&amp;gt;%
  dplyr::select(-x) %&amp;gt;%
  group_by(mu, phi) %&amp;gt;%
  unnest(tb) %&amp;gt;%
  mutate(
    .x = as.integer(levels(.x))[.x],
    Freq_cum = cumsum(Freq) / 10000,
    gamlss_F = pDPO(q = .x, mu = mu, sigma = 1 / phi)
  )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(test_rng, aes(gamlss_F, Freq_cum, color = factor(phi))) + 
  geom_abline(slope = 1, color = &amp;quot;blue&amp;quot;, linetype = &amp;quot;dashed&amp;quot;) + 
  geom_point() + geom_line() +  
  facet_grid(phi ~ mu, labeller = &amp;quot;label_both&amp;quot;) + 
  theme_minimal() + 
  labs(x = &amp;quot;Theoretical cdf (gamlss.dist)&amp;quot;, y = &amp;quot;Empirical cdf (my function)&amp;quot;) + 
  theme(legend.position = &amp;quot;none&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jepusto.com/post/Double-Poisson-in-Stan_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Looks pretty good, no?&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;using-the-custom-distribution-functions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Using the custom distribution functions&lt;/h1&gt;
&lt;p&gt;To finish out my tests of these functions, let me demonstrate their use in an actual estimation problem. I’ll generate data based on a simple generalized linear model with a single predictor &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;, where the outcome &lt;span class=&#34;math inline&#34;&gt;\(Y\)&lt;/span&gt; follows a double-Poisson distribution conditional on &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;. The data-generating process is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
X &amp;amp;\sim N(0, 1) \\
Y|X &amp;amp;\sim DPO(\mu(X), \phi) \\
\log \mu(X) &amp;amp;= 2 + 0.3 \times X
\end{aligned}
\]&lt;/span&gt;
To make things interesting, I’ll set the dispersion parameter to &lt;span class=&#34;math inline&#34;&gt;\(1 / \phi = 0.6\)&lt;/span&gt; so that the outcome is &lt;em&gt;under&lt;/em&gt;-dispersed relative to the Poisson.&lt;/p&gt;
&lt;p&gt;The following code generates a large sample from the data-generating process. To keep things R-centric, I use &lt;code&gt;gamlss.dist::rDPO&lt;/code&gt; to generate the outcome.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(20230913)
N &amp;lt;- 600
X &amp;lt;- rnorm(N)
mu &amp;lt;- exp(2 + 0.3 * X)
phi_inv &amp;lt;- 0.6
Y &amp;lt;- rDPO(N, mu = mu, sigma = phi_inv)
dat &amp;lt;- data.frame(X = X, Y = Y)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s what the sample looks like, along with a smoothed regression estimated using a basic cubic spline:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(dat, aes(X, Y)) + 
  geom_point(alpha = 0.1) + 
  geom_smooth(method = &amp;#39;gam&amp;#39;, formula = y ~ s(x, bs = &amp;quot;cs&amp;quot;)) + 
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jepusto.com/post/Double-Poisson-in-Stan_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;div id=&#34;comparison-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Comparison models&lt;/h2&gt;
&lt;p&gt;Before using the custom distribution, I’ll fit a couple of out-of-the-box models that are useful points of comparison.
Surely the simplest, quickest, and dirtiest way to estimate such a regression is with a generalized linear model, using the “quasi-Poisson” family to allow for non-unit dispersion. In R:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;quasi_fit &amp;lt;- glm(Y ~ X, family = quasipoisson(link = &amp;quot;log&amp;quot;), data = dat)
summary(quasi_fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = Y ~ X, family = quasipoisson(link = &amp;quot;log&amp;quot;), data = dat)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -2.77671  -0.58205  -0.03293   0.49158   2.52711  
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)  1.98784    0.01219  163.03   &amp;lt;2e-16 ***
## X            0.29276    0.01178   24.85   &amp;lt;2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for quasipoisson family taken to be 0.6324771)
## 
##     Null deviance: 777.74  on 599  degrees of freedom
## Residual deviance: 384.90  on 598  degrees of freedom
## AIC: NA
## 
## Number of Fisher Scoring iterations: 4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This approach recovers the data-generating parameters quite well, with a dispersion estimate of 0.632 compared to the true dispersion parameter of 0.6.&lt;/p&gt;
&lt;p&gt;Now let me fit the same generalized linear model but assuming that the outcome follows a true Poisson distribution (with unit dispersion). I’ll fit the model in a Bayesian framework with the &lt;code&gt;brms&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Poisson_fit &amp;lt;- 
  brm(
    Y ~ X, family = poisson(link = &amp;quot;log&amp;quot;),
    data = dat, 
    warmup = 500, 
    iter = 1500, 
    chains = 4, 
    cores = 4,
    seed = 20230913
  )

summary(Poisson_fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: poisson 
##   Links: mu = log 
## Formula: Y ~ X 
##    Data: dat (Number of observations: 600) 
##   Draws: 4 chains, each with iter = 1500; warmup = 500; thin = 1;
##          total post-warmup draws = 4000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     1.99      0.02     1.96     2.02 1.00     2733     2582
## X             0.29      0.01     0.26     0.32 1.00     2705     2485
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This specification recovers the intercept and slope parameters well too, but doesn’t provide any estimate of dispersion.&lt;/p&gt;
&lt;p&gt;As an alternative, I’ll also fit the model using the negative binomial distribution, which is a generalization of the Poisson that allows for over-dispersion (but not under-dispersion):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;negbin_fit &amp;lt;- 
  brm(
    Y ~ X, family = negbinomial(link = &amp;quot;log&amp;quot;),
    data = dat, 
    warmup = 500, 
    iter = 1500, 
    chains = 4, 
    cores = 4,
    seed = 20230913
  )

summary(negbin_fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: negbinomial 
##   Links: mu = log; shape = identity 
## Formula: Y ~ X 
##    Data: dat (Number of observations: 600) 
##   Draws: 4 chains, each with iter = 1500; warmup = 500; thin = 1;
##          total post-warmup draws = 4000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     1.99      0.02     1.96     2.02 1.00     3571     3057
## X             0.29      0.01     0.26     0.32 1.00     3696     3141
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## shape   313.78    130.37   136.26   622.52 1.00     3132     3122
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;brms&lt;/code&gt; package implements the negative binomial using the rate parameterization, so the &lt;code&gt;shape&lt;/code&gt; parameter corresponds to the inverse dispersion. Thus, a large shape parameter (as in the above fit) implies dispersion that is very close to one (i.e., close to the Poisson).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;double-poisson-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Double-Poisson model&lt;/h2&gt;
&lt;p&gt;Now I’ll fit the same model as previously but using my custom-built double-Poisson distribution. Following &lt;a href=&#34;https://cran.r-project.org/web/packages/brms/vignettes/brms_customfamilies.html&#34;&gt;Paul Buerkner’s vignette&lt;/a&gt; on using custom distributions in &lt;code&gt;brms&lt;/code&gt;, I’ll first specify the custom family object for the double-Poisson:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;double_Poisson &amp;lt;- custom_family(
  &amp;quot;dpo&amp;quot;, dpars = c(&amp;quot;mu&amp;quot;,&amp;quot;phi&amp;quot;),
  links = c(&amp;quot;log&amp;quot;,&amp;quot;log&amp;quot;),
  lb = c(0, 0), ub = c(NA, NA),
  type = &amp;quot;int&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I set the defaults to use a log-link for the mean (just as with the Poisson and negative binomial families) and a log-link for the inverse-dispersion.
Next, I’ll create an object to add the custom stan code from above into the code created by &lt;code&gt;brm&lt;/code&gt; for fitting the model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;double_Poisson_stanvars &amp;lt;- stanvar(scode = stancode_qr, block = &amp;quot;functions&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I’ll also need to specify a prior to use for the &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; parameter of the double-Poisson distribution:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;phi_prior &amp;lt;- prior(exponential(1), class = &amp;quot;phi&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now I’m ready to fit the model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;DPO_fit &amp;lt;- 
  brm(
    Y ~ X, family = double_Poisson,
    prior = phi_prior,
    stanvars = double_Poisson_stanvars,
    data = dat, 
    warmup = 500, 
    iter = 1500, 
    chains = 4, 
    cores = 4,
    seed = 20230913
  )

summary(DPO_fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  Family: dpo 
##   Links: mu = log; phi = identity 
## Formula: Y ~ X 
##    Data: dat (Number of observations: 600) 
##   Draws: 4 chains, each with iter = 1500; warmup = 500; thin = 1;
##          total post-warmup draws = 4000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     1.99      0.01     1.96     2.01 1.00     3468     3162
## X             0.29      0.01     0.27     0.32 1.00     3745     2786
## 
## Family Specific Parameters: 
##     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## phi     1.55      0.09     1.38     1.73 1.00     3814     2780
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The regression coefficient estimates are basically identical to those from the Poisson and negative-binomial models, estimated with slightly better precision than with the Poisson or negative binomial families. However, we get a posterior for &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; that corresponds to &lt;em&gt;under&lt;/em&gt;-dispersion. Here’s the posterior for the dispersion (i.e., &lt;span class=&#34;math inline&#34;&gt;\(1 / \phi\)&lt;/span&gt;):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mcmc_areas(DPO_fit, pars = &amp;quot;phi&amp;quot;, transformations = \(x) 1 / x) + 
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jepusto.com/post/Double-Poisson-in-Stan_files/figure-html/DPO-dispersion-1.png&#34; width=&#34;480&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-comparison&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model comparison&lt;/h2&gt;
&lt;p&gt;I’d like to get a sense of how much better the double-Poisson model does with capturing the real data-generating process compared to the simple Poisson model or the negative binomial model. There’s a wide range of diagnostics that can inform such comparisons. I’ll consider the leave-one-out information criteria (LOOIC) and also look at some posterior predictive checks.&lt;/p&gt;
&lt;p&gt;To calculate LOOIC for the double-Poisson model, I first need to provide a &lt;code&gt;log_lik&lt;/code&gt; function that &lt;code&gt;brms&lt;/code&gt; can use&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;. Here’s code, using the Stan function from above:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;expose_functions(DPO_fit, vectorize = TRUE)
log_lik_dpo &amp;lt;- function(i, prep) {
  mu &amp;lt;- brms::get_dpar(prep, &amp;quot;mu&amp;quot;, i = i)
  phi &amp;lt;- brms::get_dpar(prep, &amp;quot;phi&amp;quot;, i = i)
  y &amp;lt;- prep$data$Y[i]
  dpo_lpmf(y, mu, phi)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I can then compute LOOIC for all three models:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;loo(DPO_fit, Poisson_fit, negbin_fit)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Output of model &amp;#39;DPO_fit&amp;#39;:
## 
## Computed from 4000 by 600 log-likelihood matrix
## 
##          Estimate   SE
## elpd_loo  -1305.7 16.9
## p_loo         2.9  0.2
## looic      2611.5 33.8
## ------
## Monte Carlo SE of elpd_loo is 0.0.
## 
## All Pareto k estimates are good (k &amp;lt; 0.5).
## See help(&amp;#39;pareto-k-diagnostic&amp;#39;) for details.
## 
## Output of model &amp;#39;Poisson_fit&amp;#39;:
## 
## Computed from 4000 by 600 log-likelihood matrix
## 
##          Estimate   SE
## elpd_loo  -1330.0 11.3
## p_loo         1.3  0.1
## looic      2660.1 22.6
## ------
## Monte Carlo SE of elpd_loo is 0.0.
## 
## All Pareto k estimates are good (k &amp;lt; 0.5).
## See help(&amp;#39;pareto-k-diagnostic&amp;#39;) for details.
## 
## Output of model &amp;#39;negbin_fit&amp;#39;:
## 
## Computed from 4000 by 600 log-likelihood matrix
## 
##          Estimate   SE
## elpd_loo  -1333.2 11.1
## p_loo         1.3  0.1
## looic      2666.3 22.2
## ------
## Monte Carlo SE of elpd_loo is 0.0.
## 
## All Pareto k estimates are good (k &amp;lt; 0.5).
## See help(&amp;#39;pareto-k-diagnostic&amp;#39;) for details.
## 
## Model comparisons:
##             elpd_diff se_diff
## DPO_fit       0.0       0.0  
## Poisson_fit -24.3       6.1  
## negbin_fit  -27.4       6.3&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By these measures, the double-Poisson model has substantially better fit than either of the other models.&lt;/p&gt;
&lt;p&gt;To do posterior predictive checks, I need to provide a &lt;code&gt;posterior_predict&lt;/code&gt; function that &lt;code&gt;brms&lt;/code&gt; can use. I’ll again do an implementation that uses my custom &lt;code&gt;dpo_rng()&lt;/code&gt; from Stan.&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior_predict_dpo &amp;lt;- function(i, prep, maxval = NULL, ...) {
  mu &amp;lt;- brms::get_dpar(prep, &amp;quot;mu&amp;quot;, i = i)
  phi &amp;lt;- brms::get_dpar(prep, &amp;quot;phi&amp;quot;, i = i)
  if (is.null(maxval)) maxval &amp;lt;- 20 * mu / min(phi, 1)
  dpo_rng(mu, phi, maxval = maxval)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Functions in hand, I can now compute posterior predictions for the double-Poisson model and make pretty pictures of them, along with corresponding plots for the Poisson and negative-binomial models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Yrep_Poisson &amp;lt;- posterior_predict(Poisson_fit, draws = 400) 
color_scheme_set(&amp;quot;blue&amp;quot;)
Poisson_root &amp;lt;- ppc_rootogram(dat$Y, Yrep_Poisson, style = &amp;quot;hanging&amp;quot;) + labs(title = &amp;quot;Poisson&amp;quot;)

Yrep_negbin &amp;lt;- posterior_predict(negbin_fit, draws = 400)
color_scheme_set(&amp;quot;green&amp;quot;)
negbin_root &amp;lt;- ppc_rootogram(dat$Y, Yrep_negbin, style = &amp;quot;hanging&amp;quot;) + labs(title = &amp;quot;Negative-binomial&amp;quot;)

Yrep_dpo &amp;lt;- posterior_predict(DPO_fit, draws = 400)
color_scheme_set(&amp;quot;purple&amp;quot;)
dpo_root &amp;lt;- ppc_rootogram(dat$Y, Yrep_dpo, style = &amp;quot;hanging&amp;quot;) + labs(title = &amp;quot;Double-Poisson&amp;quot;)

dpo_root / Poisson_root / negbin_root &amp;amp;
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jepusto.com/post/Double-Poisson-in-Stan_files/figure-html/ppd-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The differences in predicted frequencies are not that obvious from these plots. The main notable difference is that the Poisson and negative-binomial distributions predict more small counts (in the range of 0 to 3) than are observed, whereas the double-Poisson does better at matching the observed frequency in this range.&lt;/p&gt;
&lt;p&gt;I think the lack of glaring differences in the above plots happens because I’m just looking at the marginal distribution of the outcome, and the (explained) variation due to the predictor dampens the degree of under-dispersion. To see this, I’ll create some plots that are grouped by quintiles of &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat$g &amp;lt;- cut(dat$X, breaks = quantile(dat$X, seq(0,1,0.2)), include.lowest = TRUE)

color_scheme_set(&amp;quot;blue&amp;quot;)
Poisson_bars &amp;lt;- ppc_bars_grouped(
  dat$Y, Yrep_Poisson, dat$g, 
  prob = 0.5, 
  facet_args = list(ncol = 5)
) + 
  labs(title = &amp;quot;Poisson&amp;quot;)

color_scheme_set(&amp;quot;green&amp;quot;)
negbin_bars &amp;lt;- ppc_bars_grouped(
  dat$Y, Yrep_negbin, dat$g, 
  prob = 0.5, 
  facet_args = list(ncol = 5)
) + 
  labs(title = &amp;quot;Negative-binomial&amp;quot;)

color_scheme_set(&amp;quot;purple&amp;quot;)
dpo_bars &amp;lt;- ppc_bars_grouped(
  dat$Y, Yrep_dpo, dat$g, 
  prob = 0.5, 
  facet_args = list(ncol = 5)
) + 
  labs(title = &amp;quot;Double-Poisson&amp;quot;)

dpo_bars / Poisson_bars / negbin_bars &amp;amp;
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jepusto.com/post/Double-Poisson-in-Stan_files/figure-html/ppd-grouped-1.png&#34; width=&#34;1152&#34; /&gt;
Still kind of subtle, I suppose, but you can see more clearly that the double-Poisson does a better job than the other distributions at matching the modes (peaks) of the empirical distribution in each of these subgroups.&lt;/p&gt;
&lt;p&gt;One last approach is to look directly at the degree of dispersion in the posterior predictive distributions relative to the actual data. I’ll calculate this dispersion by re-fitting the quick-and-dirty quasi-poisson model in each sample:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dispersion_coef &amp;lt;- function(y) {
  quasi_fit &amp;lt;- glm(y ~ dat$X, family = quasipoisson(link = &amp;quot;log&amp;quot;))
  sum(residuals(quasi_fit, type = &amp;quot;pearson&amp;quot;)^2) / quasi_fit$df.residual
}

color_scheme_set(&amp;quot;blue&amp;quot;)
Poisson_disp &amp;lt;- ppc_stat(dat$Y, Yrep_Poisson, stat = dispersion_coef, binwidth = 0.02) + 
  labs(title = &amp;quot;Poisson&amp;quot;)

color_scheme_set(&amp;quot;green&amp;quot;)
negbin_disp &amp;lt;- ppc_stat(dat$Y, Yrep_negbin, stat = dispersion_coef, binwidth = 0.02) + 
  labs(title = &amp;quot;Negative-binomial&amp;quot;)

color_scheme_set(&amp;quot;purple&amp;quot;)
dpo_disp &amp;lt;- ppc_stat(dat$Y, Yrep_dpo, stat = dispersion_coef, binwidth = 0.02) + 
  labs(title = &amp;quot;Double-Poisson&amp;quot;)

dpo_disp / Poisson_disp / negbin_disp &amp;amp;
  theme_minimal() &amp;amp; 
  xlim(c(0.45, 1.3))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jepusto.com/post/Double-Poisson-in-Stan_files/figure-html/ppc-dispersion-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From this, we can clearly see that the Poisson and negative binomial model generate data with approximately unit dispersion, which doesn’t match at all with the degree of dispersion in the observed data.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;kudos&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Kudos&lt;/h1&gt;
&lt;p&gt;So there you have it. It’s really quite feasible to build models with custom distributions. Efron (1986) also describes a double-binomial distribution (as an approximation to the “quasi-binomial” family of generalized linear models), which you could play with implementing for yourself, dear reader, if you are in the mood.
Major kudos to &lt;a href=&#34;https://paul-buerkner.github.io/&#34;&gt;Paul Buerkner&lt;/a&gt; for &lt;a href=&#34;https://paul-buerkner.github.io/brms/&#34;&gt;&lt;code&gt;brms&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://jgabry.github.io/&#34;&gt;Jonah Gabry&lt;/a&gt; and collaborators for &lt;a href=&#34;https://mc-stan.org/bayesplot/&#34;&gt;&lt;code&gt;bayesplot&lt;/code&gt;&lt;/a&gt;, and &lt;a href=&#34;https://mc-stan.org/about/team/&#34;&gt;the incredible team of folks&lt;/a&gt; developing &lt;a href=&#34;https://mc-stan.org/&#34;&gt;&lt;code&gt;Stan&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;colophon&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Colophon&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;## R version 4.2.2 (2022-10-31 ucrt)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19045)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United States.utf8 
## [2] LC_CTYPE=English_United States.utf8   
## [3] LC_MONETARY=English_United States.utf8
## [4] LC_NUMERIC=C                          
## [5] LC_TIME=English_United States.utf8    
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] loo_2.5.1           bayesplot_1.9.0     brms_2.18.0        
##  [4] Rcpp_1.0.10         rstan_2.26.23       StanHeaders_2.26.27
##  [7] gamlss.dist_6.0-3   MASS_7.3-57         patchwork_1.1.1    
## [10] forcats_0.5.1       stringr_1.5.0       dplyr_1.1.2        
## [13] purrr_1.0.2         readr_2.1.2         tidyr_1.3.0        
## [16] tibble_3.2.1        ggplot2_3.4.0       tidyverse_1.3.1    
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.4.2         backports_1.4.1      RcppEigen_0.3.3.9.2 
##   [4] plyr_1.8.8           igraph_1.3.5         splines_4.2.2       
##   [7] crosstalk_1.2.0      TH.data_1.1-2        rstantools_2.2.0    
##  [10] inline_0.3.19        digest_0.6.30        htmltools_0.5.4     
##  [13] fansi_1.0.4          magrittr_2.0.3       BH_1.78.0-0         
##  [16] checkmate_2.1.0      tzdb_0.3.0           modelr_0.1.8        
##  [19] RcppParallel_5.1.5   matrixStats_0.62.0   xts_0.12.1          
##  [22] sandwich_3.0-1       timechange_0.2.0     prettyunits_1.1.1   
##  [25] colorspace_2.1-0     rvest_1.0.2          haven_2.5.0         
##  [28] xfun_0.34            callr_3.7.2          crayon_1.5.2        
##  [31] jsonlite_1.8.4       survival_3.4-0       zoo_1.8-10          
##  [34] glue_1.6.2           gtable_0.3.1         emmeans_1.7.3       
##  [37] distributional_0.3.1 pkgbuild_1.3.1       abind_1.4-5         
##  [40] scales_1.2.1         mvtnorm_1.1-3        DBI_1.1.2           
##  [43] miniUI_0.1.1.1       xtable_1.8-4         stats4_4.2.2        
##  [46] DT_0.23              htmlwidgets_1.6.2    httr_1.4.3          
##  [49] threejs_0.3.3        posterior_1.3.1      ellipsis_0.3.2      
##  [52] pkgconfig_2.0.3      farver_2.1.1         sass_0.4.5          
##  [55] dbplyr_2.1.1         utf8_1.2.3           tidyselect_1.2.0    
##  [58] labeling_0.4.2       rlang_1.1.1          reshape2_1.4.4      
##  [61] later_1.3.0          munsell_0.5.0        cellranger_1.1.0    
##  [64] tools_4.2.2          cachem_1.0.6         cli_3.6.1           
##  [67] generics_0.1.3       broom_0.8.0          ggridges_0.5.3      
##  [70] evaluate_0.18        fastmap_1.1.0        yaml_2.3.5          
##  [73] processx_3.7.0       knitr_1.40           fs_1.6.1            
##  [76] nlme_3.1-157         mime_0.12            xml2_1.3.3          
##  [79] compiler_4.2.2       shinythemes_1.2.0    rstudioapi_0.13     
##  [82] reprex_2.0.1         bslib_0.4.2          stringi_1.7.12      
##  [85] highr_0.9            ps_1.6.0             blogdown_1.10       
##  [88] Brobdingnag_1.2-9    lattice_0.20-45      Matrix_1.5-1        
##  [91] markdown_1.7         shinyjs_2.1.0        tensorA_0.36.2      
##  [94] vctrs_0.6.3          pillar_1.9.0         lifecycle_1.0.3     
##  [97] jquerylib_0.1.4      bridgesampling_1.1-2 estimability_1.3    
## [100] httpuv_1.6.8         QuickJSR_1.0.5       R6_2.5.1            
## [103] bookdown_0.26        promises_1.2.0.1     gridExtra_2.3       
## [106] codetools_0.2-18     colourpicker_1.1.1   gtools_3.9.3        
## [109] assertthat_0.2.1     withr_2.5.0          shinystan_2.6.0     
## [112] multcomp_1.4-23      mgcv_1.8-41          parallel_4.2.2      
## [115] hms_1.1.3            grid_4.2.2           coda_0.19-4         
## [118] rmarkdown_2.18       shiny_1.7.4          lubridate_1.9.2     
## [121] base64enc_0.1-3      dygraphs_1.1.1.6&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes footnotes-end-of-document&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;To be clear up front, what I present is more complicated than really necessary because of these existing R functions to simulate values from the double-Poisson—we can just use the functions from &lt;code&gt;gamlss.dist&lt;/code&gt; for purposes of posterior predictive checks (about which more below).
I’m trying to work in Stan to the maximum extent possible solely as an excuse to learn more about the language, which I haven’t used much up until today.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;I should also note that the &lt;a href=&#34;http://www.bamlss.org/index.html&#34;&gt;&lt;code&gt;bamlss&lt;/code&gt; package&lt;/a&gt; provides similar functionality and can be combined with &lt;code&gt;gamlss.dist&lt;/code&gt; to accomplish basically the same thing as I’m going to do here.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;The simpler version is what’s needed for generating posterior predictive checks, the fancy version is just to show off how clever I am.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;Rather than exposing and calling the Stan function, one could just re-implement the log likelihood in R. (Probably the easier way in practice, but again I’m trying to learn me some Stan here…)&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;Of course, I could have saved a bunch of trouble by just using &lt;code&gt;gamlss.dist::rDPO()&lt;/code&gt; instead.&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Comparison of competing approaches to analyzing cross-classified data: Random effects models, ordinary least squares, or fixed effects with cluster robust standard errors</title>
      <link>https://www.jepusto.com/publication/competing-approaches-for-cross-classified-data/</link>
      <pubDate>Thu, 09 Mar 2023 00:00:00 +0000</pubDate>
      <guid>https://www.jepusto.com/publication/competing-approaches-for-cross-classified-data/</guid>
      <description></description>
    </item>
    
    <item>
      <title>simhelpers</title>
      <link>https://www.jepusto.com/software/simhelpers/</link>
      <pubDate>Tue, 31 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://www.jepusto.com/software/simhelpers/</guid>
      <description>&lt;p&gt;Monte Carlo simulations are computer experiments designed to study the performance of statistical methods under known data-generating conditions. The goal of simhelpers is to assist in running such simulation studies. The main tools in the package consist of functions to calculate measures of estimator performance, such as bias, root mean squared error, rejection rates. The functions also calculate the associated Monte Carlo standard errors (MCSE) of the performance measures. The functions use the tidyeval principles, so that they play well with dplyr and fit easily into a &lt;code&gt;%&amp;gt;%&lt;/code&gt;-centric workflow.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://CRAN.R-project.org/package=simhelpers&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Available on the Comprehensive R Archive Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/meghapsimatrix/simhelpers&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Source code and installation instructions on Github&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Simulating correlated standardized mean differences for meta-analysis</title>
      <link>https://www.jepusto.com/simulating-correlated-smds/</link>
      <pubDate>Mon, 30 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://www.jepusto.com/simulating-correlated-smds/</guid>
      <description>


&lt;p&gt;As I’ve discussed in &lt;a href=&#34;https://www.jepusto.com/Sometimes-aggregating-effect-sizes-is-fine&#34;&gt;previous posts&lt;/a&gt;, meta-analyses in psychology, education, and other areas often include studies that contribute multiple, statistically dependent effect size estimates.
I’m interested in methods for meta-analyzing and meta-regressing effect sizes from data structures like this, and studying this sort of thing often entails conducting Monte Carlo simulations.
Monte Carlo simulations involve generating artificial data—in this case, a set of studies, each of which has one or more dependent effect size estimates—that follows a certain distributional model, applying different analytic methods to the artificial data, and then repeating the process a bunch of times.
Because we know the true parameters that govern the data-generating process, we can evaluate the performance of the analytic methods in terms of bias, accuracy, hypothesis test calibration and power, confidence interval coverage, and the like.&lt;/p&gt;
&lt;p&gt;In this post, I’ll discuss two alternative methods to simulate meta-analytic datasets that include studies with multiple, dependent effect size estimates: simulating individual participant-level data or simulating summary statistics. I’ll focus on the case of the standardized mean difference (SMD) because it is so common in meta-analyses of intervention studies. For simplicity, I’ll assume that the effect sizes all come from simple, two-group comparisons (without any covariate adjustment or anything like that) and that the individual observations are multi-variate normally distributed within each group. Our goal will be to simulate a set of &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; studies, where study &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; is based on measuring &lt;span class=&#34;math inline&#34;&gt;\(J_k\)&lt;/span&gt; outcomes on a sample of &lt;span class=&#34;math inline&#34;&gt;\(N_k\)&lt;/span&gt; participants, all for &lt;span class=&#34;math inline&#34;&gt;\(k = 1,...,K\)&lt;/span&gt;.
Let &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\delta_k = (\delta_{1k} \cdots \delta_{J_k k})&amp;#39;\)&lt;/span&gt; be the &lt;span class=&#34;math inline&#34;&gt;\(J_k \times 1\)&lt;/span&gt; vector of true standardized mean differences for study &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;.
I’ll assume that we know these true effect size parameters for all &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; studies, so that I can avoid committing to any particular form of random effects model.&lt;/p&gt;
&lt;div id=&#34;simulating-individual-participant-level-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Simulating individual participant-level data&lt;/h1&gt;
&lt;p&gt;The most direct way to simulate this sort of effect size data is to generate outcome data for every artificial participant in every artificial study. Let &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Y}_{ik}^T\)&lt;/span&gt; be the &lt;span class=&#34;math inline&#34;&gt;\(J_k \times 1\)&lt;/span&gt; vector of outcomes for treatment group participant &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; in study &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, and let &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Y}_{ik}^C\)&lt;/span&gt; be the &lt;span class=&#34;math inline&#34;&gt;\(J_k \times 1\)&lt;/span&gt; vector outcomes for control group participant &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; in study &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;, for &lt;span class=&#34;math inline&#34;&gt;\(i=1,...,N_k / 2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(k = 1,...,K\)&lt;/span&gt;. Assuming multi-variate normality of the outcomes, we can generate these outcome vectors as
&lt;span class=&#34;math display&#34;&gt;\[
\mathbf{Y}_{ik}^T \sim N\left(\boldsymbol\delta_k, \boldsymbol\Psi_k\right) \qquad \text{and}\qquad \mathbf{Y}_{ik}^C \sim N\left(\mathbf{0}, \boldsymbol\Psi_k\right),
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\Psi_k\)&lt;/span&gt; is the population correlation matrix of the outcomes in study &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt;.
Note that I am setting the mean outcomes of the control group participants to zero and also specifying that the outcomes all have unit variance within each group.
After simulating data based on these distributions, the effect size estimates for each outcome can be calculated directly, following standard formulas.&lt;/p&gt;
&lt;p&gt;Here’s what this approach looks like in code.
It is helpful to simplify things by focusing on simulating just a single study with multiple, correlated effect sizes.
Focusing first on just the input parameters, a function might look like the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r_SMDs_raw &amp;lt;- function(delta, J, N, Psi) {
  # stuff
  return(ES_data)  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the above function skeleton, &lt;code&gt;delta&lt;/code&gt; would be the true effect size parameter &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\delta_k\)&lt;/span&gt;, &lt;code&gt;J&lt;/code&gt; would be the number of effect sizes to generate &lt;span class=&#34;math inline&#34;&gt;\((J_k)\)&lt;/span&gt;, &lt;code&gt;N&lt;/code&gt; is the total number of participants &lt;span class=&#34;math inline&#34;&gt;\((N_k)\)&lt;/span&gt;, and &lt;code&gt;Psi&lt;/code&gt; is a matrix of correlations between the outcomes &lt;span class=&#34;math inline&#34;&gt;\((\Psi_k)\)&lt;/span&gt;.
From these parameters, we’ll generate raw data, calculate effect size estimates and standard errors, and return the results in a little dataset.&lt;/p&gt;
&lt;p&gt;To make the function a little bit easier to use, I’m going overload the &lt;code&gt;Psi&lt;/code&gt; argument so that it can be a single number, indicating a common correlation between the outcomes. Thus, instead of having to feed in a &lt;span class=&#34;math inline&#34;&gt;\(J_k \times J_k\)&lt;/span&gt; matrix, you can specify a single correlation &lt;span class=&#34;math inline&#34;&gt;\(r_k\)&lt;/span&gt;, and the function will assume that all of the outcomes are equicorrelated. In code, the logic is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;if (!is.matrix(Psi)) Psi &amp;lt;- Psi + diag(1 - Psi, nrow = J)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s the function with the innards:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r_SMDs_raw &amp;lt;- function(delta, J, N, Psi) {

  require(mvtnorm) # for simulating multi-variate normal data
  
  # create Psi matrix assuming equicorrelation
  if (!is.matrix(Psi)) Psi &amp;lt;- Psi + diag(1 - Psi, nrow = J)
  
  # generate control group summary statistics
  Y_C &amp;lt;- rmvnorm(n = N / 2, mean = rep(0, J), sigma = Psi)
  ybar_C &amp;lt;- colMeans(Y_C)
  sd_C &amp;lt;- apply(Y_C, 2, sd)
  
  # generate treatment group summary statistics
  delta &amp;lt;- rep(delta, length.out = J)
  Y_T &amp;lt;- rmvnorm(n = N / 2, mean = delta, sigma = Psi)
  ybar_T &amp;lt;- colMeans(Y_T)
  sd_T &amp;lt;- apply(Y_T, 2, sd)

  # calculate Cohen&amp;#39;s d
  sd_pool &amp;lt;- sqrt((sd_C^2 + sd_T^2) / 2)
  ES &amp;lt;- (ybar_T - ybar_C) / sd_pool
  
  # calculate SE of d
  SE &amp;lt;- sqrt(4 / N + ES^2 / (2 * (N - 2)))

  data.frame(ES = ES, SE = SE, N = N)

}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In action:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;delta &amp;lt;- rnorm(4, mean = 0.2, sd = 0.1)
r_SMDs_raw(delta = delta, J = 4, N = 40, Psi = 0.6)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: mvtnorm&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##            ES        SE  N
## 1 -0.19106514 0.3169863 40
## 2  0.18427227 0.3169334 40
## 3  0.25646209 0.3175932 40
## 4  0.00210429 0.3162279 40&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Or if you’d rather specify the full &lt;span class=&#34;math inline&#34;&gt;\(\Psi_k\)&lt;/span&gt; matrix yourself:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Psi_k &amp;lt;- 0.6 + diag(0.4, nrow = 4)
Psi_k&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      [,1] [,2] [,3] [,4]
## [1,]  1.0  0.6  0.6  0.6
## [2,]  0.6  1.0  0.6  0.6
## [3,]  0.6  0.6  1.0  0.6
## [4,]  0.6  0.6  0.6  1.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r_SMDs_raw(delta = delta, J = 4, N = 40, Psi = Psi_k)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##           ES        SE  N
## 1 -0.1597097 0.3167580 40
## 2 -0.1717717 0.3168410 40
## 3 -0.4369032 0.3201744 40
## 4  0.0657410 0.3163177 40&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;exercises&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exercises&lt;/h2&gt;
&lt;p&gt;The function above is serviceable but quite basic. I can think of several additional features that one might like to have for use in research simulations, but I’m feeling both cheeky and lazy at the moment, so I’ll leave them for you, dear reader. Here are some suggested exercises:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Add an argument to the function, &lt;code&gt;Hedges_g = TRUE&lt;/code&gt;, which controls where the simulated effect size is Hedges’ &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt; or Cohen’s &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;. If it is Hedges’ g, make sure that the standard error is corrected too.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Add an argument to the function, &lt;code&gt;p_val = TRUE&lt;/code&gt;, which allows the user to control whether or not to return &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-values from the test of mean differences for each outcome. Note that the p-values should be for a test of the &lt;em&gt;raw&lt;/em&gt; mean differences between groups, rather than a test of the effect size &lt;span class=&#34;math inline&#34;&gt;\(\delta_{jk} = 0\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Add an argument to the function, &lt;code&gt;corr_mat = FALSE&lt;/code&gt;, which controls whether the function returns just the simulated effect sizes and SEs or both the simulated effect sizes and the full sampling variance-covariance matrix of the effect sizes. See &lt;a href=&#34;https://www.jepusto.com/correlations-between-SMDs&#34;&gt;here&lt;/a&gt; for the relevant formulas.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;simulating-summary-statistics&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Simulating summary statistics&lt;/h1&gt;
&lt;p&gt;Another approach to simulating SMDs is to sample from the distribution of the &lt;em&gt;summary statistics&lt;/em&gt; used in calculating the effect size. This approach should simplify the code, at the cost of having to use a bit of distribution theory. Let &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\bar{y}}_{Tk}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{\bar{y}}_{Ck}\)&lt;/span&gt; be the &lt;span class=&#34;math inline&#34;&gt;\(J_k \times 1\)&lt;/span&gt; vectors of sample means for the treatment and control groups, respectively. Let &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{S}_k\)&lt;/span&gt; be the &lt;span class=&#34;math inline&#34;&gt;\(J_k \times J_k\)&lt;/span&gt; sample covariance matrix of the outcomes, pooled across the treatment and control groups. Again assuming multi-variate normality, and following the same notation as above:
&lt;span class=&#34;math display&#34;&gt;\[
\mathbf{\bar{y}}_{Ck} \sim N\left(\mathbf{0}, \frac{2}{N_k} \boldsymbol\Psi_k\right), \qquad \mathbf{\bar{y}}_{Tk} \sim N\left(\boldsymbol\delta_k, \frac{2}{N_k} \boldsymbol\Psi_k\right),
\]&lt;/span&gt;
and
&lt;span class=&#34;math display&#34;&gt;\[
\left(\mathbf{\bar{y}}_{Tk} - \mathbf{\bar{y}}_{Ck}\right) \sim N\left(\boldsymbol\delta_k, \frac{4}{N_k} \boldsymbol\Psi_k\right).
\]&lt;/span&gt;
This shows how we could directly simulate the numerator of the standardized mean difference.&lt;/p&gt;
&lt;p&gt;A &lt;a href=&#34;https://www.jepusto.com/distribution-of-sample-variances&#34;&gt;further bit of distribution theory&lt;/a&gt; says that the pooled sample covariance matrix follows a multiple of a &lt;a href=&#34;https://en.wikipedia.org/wiki/Wishart_distribution&#34;&gt;Wishart distribution&lt;/a&gt; with &lt;span class=&#34;math inline&#34;&gt;\(N_k - 2\)&lt;/span&gt; degrees of freedom and scale matrix &lt;span class=&#34;math inline&#34;&gt;\(\Psi_k\)&lt;/span&gt;:
&lt;span class=&#34;math display&#34;&gt;\[
(N_k - 2) \mathbf{S}_k \sim Wishart\left(N_k - 2, \Psi_k \right).
\]&lt;/span&gt;
Thus, to simulate the denominators of the SMD estimates, we can simulate a single Wishart matrix, pull out the diagonal entries, divide by &lt;span class=&#34;math inline&#34;&gt;\(N_k - 2\)&lt;/span&gt;, and take the square root. In all, we draw a single &lt;span class=&#34;math inline&#34;&gt;\(J_k \times 1\)&lt;/span&gt; observation from a multi-variate normal distribution and a single &lt;span class=&#34;math inline&#34;&gt;\(J_k \times J_k\)&lt;/span&gt; observation from a Wishart distribution. In contrast, the raw data approach requires simulating &lt;span class=&#34;math inline&#34;&gt;\(N_k\)&lt;/span&gt; observations from a multi-variate normal distribution, then calculating &lt;span class=&#34;math inline&#34;&gt;\(4 J_k\)&lt;/span&gt; summary statistics (M and SD for each group on each outcome).&lt;/p&gt;
&lt;div id=&#34;exercises-1&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exercises&lt;/h2&gt;
&lt;p&gt;Once again, I’ll leave it to you, dear reader, to do the fun programming bits:&lt;/p&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Create a modified version of the function &lt;code&gt;r_SMDs_raw&lt;/code&gt; that simulates summary statistics instead of raw data (Call it &lt;code&gt;r_SMDs_stats&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;Use the &lt;code&gt;microbenchmark&lt;/code&gt; package (or your preferred benchmarking tool) to compare the computational efficiency of both versions of the function.&lt;/li&gt;
&lt;li&gt;Check your work! Verify that both versions of the function generate the same distributions if the same parameters are used as input.&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;which-approach-is-better&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Which approach is better?&lt;/h1&gt;
&lt;p&gt;Like many things in research, there’s no clearly superior method here. The advantage of the summary statistics approach is computational efficiency. It should generally be faster than the raw data approach, and if you need to generate 10,000 meta-analysis each with 80 studies in them, the computational savings might add up. On the other hand, computational efficiency isn’t everything.&lt;/p&gt;
&lt;p&gt;I see two potential advantages of the raw data approach. First is interpretability: simulating raw data is likely easier to understand. It feels tangible and familiar, harkening back to those bygone days we spent learning ANOVA, whereas the summary statistics approach requires a bit of distribution theory to follow (bookmark this blog post!). Second is extensibility: it is relatively straightforward to extend the approach to use other distributional models for the raw dat (perhaps you want to look at outcomes that follow a &lt;a href=&#34;https://en.wikipedia.org/wiki/Multivariate_t-distribution&#34;&gt;multi-variate &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; distribution&lt;/a&gt;?) or more complicated estimators of the SMD (difference-in-differences? covariate-adjusted? cluster-randomized trial?). To use the summary statistics approach in more complicated scenarios, you’d have to work out the sampling distributions for yourself, or locate the right reference.&lt;/p&gt;
&lt;p&gt;Of course, there’s also no need to choose between these two approaches. As I’m trying to hint at in Exercise 6, it’s actually useful to write both. Then, you can use the (potentially slower) raw data version to verify that the summary statistics version is correct.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simulating-full-meta-analyses&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Simulating full meta-analyses&lt;/h1&gt;
&lt;p&gt;So far we’ve got a data-generating function that simulates a single study’s worth of effect size estimates. To study meta-analytic methods, we’ll need to build out the function to simulate multiple studies. To do so, I think it’s useful to use the technique of &lt;a href=&#34;https://r4ds.had.co.nz/iteration.html&#34;&gt;mapping&lt;/a&gt;, as implemented in the &lt;code&gt;purrr&lt;/code&gt; package’s &lt;code&gt;map_*&lt;/code&gt; functions. The idea here is to first generate a “menu” of study-specific parameters for each of &lt;span class=&#34;math inline&#34;&gt;\(K\)&lt;/span&gt; studies, then apply the &lt;code&gt;r_SMDs&lt;/code&gt; function to each parameter set.&lt;/p&gt;
&lt;p&gt;Let’s consider how to do this for a simple random effects model, where the true effect size parameter is constant within each study (i.e., &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\delta_k = (\delta_k \cdots \delta_k)&amp;#39;\)&lt;/span&gt;), and in a model without covariates. We’ll need to generate a true effect for each study, along with a sample size, an outcome dimension, and a correlation between outcomes. For the true effects, I’ll assume that
&lt;span class=&#34;math display&#34;&gt;\[
\delta_k \sim N(\mu, \tau^2),
\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[
J_k \sim 2 + Poisson(3),
\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[
N_k \sim 20 + 2 \times Poisson(10),
\]&lt;/span&gt;
and
&lt;span class=&#34;math display&#34;&gt;\[
r_k \sim Beta\left(\rho \nu, (1 - \rho)\nu\right),
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\rho = \text{E}(r_k)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\nu &amp;gt; 0\)&lt;/span&gt; controls the variability of &lt;span class=&#34;math inline&#34;&gt;\(r_k\)&lt;/span&gt; across studies, with smaller &lt;span class=&#34;math inline&#34;&gt;\(\nu\)&lt;/span&gt; corresponding to more variable correlations.
Specifically, &lt;span class=&#34;math inline&#34;&gt;\(\text{Var}(r_k) = \rho (1 - \rho) / (1 + \nu)\)&lt;/span&gt;.
These distributions are just made up, without any particular justification.&lt;/p&gt;
&lt;p&gt;Here’s what these distributional models look like in R code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;K &amp;lt;- 6
mu &amp;lt;- 0.2
tau &amp;lt;- 0.05
J_mean &amp;lt;- 5
N_mean &amp;lt;- 45
rho &amp;lt;- 0.6
nu &amp;lt;- 39

study_data &amp;lt;- 
  data.frame(
    delta = rnorm(K, mean = mu, sd = tau),
    J = 2 + rpois(K, J_mean - 2),
    N = 20 + 2 * rpois(K, (N_mean - 20) / 2),
    Psi = rbeta(K, rho * nu, (1 - rho) * nu)
  )

study_data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       delta J  N       Psi
## 1 0.1749657 6 56 0.6670410
## 2 0.1371771 4 52 0.7952095
## 3 0.1430044 2 46 0.5551301
## 4 0.1953675 6 46 0.5339670
## 5 0.1653242 4 42 0.5623903
## 6 0.1419457 7 40 0.6615825&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once we have the “menu” of study-level characteristics, it’s just a matter of mapping the parameters to the data-generating function. One way to do this is with &lt;code&gt;pmap_df&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(purrr)
meta_data &amp;lt;- pmap_df(study_data, r_SMDs_raw, .id = &amp;quot;study&amp;quot;)
meta_data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    study           ES        SE  N
## 1      1  0.427048814 0.2704019 56
## 2      1  0.206502285 0.2679989 56
## 3      1  0.270244756 0.2685234 56
## 4      1  0.423149362 0.2703451 56
## 5      1  0.525878094 0.2720096 56
## 6      1  0.746186579 0.2767383 56
## 7      2 -0.005809721 0.2773507 52
## 8      2 -0.082222645 0.2774719 52
## 9      2  0.114670949 0.2775871 52
## 10     2 -0.001432641 0.2773501 52
## 11     3 -0.031231291 0.2949027 46
## 12     3  0.302264458 0.2966391 46
## 13     4  0.085338908 0.2950242 46
## 14     4 -0.062511255 0.2949592 46
## 15     4 -0.040178730 0.2949150 46
## 16     4 -0.082519741 0.2950151 46
## 17     4  0.207953122 0.2957160 46
## 18     4 -0.005713721 0.2948845 46
## 19     5  0.293666394 0.3103483 42
## 20     5  0.258312309 0.3099551 42
## 21     5  0.362126706 0.3112512 42
## 22     5  0.177656049 0.3092452 42
## 23     6 -0.115158991 0.3165035 40
## 24     6  0.094349350 0.3164129 40
## 25     6 -0.052996601 0.3162862 40
## 26     6 -0.042766762 0.3162658 40
## 27     6 -0.314584445 0.3182800 40
## 28     6  0.078519103 0.3163560 40
## 29     6 -0.103034241 0.3164486 40&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;table(meta_data$study)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 1 2 3 4 5 6 
## 6 4 2 6 4 7&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Putting it all together into a function, we have&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;r_meta &amp;lt;- function(K, mu, tau, J_mean, N_mean, rho, nu) {
  require(purrr)
  
  study_data &amp;lt;- 
    data.frame(
      delta = rnorm(K, mean = mu, sd = tau),
      J = 2 + rpois(K, J_mean - 2),
      N = 20 + 2 * rpois(K, (N_mean - 20) / 2),
      Psi = rbeta(K, rho * nu, (1 - rho) * nu)
    )
  
  pmap_df(study_data, r_SMDs_raw, .id = &amp;quot;study&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;exercises-2&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exercises&lt;/h2&gt;
&lt;ol start=&#34;7&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;Modify &lt;code&gt;r_meta&lt;/code&gt; so that it uses &lt;code&gt;r_SMDs_stats&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Add options to &lt;code&gt;r_meta&lt;/code&gt; for &lt;code&gt;Hedges_g&lt;/code&gt;, &lt;code&gt;p_val = TRUE&lt;/code&gt;, and &lt;code&gt;corr_mat = FALSE&lt;/code&gt; and ensure that these get passed along to the &lt;code&gt;r_SMDs&lt;/code&gt; function.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;One way to check that the &lt;code&gt;r_meta&lt;/code&gt; function is working properly is to generate a very large meta-analytic dataset, then to verify that the generated distributions align with expectations. Here’s a very large meta-analytic dataset:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;meta_data &amp;lt;- 
  r_meta(100000, mu = 0.2, tau = 0.05, 
         J_mean = 5, N_mean = 40, 
         rho = 0.6, nu = 39)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Compare the distribution of the simulated dataset against what you would expect to get based on the input parameters.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Modify the &lt;code&gt;r_meta&lt;/code&gt; function so that &lt;span class=&#34;math inline&#34;&gt;\(J_k\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(N_k\)&lt;/span&gt; are correlated, according to
&lt;span class=&#34;math display&#34;&gt;\[
\begin{align}
J_k &amp;amp;\sim 2 + Poisson(\mu_J - 2) \\
N_k &amp;amp;\sim 20 + 2 \times Poisson\left(\frac{1}{2}(\mu_N - 20) + \alpha (J_k - \mu_J) \right)
\end{align}
\]&lt;/span&gt;
for user-specified values of &lt;span class=&#34;math inline&#34;&gt;\(\mu_J\)&lt;/span&gt; (the average number of outcomes per study), &lt;span class=&#34;math inline&#34;&gt;\(\mu_N\)&lt;/span&gt; (the average total sample size per study), and &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;, which controls the degree of dependence between &lt;span class=&#34;math inline&#34;&gt;\(J_k\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(N_k\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;div id=&#34;a-challenge&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A challenge&lt;/h2&gt;
&lt;p&gt;The meta-analytic model that we’re using here is quite simple—simplistic, even—and for some simulation studies, something more complex might be needed. For example, we might need to generate data from a model that includes within-study random effects, as in:
&lt;span class=&#34;math display&#34;&gt;\[
\delta_{jk} = \mu + u_k + v_{jk}, \quad \text{where}\quad u_k \sim N(0, \tau^2), \quad v_{jk} \sim N(0, \omega^2).
\]&lt;/span&gt;
Even more complex would be to simulate from a multi-level meta-regression model
&lt;span class=&#34;math display&#34;&gt;\[
\delta_{jk} = \mathbf{x}_{jk} \boldsymbol\beta + u_k + v_{jk}, \quad \text{where}\quad u_k \sim N(0, \tau^2), \quad v_{jk} \sim N(0, \omega^2),
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{x}_{jk}\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(1 \times p\)&lt;/span&gt; row-vector of covariates describing outcome &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; in study &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\beta\)&lt;/span&gt; is a &lt;span class=&#34;math inline&#34;&gt;\(p \times 1\)&lt;/span&gt; vector of meta-regression coefficients. In past work, I’ve done this by writing a data-generating function that takes a fixed design matrix &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X} = \left(\mathbf{x}_{11}&amp;#39; \cdots \mathbf{x}_{J_K K}&amp;#39;\right)&amp;#39;\)&lt;/span&gt; as an input argument, along with &lt;span class=&#34;math inline&#34;&gt;\(\boldsymbol\beta\)&lt;/span&gt;. The design matrix would also include an identifier for each unique study. There are surely better (simpler, easier to follow) ways to implement the multi-level meta-regression model. I’ll once again leave it to you to work out an approach.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Easily simulate thousands of single-case designs</title>
      <link>https://www.jepusto.com/easily-simulate-thousands-of-single-case-designs/</link>
      <pubDate>Thu, 21 Jun 2018 00:00:00 +0000</pubDate>
      <guid>https://www.jepusto.com/easily-simulate-thousands-of-single-case-designs/</guid>
      <description>


&lt;p&gt;Earlier this month, I taught at the &lt;a href=&#34;https://scdinstitute2018.com/&#34;&gt;Summer Research Training Institute on Single-Case Intervention Design and Analysis workshop&lt;/a&gt;, sponsored by the Institute of Education Sciences’ National Center for Special Education Research.
While I was there, I shared &lt;a href=&#34;https://jepusto.shinyapps.io/ARPsimulator/&#34;&gt;a web-app for simulating data from a single-case design&lt;/a&gt;.
This is a tool that I put together a couple of years ago as part of my &lt;a href=&#34;https://www.jepusto.com/software/arpobservation/&#34;&gt;ARPobservation R package&lt;/a&gt;, but haven’t ever really publicized or done anything formal with.
It provides an easy way to simulate “mock” data from a single-case design where the dependent variable is measured using systematic direct observation of behavior.
The simulated data can be viewed in the form of a graph or downloaded as a csv file.
And it’s quite fast—simulating 1000’s of mock single-case designs takes only a few seconds.
The tool also provides a visualization of the distribution of effect size estimates that you could anticipate observing in a single-case design, given a set of assumptions about how the dependent variable is measured and how it changes in response to treatment.&lt;/p&gt;
&lt;div id=&#34;demo&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Demo&lt;/h1&gt;
&lt;p&gt;Here’s an example of the sort of data that the tool generates and the assumptions it asks you to make.
Say that you’re interested in evaluating the effect of a Social Stories intervention on the behavior of a child with autism spectrum disorder, and that you plan to use a treatment reversal design.
Your primary dependent variable is inappropriate play behavior, measured using frequency counts over ten minute observation sessions.&lt;br /&gt;
The initial baseline and treatment phases will be 7 sessions long.
At baseline, the child engages in inappropriate play at a rate of about 0.8 per minute.
You anticipate that the intervention could reduce inappropriate play by as much as 90% from baseline.
Enter all of these details and assumptions into the simulator, and it will generate a graph like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jepusto.com/img/Crozier-Tincani-simulated-data-A.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Hit the “Simulate!” button again and you might get something like this:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jepusto.com/img/Crozier-Tincani-simulated-data-B.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Or one of these:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jepusto.com/img/Crozier-Tincani-simulated-data-C.png&#34; /&gt;
&lt;img src=&#34;https://www.jepusto.com/img/Crozier-Tincani-simulated-data-A.png&#34; /&gt;
&lt;img src=&#34;https://www.jepusto.com/img/Crozier-Tincani-simulated-data-A.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jepusto.com/img/Crozier-Tincani-simulated-data-D.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jepusto.com/img/Crozier-Tincani-simulated-data-E.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;All of the above graphs were generated from the same hypothetical model—the variation in the clarity and strength of the functional relation is due to random error alone.
The simulator can also produce graphs that show multiple realizations of the data-generating process. Here’s one with five replications:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jepusto.com/img/Crozier-Tincani-simulated-data-5.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;And here’s the same figure, but with trend lines added:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jepusto.com/img/Crozier-Tincani-simulated-data-trend.png&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The trend lines represent the overall average level of the dependent variable during each session, across infinitely replications of the study.
The variability around the trend line provides a sense of the extent of random error in the measurements of the dependent variable.&lt;/p&gt;
&lt;p&gt;I think it’s a rather interesting exercise to try and draw inferences based on visual inspection of randomly generated graphs like this—particularly because it forces you to grapple with random measurement error in a way that using only real data (or only hand-drawn mock data) doesn’t allow.
It seems like it could really help a visual analyst to calibrate their interpretations of single-case graphs with visually apparent time trends, outliers, etc.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;use-cases&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Use cases&lt;/h1&gt;
&lt;p&gt;So far, this tool is really only a toy—something that I’ve puttered with off and on for a while, but never developed or applied for any substantive purpose.
However, it occurs to me that it (or something similar to it) might have a number of purposes related to planning single-case studies, studying the process of visual inspection, or training single-case researchers.&lt;/p&gt;
&lt;p&gt;When I originally put the tool together, the leading case I imagined was to use the tool to help researchers make principled decisions about how to measure dependent variables in single-case designs.
By using the tool to simulate hypothetical single-case studies, a researcher would be able to experiment with different measurement strategies—such as using partial interval recording instead of continuous duration recording, using shorter or longer observation sessions, or using short or longer baseline phases—before collecting data on real-life behavior in the field.
I’m not sure if this is something that well-trained single-case researchers would actually find helpful, but it seems like it might help a novice (like me!) to temper one’s expectations or to move towards a more reliable measurement system.&lt;/p&gt;
&lt;p&gt;There’s been quite a bit of research examining the reliability and error rates of inferences based on visual inspection (see &lt;a href=&#34;http://dx.doi.org/10.1037/14376-004&#34;&gt;Chapter 4 of Kratochwill &amp;amp; Levin, 2014&lt;/a&gt; for a review of some of this literature).
Some of this work has compared the inferences drawn by novices versus experts or by un-aided visual inspection versus visual inspection supplemented with graphical guides (like trend lines).
But there are many other factors that could be investigated too, such as phase lengths (this could help to better justify the WWC single-case design standards around minimum phase lengths), use of different measurement systems, or use of different design elements on single-case graphs (can we get some color on these graphs, folks?!? And stop plotting 14 different dependent variables on the same graph?!?).
The simulator would be an easy way to generate the stimuli one would need to do this sort of work.&lt;/p&gt;
&lt;p&gt;A closely related use-case is to generate stimuli for training researchers to do systematic visual inspection.
Some of the SCD Institute instructors (including Tom Kratochwill, Rob Horner, Joel Levin, along with some of their other colleagues) have developed the website &lt;a href=&#34;http://www.singlecase.org&#34;&gt;www.singlecase.org&lt;/a&gt; with a bunch of exercises meant to help researchers develop and test their visual analysis skills.
It looks to me like the site uses simulated data (though I’m not entirely sure).
The ARPsimulator tool could be used to do something similar, but based on a data-generating process that captures many of the features of systematic direct observation data.
This might let researchers test their skills under more challenging and ambiguous, yet plausible, conditions, similar to what they will encounter when collecting real data in the field.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;future-directions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Future directions&lt;/h1&gt;
&lt;p&gt;A number of future directions for this project have crossed my mind:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Currently, the outcome data are simulated as independent across observation sessions (given the true time trend). It wouldn’t be too hard to add a further option to generate auto-correlated data, although this would further increase the complexity of the model. Perhaps there would be a way to add this as an “advanced” option that would be concealed unless the user asks for it (i.e., “Are you Really Sure you want to go down this rabbit hole?”). So far, I have avoided adding these features because I’m not sure what reasonable defaults would be.&lt;/li&gt;
&lt;li&gt;Joel Levin, John Ferron, and some of the other SCD Institute instructors are big proponents of incorporating randomization procedures into the design of single-case studies, at least when circumstances allow. Currently, the ARPsimulator generates data based on a fixed, pre-specified design, such as an ABAB design with 6 sessions per phase or a multiple baseline design with 25 sessions total and intervention start-times of 8, 14, and 20. It wouldn’t be too hard to incorporate randomized phase-changes into the simulator. This might make a nice, contained project for a student who wants to learn more about randomization designs.&lt;/li&gt;
&lt;li&gt;Along similar lines, John Ferron has &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.3200/JEXE.75.1.66-81&#34;&gt;developed&lt;/a&gt; and &lt;a href=&#34;https://doi.org/10.1002/jaba.410&#34;&gt;evaluated&lt;/a&gt; masked visual analysis procedures, which blend randomization and traditional response-guided approaches to designing single-case studies. It would take a bit more work, but it would be pretty nifty to incorporate these designs into ARPsimulator too.&lt;/li&gt;
&lt;li&gt;Currently, the model behind ARPsimulator asks the user to specify a fixed baseline level of behavior, and this level of behavior is used for every simulated case—even in designs involving multiple cases. A more realistic (albeit more complicated) data-generating model would allow for between-case variation in the baseline level of behavior.&lt;/li&gt;
&lt;li&gt;Perhaps the most important outstanding question about the premise of this work is just how well the alternating renewal process model captures the features of real single-case data. Validating the model against empirical data from single-case studies would allow use to assess whether it is really a realistic approach to simulation, at least for certain classes of behavior. Another product of such an investigation would be to develop realistic default assumptions for the model’s parameters.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At the moment I have no plans to implement any of these unless there’s a reasonably focused need (sadly, I don’t have time to putter and putz to the same extent that I used to).
If you, dear reader, would be interested in helping to pursue any of these directions, or if you have other, better ideas for how to make use of this tool, I would love to hear from you.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>New paper: procedural sensitivities of effect size measures for SCDs</title>
      <link>https://www.jepusto.com/procedural-sensitivities-paper/</link>
      <pubDate>Thu, 11 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://www.jepusto.com/procedural-sensitivities-paper/</guid>
      <description>


&lt;p&gt;I’m very happy to share that my article “Procedural sensitivities of effect sizes for single-case designs with directly observed behavioral outcome measures” has been accepted at &lt;em&gt;Psychological Methods&lt;/em&gt;. There’s no need to delay in reading it, since you can check out the &lt;a href=&#34;https://psyarxiv.com/vxa86&#34;&gt;pre-print&lt;/a&gt; and &lt;a href=&#34;https://osf.io/hkzsm/&#34;&gt;supporting materials&lt;/a&gt;. Here’s the abstract:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A wide variety of effect size indices have been proposed for quantifying the magnitude of treatment effects in single-case designs. Commonly used measures include parametric indices such as the standardized mean difference, as well as non-overlap measures such as the percentage of non-overlapping data, improvement rate difference, and non-overlap of all pairs. Currently, little is known about the properties of these indices when applied to behavioral data collected by systematic direct observation, even though systematic direct observation is the most common method for outcome measurement in single-case research. This study uses Monte Carlo simulation to investigate the properties of several widely used single-case effect size measures when applied to systematic direct observation data. Results indicate that the magnitude of the non-overlap measures and of the standardized mean difference can be strongly influenced by procedural details of the study’s design, which is a significant limitation to using these indices as effect sizes for meta-analysis of single-case designs. A less widely used parametric index, the log-response ratio, has the advantage of being insensitive to sample size and observation session length, although its magnitude is influenced by the use of partial interval recording.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This paper was a long time coming. The core idea came out of a grant proposal I wrote during the summer of 2014, which I fleshed out for a &lt;a href=&#34;https://www.jepusto.com/files/AERA-2015-poster-Non-overlap-measures.pdf&#34;&gt;poster presented at AERA&lt;/a&gt; in April of 2015. After finishing a draft of the paper, I tried to publish it in a special education journal, reasoning that the main audience for the paper is researchers interested in meta-analyzing single case research studies that are commonly used in some parts of special education. That turned out to be a non-starter. Four rejection letters later, I re-worked the paper a bit to give more technical details, then submitted it to a more methods-ish journal. This yielded an R&amp;amp;R, I revised the paper extensively, resubmitted it, and it was declined. Buying in fully to the sunk costs fallacy, I sent the paper to Psychological Methods. This time, I received very extensive and helpful feedback from several anonymous reviewers and an associate editor (thank you, anonymous peers!), which helped me to revise the paper yet again, and this time it was accepted. Sixth time is the charm, as they say.&lt;/p&gt;
&lt;p&gt;Here’s the complete time-line of submissions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;August 5, 2015: submitted to journal #1 (special education)&lt;/li&gt;
&lt;li&gt;August 28, 2015: desk reject decision from journal #1&lt;/li&gt;
&lt;li&gt;September 3, 2015: submitted to journal #2 (special education)&lt;/li&gt;
&lt;li&gt;November 6, 2015: reject decision (after peer review) from journal #2&lt;/li&gt;
&lt;li&gt;November 18, 2015: submitted to journal #3 (special education)&lt;/li&gt;
&lt;li&gt;November 22, 2015: desk reject decision from journal #3 as not appropriate for their audience. I was grateful to get a quick decision.&lt;/li&gt;
&lt;li&gt;November 23, 2015: submitted to journal #4 (special education)&lt;/li&gt;
&lt;li&gt;February 17, 2016: reject decision (after peer review) from journal #4&lt;/li&gt;
&lt;li&gt;April 19, 2016: submitted to journal #5 (methods)&lt;/li&gt;
&lt;li&gt;August 16, 2016: revise-and-resubmit decision from journal #5&lt;/li&gt;
&lt;li&gt;October 14, 2016: re-submitted to journal #5&lt;/li&gt;
&lt;li&gt;February 2, 2017: reject decision from journal #5&lt;/li&gt;
&lt;li&gt;May 10, 2017: submitted to &lt;em&gt;Psychological Methods&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;September 1, 2017: revise-and-resubmit decision from &lt;em&gt;Psychological Methods&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;September 26, 2017: re-submitted to &lt;em&gt;Psychological Methods&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;November 22, 2017: conditional acceptance&lt;/li&gt;
&lt;li&gt;December 6, 2017: re-submitted with minor revisions&lt;/li&gt;
&lt;li&gt;January 10, 2018: accepted at &lt;em&gt;Psychological Methods&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>You wanna PEESE of d&#39;s?</title>
      <link>https://www.jepusto.com/pet-peese-performance/</link>
      <pubDate>Thu, 27 Apr 2017 00:00:00 +0000</pubDate>
      <guid>https://www.jepusto.com/pet-peese-performance/</guid>
      <description>


&lt;p&gt;Publication bias—or more generally, outcome reporting bias or dissemination bias—is recognized as a critical threat to the validity of findings from research syntheses. In the areas with which I am most familiar (education and psychology), it has become more or less a requirement for research synthesis projects to conduct analyses to detect the presence of systematic outcome reporting biases. Some analyses go further by trying correct for its distorting effects on average effect size estimates. Widely known analytic techniques for doing so include Begg and Mazumdar’s &lt;a href=&#34;https://dx.doi.org/10.2307/2533446&#34;&gt;rank-correlation test&lt;/a&gt;, the Trim-and-Fill technique proposed by &lt;a href=&#34;https://dx.doi.org/10.2307/2669529&#34;&gt;Duval and Tweedie&lt;/a&gt;, and &lt;a href=&#34;https://dx.doi.org/10.1136/bmj.315.7109.629&#34;&gt;Egger regression&lt;/a&gt; (in its &lt;a href=&#34;https://dx.doi.org/10.1186/1471-2288-9-2&#34;&gt;many variants&lt;/a&gt;). Another class of methods involves selection models (or weight function models), as proposed by &lt;a href=&#34;https://dx.doi.org/10.1007/BF02294384&#34;&gt;Hedges and Vevea&lt;/a&gt;, &lt;a href=&#34;https://dx.doi.org/10.1037/1082-989X.10.4.428&#34;&gt;Vevea and Woods&lt;/a&gt;, and others. As far as I can tell, selection models are well known among methodologists but very seldom applied due to their complexity and lack of ready-to-use software (though &lt;a href=&#34;https://CRAN.R-project.org/package=weightr&#34;&gt;an R package&lt;/a&gt; has recently become available). More recent proposals include the PET-PEESE technique introduced by &lt;a href=&#34;https://dx.doi.org/10.1002/jrsm.1095&#34;&gt;Stanley and Doucouliagos&lt;/a&gt;; Simonsohn, Nelson, and Simmon’s &lt;a href=&#34;https://dx.doi.org/10.1177/1745691614553988&#34;&gt;p-curve technique&lt;/a&gt;; Van Assen, Van Aert, and Wichert’s &lt;a href=&#34;http://dx.doi.org/10.1037/met0000025&#34;&gt;p-uniform&lt;/a&gt;, and others. The list of techniques grows by the day.&lt;/p&gt;
&lt;p&gt;Among these methods, Egger regression, PET, and PEESE are superficially quite appealing due to their simplicity. These methods each involve estimating a fairly simple meta-regression model, using as the covariate the sampling variance of the effect size or some transformation thereof. PET uses the standard error of the effect size as the regressor; PEESE uses the sampling variance (i.e., the squared standard error); PET-PEESE involves first testing whether the PET estimate is statistically significant, using PEESE if it is or PET otherwise. The intercept from one of these regressions is the average effect size estimate from a study with zero sampling variance; the estimated intercept is used as a “bias-corrected” estimator of the population average effect. These methods are also appealing due to their extensibility. Because they are just meta-regressions, it is comparatively easy to extend them to meta-regression models that control for further covariates, to use robust variance estimation to account for dependencies among effect size estimates, etc.&lt;/p&gt;
&lt;p&gt;In &lt;a href=&#34;http://datacolada.org/59&#34;&gt;a recent blog post&lt;/a&gt;, Uri Simonsohn reports some simulation evidence indicating that the PET-PEESE estimator can have large biases under certain conditions, &lt;em&gt;even in the absence of publication bias&lt;/em&gt;. The simulations are based on standardized mean differences from two-group experiments and involve simulating collections of studies that include many with small sample sizes, as might be found in certain areas of psychology. On the basis of these performance assessments, he argues that this purported cure is actually worse than the disease—that PET-PEESE should &lt;em&gt;not&lt;/em&gt; be used in meta-analyses of psychological research because it performs too poorly to be trusted. In &lt;a href=&#34;http://datacolada.org/wp-content/uploads/2017/04/Response-by-Joe-Hilgard-to-Colada-59.pdf&#34;&gt;a response to Uri’s post&lt;/a&gt;, &lt;a href=&#34;http://crystalprisonzone.blogspot.com/&#34;&gt;Joe Hilgard&lt;/a&gt; suggests that some simple modifications to the method can improve its performance. Specifically, Joe suggests using a function of sample size as the covariate (in place of the standard error or sampling variance of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;), and also using PET or PEESE as stand-alone estimators, rather than using them contingent on a significance test.&lt;/p&gt;
&lt;p&gt;In this post, I follow up Joe’s suggestions while replicating and expanding upon Uri’s simulations, to try and provide a fuller picture of the relative performance of these estimators. In brief, the simulations show that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tests for small-sample bias that use PET or PEESE can have wildly incorrect type-I error rates in the absence of publication bias. Don’t use them.&lt;/li&gt;
&lt;li&gt;The sample-size variants of PET and PEESE &lt;strong&gt;do&lt;/strong&gt; maintain the correct type-I error rates in the absence of publication bias.&lt;/li&gt;
&lt;li&gt;The sample-size variants of PET and PEESE are exactly unbiased in the absence of publication bias.&lt;/li&gt;
&lt;li&gt;However, these adjusted estimators still have a cost, being less precise than the conventional fixed-effect estimator.&lt;/li&gt;
&lt;li&gt;In the presence of varying degrees of publication bias, none of the estimators consistently out-perform the others. If you really really need to use a regression-based correction, the sample-size variant of PEESE seems like it might be a reasonable default method, but it’s still really pretty rough.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;why-use-sample-size&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Why use sample size?&lt;/h1&gt;
&lt;p&gt;To see why it makes sense to use a function of sample size as the covariate for PET-PEESE analyses, rather than using the standard error of the effect size estimate, let’s look at the formulas. Say that we have a standardized mean difference estimate from a two-group design (without covariates) with sample sizes &lt;span class=&#34;math inline&#34;&gt;\(n_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(n_1\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
d = \frac{\bar{y}_1 - \bar{y}_0}{s_p},
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}_0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}_1\)&lt;/span&gt; are the sample means within each group and &lt;span class=&#34;math inline&#34;&gt;\(s_p^2\)&lt;/span&gt; is the pooled sample variance. Following convention, we’ll assume that the outcomes are normally distributed within each group, and the groups have common variance. The exact sampling variance of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; is a rather complicated formula, but one which can be approximated reasonably well as&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Var}(d) \approx \frac{n_0 + n_1}{n_0 n_1} + \frac{\delta^2}{2(n_0 + n_1)},
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt; is the &lt;em&gt;true&lt;/em&gt; standardized mean difference parameter. This formula is a delta-method approximation. The first term captures the variance of the numerator of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;, so it gets at how precisely the &lt;em&gt;unstandardized&lt;/em&gt; difference in means is estimated. The second term captures the variance of the denominator of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;, so it gets at how precisely the &lt;em&gt;scale&lt;/em&gt; of the outcome is estimated. The second term also involves the unknown parameter &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt;, which must be estimated in practice. The conventional formula for the estimated sampling variance of &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; substitutes &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; in place of &lt;span class=&#34;math inline&#34;&gt;\(\delta\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
V = \frac{n_0 + n_1}{n_0 n_1} + \frac{d^2}{2(n_0 + n_1)}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;In PET-PEESE analysis, &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; or its square root is used as a covariate in a regression of the effect sizes, as a means of adjusting for publication bias. There are two odd things about this. First, publication bias is about the statistical significance of the group differences, but statistical significance &lt;strong&gt;&lt;em&gt;does not depend on the scale of the outcome&lt;/em&gt;&lt;/strong&gt;. The test of the null hypothesis of no differences between groups is &lt;strong&gt;&lt;em&gt;not&lt;/em&gt;&lt;/strong&gt; based on &lt;span class=&#34;math inline&#34;&gt;\(d / \sqrt{V}\)&lt;/span&gt;. Instead, it is a function of the &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; statistic:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
t = d / \sqrt{\frac{n_0 + n_1}{n_0 n_1}}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Consequently, it makes sense to use &lt;strong&gt;&lt;em&gt;only the first term of &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt;&lt;/em&gt;&lt;/strong&gt; as a covariate for purposes of detecting publication biases.&lt;/p&gt;
&lt;p&gt;The second odd thing is that &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; is generally going to be correlated with &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; because we have to use &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; to calculate &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt;. As &lt;a href=&#34;http://datacolada.org/wp-content/uploads/2017/04/Response-by-Joe-Hilgard-to-Colada-59.pdf&#34;&gt;Joe explained in his response to Uri&lt;/a&gt;, this means that there will be a non-zero correlation between &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; (or between &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{V}\)&lt;/span&gt;) except in some very specific cases, even in the absence of any publication bias. Pretty funky.&lt;/p&gt;
&lt;p&gt;This second problem with regression tests for publication bias has been recognized for a while in the literature (e.g., &lt;a href=&#34;https://dx.doi.org/10.1002/sim.698&#34;&gt;Macaskill, Walter, &amp;amp; Irwig, 2001&lt;/a&gt;; &lt;a href=&#34;https://dx.doi.org/10.1001/jama.295.6.676&#34;&gt;Peters et al., 2006&lt;/a&gt;; &lt;a href=&#34;https://dx.doi.org/10.1186/1471-2288-9-2&#34;&gt;Moreno et al., 2009&lt;/a&gt;), but most of the work here has focused on other effect size measures, like odds ratios, that are relevant in clinical medicine. The behavior of these estimators might well differ for &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt;’s because the dependence between the effect measure and its variance has a different structure.&lt;/p&gt;
&lt;p&gt;Below I’ll investigate how this stuff works with standardized mean differences, which haven’t been studied as extensively as odds ratios. Actually, I know of only two simulation studies that examined the performance of PET-PEESE methods with standardized mean difference estimates: &lt;a href=&#34;http://dx.doi.org/10.2139/ssrn.2659409&#34;&gt;Inzlicht, Gervais, and Berkman (2015)&lt;/a&gt; and &lt;a href=&#34;https://dx.doi.org/10.1177/1948550617693062&#34;&gt;Stanley (2017)&lt;/a&gt;. (Know of others? Leave a comment!) Neither considered using sample-size variants of PET-PEESE. The only source I know of that &lt;em&gt;did&lt;/em&gt; consider this is this &lt;a href=&#34;http://willgervais.com/blog/2015/6/29/pet-peese-vs-peters&#34;&gt;blog post from Will Gervais&lt;/a&gt;, which starts out optimistic about the sample-size variants but ends on a discouraged note. The simulations below build upon Will’s work, as well as Uri’s, by 1) considering a more extensive set of data-generating processes and 2) examining accuracy in addition to bias.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simulation-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Simulation model&lt;/h1&gt;
&lt;p&gt;The simulations are based on the following data-generating model, which closely follows &lt;a href=&#34;http://datacolada.org/59&#34;&gt;the structure that Uri used&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Per-cell sample size is generated as &lt;span class=&#34;math inline&#34;&gt;\(n = 12 + B (n_{max} - 12)\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(B \sim Beta(\alpha, \beta)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(n_{max}\)&lt;/span&gt; is the maximum observed sample size. I take &lt;span class=&#34;math inline&#34;&gt;\(n_{max} = 50\)&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;\(120\)&lt;/span&gt; and look at three sample size distributions (note that these distributions are pre-selection, so the observed sample size distributions will deviate from these if there is selective publication):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\alpha = \beta = 1\)&lt;/span&gt; corresponds to a uniform distribution on &lt;span class=&#34;math inline&#34;&gt;\([12,n_{max}]\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\alpha = 1, \beta = 3\)&lt;/span&gt; is a distribution with more small studies; and&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\alpha = 3, \beta = 1\)&lt;/span&gt; is a distribution with more large studies.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;True effects are simulated as &lt;span class=&#34;math inline&#34;&gt;\(\delta \sim N(\mu, \sigma^2)\)&lt;/span&gt;, for &lt;span class=&#34;math inline&#34;&gt;\(\mu = 0, 0.1, 0.2, ..., 1.0\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma = 0.0, 0.1, 0.2, 0.4\)&lt;/span&gt;. Note that the values of &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; are &lt;em&gt;standard deviations&lt;/em&gt; of the true effects, with &lt;span class=&#34;math inline&#34;&gt;\(\sigma = 0.0\)&lt;/span&gt; corresponding to the constant effect model and &lt;span class=&#34;math inline&#34;&gt;\(\sigma = 0.4\)&lt;/span&gt; corresponding to rather substantial effect heterogeneity.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Standardized mean difference effect size estimates are generated as in a two-group between-subjects experiment with equal per-cell sample sizes. I do this by taking &lt;span class=&#34;math inline&#34;&gt;\(t = D / \sqrt{S / [2(n - 1)]}\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(D \sim N(\delta \sqrt{n / 2}, 1)\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(S \sim \chi^2_{2(n - 1)}\)&lt;/span&gt;, then calculating&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
  d = \left(1 - \frac{3}{8 n - 9}\right) \times \sqrt{\frac{2}{n}} \times t.
  \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;(That first term is Hedges’ &lt;span class=&#34;math inline&#34;&gt;\(g\)&lt;/span&gt; correction, cuz that’s how I roll.)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Observed effects are filtered based on statistical significance. Let &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; be the p-value corresponding to the observed &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; and the one-tailed hypothesis test of &lt;span class=&#34;math inline&#34;&gt;\(\delta \leq 0\)&lt;/span&gt;. If &lt;span class=&#34;math inline&#34;&gt;\(p &amp;lt; .025\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; is observed with probability 1. If &lt;span class=&#34;math inline&#34;&gt;\(p \geq .025\)&lt;/span&gt;, then &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; is observed with probability &lt;span class=&#34;math inline&#34;&gt;\(\pi\)&lt;/span&gt;. Noted that this mechanism corresponds to filtering based on two-sided hypothesis tests, where effects are filtered if they are statistically non-significant effects or statistically significant but in the wrong direction. I look at three scenarios:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\pi = 1.0\)&lt;/span&gt; corresponds to no selective publication (all simulated effects are observed);&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\pi = 0.2\)&lt;/span&gt; corresponds to an intermediate degree of selective publication (some but not non-significant effects are observed); and&lt;/li&gt;
&lt;li&gt;&lt;span class=&#34;math inline&#34;&gt;\(\pi = 0.0\)&lt;/span&gt; corresponds to very strong selective publication (only statistically significant effects are observed).&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Each meta-analysis includes a total of &lt;span class=&#34;math inline&#34;&gt;\(k = 100\)&lt;/span&gt; observed studies. Note that in scenarios with publication bias, more (sometimes many more) than 100 studies are generated in order to get 100 observed effects.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For each simulated meta-sample, I calculated the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the usual fixed-effect meta-analytic average (I skipped random effects for simplicity);&lt;/li&gt;
&lt;li&gt;the PET estimator (including intercept and slope);&lt;/li&gt;
&lt;li&gt;the PEESE estimator (including intercept and slope);&lt;/li&gt;
&lt;li&gt;PET-PEESE, which is equal to the PEESE intercept if &lt;span class=&#34;math inline&#34;&gt;\(H_0: \beta_0 \leq 0\)&lt;/span&gt; is rejected at the 10% level, and is otherwise equal to the PET intercept (this definition follows &lt;a href=&#34;https://dx.doi.org/10.1177/1948550617693062&#34;&gt;Stanley, 2017&lt;/a&gt;);&lt;/li&gt;
&lt;li&gt;the modified PET estimator, which I’ll call “SPET” for “sample-size PET” (suggestions for better names welcome);&lt;/li&gt;
&lt;li&gt;the modified PEESE estimator, which I’ll call “SPEESE”; and&lt;/li&gt;
&lt;li&gt;SPET-SPEESE, which follows the same conditional logic as PET-PEESE.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Simulation results are summarized across 4000 replications. The R code for all this &lt;a href=&#34;https://www.jepusto.com/R/PET-PEESE-performance-simulations.R&#34;&gt;lives here&lt;/a&gt;. Complete numerical results &lt;a href=&#34;https://www.jepusto.com/files/PET-PEESE-Simulation-Results.Rdata&#34;&gt;live here&lt;/a&gt;. Code for creating the graphs below &lt;a href=&#34;https://www.jepusto.com/R/PET-PEESE-performance-graphs.R&#34;&gt;lives here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;results&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Results&lt;/h1&gt;
&lt;div id=&#34;false-positive-rates-for-publication-bias-detection&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;False-positive rates for publication bias detection&lt;/h3&gt;
&lt;p&gt;First, let’s consider the performance of PET and PEESE as tests for detecting publication bias. Here, a statistically significant estimate for the coefficient on the SE (for PET) or on &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; (for PEESE) is taken as evidence of small-sample bias. For that logic to hold, the tests should maintain the nominal error rates in the absence of publication bias.&lt;/p&gt;
&lt;p&gt;The figure below depicts the Type-I error rates of the PET and PEESE tests when &lt;span class=&#34;math inline&#34;&gt;\(\pi = 1\)&lt;/span&gt; (so no publication bias at all), for a one-sided test of &lt;span class=&#34;math inline&#34;&gt;\(H_0: \beta_1 \leq 0\)&lt;/span&gt; at the nominal level of &lt;span class=&#34;math inline&#34;&gt;\(\alpha = .05\)&lt;/span&gt;. Rejection rates are plotted for varying true mean effects, levels of heterogeneity, and sample size distributions. Separate colors are used for maximum sample sizes of 50 or 120.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jepusto.com/post/PET-PEESE-performance_files/figure-html/PET-PEESE-rejection-rates-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Both tests are horribly mis-calibrated, tending to reject the null hypothesis far more often than they should. This happens because there is a non-zero correlation between &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt;, even in the absence of publication bias. Thus, it does not follow that rejecting &lt;span class=&#34;math inline&#34;&gt;\(H_0: \beta_1 \leq 0\)&lt;/span&gt; implies rejection of the hypothesis that there is no publication bias. (Sorry, that’s at least a triple negative!)&lt;/p&gt;
&lt;p&gt;Here’s the same graph, but using the SPET and SPEESE estimators:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jepusto.com/post/PET-PEESE-performance_files/figure-html/SPET-SPEESE-rejection-rates-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Yes, this may be the World’s Most Boring Figure, but it does make clear that both the SPET and SPEESE tests maintain the correct Type-I error rate. (Any variation in rejection rates is just Monte Carlo error.) Thus, it seems pretty clear that if we want to test for small-sample bias, SPET or SPEESE should be used rather than PET or PEESE.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bias-of-bias-corrected-estimators&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Bias of bias-corrected estimators&lt;/h3&gt;
&lt;p&gt;Now let’s consider the performance of these methods as estimators of the population mean effect. &lt;a href=&#34;http://datacolada.org/59&#34;&gt;Uri’s analysis&lt;/a&gt; focused on the bias of the estimators, meaning the difference between the average value of the estimator (across repeated samples) and the true parameter. The plot below depicts the expected level of PET, PEESE, and PET-PEESE as a function of the true mean effect, using the uniform distribution of studies and a maximum sample size of &lt;span class=&#34;math inline&#34;&gt;\(n = 50\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jepusto.com/post/PET-PEESE-performance_files/figure-html/bias-of-PET-PEESE-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;All three of these estimators are pretty bad in terms of bias. In the absence of publication bias, they consistently &lt;em&gt;under&lt;/em&gt;-estimate the true mean effect. With intermediate or strong publication bias, PET and PET-PEESE have a consistent downward bias. As an unconditional estimator, PEESE tends to have a positive bias when the true effect is small, but this decreases and becomes negative as the true effect increases. For all three estimators, bias increases as the degree of heterogeneity increases.&lt;/p&gt;
&lt;p&gt;Here is how these estimators compare to the modified SPET, SPEESE, and SPET-SPEESE estimators, as well as to the usual fixed-effect average with no correction for publication bias:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jepusto.com/post/PET-PEESE-performance_files/figure-html/bias-of-SPET-SPEESE-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In the left column, we see that SPET and SPEESE are exactly unbiased (and SPET-SPEESE is nearly so) in the absence of selective publication. So is regular old fixed effect meta-analysis, of course. In the middle and right columns, studies are selected based partially or fully on statistical significance, and things get messy. Overall, there’s no consistent winner between PEESE versus SPEESE. At small or moderate levels of between-study heterogeneity, and when the true mean effect is small, PEESE, SPEESE, and SPET-SPEESE have fairly similar biases, but PEESE appears to have a slight edge. This seems to me to be nothing but a fortuitous accident, in that the bias induced by the correlation between &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; just happens to work in the right direction. Then, as the true mean effect increases, SPEESE and SPET-SPEESE start to edge out PEESE. This makes sense because the bias induced by the correlation between &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; will be larger when the true effect sizes are larger.&lt;br /&gt;
These trends seem mostly to hold for the other sample size distributions I examined too, although the biases of PEESE and PET-PEESE aren’t as severe when the maximum sample size is larger. You can see for yourself here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.jepusto.com/rmarkdown-libs/figure-html4/more-bias-of-SPET-SPEESE-1.png&#34;&gt;Uniform distribution of studies, maximum sample size of 120&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.jepusto.com/rmarkdown-libs/figure-html4/more-bias-of-SPET-SPEESE-2.png&#34;&gt;More small studies, maximum sample size of 50&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.jepusto.com/rmarkdown-libs/figure-html4/more-bias-of-SPET-SPEESE-3.png&#34;&gt;More small studies, maximum sample size of 120&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.jepusto.com/rmarkdown-libs/figure-html4/more-bias-of-SPET-SPEESE-4.png&#34;&gt;More large studies, maximum sample size of 50&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.jepusto.com/rmarkdown-libs/figure-html4/more-bias-of-SPET-SPEESE-5.png&#34;&gt;More large studies, maximum sample size of 120&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;accuracy-of-bias-corrected-estimators&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Accuracy of bias-corrected estimators&lt;/h3&gt;
&lt;p&gt;Bias isn’t everything, of course. Now let’s look at the overall accuracy of these estimators, as measured by root mean squared error (RMSE). RMSE is a function of both bias and sampling variance, and so is one way to weigh an estimator that is biased but fairly precise against an estimator that is perfectly unbiased but noisy. The following chart plots the RMSE of all of the estimators (following the same layout as above, just with a different vertical axis):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jepusto.com/post/PET-PEESE-performance_files/figure-html/RMSE-plots-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Starting in the left column where there’s no selective publication, we can see that the normal fixed-effect average has the smallest RMSE (and so is most accurate). The next most accurate is SPEESE, which uniformly beats out PEESE, PET-PEESE, SPET, and SPET-SPEESE. It’s worth noting, though, that there is a fairly large penalty for using SPEESE when it is unnecessary: even with a quite large sample of 100 studies, SPEESE still has twice the RMSE of the FE estimator.&lt;/p&gt;
&lt;p&gt;The middle column shows these estimators’ RMSE when there is an intermediate degree of selective publication. Because of the “fortuitous accident” of how the correlation between &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; affects the PEESE estimator, it is more accurate than SPEESE for small values of the true mean effect. Its advantage is larger when heterogeneity is larger, and heterogeneity also affects the point (i.e., what true mean effect) at which SPEESE catches up with PEESE. Then at larger true mean effects, the accuracy of SPEESE continues to improve while the accuracy of PEESE degrades. It is also interesting to note that at this intermediate degree of selective publication, none of the other bias-correction estimators (PET-PEESE, SPET, SPET-SPEESE) compete with PEESE and SPEESE.&lt;/p&gt;
&lt;p&gt;Finally, the right column plots RMSE when there’s strong selective publication, so only statistically significant effects appear. Just as in the middle column, PEESE edges out SPEESE for smaller values of the true mean effect. For very small true effects, both of these estimators are edged out by PET-PEESE and SPET-SPEESE. This only holds over a very small range for the true mean effect though, and for true effects above that range these conditional estimators perform poorly—consistently worse than just using PEESE or SPEESE.&lt;/p&gt;
&lt;p&gt;Here are charts for the other sample size distributions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.jepusto.com/rmarkdown-libs/figure-html4/more-RMSE-plots-1.png&#34;&gt;Uniform distribution of studies, maximum sample size of 120&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.jepusto.com/rmarkdown-libs/figure-html4/more-RMSE-plots-2.png&#34;&gt;More small studies, maximum sample size of 50&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.jepusto.com/rmarkdown-libs/figure-html4/more-RMSE-plots-3.png&#34;&gt;More small studies, maximum sample size of 120&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.jepusto.com/rmarkdown-libs/figure-html4/more-RMSE-plots-4.png&#34;&gt;More large studies, maximum sample size of 50&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.jepusto.com/rmarkdown-libs/figure-html4/more-RMSE-plots-5.png&#34;&gt;More large studies, maximum sample size of 120&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The trends that I’ve noted mostly seem to hold for the other sample size distributions (but correct me if you disagree! I’m getting kind of bleary-eyed at the moment…). One difference worth noting is that when the sample size distribution skews towards having more large studies, the accuracy of the regular fixed-effect estimator improves a bit. At intermediate degrees of selective publication, the fixed-effect estimator is &lt;em&gt;consistently&lt;/em&gt; more accurate than SPEESE, and mostly more accurate than PEESE too. With strong selective publication, though, the FE estimator blows up just as before.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusions-caveats-further-thoughts&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusions, caveats, further thoughts&lt;/h1&gt;
&lt;p&gt;Where does this leave us? The one thing that seems pretty clear is that if the meta-analyst’s goal is to test for potential small-sample bias, then SPET or SPEESE should be used rather than PET or PEESE. Beyond that, we’re in a bit of a morass. None of the estimators consistently out-performs the others across the conditions of the simulation. It’s only under certain conditions that any of the bias-correction methods are more accurate than using the regular FE estimator, and those conditions aren’t easy to identify in a real data analysis because they depend on the degree of publication bias.&lt;/p&gt;
&lt;div id=&#34;caveats&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Caveats&lt;/h3&gt;
&lt;p&gt;These findings are also pretty tentative because of the limitations of the simulation conditions examined here. The distribution of sample sizes seems to affect the relative accuracy of the estimators to a certain degree, but I’ve only looked at a limited set of possibilities, and also limited consideration to rather large meta-samples of 100 studies.&lt;/p&gt;
&lt;p&gt;Another caveat is that the simulations are based on &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; estimates from a two-group, between-subjects design with no covariates. In many applications, there is considerably more diversity in study designs. A given meta-analysis might include two-group, post-test only designs as well as between-subjects designs with a pre-test covariate or with repeated measures, as well as two-group designs with multiple (or multi-dimensional) outcomes. All of this introduces further layers of complexity into the relationship between sample size, effect magnitude, and selective publication.&lt;/p&gt;
&lt;p&gt;A further, quite important caveat is that selective publication is not the only possible explanation for a correlation between effect size and sample sizes. &lt;a href=&#34;http://datacolada.org/58&#34;&gt;In another recent post&lt;/a&gt;, Uri sketches a scenario where investigators choose sample size to achieve adequate power (so following best practice!) for predicted effect sizes. If 1) true effects are heterogeneous and 2) investigators’ predictions are correlated with true effect sizes, then a meta-analysis will have effect size estimates that are correlated with sample size even in the absence of publication bias. A &lt;a href=&#34;http://bayesfactor.blogspot.com/2016/01/asymmetric-funnel-plots-without.html&#34;&gt;blog post by Richard Morey&lt;/a&gt; illustrates another possibility that leads to effect-sample size correlation, in which resource constraints induce negative correlation between sample size and the reliability of the outcome measure.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;hold-me-hostage&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Hold me hostage&lt;/h3&gt;
&lt;p&gt;It seems to me that one lesson we can draw from this is that these regression-based corrections are pretty meager as analytic methods. We need to understand the &lt;em&gt;mechanism&lt;/em&gt; of selective publication in order to be able to correct for its consequences, but the regression-based corrections don’t provide direct information here (even though their performance depends on it!). I think this speaks to the need for methods that directly model the mechanism, which means turning to selection models and studying the distribution of p-values. Also, without bringing in other pieces of information (like p-values), it seems more or less impossible to tease apart selective publication from other possible explanations for effect-sample size correlation.&lt;/p&gt;
&lt;p&gt;If I had to pick one of the regression-based bias-correction method to use in an application—as in, if you handcuffed me to my laptop and threatened to not let me go until I analyzed your effect sizes—then on the basis of these simulation exercises, I think I would probably go with SPEESE as a default, and perhaps also report PEESE, but I wouldn’t bother with any of the others. Even though SPEESE is less accurate than PEESE and some other estimators under certain conditions, on a practical level it seems kind of silly to use different estimators when testing for publication bias versus trying to correct for it. And whatever advantage that regular PEESE has over SPEESE strikes me as kind of like cheating—it relies on an induced correlation between &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt; to gain an accuracy advantage under certain conditions, but that correlation causes big problems under other conditions.&lt;/p&gt;
&lt;p&gt;Even if you chained me to the laptop, I would also definitely include a caution that these estimators should be interpreted more as sensitivity analyses than as bias-corrected estimates of the overall mean effect. This is roughly in line with the conclusions of &lt;a href=&#34;http://dx.doi.org/10.2139/ssrn.2659409&#34;&gt;Inzlicht, Gervais, and Berkman (2015)&lt;/a&gt;. From their abstract:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Our simulations revealed that not one of the bias-correction techniques revealed itself superior in all conditions, with corrections performing adequately in some situations but inadequately in others. Such a result implies that meta-analysts ought to present a range of possible effect sizes and to consider them all as being possible.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Their conclusion was in reference to PET, PEESE, and PET-PEESE. Unfortunately, the tweaks of SPET and SPEESE don’t clarify the situation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;outstanding-questions&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Outstanding questions&lt;/h3&gt;
&lt;p&gt;These exercises have left me wondering about a couple of things, which I’ll just mention briefly:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I haven’t calculated confidence interval coverage levels for these simulations. I should probably add that but need to move on at the moment.&lt;/li&gt;
&lt;li&gt;The ever-popular Trim-and-Fill procedure is based on the assumption that a funnel plot will be symmetric in the absence of publication bias. This assumption won’t hold if there’s correlation between &lt;span class=&#34;math inline&#34;&gt;\(d\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt;, and so it would be interesting to see if using a function of sample size (i.e., just the first term of &lt;span class=&#34;math inline&#34;&gt;\(V\)&lt;/span&gt;) could improve the performance of Trim-and-Fill.&lt;/li&gt;
&lt;li&gt;Under the model examined here, the bias in PET, PEESE, SPET, and SPEESE comes from the fact that the relevant regression relationships aren’t actually linear under selective publication. I do wonder whether using some more flexible sort of regression model (perhaps including a non-linear term) could reduce bias. The trick would be to find something that’s still constrained enough so that bias improvements aren’t swamped by increased variance.&lt;/li&gt;
&lt;li&gt;Many of the applications that I am familiar with involve syntheses where some studies contribute multiple effect size estimates, which might also be inter-correlated. Very little work has examined how regression corrections like PET-PEESE perform in such settings (the only study I know of is &lt;a href=&#34;http://www.economics-ejournal.org/economics/discussionpapers/2015-9&#34;&gt;Reed, 2015&lt;/a&gt;, which involves a specialized and I think rather unusual data-generating model). For that matter, I don’t know of any work that looks at other publication bias correction methods either. Or what selective publication even means in this setting. Somebody should really work on that.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>New working paper: Procedural sensitivities of SCD effect sizes</title>
      <link>https://www.jepusto.com/scd-effect-size-sensitivities/</link>
      <pubDate>Mon, 17 Oct 2016 00:00:00 +0000</pubDate>
      <guid>https://www.jepusto.com/scd-effect-size-sensitivities/</guid>
      <description>


&lt;p&gt;I’ve just posted a new version of my working paper, &lt;em&gt;Procedural sensitivities of effect sizes for single-case designs with behavioral outcome measures&lt;/em&gt;. The abstract is below. This version is a major update of an &lt;a href=&#34;https://www.jepusto.com/files/Pustejovsky-2015-Nov-Non-overlap-measures.pdf&#34;&gt;earlier paper&lt;/a&gt; that focused only on the non-overlap measures. The new version also includes analysis of two other effect sizes (the within-case standardized mean difference and the log response ratio) as well as additional results and more succinct summaries of the main findings.&lt;/p&gt;
&lt;p&gt;The paper itself is available on the Open Science Framework (&lt;a href=&#34;https://osf.io/pxn24/&#34;&gt;here&lt;/a&gt;), as are the &lt;a href=&#34;https://osf.io/hkzsm/&#34;&gt;supplementary materials&lt;/a&gt; and &lt;a href=&#34;https://osf.io/j4gvt/&#34;&gt;Source code&lt;/a&gt;. I also created interaction versions of the graphics from the main paper and the supplementary materials, which can be viewed in &lt;a href=&#34;https://jepusto.shinyapps.io/SCD-effect-size-sensitivities/&#34;&gt;this shiny app&lt;/a&gt;. I would welcome any comments, questions, or feedback that readers may have.&lt;/p&gt;
&lt;div id=&#34;abstract&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Abstract&lt;/h3&gt;
&lt;p&gt;A wide variety of effect size indices have been proposed for quantifying the magnitude of treatment effects in single-case designs. Commonly used measures include parametric indices such as the standardized mean difference, as well as non-overlap measures, such as the percentage of non-overlapping data, improvement rate difference, and non-overlap of all pairs. Currently, little is known about the properties of these indices when applied to behavioral data collected by systematic direct observation, even though systematic direct observation is the most common approach to outcome measurement in single-case research. This study uses computer simulation to investigate the properties of several single-case effect size measures when applied to systematic direct observation data. Results indicate that the magnitude of the non-overlap measures and of the standardized mean difference can be strongly influenced by procedural details of the study’s design, which is a significant limitation to using these indices as effect sizes for meta-analysis of single-case designs. A less widely used parametric index, the log-response ratio, has the advantage of being insensitive to sample size and observation session length, although its magnitude is influenced by the use of partial interval recording.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Simulation studies in R (Fall, 2016 version)</title>
      <link>https://www.jepusto.com/simulation-studies-in-r-2016/</link>
      <pubDate>Wed, 28 Sep 2016 00:00:00 +0000</pubDate>
      <guid>https://www.jepusto.com/simulation-studies-in-r-2016/</guid>
      <description>


&lt;p&gt;In today’s Quant Methods colloquium, I gave an introduction to the logic and purposes of Monte Carlo simulation studies, with examples written in R.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.jepusto.com/files/Simulations-in-R-2016.html&#34;&gt;Here are the slides&lt;/a&gt; from my presentation.&lt;/li&gt;
&lt;li&gt;You can find the code that generates the slides &lt;a href=&#34;https://gist.github.com/jepusto/bf6cdb6e393f54470ba4d016199c6eb8&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Here is my &lt;a href=&#34;https://www.jepusto.com/Designing-simulation-studies-using-R&#34;&gt;presentation on the same topic&lt;/a&gt; from a couple of years ago.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://varianceexplained.org/r/beta_binomial_baseball/&#34;&gt;David Robinson’s blog&lt;/a&gt; has a much more in-depth discussion of beta-binomial regression.&lt;/li&gt;
&lt;li&gt;The data I used is from &lt;a href=&#34;http://www.seanlahman.com/baseball-database.html&#34;&gt;Lahman’s baseball database&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>ARPobservation</title>
      <link>https://www.jepusto.com/software/arpobservation/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://www.jepusto.com/software/arpobservation/</guid>
      <description>&lt;p&gt;An R package for simulating different methods of recording data based on direct observation of behavior, where behavior is modeled by an alternating renewal process.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://cran.r-project.org/package=ARPobservation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Available on the Comprehensive R Archive Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.jepusto.com/getting-started-with-ARPobservation&#34;&gt;Installation instructions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/jepusto/ARPobservation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Source code on Github&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://jepusto.shinyapps.io/ARPsimulator/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ARPsimulator&lt;/a&gt;: An interactive web application for simulating systematic direct observation data based on the alternating renewal process model.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Update: parallel R on the TACC</title>
      <link>https://www.jepusto.com/parallel-r-on-tacc-update/</link>
      <pubDate>Tue, 08 Apr 2014 00:00:00 +0000</pubDate>
      <guid>https://www.jepusto.com/parallel-r-on-tacc-update/</guid>
      <description>


&lt;p&gt;I have learned from &lt;a href=&#34;https://www.tacc.utexas.edu/staff/yaakoub-el-khamra&#34;&gt;Mr. Yaakoub El Khamra&lt;/a&gt; that he and the good folks at TACC have made some modifications to TACC’s custom MPI implementation and R build in order to correct bugs in Rmpi and snow that were causing crashes. &lt;a href=&#34;https://www.jepusto.com/parallel-R-on-TACC&#34;&gt;My earlier post&lt;/a&gt; has been updated to reflect the modifications. The main changes are:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The version of MVAPICH2 has changed to 2.0b&lt;/li&gt;
&lt;li&gt;Changes to the Rmpi and snow packages necessitate using the latest version of R (Warm Puppy, 3.0.3). This version is available in the &lt;code&gt;Rstats&lt;/code&gt; module.&lt;/li&gt;
&lt;li&gt;For improved reproducibility, I modified the R code so that the simulation driver function uses a seed value.&lt;/li&gt;
&lt;li&gt;I had to switch from &lt;code&gt;maply&lt;/code&gt; to &lt;code&gt;mdply&lt;/code&gt; as a result of (3).&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Running R in parallel on the TACC</title>
      <link>https://www.jepusto.com/parallel-r-on-tacc/</link>
      <pubDate>Fri, 20 Dec 2013 00:00:00 +0000</pubDate>
      <guid>https://www.jepusto.com/parallel-r-on-tacc/</guid>
      <description>


&lt;p&gt;UPDATE (4/8/2014): I have learned from &lt;a href=&#34;https://www.tacc.utexas.edu/staff/yaakoub-el-khamra&#34;&gt;Mr. Yaakoub El Khamra&lt;/a&gt; that he and the good folks at TACC have made some modifications to TACC’s custom MPI implementation and R build in order to correct bugs in Rmpi and snow that were causing crashes. This post &lt;a href=&#34;https://www.jepusto.com/parallel-R-on-TACC-update&#34;&gt;has been updated&lt;/a&gt; to reflect the modifications.&lt;/p&gt;
&lt;p&gt;I’ve started to use the Texas Advanced Computing Cluster to run statistical simulations in R. It takes a little bit of time to get up and running, but once you do it is an amazing tool. To get started, you’ll need&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;An account on the &lt;a href=&#34;https://www.tacc.utexas.edu/&#34;&gt;TACC&lt;/a&gt; and an allocation of computing time.&lt;/li&gt;
&lt;li&gt;An ssh client like &lt;a href=&#34;http://www.chiark.greenend.org.uk/~sgtatham/putty/&#34;&gt;PUTTY&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Some R code that can be adapted to run in parallel.&lt;/li&gt;
&lt;li&gt;A SLURM script that tells the server (called Stampede) how to run the R.&lt;/li&gt;
&lt;/ol&gt;
&lt;div id=&#34;the-r-script&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The R script&lt;/h3&gt;
&lt;p&gt;I’ve been running my simulations using a combination of several packages that provide very high-level functionality for parallel computing, namely &lt;code&gt;foreach&lt;/code&gt;, &lt;code&gt;doSNOW&lt;/code&gt;, and the &lt;code&gt;maply&lt;/code&gt; function in &lt;code&gt;plyr&lt;/code&gt;. All of this runs on top of an &lt;code&gt;Rmpi&lt;/code&gt; implementation developed by the folks at TACC (&lt;a href=&#34;https://portal.tacc.utexas.edu/documents/13601/901835/Parallel_R_Final.pdf/&#34;&gt;more details here&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;In &lt;a href=&#34;https://www.jepusto.com/Designing-simulation-studies-using-R/&#34;&gt;an earlier post&lt;/a&gt;, I shared code for running a very simple simulation of the Behrens-Fisher problem. Here’s &lt;a href=&#34;https://gist.github.com/jepusto/8059893&#34;&gt;adapted code&lt;/a&gt; for running the same simulation on Stampede. The main difference is that there are a few extra lines of code to set up a cluster, seed a random number generator, and pass necessary objects (saved in &lt;code&gt;source_func&lt;/code&gt;) to the nodes of the cluster:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(Rmpi)
library(snow)
library(foreach)
library(iterators)
library(doSNOW)
library(plyr)

# set up parallel processing
cluster &amp;lt;- getMPIcluster()
registerDoSNOW(cluster)

# export source functions
clusterExport(cluster, source_func)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Once it is all set up, running the code is just a matter of turning on the parallel option in &lt;code&gt;mdply&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;BFresults &amp;lt;- mdply(parms, .fun = run_sim, .drop=FALSE, .parallel=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I fully admit that my method of passing source functions is rather kludgy. One alternative would be to save all of the source functions in a separate file (say, &lt;code&gt;source_functions.R&lt;/code&gt;), then &lt;code&gt;source&lt;/code&gt; the file at the beginning of the simulation script:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rm(list=ls())
source(&amp;quot;source_functions.R&amp;quot;)
print(source_func &amp;lt;- ls())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Another, more elegant alternative would be to put all of your source functions in a little package (say, &lt;code&gt;BehrensFisher&lt;/code&gt;), install the package, and then pass the package in the &lt;code&gt;maply&lt;/code&gt; call:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;BFresults &amp;lt;- mdply(parms, .fun = run_sim, .drop=FALSE, .parallel=TRUE, .paropts = list(.packages=&amp;quot;BehrensFisher&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Of course, developing a package involves a bit more work on the front end.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-slurm-script&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The SLURM script&lt;/h3&gt;
&lt;p&gt;Suppose that you’ve got your R code saved in a file called &lt;code&gt;Behrens_Fisher.R&lt;/code&gt;. Here’s an example of a SLURM script that runs the R script after configuring an Rmpi cluster:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;#!/bin/bash
#SBATCH -J Behrens          # Job name
#SBATCH -o Behrens.o%j      # Name of stdout output file (%j expands to jobId)
#SBATCH -e Behrens.o%j      # Name of stderr output file(%j expands to jobId)
#SBATCH -n 32               # Total number of mpi tasks requested
#SBATCH -p normal           # Submit to the &amp;#39;normal&amp;#39; or &amp;#39;development&amp;#39; queue
#SBATCH -t 0:20:00          # Run time (hh:mm:ss)
#SBATCH -A A-yourproject    # Allocation name to charge job against
#SBATCH --mail-user=you@email.address # specify email address for notifications
#SBATCH --mail-type=begin   # email when job begins
#SBATCH --mail-type=end     # email when job ends

# load R module
module load Rstats           

# call R code from RMPISNOW
ibrun RMPISNOW &amp;lt; Behrens_Fisher.R &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The file should be saved in a plain text file called something like &lt;code&gt;run_BF.slurm&lt;/code&gt;. The file has to use ANSI encoding and Unix-type end-of-line encoding; &lt;a href=&#34;http://notepad-plus-plus.org/&#34;&gt;Notepad++&lt;/a&gt; is a text editor that can create files in this format.&lt;/p&gt;
&lt;p&gt;Note that for full efficiency, the &lt;code&gt;-n&lt;/code&gt; option should be a multiple of 16 because their are 16 cores per compute node. Further details about SBATCH options can be found &lt;a href=&#34;https://portal.tacc.utexas.edu/user-guides/stampede#running-slurm-jobcontrol&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;running-on-stampede&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Running on Stampede&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://portal.tacc.utexas.edu/user-guides/stampede#access&#34;&gt;Follow these directions&lt;/a&gt; to log in to the Stampede server. Here’s the &lt;a href=&#34;https://portal.tacc.utexas.edu/user-guides/stampede&#34;&gt;User Guide&lt;/a&gt; for Stampede. The first thing you’ll need to do is ensure that you’ve got the proper version of MVAPICH loaded. To do that, type&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;module swap intel intel/14.0.1.106
module setdefault&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The second line sets this as the default, so you won’t need to do this step again.&lt;/p&gt;
&lt;p&gt;Second, you’ll need to install whatever R packages you’ll need to run your code. To do that, type the following at the &lt;code&gt;login4$&lt;/code&gt; prompt:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;login4$module load Rstats
login4$R&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This will start an interactive R session. From the R prompt, use &lt;code&gt;install.packages&lt;/code&gt; to download and install, e.g.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;plyr&amp;quot;,&amp;quot;reshape&amp;quot;,&amp;quot;doSNOW&amp;quot;,&amp;quot;foreach&amp;quot;,&amp;quot;iterators&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The packages will be installed in a local library. Now type &lt;code&gt;q()&lt;/code&gt; to quit R.&lt;/p&gt;
&lt;p&gt;Next, make a new directory for your project:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;login4$mkdir project_name
login4$cd project_name&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Upload your files to the directory (using &lt;a href=&#34;http://the.earth.li/~sgtatham/putty/0.63/htmldoc/Chapter6.html&#34;&gt;psftp&lt;/a&gt;, for instance). Check that your R script is properly configured by viewing it in Vim.&lt;/p&gt;
&lt;p&gt;Finally, submit your job by typing&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;login4$sbatch run_BF.slurm&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or whatever your SLURM script is called. To check the status of the submitted job, type &lt;code&gt;showq -u&lt;/code&gt; followed by your TACC user name (more details &lt;a href=&#34;https://portal.tacc.utexas.edu/user-guides/stampede#running-slurm-jobcontrol-squeue&#34;&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;further-thoughts&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Further thoughts&lt;/h3&gt;
&lt;p&gt;TACC accounts come with a limited number of computing hours, so you should be careful to write efficient code. Before you even start worrying about running on TACC, you should profile your code and try to find ways to speed up the computations. (Some simple improvements in my Behrens-Fisher code would make it run MUCH faster.) Once you’ve done what you can in terms of efficiency, you should do some small test runs on Stampede. For example, you could try running only a few iterations for each combination of factors, and/or running only some of the combinations rather than the full factorial design. Based on the run-time for these jobs, you’ll then be able to estimate how long the full code would take. If it’s acceptable (and within your allocation), then go ahead and &lt;code&gt;sbatch&lt;/code&gt; the full job. If it’s not, you might reconsider the number of factor levels in your design or the number of iterations you need. I might have more comments about those some other time.&lt;/p&gt;
&lt;p&gt;Comments? Suggestions? Corrections? Drop a comment.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Designing simulation studies using R</title>
      <link>https://www.jepusto.com/designing-simulation-studies-using-r/</link>
      <pubDate>Fri, 06 Dec 2013 00:00:00 +0000</pubDate>
      <guid>https://www.jepusto.com/designing-simulation-studies-using-r/</guid>
      <description>


&lt;p&gt;&lt;a href=&#34;https://www.jepusto.com/files/Designing-simulation-studies-using-R.pdf&#34;&gt;Here are the slides&lt;/a&gt; from my presentation at this afternoon’s Quant. Methods brown bag. I gave a very quick introduction to using R for conducting simulation studies. I hope it was enough to get people intrigued about the possibilities of using R in their own work.&lt;/p&gt;
&lt;p&gt;The second half of the presentation sketched out a quick-and-dirty simulation of the &lt;a href=&#34;http://en.wikipedia.org/wiki/Behrens%E2%80%93Fisher_problem&#34;&gt;Behrens-Fisher problem&lt;/a&gt;, or more specifically the coverage rates of 95% confidence intervals using Welch’s degrees of freedom approximation, given independent samples with unequal variances. Here is &lt;a href=&#34;https://gist.github.com/jepusto/7686463&#34;&gt;the complete code&lt;/a&gt;. As I mentioned in the talk, there’s lots of room for improvement. The main point that I was trying to illustrate is that simulations have five distinct pieces:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;a data generating model,&lt;/li&gt;
&lt;li&gt;an estimation procedure,&lt;/li&gt;
&lt;li&gt;performance criteria,&lt;/li&gt;
&lt;li&gt;an experimental design (parameter values and sample dimensions), and&lt;/li&gt;
&lt;li&gt;analysis and results.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;It is useful to write simulation code that reflects the structure, so that it is easy for you (or other people) to read, revise, extend, or re-run it. And then post it on your blog.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>To what extent does partial interval recording over-estimate prevalence?</title>
      <link>https://www.jepusto.com/pir-overestimates-prevalence/</link>
      <pubDate>Sat, 26 Oct 2013 00:00:00 +0000</pubDate>
      <guid>https://www.jepusto.com/pir-overestimates-prevalence/</guid>
      <description>


&lt;p&gt;It is well known that the partial interval recording procedure produces an over-estimate of the prevalence of a behavior. Here I will demonstrate how to use the ARPobservation package to study the extent of this bias. First though, I’ll need to define the terms prevalence and incidence and also take a detour through continuous duration recording.&lt;/p&gt;
&lt;div id=&#34;prevalence-and-incidence&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Prevalence and incidence&lt;/h2&gt;
&lt;p&gt;First off, what do I mean by prevalence? In an alternating renewal process, &lt;strong&gt;prevalence&lt;/strong&gt; is the long-run proportion of time that the behavior occurs. I’ll call prevalence &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; (“phi”). So far, I’ve described alternating renewal processes in terms of their average event duration (which I’ll call &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; or “mu”) and the average interim time (which I’ll call &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt; or “lambda”). Prevalence is related to these quantities mathematically as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \phi = \frac{\mu}{\mu + \lambda}. \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;So given &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;, we can figure out &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Another characteristic of behavior that can be determined by the average event duration and average interim time is &lt;strong&gt;incidence&lt;/strong&gt;, or the rate of event occurrence per unit of time. I’ll call incidence &lt;span class=&#34;math inline&#34;&gt;\(\zeta\)&lt;/span&gt; (“zeta”). In an alternating renewal process,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \zeta = \frac{1}{\mu + \lambda}. \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This makes intuitive sense, because &lt;span class=&#34;math inline&#34;&gt;\(\mu + \lambda\)&lt;/span&gt; is the average time in between the start of each event, so its inverse should be the average number of times that an event starts per unit of time. (Note that though this is quite intuitive, it’s also very difficult to prove mathematically.) Given &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\lambda\)&lt;/span&gt;, we can figure out &lt;span class=&#34;math inline&#34;&gt;\(\zeta\)&lt;/span&gt;. Conversely, if we know &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\zeta\)&lt;/span&gt;, we can solve for &lt;span class=&#34;math inline&#34;&gt;\(\mu = \phi / \zeta\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\lambda = (1 - \phi) / \zeta\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;continuous-duration-recording&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Continuous duration recording&lt;/h2&gt;
&lt;p&gt;It can be shown mathematically that, on average, data produced by continuous duration recording (CDR) will be equal to the prevalence of the behavior. In statistical parlance, CDR data produces an &lt;em&gt;unbiased&lt;/em&gt; estimate of prevalence. Since this is a mathematical fact, it’s a good idea to check that the software gives the same result (if it doesn’t, there must be something wrong with the code).&lt;/p&gt;
&lt;p&gt;In order to simulate behavior streams, the software needs values for the average event duration and average interim time. But I want to think in terms of prevalence and incidence, so I’ll first pick a value for incidence. Say that a new behavioral event starts once per minute on average, so incidence (in events per second) would be &lt;span class=&#34;math inline&#34;&gt;\(\zeta = 1 / 60\)&lt;/span&gt;. I’ll then vary prevalence across the range from zero to one. For each value of prevalence, I’ll generate 10 behavior streams (if you’d like to do more, go ahead!).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ARPobservation)
set.seed(8)
zeta &amp;lt;- 1 / 60
phi &amp;lt;- rep(seq(0.01, 0.99, 0.01), each = 10)

# Now solve for mu and lambda
mu &amp;lt;- phi / zeta
lambda &amp;lt;- (1 - phi) / zeta

iterations &amp;lt;- length(phi) # total number of behavior streams to generate&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Two last elements are needed before I can get to the simulating: I need to decide what distributions to use for event durations and interim times, and I need to decide how long the observation session should last. To keep things simple, for the time being I’ll use exponential distributions. I’ll also suppose that we observe for 10 min = 600 s, so that on average we should observe 10 events per session. Now I can simulate a bunch of behavior streams and apply the CDR procedure to them.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;BS &amp;lt;- r_behavior_stream(n = iterations, mu = mu, lambda = lambda, F_event = F_exp(), F_interim = F_exp(), stream_length = 600)
CDR &amp;lt;- continuous_duration_recording(BS)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To check that the CDR procedure is unbiased, I’ll plot the CDR data versus the true value of prevalence, and run a smoothing line through the cloud of data-points:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
qplot(x = phi, y = CDR, geom = &amp;quot;point&amp;quot;) + geom_smooth(method = &amp;quot;loess&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jepusto.com/post/PIR-overestimates-prevalence_files/figure-html/CDR_bias-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The blue line is nearly identical to the line &lt;code&gt;y = x&lt;/code&gt;, meaning that the average of CDR data is equal to prevalence. Good news–the software appears to be working correctly!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;partial-interval-recording&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Partial interval recording&lt;/h2&gt;
&lt;p&gt;Now to partial interval recording (PIR). There are two different ways to think about how PIR data over-estimates prevalence. The conventional statistical approach follows the same logic as above, comparing the average value of PIR data to the true value of prevalence, &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;. Using the same simulated data streams as above, with 15 s intervals and 5 s of rest time after each interval…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;PIR &amp;lt;- interval_recording(BS, interval_length = 20, rest_length = 5)

qplot(x = phi, y = PIR, geom = &amp;quot;point&amp;quot;, ylim = c(-0.02,1.02)) + 
  geom_smooth(method = &amp;quot;loess&amp;quot;, se = FALSE) + 
  geom_abline(intercept = 0, slope = 1, linetype = &amp;quot;dashed&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jepusto.com/post/PIR-overestimates-prevalence_files/figure-html/PIR_bias-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The blue line indicates the average value of PIR data across the simulations for a given value of prevalence. The dashed line indicates &lt;code&gt;y = x&lt;/code&gt;, so clearly PIR data over-estimates prevalence.&lt;/p&gt;
&lt;p&gt;Previous studies in the Applied Behavior Analysis literature have taken a slightly different approach to thinking about over-estimation. Rather than comparing PIR data to the prevalence parameter &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;, PIR data is instead compared to the &lt;em&gt;sample&lt;/em&gt; value of prevalence, which is equivalent to the CDR proportion. Following this logic, I apply the PIR and CDR procedures to the same simulated behavior streams, then plot PIR versus CDR.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;obs_data &amp;lt;- reported_observations(BS, data_types = c(&amp;quot;C&amp;quot;,&amp;quot;P&amp;quot;), interval_length = 20, rest_length = 5)

qplot(x = CDR, y = PIR, data = obs_data, geom = &amp;quot;point&amp;quot;, ylim = c(-0.02,1.02)) + 
  geom_smooth(method = &amp;quot;loess&amp;quot;, se = FALSE) + 
  geom_abline(intercept = 0, slope = 1, linetype = &amp;quot;dashed&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jepusto.com/post/PIR-overestimates-prevalence_files/figure-html/PIR_CDR-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The blue fitted line is slightly different than with the other approach, but the general conclusion is the same: PIR data over-estimates prevalence.&lt;/p&gt;
&lt;p&gt;But by how much? That’s actually a tricky question to answer, because the extent of the bias depends on a bunch of factors:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the true prevalence &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt;,&lt;/li&gt;
&lt;li&gt;the true incidence &lt;span class=&#34;math inline&#34;&gt;\(\zeta\)&lt;/span&gt;,&lt;/li&gt;
&lt;li&gt;the length of the intervals, and&lt;/li&gt;
&lt;li&gt;the distribution of interim times &lt;code&gt;F_lambda&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(Curiously enough, the bias doesn’t depend on the distribution of event durations &lt;code&gt;F_mu&lt;/code&gt;.)&lt;/p&gt;
&lt;div id=&#34;interval-length&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Interval length&lt;/h4&gt;
&lt;p&gt;To see that the bias depends on the length of intervals used, I’ll compare 15 s intervals with 5 s rest times versus 25 s intervals with 5 s rest times. For a session of length 600 s, the latter procedure will yield 20 intervals.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;PIR_25 &amp;lt;- interval_recording(BS, interval_length = 30, rest_length = 5)
obs_data &amp;lt;- cbind(obs_data, PIR_25)
qplot(x = CDR, y = PIR, data = obs_data, geom = &amp;quot;smooth&amp;quot;, method = &amp;quot;loess&amp;quot;, ylim = c(-0.02,1.02)) + 
  geom_smooth(aes(y = PIR_25), method = &amp;quot;loess&amp;quot;, se = FALSE, col = &amp;quot;red&amp;quot;) + 
  geom_abline(intercept = 0, slope = 1, linetype = &amp;quot;dashed&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula &amp;#39;y ~ x&amp;#39;
## `geom_smooth()` using formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jepusto.com/post/PIR-overestimates-prevalence_files/figure-html/PIR_length-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The red line indicates that the longer interval time leads to a larger degree of over-estimation. (For clarity, I’ve removed the points in the scatter-plot.)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;interim-time-distribution&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Interim time distribution&lt;/h4&gt;
&lt;p&gt;It isn’t terribly troubling that the bias of PIR data depends on the interval length, because the observer will generally know (and will hopefully report in any write-up of their experiment) the interval length that was used. Much more troubling is the fact that the bias depends on the &lt;em&gt;distribution&lt;/em&gt; of interim times, because this is something that the observer or analyst won’t usually have much information about. To see how this bias works, I’ll compare behavior streams generated using an exponential distribution for the interim times with thos generated using a gamma distribution with shape parameter 3 (this distribution is much less dispersed than the exponential).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;BS_exp &amp;lt;- r_behavior_stream(n = iterations, mu = mu, lambda = lambda, F_event = F_exp(), F_interim = F_exp(), stream_length = 600)
obs_exp &amp;lt;- reported_observations(BS_exp, data_types = c(&amp;quot;C&amp;quot;,&amp;quot;P&amp;quot;), interval_length = 20, rest_length = 5)
obs_exp$F_lambda &amp;lt;- &amp;quot;Exponential&amp;quot;

BS_gam &amp;lt;- r_behavior_stream(n = iterations, mu = mu, lambda = lambda, F_event = F_exp(), F_interim = F_gam(shape = 3), stream_length = 600)
obs_gam &amp;lt;- reported_observations(BS_gam, data_types = c(&amp;quot;C&amp;quot;,&amp;quot;P&amp;quot;), interval_length = 20, rest_length = 5)
obs_gam$F_lambda &amp;lt;- &amp;quot;Gamma(3)&amp;quot;

obs_data &amp;lt;- rbind(obs_exp, obs_gam)
qplot(x = C, y = P, color = F_lambda, 
      data = obs_data, geom = &amp;quot;smooth&amp;quot;, method = &amp;quot;loess&amp;quot;, se = FALSE, ylim = c(-0.02, 1.02))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `geom_smooth()` using formula &amp;#39;y ~ x&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://www.jepusto.com/post/PIR-overestimates-prevalence_files/figure-html/PIR_interim_dist-1.png&#34; width=&#34;768&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The gamma(3) interim time distribution leads to a slightly larger positive bias.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Getting started with ARPobservation</title>
      <link>https://www.jepusto.com/getting-started-with-arpobservation/</link>
      <pubDate>Thu, 24 Oct 2013 00:00:00 +0000</pubDate>
      <guid>https://www.jepusto.com/getting-started-with-arpobservation/</guid>
      <description>


&lt;p&gt;&lt;strong&gt;UPDATED 5/29/2014 after posting the package to CRAN&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Here are step-by-step instructions on how to download and install ARPobservation. For the time being, ARPobservation is available as a pre-compiled binary for Windows. For Mac/Linux, you’ll have to download the source from Github.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://cran.us.r-project.org/&#34;&gt;Download&lt;/a&gt; and install R. R is free, open-source software that is used by many data analysts and statisticians. ARPobservation is a contributed package that runs within R, so you’ll need to get the base software first.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;(Optional but recommended) &lt;a href=&#34;http://www.rstudio.com/&#34;&gt;Download&lt;/a&gt; and install RStudio, which is a very nice front-end interface to R.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Open R or RStudio and type the following sequence of commands in the console:&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;ARPobservation&amp;quot;)
library(ARPobservation)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You’ll only need to do the above once. Once you’ve got the package installed, type the following in order to access the package within an R session: &lt;code&gt;library(ARPobservation)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To open the package documentation, type &lt;code&gt;package?ARPobservation&lt;/code&gt;. To access the documentation for an individual function in this package, just type &lt;code&gt;?&lt;/code&gt; followed by the name of the function. For instance, one of the main functions in the package is called &lt;code&gt;r_behavior_stream&lt;/code&gt;; to access its documentation, type &lt;code&gt;?r_behavior_stream&lt;/code&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
