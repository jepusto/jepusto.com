---
title: Rejection sampler for Efron's double Poisson distribution
authors:
- admin
date: '2023-01-25'
codefolding_show: 'show'
slug: rejection-sampler-double-poisson
categories: []
draft: true
tags:
  - simulation
  - distribution-theory
header:
  caption: ''
  image: ''
---

$$
\def\Pr{{\text{Pr}}}
\def\E{{\text{E}}}
\def\Var{{\text{Var}}}
\def\Cov{{\text{Cov}}}
\def\bm{\mathbf}
\def\bs{\boldsymbol}
$$
For a project I am working on, we are using [Stan](https://mc-stan.org/) to fit generalized random effects location-scale models to a bunch of count data. We're interested in using the double-Poisson distribution, as described by [Efron (1986)](https://doi.org/10.2307/2289002), but it's turning out to be a bit tricky. The double-Poisson distribution is not implemented in Stan, so we've had to write our own distribution function. That's fine and not particularly difficult. What's more challenging is that also we need to also write a Stan function to generate random samples from the double-Poisson, so that we can generate posterior predictive checks.[^gamlls] In this post, I'll look at how to approach this problem using a rejection sampler. 

[^gamlls]: The incredible [`gamlss.dist` package](https://cran.r-project.org/package=gamlss.dist) does include a sampler for the double-Poisson distribution. It appears to be implemented using the standard inversion method, by applying the double-Poisson quantile function to a random sample from a uniform distribution.

To implement a rejection sampler, we need to find a distribution 1) from which it is easy to draw random samples and 2) that approximates the double-Poisson. Specifically, we need to find a density $g(x)$ and a constant $k$ that satisfies the following criterion, with $f(x)$ denoting the double-Poisson density:
$$
k \geq \sup_x \frac{f(x)}{g(x)},
$$
with $x$ ranging over the non-negative integers. Equivalently, we seek
$$
\ln k \geq \sup_x \ d(x), \quad \text{where} \quad d(x) = \ln f(x) - \ln g(x).
$$

[Efron (1986)](https://doi.org/10.2307/2289002) gives the following expression for the density of the double-Poisson distribution with mean $\mu$ and inverse-disperson $\theta$:
$$
f(x | \mu, \theta) = \frac{\theta^{1/2} e^{-\theta \mu}}{c(\mu,\theta)} \left(\frac{e^{-x} x^x}{x!}\right) \left(\frac{e \mu}{x}\right)^{\theta x},
$$
where $c(\mu,\theta)$ is a scaling constant to ensure that the density sums to one, which is closely approximated by 
$$
c(\mu, \theta) \approx 1 + \frac{1 - \theta}{12 \mu \theta}\left(1 + \frac{1}{\mu \theta}\right).
$$
Ignoring the scaling constant, we have
$$
\ln f(x | \mu, \theta) = \frac{1}{2} \ln \theta - \theta \mu - (1 - \theta - \theta \ln \mu) x + (1 - \theta) x \ln(x) - \ln \left(x!\right),
$$
where $x \ln (x)$ is evaluated as 0.

Finding an approximating distribution is tricky because the double-Poisson admits both over-dispersion (where $\Var(X) > \E(X)$) and under-dispersion (where $\Var(X) < \E(X)$). In contrast, many other readily available distributions such as the negative binomial only allow for over-dispersion relative to the Poisson. 

# Over-dispersion 

Let us consider using a negative binomial distribution for $g(x)$, for the case where the double-Poisson is over-dispersed with $\theta < 1$. The negative binomial density is
$$
g(x | p, r) = {{x + r - 1}\choose{x}} (1 - p)^x p^r
$$
with log density
$$
\ln g(x | p, r) = \ln \left((x + r - 1)!\right) - \ln \left((r - 1)!\right) - \ln \left(x!\right) + x \ln(1 - p) + r \ln (p).
$$
The difference function is thus
$$
\begin{aligned}
d(x) &= \ln f(x | \mu, \theta) - \ln g(x | p, r) \\
&= \frac{1}{2} \ln \theta - \theta \mu - r \ln p \\
& \qquad + \left[\theta \ln \mu - (1 - \theta) - \ln (1 - p)\right] x \\
& \qquad \qquad + (1 - \theta) x \ln(x) - \left[\ln \left((x + r - 1)!\right) - \ln \left((r - 1)!\right)\right] \\
& = (1 - \theta) x \ln(x) + b x - \ln \Gamma(x + r - 1) + c
\end{aligned}
$$
where $b = \theta \ln \mu - (1 - \theta) - \ln (1 - p)$ and $c = \frac{1}{2} \ln \theta - \theta \mu - r \ln p + \ln \Gamma(r - 1)$. The first derivative of $d(x)$ with respect to $x$ is then
$$
d'(x) = (1 - \theta) \ln x - \psi_0(x + r - 1) + \theta \ln \mu - \ln(1 - p),
$$
where $\psi_0()$ is the [digamma function](https://en.wikipedia.org/wiki/Digamma_function). 

Let's assume that we want $d'(\mu) = 0$. This implies that 
$$
\ln(1 - p) = \ln \mu - \psi_0(\mu + r - 1)
$$
and thus
$$
\ln p = \ln\left[1 - \mu \exp \psi_0(\mu + r - 1)\right]
$$
We can then write 
$$
\begin{aligned}
d(\mu) &= (1 - \theta) \mu \ln \mu + b \mu - \ln \Gamma(\mu + r - 1) + c \\
&= (1 - \theta) \mu \ln \mu + \theta \mu \ln \mu - \mu (1 - \theta) - \mu\ln (1 - p) - \ln \Gamma(\mu + r - 1) + \frac{1}{2} \ln \theta - \theta \mu - r \ln p + \ln \Gamma(r - 1) \\
&= \mu \ln \mu - \mu - \mu \ln(1 - p) - \ln \Gamma(\mu + r - 1) + \frac{1}{2} \ln \theta - r \ln p + \ln \Gamma(r - 1) \\
&= \mu \psi_0(\mu + r - 1) - r \ln\left[1 - \mu \exp \psi_0(\mu + r - 1)\right] - \ln \Gamma(\mu + r - 1) + \ln \Gamma(r - 1)  - \mu + \frac{1}{2} \ln \theta.
\end{aligned}
$$
Now, we would like to find a value of $r$ so that the approximating distribution is as similar as possible to $f(x)$. One way to approach this is to minimize $d(\mu)$ with respect to $r$. Let's therefore look at the derivative
$$
\frac{\partial}{\partial r} d(\mu) = \mu \psi_1(\mu + r - 1)\left(\frac{1 - (\mu - r)\exp \psi_0(\mu + r - 1)}{1 - \mu \exp \psi_0(\mu + r - 1)}\right) - \ln\left[1 - \mu \exp \psi_0(\mu + r - 1)\right] - \psi_0(\mu + r - 1) + \psi_0(r - 1).
$$