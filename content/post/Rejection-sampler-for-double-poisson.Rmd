---
title: Rejection sampler for Efron's double Poisson distribution
authors:
- admin
date: '2023-01-25'
codefolding_show: 'show'
slug: rejection-sampler-double-poisson
categories: []
draft: true
tags:
  - simulation
  - distribution-theory
header:
  caption: ''
  image: ''
---

$$
\def\Pr{{\text{Pr}}}
\def\E{{\text{E}}}
\def\Var{{\text{Var}}}
\def\Cov{{\text{Cov}}}
\def\bm{\mathbf}
\def\bs{\boldsymbol}
$$
For a project I am working on, we are using [Stan](https://mc-stan.org/) to fit generalized random effects location-scale models to a bunch of count data. We're interested in using the double-Poisson distribution, as described by [Efron (1986)](https://doi.org/10.2307/2289002), but it's turning out to be a bit tricky. The double-Poisson distribution is not implemented in Stan, so we've had to write our own distribution function. That's fine and not particularly difficult. What's more challenging is that also we need to also write a Stan function to generate random samples from the double-Poisson, so that we can generate posterior predictive checks.[^gamlls] In this post, I'll look at how to approach this problem using a rejection sampler. 

[^gamlls]: The incredible [`gamlss.dist` package](https://cran.r-project.org/package=gamlss.dist) does include a sampler for the double-Poisson distribution. It appears to be implemented using the standard inversion method, by applying the double-Poisson quantile function to a random sample from a uniform distribution.

To implement a rejection sampler, we need to find a distribution 1) from which it is easy to draw random samples and 2) that approximates the double-Poisson. Specifically, we need to find a density $g(x)$ and a constant $M$ that satisfies the following criterion, with $f(x)$ denoting the double-Poisson density:
$$
M \geq \sup_x \frac{f(x)}{g(x)},
$$
with $x$ ranging over the non-negative integers. Equivalently, we seek
$$
\ln M \geq \sup_x \ d(x), \quad \text{where} \quad d(x) = \ln f(x) - \ln g(x).
$$

[Efron (1986)](https://doi.org/10.2307/2289002) gives the following expression for the density of the double-Poisson distribution with mean $\mu$ and inverse-disperson $\theta$:
$$
f(x | \mu, \theta) = \frac{\theta^{1/2} e^{-\theta \mu}}{c(\mu,\theta)} \left(\frac{e^{-x} x^x}{x!}\right) \left(\frac{e \mu}{x}\right)^{\theta x},
$$
where $c(\mu,\theta)$ is a scaling constant to ensure that the density sums to one, which is closely approximated by 
$$
c(\mu, \theta) \approx 1 + \frac{1 - \theta}{12 \mu \theta}\left(1 + \frac{1}{\mu \theta}\right).
$$
We then have
$$
\ln f(x | \mu, \theta) = \frac{1}{2} \ln \theta - \theta \mu - c(\mu, \theta) - (1 - \theta - \theta \ln \mu) x + (1 - \theta) x \ln(x) - \ln \left(x!\right),
$$
where $x \ln (x)$ is evaluated as 0.

Finding an approximating distribution is tricky because the double-Poisson admits both over-dispersion (where $\Var(X) > \E(X)$) and under-dispersion (where $\Var(X) < \E(X)$). In contrast, many other readily available distributions such as the negative binomial only allow for over-dispersion relative to the Poisson. 

# Over-dispersion 

Let us consider using a negative binomial distribution for $g(x)$, for the case where the double-Poisson is over-dispersed with $\theta < 1$. The negative binomial density is
$$
g(x | p, r) = {{x + r - 1}\choose{x}} (1 - p)^x p^r
$$
with log density
$$
\ln g(x | p, r) = \ln \left((x + r - 1)!\right) - \ln \left((r - 1)!\right) - \ln \left(x!\right) + x \ln(1 - p) + r \ln (p).
$$
The difference function is thus
$$
\begin{aligned}
d(x) &= \ln f(x | \mu, \theta) - \ln g(x | p, r) \\
&= \frac{1}{2} \ln \theta - \theta \mu - r \ln p \\
& \qquad + \left[\theta \ln \mu - (1 - \theta) - \ln (1 - p)\right] x \\
& \qquad \qquad + (1 - \theta) x \ln(x) - \left[\ln \left((x + r - 1)!\right) - \ln \left((r - 1)!\right)\right] \\
& = (1 - \theta) x \ln(x) \\
& \qquad + \left[\theta \ln \mu - (1 - \theta) - \ln (1 - p)\right] x \\
& \qquad \qquad - \ln \Gamma(x + r - 1) \\
& \qquad \qquad \qquad + \frac{1}{2} \ln \theta - \theta \mu - r \ln p + \ln \Gamma(r - 1).
\end{aligned}
$$
The first derivative of $d(x)$ with respect to $p$ is then
$$
\frac{\partial}{\partial p}d(x) = \frac{x}{1 - p} - \frac{r}{p},
$$
which implies that $d(x)$ is minimized when $p = \frac{r}{r + x}$. Thus, we can write $d(x)$ in terms of $\mu$, $\theta$, and $r$ as
$$
\begin{aligned}
d(x) &= -\theta x \ln(x) + \left[\theta \ln \mu + \theta - 1\right] x \\
& \qquad + (x + r) \ln(x + r) - \ln \Gamma(x + r - 1) \\
& \qquad \qquad + \frac{1}{2} \ln \theta - \theta \mu - r \ln r + \ln \Gamma(r - 1).
\end{aligned}
$$
Now, it can be shown that 
$$
\begin{aligned}
\ln \Gamma(x + r - 1) &\geq (x + r) \ln (x + r) - (x + r - 1) - \ln(x + r) - \ln(x + r - 1) \\
&\geq (x + r) \ln (x + r) - (x + r - 1) - 2\ln(x + r) 
\end{aligned}
$$
by which it follows that 
$$
\begin{aligned}
d(x) \leq \tilde{d}(x) &= -\theta x \ln(x) + \left[\theta \ln \mu + \theta - 1\right] x \\
& \qquad + (x + r) \ln(x + r) -  (x + r) \ln (x + r) + (x + r - 1) + 2 \ln(x + r) \\
& \qquad \qquad + \frac{1}{2} \ln \theta - \theta \mu - r \ln r + \ln \Gamma(r - 1) \\
&= \theta x -\theta x \left[\ln(x) - \ln \mu\right] + 2\ln(x + r) \\
& \qquad \qquad + r - 1 + \frac{1}{2} \ln \theta - \theta \mu - r \ln r + \ln \Gamma(r - 1).
\end{aligned}
$$
The derivative of $\tilde{d}(x)$ with respect to $x$ is
$$
\tilde{d}'(x) = \frac{2}{x + r} - \theta \ln\left(\frac{x}{\mu}\right).
$$
Let $x^*$ denote the maximizing value of $\tilde{d}(x)$, so that $\tilde{d}'(x^*) = 0$. It follows that
$$
\frac{2}{x^* + r} = \theta \ln\left(\frac{x^*}{\mu}\right).
$$
Therefore, $x^* \in \left(\mu, \mu \exp\left(\frac{2}{\theta(\mu + r)}\right)\right)$.

Substituting into $\tilde{d}(x^*)$ and re-arranging a bit gives
$$
\begin{aligned}
\tilde{d}(x^*) &= x^* \theta \ln \mu -\theta x^* \left[\ln(x^*) - 1\right] + 2 \ln(x^* + r) \\
& \qquad \qquad + \frac{1}{2} \ln \theta - \theta \mu - 1 - r \left(\ln r - 1\right) + \ln \Gamma(r - 1).
\end{aligned}
$$
We seek a value of $r$ that minimizes the discrepancy at $x^*$. Taking the derivative with respect to $r$ (and treating $x^*$ as an implicit function of $r$),
$$
\begin{aligned}
\frac{\partial}{\partial r} \tilde{d}(x^*) &= - \theta \ln\left(\frac{x^*}{\mu}\right)\left(\frac{\partial x^*}{\partial r}\right) + \frac{2}{x^* + r}\left(\frac{\partial x^*}{\partial r} + 1\right) + \psi(r - 1) - \ln(r) \\
&= \left[\frac{2}{x^* + r} - \theta \ln\left(\frac{x^*}{\mu}\right)\right] \frac{\partial x^*}{\partial r} + \frac{2}{x^* + r} + \psi(r - 1) - \ln(r) \\
&= \frac{2}{x^* + r} + \psi(r - 1) - \ln(r)
\end{aligned}
$$

where $\psi()$ is the [digamma function](https://en.wikipedia.org/wiki/Digamma_function). Thus, take $r*$ and $x^*$ to be the solution to $\frac{2}{x^* + r} + \psi(r^* - 1) - \ln(r^*) = 0$ and set $p^* = \frac{x^*}{x^* + r^*}$. Set $M = \exp \tilde{d}(x^*)$. Since 
$$
\ln M = \tilde{d}(x^*) \geq \sup_x d(x),
$$
we can use $M$ and g(x; p^*, r^*)$ for rejection sampling.

Here is R code to calculate $r^*$, $x^*$, and $p^*$:
```{r}
library(ggplot2)

x_star <- function(r, mu, theta) {
  upper <- mu * exp(2 / (theta * (mu + r)))
  x_root <- uniroot(\(x) 2 / (x + r) - theta * log(x / mu), lower = mu, upper = upper)
  x_root$root
}

d_tilde <- function(x, r, mu, theta) {
  theta * (log(mu) + 1) * x - theta * x * log(x) + 2 * log(x + r) + 
    log(theta) / 2 - theta * mu - r * log(r) + r - 1 + lgamma(r - 1)
}

d_tilde_min <- function(x, mu, theta) {
  if (length(x) > 1L) {
    res <- sapply(x, d_tilde_min, mu = mu, theta = theta)
    return(res)
  }
  lower <- uniroot(\(r) 2 / (mu + r) + digamma(r - 1) - log(r), interval = c(1 + 1e-10, 200))
  upper <- uniroot(\(r) 2 / r + digamma(r - 1) - log(r), interval = c(1 + 1e-10, 200))
  r <- uniroot(\(r) 2 / (x_star(r, mu, theta) + r) + digamma(r - 1) - log(r), 
               interval = c(lower$root, upper$root))
  x_min <- x_star(x, mu, theta)
  d_tilde(x = x_min, r = x, mu = mu, theta = theta)
}


ggplot() +
  xlim(1.00001, 30) +
  ylim(0, 4) + 
  geom_function(fun = d_tilde_min, 
                args = list(mu = 5, theta = 0.9), 
                color = "red")

```

