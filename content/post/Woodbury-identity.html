---
title: The Woodbury identity
subtitle: A life-hack for analyzing hierarchical models
authors:
- admin
date: '2020-12-04'
draft: true
slug: Woodbury-identity
categories: []
tags:
  - hierarchical models
  - matrix algebra
header:
  caption: ''
  image: ''
---



<p>As in many parts of life, statistics is full of little bits of knowledge that are extremely handy if you know them, but which hardly anybody ever bothers to mention. You would think, if something is so useful, perhaps your professors would spend a fair bit of time explaining it to you. But maybe the stuff seems trivial, obvious, or simple to them, so they don’t bother.</p>
<p>One example of this is Excel keyboard shortcuts. In a previous life, I was an Excel jockey so I learned all the keyboard shortcuts, such as how to move the cursor to the last cell in a continuous block of entries (<code>ctrl</code> + an arrow key). Whenever I do this while sharing a screen in a meeting, someone is invariably amazed and wants to know what dark sorcery I’m conjuring. It’s a simple, handy trick—especially if you’re working with a really large dataset with a lot of rows! But it’s also something that there’s no reason to expect anyone to figure out on their own, and that no stats or quant methods professor is going to spend class time demonstrating.</p>
<p>Let me explain another, slightly more involved example, involving one of my favorite pieces of matrix algebra. There’s a thing called the Woodbury identity, also known as the Sherman-Morrison-Woodbury identity, that is a little life hack for inverting certain types of matrices. It has a <a href="https://en.wikipedia.org/wiki/Woodbury_matrix_identity">Wikipedia page</a>, which I have visited many times. It is a very handy bit of math, if you happen to be a statistics student working with hierarchical models (such as meta-analytic models). I’ll give a statement of the identity, then explain a bit about the connection to hierarchical models.</p>
<div id="the-woodbury-identity" class="section level1">
<h1>The Woodbury identity</h1>
<p>Say that you’ve got four matrices, an <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(\mathbf{A}\)</span>, a <span class="math inline">\(k \times k\)</span> matrix <span class="math inline">\(\mathbf{C}\)</span>, an <span class="math inline">\(n \times k\)</span> matrix <span class="math inline">\(\mathbf{U}\)</span>, and a $k <span class="math inline">\(n\)</span> matrix <span class="math inline">\(\mathbf{V}\)</span>. Assume that <span class="math inline">\(\mathbf{A}\)</span> and <span class="math inline">\(\mathbf{C}\)</span> are invertible. The Woodbury identity tells you how to get the inverse of a certain combination of these matrices:
<span class="math display">\[
\left(\mathbf{A} + \mathbf{U} \mathbf{C} \mathbf{V}\right)^{-1} = \mathbf{A}^{-1} - \mathbf{A}^{-1} \mathbf{U} \left(\mathbf{C}^{-1} + \mathbf{V} \mathbf{A}^{-1} \mathbf{U} \right)^{-1} \mathbf{V} \mathbf{A}^{-1}.
\]</span>
Admit it, you’re impressed. “Dude! Mind. Blown.” you’re probably saying to yourself right now. Okay, or maybe you’re still a touch skeptical that this little formula is worth knowing. Let me explain the connection to hierarchical models.</p>
</div>
<div id="hierarchical-models" class="section level1">
<h1>Hierarchical models</h1>
<p>Hierarchical linear models are a mainstay of statistical analysis in many, many areas of application, including education research, where we often deal with data collected on individuals (students, teachers) nested within larger aggregate units (like schools). In meta-analysis, these models come up if we’re dealing with samples that have more than relevant outcome, so that we have multiple effect size estimates nested within a given sample or study.
Suppose we have a hierarchical structure with <span class="math inline">\(J\)</span> clusters, where cluster <span class="math inline">\(j\)</span> has <span class="math inline">\(n_j\)</span> individual observations. A quite general way of expressing a hierarchical model for such a data structure is
<span class="math display">\[
\mathbf{Y}_j = \mathbf{X}_j \boldsymbol\beta + \mathbf{Z}_j \boldsymbol\eta_j + \boldsymbol\epsilon_j,
\]</span>
for <span class="math inline">\(j = 1,...,J\)</span>, where, for cluster <span class="math inline">\(j\)</span>:</p>
<ul>
<li><span class="math inline">\(\mathbf{Y}_j\)</span> is an <span class="math inline">\(n_j \times 1\)</span> vector of outcomes,</li>
<li><span class="math inline">\(\mathbf{X}_j\)</span> is an <span class="math inline">\(n_j \times p\)</span> design matrix for the fixed effects,</li>
<li><span class="math inline">\(\boldsymbol\beta\)</span> is a <span class="math inline">\(p \times 1\)</span> vector of fixed effect coefficients,</li>
<li><span class="math inline">\(\mathbf{Z}_j\)</span> is an <span class="math inline">\(n_j \times q\)</span> design matrix for the random effects,</li>
<li><span class="math inline">\(\boldsymbol\eta_j\)</span> is a <span class="math inline">\(q \times 1\)</span> vector of random effects, and</li>
<li><span class="math inline">\(\boldsymbol\epsilon_j\)</span> is an <span class="math inline">\(n_j \times 1\)</span> vector of level-1 errors.</li>
</ul>
<p>In this model, we assume that the random effects have mean zero and unknown variance-covariance matrix <span class="math inline">\(\mathbf{T}\)</span>, which often assumed to be an unstructured, symmetric and invertible matrix; we assume that the level-1 errors are also mean zero with variance-covariance matrix <span class="math inline">\(\boldsymbol\Sigma_j\)</span>; and we assume that <span class="math inline">\(\boldsymbol\eta_j\)</span> is independent of <span class="math inline">\(\boldsymbol\epsilon_j\)</span>. In many instances, we might assume that the entries of <span class="math inline">\(\mathbf{e}_j\)</span> are all independent, so <span class="math inline">\(\boldsymbol\Sigma\)</span> will be a multiple of an identity matrix, <span class="math inline">\(\boldsymbol\Sigma_j = \sigma^2 \mathbf{I}_j\)</span>. In other instances (such as models for longitudinal data), <span class="math inline">\(\boldsymbol\Sigma\)</span> might be a patterned matrix that includes off-diagonal terms, such as an auto-regressive structure.</p>
<p>What is the marginal variance of <span class="math inline">\(\mathbf{Y}_j | \mathbf{X}_j\)</span> in this model? In other words, if we combine the variance due to the random effects and the variance of the level-1 errors, what do we get? We get
<span class="math display">\[
\text{Var}\left(\mathbf{Y}_j | \mathbf{X}_j \right) = \mathbf{V}_j = \mathbf{Z}_j \mathbf{T} \mathbf{Z}_j&#39; + \boldsymbol\Sigma_j,
\]</span>
a matrix that, if you reverse the terms, looks like
<span class="math display">\[
\mathbf{V}_j = \boldsymbol\Sigma_j + \mathbf{Z}_j \mathbf{T} \mathbf{Z}_j&#39;
\]</span>
a simple form of the combination of matrices in the left-hand side of the Woodbury identity. The identity tells you how to invert this matrix.</p>
<p>Why would we care about inverting this variance-covariance matrix? One good reason is that the fixed effect coefficients in the hierarchical model are estimated by weighted least squares, where the weight matrices are the inverse of an estimate of <span class="math inline">\(\mathbf{V}_j\)</span>. Thus, to understand how the weights in a hierarchical model work, it’s quite useful to be able to invert <span class="math inline">\(\mathbf{V}_j\)</span>. Another good (related) reason is that the sampling variance of the fixed effect estimates is approximately
<span class="math display">\[
\text{Var}(\boldsymbol{\hat\beta}) \approx \left(\sum_{j=1}^J \mathbf{X}_j&#39;\mathbf{V}_j^{-1} \mathbf{X}_j \right)^{-1}
\]</span>
(it would be exact if we knew the parameters of <span class="math inline">\(\mathbf{V}_j\)</span> with certainty). So if we want to understand the precision of <span class="math inline">\(\boldsymbol{\hat\beta}\)</span> or the power of hypothesis involving <span class="math inline">\(\boldsymbol{\hat\beta}\)</span>, then it’s again handy to be able to invert <span class="math inline">\(\mathbf{V}_j\)</span>.</p>
<p>Directly applying the identity, we get
<span class="math display">\[
\mathbf{V}_j^{-1} = \boldsymbol\Sigma_j^{-1} - \boldsymbol\Sigma_j^{-1} \mathbf{Z}_j \left(\mathbf{T}^{-1} + \mathbf{Z}_j&#39;\boldsymbol\Sigma_j^{-1}\mathbf{Z}_j \right)^{-1} \mathbf{Z}_j&#39; \boldsymbol\Sigma_j^{-1}
\]</span>
This expression looks like a bit of a mess, I’ll admit, but it can be useful. Things simplify quite a bit of <span class="math inline">\(\boldsymbol\Sigma_j^{-1}\)</span> has a form that is easy to invert (like a multiple of an identity matrix) and if the dimension of the random effects <span class="math inline">\(q\)</span> is small. Under these conditions, <span class="math inline">\(\boldsymbol\Sigma_j^{-1}\)</span> is easy to work with, <span class="math inline">\(\mathbf{T}^{-1}\)</span> is manageable because it has small dimensions, and <span class="math inline">\(\mathbf{Z}_j&#39;\boldsymbol\Sigma_j^{-1}\mathbf{Z}_j\)</span> becomes manageable because it also has small dimensions (<span class="math inline">\(q \times q\)</span>, in both cases).</p>
<div id="random-intercepts" class="section level2">
<h2>Random intercepts</h2>
<p>As an example, consider a very simple model that includes only random intercepts, so <span class="math inline">\(\mathbf{Z}_j = \mathbf{1}_j\)</span>, an <span class="math inline">\(n_j \times 1\)</span> vector with every entry equal to 1, and <span class="math inline">\(\mathbf{T}\)</span> is simply <span class="math inline">\(\tau^2\)</span>, variance of the random intercepts. For simplicity, let’s also assume that the level-1 errors are independent, so <span class="math inline">\(\boldsymbol\Sigma_j = \sigma^2 \mathbf{I}_j\)</span> and <span class="math inline">\(\boldsymbol\Sigma_j^{-1} = \sigma^{-2} \mathbf{I}_j\)</span>. Applying the Woodbury identity,
<span class="math display">\[
\begin{aligned}
\mathbf{V}_j^{-1} &amp;= \boldsymbol\Sigma_j^{-1} - \boldsymbol\Sigma_j^{-1} \mathbf{1}_j \left(\mathbf{T}^{-1} + \mathbf{1}_j&#39;\boldsymbol\Sigma_j^{-1}\mathbf{1}_j \right)^{-1} \mathbf{1}_j&#39; \boldsymbol\Sigma_j^{-1} \\
&amp;= \sigma^{-2} \mathbf{I}_j - \sigma^{-4} \mathbf{1}_j \left(\tau^{-2} + \sigma^{-2} \mathbf{1}_j&#39;\mathbf{1}_j \right)^{-1} \mathbf{1}_j&#39; \\
&amp;= \sigma^{-2} \mathbf{I}_j - \sigma^{-4} \left(\tau^{-2} + \sigma^{-2} n_j \right)^{-1} \mathbf{1}_j \mathbf{1}_j&#39; \\
&amp;= \sigma^{-2} \left(\mathbf{I}_j - \frac{\tau^2} {\sigma^2 + n_j \tau^2} \mathbf{1}_j \mathbf{1}_j&#39;\right).
\end{aligned}
\]</span>
Try checking this for yourself by carrying through the matrix algebra for <span class="math inline">\(\mathbf{V}_j \mathbf{V}_j^{-1}\)</span>.</p>
<p>Now suppose that the design matrix was also quite simple, consisting of just an intercept term <span class="math inline">\(\mathbf{X}_j = \mathbf{1}_j\)</span>, so that <span class="math inline">\(\boldsymbol\beta = \beta\)</span> is simply a population mean. How precise is the estimate of the population mean from this hierarchical model? Well, the sampling variance of the estimator <span class="math inline">\(\hat\beta\)</span> is approximately
<span class="math display">\[
\begin{aligned}
\text{Var}(\hat\beta) &amp;\approx \left(\sum_{j=1}^J \mathbf{1}_j&#39;\mathbf{V}_j^{-1} \mathbf{1}_j \right)^{-1} \\
&amp;= \left(\sigma^{-2}\sum_{j=1}^J \mathbf{1}_j&#39; \left(\mathbf{I}_j - \frac{\tau^2} {\sigma^2 + n_j \tau^2} \mathbf{1}_j \mathbf{1}_j&#39;\right) \mathbf{1}_j \right)^{-1} \\
&amp;= \left(\sigma^{-2} \sum_{j=1}^J n_j \left(1 - \frac{n_j \tau^2} {\sigma^2 + n_j \tau^2} \right)  \right)^{-1} \\ 
&amp;= \left( \sigma^{-2} \sum_{j=1}^J \frac{n_j \sigma^2} {\sigma^2 + n_j \tau^2} \right)^{-1} \\ 
&amp;= \left(\sum_{j=1}^J \frac{n_j} {\sigma^2 + n_j \tau^2} \right)^{-1} \\
&amp;= \left(\sigma^2 + \tau^2\right) \left(\sum_{j=1}^J \frac{n_j} {1 + (n_j - 1) \rho} \right)^{-1},
\end{aligned}
\]</span>
where <span class="math inline">\(\rho = \tau^2 / (\tau^2 + \sigma^2)\)</span> is the intra-class correlation. Squint at this expression for a bit and you can see how the ICC influences the varince. If <span class="math inline">\(\rho\)</span> is near zero, then the sampling variance will be close to <span class="math inline">\(\left(\sigma^2 + \tau^2\right) / N\)</span>, which is what you would get if you treated every observation as independent. If <span class="math inline">\(\rho\)</span> is near 1, then the sampling variance ends up being nearly <span class="math inline">\(\left(\sigma^2 + \tau^2\right) / J\)</span>, which is what you would get if you treated every cluster as a single observation. For intermediate ICCs, the sample size from cluster <span class="math inline">\(j\)</span> (in the numerator of the fraction inside the summation) is getting cut down to size accordingly.</p>
<p>The estimator of the population mean is a weighted average of the outcomes. Specifically,
<span class="math display">\[
\hat\beta = \left(\sum_{j=1}^J \mathbf{1}_j&#39;\mathbf{\hat{V}}_j^{-1} \mathbf{1}_j \right)^{-1} \sum_{j=1}^J \mathbf{1}_j&#39;\mathbf{\hat{V}}_j^{-1} \mathbf{Y}_j,
\]</span>
where <span class="math inline">\(\mathbf{\hat{V}}_j\)</span> is an estimator of <span class="math inline">\(\mathbf{V}_j\)</span>. If you carry through the matrix algebra, you’ll find that
<span class="math display">\[
\begin{aligned}
\hat\beta &amp;= \left(\sum_{j=1}^J \frac{n_j} {\sigma^2 + n_j \tau^2} \right)^{-1} \sum_{j=1}^J \frac{\mathbf{1}_j&#39;\mathbf{Y}_j}{\sigma^2 + n_j \tau^2} \\
&amp;= \frac{1}{W} \sum_{j=1}^J \sum_{i=1}^{n_j} w_j y_{ij},
\end{aligned}
\]</span>
where <span class="math inline">\(w_j = \frac{1}{1 + (n_j - 1) \rho}\)</span> and <span class="math inline">\(\displaystyle{W = \sum_{j=1}^J n_j w_j}\)</span>. From this, we can see that the weight of a given observation depends on the ICC and the size of the cluster. If the ICC is low, then weights will be close to 1. For higher ICCs, observations in larger clusters get proportionately <em>less</em> weight than observations in smaller clusters.</p>
</div>
<div id="a-meta-analysis-example" class="section level2">
<h2>A meta-analysis example</h2>
<p>In <a href="/weighting-in-multivariate-meta-analysis/">previous post</a> on multi-variate meta-analysis, I examined how the weights</p>
</div>
</div>
