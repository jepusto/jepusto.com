---
title: Approximating the distribution of cluster-robust Wald statistics
authors:
  - admin
summary: ""
date: '2024-01-20'
draft: true
codefolding_show: 'hide'
slug: cluster-robust-Wald-statistics
categories: []
tags:
  - robust variance estimation
  - distribution theory
header:
  caption: ''
  image: ''
---



<p><span class="math display">\[
\def\Pr{{\text{Pr}}}
\def\E{{\text{E}}}
\def\Var{{\text{Var}}}
\def\Cov{{\text{Cov}}}
\def\cor{{\text{cor}}}
\def\bm{\mathbf}
\def\bs{\boldsymbol}
\]</span></p>
<p>In <a href="http://doi.org/10.3102/1076998615606099">Tipton and Pustejovsky (2015)</a>, we examined several different small-sample approximations for cluster-robust Wald test statistics, which are like <span class="math inline">\(F\)</span> statistics but based on cluster-robust variance estimators. These statistics are, frankly, kind of weird and awkward to work with, and the approximations that we examined were far from perfect. In this post, I will look in detail at the robust Wald statistic for a one-way ANOVA problem with clusters of dependent observations.</p>
<p>Consider a setup where clusters can be classified into one of <span class="math inline">\(C\)</span> categories, with each cluster of observations falling into a single category. Let <span class="math inline">\(\bs\mu = \left[\mu_c \right]_{c=1}^C\)</span> denote the means of these categories. Suppose we have an estimator of those means <span class="math inline">\(\bs{\hat\mu} = \left[\hat\mu_c\right]_{c=1}^C\)</span> and a corresponding cluster-robust variance estimator <span class="math inline">\(\bm{V}^R = \bigoplus_{c=1}^C V^R_c\)</span>. Note that <span class="math inline">\(\bm{V}^R\)</span> is diagonal because the estimators for each category are independent.
Assume that the robust variance estimator is unbiased so <span class="math inline">\(\E\left(V^R_c\right) = \Var\left( \hat\mu_c \right) = \psi_c\)</span> for <span class="math inline">\(c = 1,...,C\)</span>.
Let <span class="math inline">\(\bs\Psi = \bigoplus_{c=1}^C \psi_c\)</span>.</p>
<p>Suppose that we want to test the null hypothesis that that means of the categories are all equal, <span class="math inline">\(H_0: \mu_1 = \mu_2 = \cdots = \mu_C\)</span>. We can express this null using a <span class="math inline">\(q \times C\)</span> contrast matrix <span class="math inline">\(\bm{C} = \left[-\bm{1}_q \ \bm{I}_q \right]\)</span>, where <span class="math inline">\(q = C - 1\)</span>. The null hypothesis is then <span class="math inline">\(\bm{C} \bs\mu = \bm{0}_q\)</span>. The corresponding cluster-robust Wald statistic is
<span class="math display">\[
Q = \bs{\hat\mu}&#39; \bm{C}&#39; \left(\bm{C} \bm{V}^R \bm{C}&#39;\right)^{-1} \bm{C} \bs{\hat\mu}.
\]</span>
Under the null hypothesis, the distribution of <span class="math inline">\(Q\)</span> will converge to a <span class="math inline">\(\chi^2_q\)</span> as the number of clusters in each category grows large. However, with a limited number of clusters in some of the categories, this approximate reference distribution is not very accurate and tests based on it can have wildly inflated type I error rates.</p>
<p>In the paper, we considered several different ways of approximating the distribution of <span class="math inline">\(Q\)</span> that work at smaller sample sizes.
Several of the small-sample approximations that we considered are based on representing the test statistic as
<span class="math display">\[
Q = \bm{z}&#39; \bm{D}^{-1} \bm{z},
\]</span>
where <span class="math inline">\(\bs\Omega = \bm{C} \bs\Psi \bm{C}&#39;\)</span>, <span class="math inline">\(\bm{z} = \bs\Omega^{-1/2}\bm{C}\hat\mu_c\)</span>, <span class="math inline">\(\bm{G} = \bs\Omega^{-1/2} \bm{C}\)</span>, and
<span class="math display">\[
\bm{D} = \bm{G} \bm{V}^R \bm{G}&#39;.
\]</span>
To make sense of the approximations, I will look at the form of <span class="math inline">\(\bm{D}\)</span>.</p>
<p>One class of approaches to approximating the sampling distribution of <span class="math inline">\(Q\)</span> is to use a Hotelling’s <span class="math inline">\(T^2\)</span> distribution with degrees of freedom <span class="math inline">\(\eta\)</span>. Given the degrees of freedom, Hotelling’s <span class="math inline">\(T^2\)</span> is a multiple of an <span class="math inline">\(F\)</span> distribution:
<span class="math display">\[
\frac{\eta - q + 1}{\eta q} Q \sim F(q, \eta - q + 1).
\]</span>
The question is then how to determine <span class="math inline">\(\eta\)</span>.</p>
<p>We considered several different approximations for <span class="math inline">\(\eta\)</span>, including one proposed by Zhang (<a href="https://doi.org/10.1016/j.jspi.2011.07.023">2012</a>, <a href="https://doi.org/10.14419/ijasp.v1i2.908">2013</a>). These degrees of freedom are given by
<span class="math display">\[
\eta_Z = \frac{q(q + 1)}{\sum_{s=1}^q \sum_{t = 1}^q \Var(d_{st})},
\]</span>
where <span class="math inline">\(d_{st}\)</span> is the entry in row <span class="math inline">\(s\)</span>, column <span class="math inline">\(t\)</span> of <span class="math inline">\(\bm{D}\)</span>.</p>
<p>Before going further, it’s useful to observe that <span class="math inline">\(\bm{D}\)</span> is invariant to linear transformations of <span class="math inline">\(\bm{C}\)</span>. In particular, an equivalent way to write the null hypothesis is as <span class="math inline">\(H_0: \bs\Psi_{\circ}^{-1/2} \bm{C} = \bm{0}_q\)</span>, where <span class="math inline">\(\bs\Psi_{\circ} = \bigoplus_{c=2}^C \psi_c\)</span> is the diagonal of the true sampling variances of categories 2 through <span class="math inline">\(C\)</span>, omitting the first category. Thus, let me redefine
<span class="math display">\[
\bs\Omega = \bs\Psi_{\circ}^{-1/2} \bm{C} \bs\Psi \bm{C}&#39;\bs\Psi_{\circ}^{-1/2},
\]</span>
<span class="math inline">\(\bm{z} = \bs\Omega^{-1/2}\bs\Psi_{\circ}^{-1/2}\bm{C}\hat\mu_c\)</span>, and
<span class="math display">\[
\bm{G} = \bs\Omega^{-1/2} \bs\Psi_{\circ}^{-1/2} \bm{C}.
\]</span>
This transformation of the constraint matrix will make it possible to find a closed-form expression for <span class="math inline">\(\bs\Omega^{-1/2}\)</span>.</p>
<p>Now, observe that
<span class="math display">\[
\begin{aligned}
\bs\Omega &amp;= \bs\Psi_{\circ}^{-1/2} \bm{C} \bs\Psi \bm{C}&#39;\bs\Psi_{\circ}^{-1/2} \\
&amp;= \bs\Psi_{\circ}^{-1/2} \left(\bs\Psi_{\circ} + \psi_1 \bm{1}_q \bm{1}_q&#39;\right)\bs\Psi_{\circ}^{-1/2} \\
&amp;= \bm{I}_q + \psi_1 \bm{f} \bm{f}&#39;,
\end{aligned}
\]</span>
where <span class="math inline">\(\bm{f} = \bs\Psi_{\circ}^{-1/2} \bm{1}_q = \left[ \psi_c^{-1/2}\right]_{c = 2}^C\)</span>. From the <a href="\bs\Psi_%7B\circ%7D%5E%7B-1/2%7D">Woodbury identity</a>,
<span class="math display">\[
\bs\Omega^{-1} = \bm{I} - \frac{1}{W} \bm{f} \bm{f}&#39;,
\]</span>
where <span class="math inline">\(W = \sum_{c=1}^C \frac{1}{\psi_c}\)</span>.</p>
<p><a href="https://doi.org/10.1137/22M1471559">Fasi, Higham, and Liu (2023)</a> provide formulas for <span class="math inline">\(p^{th}\)</span> roots of low-rank updates to scaled identity matrices. Their results provide a neat closed-form expression for <span class="math inline">\(\bs\Omega^{-1/2}\)</span>. From their Equation (1.9),
<span class="math display">\[
\bs\Omega^{-1/2} = \mathbf{I}_q - \kappa \ \bm{f} \bm{f}&#39;,
\]</span>
where <span class="math inline">\(\kappa = \frac{\sqrt{\psi_1}}{W \sqrt{\psi_1} + \sqrt{W}}\)</span>.
Further, we can write the <span class="math inline">\(q \times C\)</span> matrix <span class="math inline">\(\bm{G}\)</span> as
<span class="math display">\[
\begin{aligned}
\bm{G} &amp;= \bs\Omega^{-1/2} \bs\Psi_{\circ}^{-1/2} \bm{C} \\
&amp;= \left( \mathbf{I}_q - \kappa \ \bm{f} \bm{f}&#39; \right) \bs\Psi_{\circ}^{-1/2} \left[-\bm{1}_q, \ \bm{I}_q \right] \\
&amp;= \left[\frac{\kappa(W \psi_1 - 1) - \psi_1}{\psi_1} \bm{f},  \left( \mathbf{I}_q - \kappa \ \bm{f} \bm{f}&#39; \right) \bs\Psi_{\circ}^{-1/2}\right],
\end{aligned}
\]</span>
with entries given by
<span class="math display">\[
g_{sc} = \begin{cases}
\frac{\kappa(W \psi_1 - 1) - \psi_1}{\psi_1 \sqrt{\psi_{s+1}}} &amp; \text{if} \quad c = 1 \\
\frac{I(s+1 = c)}{\sqrt{\psi_{c}}} - \frac{\kappa}{\psi_c \sqrt{\psi_{s+1}}} &amp; \text{if} \quad c &gt; 1.
\end{cases}
\]</span>
Because <span class="math inline">\(\bm{D} = \bm{G} \bm{V}^R \bm{G}&#39;\)</span> and <span class="math inline">\(\bm{V}^R\)</span> is diagonal, we can write the entries of <span class="math inline">\(\bm{D}\)</span> as
<span class="math display">\[
d_{st} = \sum_{c=1}^C g_{sc} g_{tc} V^R_c.
\]</span>
And because the variance estimators for each category are independent,
<span class="math display">\[
\Var(d_{st}) = \sum_{c=1}^C g_{sc}^2 g_{tc}^2 \Var(V^R_c).
\]</span>
In <a href="/publication/power-approximations-for-dependent-effects/">prior work</a>, we derived expressions for the Satterthwaite degrees of freedom for variances of average effect sizes, and the same formulas can be applied here with the category-specific <span class="math inline">\(V^R_c\)</span>. Let me write <span class="math inline">\(\nu_c = 2\left[\E(V^R_c)\right]^2 / \Var(V^R_c)\)</span> for the degrees of freedom corresponding to category <span class="math inline">\(c\)</span>. Then
<span class="math display">\[
\Var(d_{st}) = 2 \sum_{c=1}^C g_{sc}^2 g_{tc}^2 \frac{\psi_c^2}{\nu_c}.
\]</span>
We can use this to obtain an expression for Zhang’s approximate degrees of freedom:
<span class="math display">\[
\begin{aligned}
q(q + 1)\eta_Z^{-1} &amp;= \sum_{s=1}^q \sum_{t = 1}^q \Var(d_{st}) \\
&amp;= 2\sum_{s=1}^q \sum_{t = 1}^q \sum_{c=1}^C g_{sc}^2 g_{tc}^2 \frac{\psi_c^2}{\nu_c} \\
&amp;= 2\sum_{c=1}^C \frac{\psi_c^2}{\nu_c} \left(\sum_{s=1}^q g_{sc}^2\right)^2.
\end{aligned}
\]</span>
Now, all we need to do is simplify…
<span class="math display">\[
\begin{aligned}
\sum_{s=1}^q g_{s1}^2 &amp;= \sum_{s=1}^q \frac{\left(\kappa(W \psi_1 - 1) - \psi_1\right)^2}{\psi_1^2 \psi_{s+1}} \\
&amp;= \frac{\left(\kappa(W \psi_1 - 1) - \psi_1\right)^2}{\psi_1^2} \sum_{c=2}^C \frac{1}{\psi_{s+1}} \\
&amp;= \frac{\left(\kappa(W \psi_1 - 1) - \psi_1\right)^2}{\psi_1^2} \frac{(W \psi_1 - 1)}{\psi_1} \\
&amp;= \text{...a bunch of tedious algebra...} \\
&amp;= \frac{1}{\psi_1^2} \left(\psi_1 - \frac{1}{W}\right)
\end{aligned}
\]</span>
and, for <span class="math inline">\(c = 2,...,C\)</span>,
<span class="math display">\[
\begin{aligned}
\sum_{s=1}^q g_{sc}^2 &amp;= \sum_{s=1}^q \left(\frac{I(s+1 = c)}{\sqrt{\psi_{c}}} - \frac{\kappa}{\psi_c \sqrt{\psi_{s+1}}}\right)^2 \\
&amp;= \frac{1}{\psi_c} - \frac{2 \kappa}{\psi_c^2} + \frac{\kappa^2}{\psi_c^2}\sum_{s=1}^q \frac{1}{\psi_{s+1}} \\
&amp;= \frac{1}{\psi_c} - \frac{2 \kappa}{\psi_c^2} + \frac{\kappa^2}{\psi_c^2}\frac{(W \psi_1 - 1)}{\psi_1} \\
&amp;= \text{...a bunch of tedious algebra...} \\
&amp;= \frac{1}{\psi_c^2} \left(\psi_c - \frac{1}{W}\right)
\end{aligned}
\]</span>
Thus,
<span class="math display">\[
\begin{aligned}
q(q + 1)\eta_Z^{-1} &amp;= 2\sum_{c=1}^C \frac{\psi_c^2}{\nu_c} \left(\sum_{s=1}^q g_{sc}^2\right)^2 \\
&amp;= 2\sum_{c=1}^C \frac{1}{\nu_c \psi_c^2}\left(\psi_c - \frac{1}{W}\right)^2 \\
&amp;= 2\sum_{c=1}^C \frac{1}{\nu_c}\left(1 - \frac{1}{\psi_c W}\right)^2
\end{aligned}
\]</span>
or, rearranging,
<span class="math display">\[
\eta_Z = \frac{q(q + 1)}{2 \sum_{c=1}^C \frac{1}{\nu_c}\left(1 - \frac{1}{\psi_c W}\right)^2}.
\]</span>
It’s a surprisingly clean formula!
Once these degrees of freedom are calculated, the degrees of freedom for the reference <span class="math inline">\(F\)</span> distribution would be <span class="math inline">\(q\)</span> and <span class="math inline">\(\eta_Z - q + 1\)</span>.</p>
<p>In the paper, we also considered two other degrees of freedom approximations, which involve not only the variances of <span class="math inline">\(d_{st}\)</span> but also the covariances between entries. In principle, one could follow similar algebra to get expressions for these other degrees of freedom as well. However, our simulations indicated that the other degrees of freedom approximations tend to be overly conservative and produce type-I error rates way below the nominal level (essentially, hardly ever rejecting the null) and less accurate than HTZ. So, there’s not much reason to work through them unless you find algebra enjoyable for its own sake.</p>
<div id="numerical-validation" class="section level1">
<h1>Numerical validation</h1>
<pre class="r"><code>library(tidyverse)
library(metadat)
library(metafor)
library(clubSandwich)

dat.tannersmith2016$ESid &lt;- 1:nrow(dat.tannersmith2016)
rho &lt;- 0.7
Vmat &lt;- with(dat.tannersmith2016, impute_covariance_matrix(vi = vi, cluster = studyid, r = rho, smooth_vi = TRUE))

TS_fit &lt;- rma.mv(yi ~ 0 + sexmix, V = Vmat, 
                 random = ~ 1 | studyid / ESid,
                 data = dat.tannersmith2016,
                 method = &quot;REML&quot;, sparse = TRUE)
TS_cat &lt;- conf_int(TS_fit, vcov = &quot;CR2&quot;)

Cmat &lt;- constrain_equal(&quot;^sexmix&quot;, reg_ex = TRUE, 
                        coefs = coef(TS_fit))
Cmat_pivot &lt;- vcov(TS_fit)[-1,-1] %*% Cmat

Wald_cat &lt;- Wald_test(TS_fit, constraints = Cmat, vcov = &quot;CR2&quot;)
Wald_test(TS_fit, constraints = Cmat_pivot, vcov = &quot;CR2&quot;)</code></pre>
<pre><code>##  test  Fstat df_num df_denom p_val sig
##   HTZ 0.0804      2     7.63 0.924</code></pre>
<pre class="r"><code># calculate category-specific degrees of freedom
tau_sq &lt;- TS_fit$sigma2[1]
omega_sq &lt;- TS_fit$sigma2[2]

cat_calcs &lt;- 
  dat.tannersmith2016 %&gt;%
  group_by(sexmix, studyid) %&gt;%
  summarise(
    ybar = mean(yi),
    kj = n(),
    sigma2j = mean(vi),
    .groups = &quot;drop_last&quot;
  ) %&gt;%
  mutate(
    wj = kj/ (kj * tau_sq + (kj - 1) * rho * sigma2j + omega_sq + sigma2j)
  ) %&gt;%
  summarise(
    ybar_c = weighted.mean(ybar, wj),
    W_c = sum(wj),
    E_VR = 1 / W_c,
    nu_VR = 1 / (sum(wj^2 / (W_c - wj)^2) - (2 / W_c) * sum(wj^3 / (W_c - wj)^2) + (1 / W_c^2) * sum(wj^2 / (W_c - wj))^2)
  )

df_Z &lt;- 
  cat_calcs %&gt;%
  summarize(
    q = n() - 1,
    num = q * (q + 1),
    den = 2 * sum((1 - 1 / (E_VR * sum(W_c)))^2 / nu_VR),
    df_Z = num / den
  ) %&gt;%
  pull(df_Z)

mu_hat &lt;- coef(TS_fit)
VR &lt;- vcovCR(TS_fit, type = &quot;CR2&quot;) %&gt;% as.matrix()
q &lt;- nrow(Cmat)
Q &lt;- as.numeric(t(Cmat %*% mu_hat) %*% solve(Cmat %*% VR %*% t(Cmat)) %*% (Cmat %*% mu_hat))
delta &lt;- (df_Z - q + 1) / (df_Z)
Fstat &lt;- delta * Q / q
df_num &lt;- q
df_den &lt;- df_Z - q + 1
all.equal(df_num, Wald_cat$df_num)</code></pre>
<pre><code>## [1] TRUE</code></pre>
<pre class="r"><code>all.equal(df_den, Wald_cat$df_denom)</code></pre>
<pre><code>## [1] TRUE</code></pre>
<pre class="r"><code>all.equal(delta, Wald_cat$delta)</code></pre>
<pre><code>## [1] TRUE</code></pre>
<pre class="r"><code>all.equal(Fstat, Wald_cat$Fstat)</code></pre>
<pre><code>## [1] TRUE</code></pre>
</div>
