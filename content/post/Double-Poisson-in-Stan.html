---
title: Implementing Efron's double Poisson distribution in Stan
authors:
- admin
date: '2023-09-12'
codefolding_show: 'show'
slug: double-poisson-in-Stan
categories: []
draft: true
tags:
  - simulation
  - distribution-theory
header:
  caption: ''
  image: ''
---



<p><span class="math display">\[
\def\Pr{{\text{Pr}}}
\def\E{{\text{E}}}
\def\Var{{\text{Var}}}
\def\Cov{{\text{Cov}}}
\def\bm{\mathbf}
\def\bs{\boldsymbol}
\]</span>
For a project I am working on, we are using <a href="https://mc-stan.org/">Stan</a> to fit generalized random effects location-scale models to a bunch of count data. We’re interested in using the double-Poisson distribution, as described by <a href="https://doi.org/10.2307/2289002">Efron (1986)</a>, but it’s turning out to be a bit tricky. The double-Poisson distribution is not implemented in Stan, so we’ve had to write our own distribution function. That’s fine and not particularly difficult. What’s more challenging is that also we need to also write a Stan function to generate random samples from the double-Poisson, so that we can generate posterior predictive checks. In this post, I’ll walk through the implementation of the custom distribution functions needed to use the double-Poisson in Stan.</p>
<p>The incredible <a href="https://cran.r-project.org/package=gamlss.dist"><code>gamlss.dist</code> package</a> provides a full set of distributional functions for the double-Poisson distribution, including a sampler. Thus, I can validate my Stan functions against the functions from <code>gamlss.dist</code>.</p>
<pre class="r"><code>library(tidyverse)
library(gamlss.dist) # DPO distribution functions
library(rstan)       # Stan interface to R
library(brms)        # fitting generalized linear models
library(bayesplot)   # Examine fitted models
library(loo)         # Model fit measures
rstan_options(auto_write = TRUE)</code></pre>
<div id="the-double-poisson" class="section level2">
<h2>The double-Poisson</h2>
<p>The double-Poisson distribution is a discrete distribution for non-negative counts, with support <span class="math inline">\(\mathcal{S}_X = \{0, 1, 2, 3, ...\}\)</span>.
It is an interesting distribution because it admits for both over- and under-dispersion relative to the Poisson distribution, whereas most of the conventional alternatives such as the negative binomial distribution or Poisson-normal mixture distribution allow only for over-dispersion.
The mean-variance relationship of the double-Poisson is approximately constant; for <span class="math inline">\(X \sim DPO(\mu, \theta)\)</span>, <span class="math inline">\(\text{E}(X) \approx \mu\)</span> and <span class="math inline">\(\text{Var}(X) \approx \mu / \theta\)</span>, so that the double-Poisson distribution approximately satisfies the assumptions of a quasi-Poisson generalized linear model (although not quite exactly so).</p>
<p><a href="https://doi.org/10.2307/2289002">Efron (1986)</a> gives the following expression for the density of the double-Poisson distribution with mean <span class="math inline">\(\mu\)</span> and inverse-disperson <span class="math inline">\(\theta\)</span>:
<span class="math display">\[
f(x | \mu, \theta) = \frac{\theta^{1/2} e^{-\theta \mu}}{c(\mu,\theta)} \left(\frac{e^{-x} x^x}{x!}\right) \left(\frac{e \mu}{x}\right)^{\theta x},
\]</span>
where <span class="math inline">\(c(\mu,\theta)\)</span> is a scaling constant to ensure that the density sums to one, which is closely approximated by
<span class="math display">\[
c(\mu, \theta) \approx 1 + \frac{1 - \theta}{12 \mu \theta}\left(1 + \frac{1}{\mu \theta}\right).
\]</span>
We then have
<span class="math display">\[
\ln f(x | \mu, \theta) = \frac{1}{2} \ln \theta - \theta \mu - \ln c(\mu, \theta) + x (\theta + \theta \ln \mu - 1) + (1 - \theta) x \ln(x) - \ln \left(x!\right),
\]</span>
where <span class="math inline">\(x \ln (x)\)</span> is evaluated as 0.</p>
</div>
<div id="log-of-the-probability-mass-function" class="section level1">
<h1>Log of the probability mass function</h1>
<p>For purposes of using this distribution in Stan, it’s sufficient to provide the log of the probability mass function up to a constant—there’s no need to normalize it to sum to one. Thus, we can ignore the <span class="math inline">\(c(\mu, \theta)\)</span> term above. Here’s a Stan function implementing the lpmf:</p>
<pre class="r"><code>stancode_lpmf &lt;- &quot;
functions {
  real dpo_lpmf(int X, real mu, real theta) {
    real ans;
    real A = inv(2) * log(theta) - theta * mu;
    if (X == 0)
      ans = A;
    else
      ans = A + X * (theta * (1 + log(mu)) - 1) - lgamma(X + 1) + (1 - theta) * X * log(X);
    return ans;
  }
}
&quot;</code></pre>
<p>To check that this is accurate, I’ll compare the Stan function to the corresponding function from <code>gamlss.dist</code> for a couple of different parameter values and for <span class="math inline">\(x = 0,...,100\)</span>. If my function is accurate, the calculated log-probabilities should differ by a constant value for each set of parameters.</p>
<pre class="r"><code>writeLines(stancode_lpmf, &quot;DPO-lpmf.stan&quot;)
expose_stan_functions(&quot;DPO-lpmf.stan&quot;)

test_lpmf &lt;- 
  expand_grid(
    mu = c(2, 5, 10, 20),
    theta = c(0.1, 0.2, 0.5, 1, 2, 5, 10),
    X = 0:100
  ) %&gt;%
  mutate(
    gamlss_lpmf = dDPO(x = X, mu = mu, sigma = 1 / theta, log = TRUE),
    my_lpmf = pmap_dbl(.l = list(X = X, mu = mu, theta = theta), .f = dpo_lpmf),
    diff = my_lpmf - gamlss_lpmf
  )</code></pre>
<pre class="r"><code>ggplot(test_lpmf, aes(factor(theta), diff, color = factor(theta))) + 
  geom_boxplot() + 
  facet_wrap( ~ mu, labeller = &quot;label_both&quot;, ncol = 2) + 
  theme_minimal() + 
  labs(x = &quot;theta&quot;) + 
  theme(legend.position = &quot;none&quot;)</code></pre>
<p><img src="/post/Double-Poisson-in-Stan_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>Checks out. Onward!</p>
</div>
<div id="cumulative-distribution-function" class="section level1">
<h1>Cumulative distribution function</h1>
<p>I’ll next implement a function to evaluate the cumulative distriution function over a range of values. This is an expensive calculation, but it can be improved a little bit by noting the relationship between sequential values of the probability mass function. Letting <span class="math inline">\(d = \exp \left(\theta + \theta \ln \mu - 1 \right)\)</span>, observe that
<span class="math display">\[
\begin{aligned}
f(0 | \mu, \theta) &amp;= \frac{\theta^{1/2} e^{-\theta \mu}}{c(\mu,\theta)} \\
f(1 | \mu, \theta) &amp;= f(0 | \mu, \theta) \times d \\
f(x | \mu, \theta) &amp;= f(x - 1 | \mu, \theta) \times d \times \frac{\exp\left[(1 - \theta)(x - 1)\left(\ln(x) - \ln(x - 1)\right) \right]}{x^\theta}
\end{aligned}
\]</span>
where the last expression holds for <span class="math inline">\(x \geq 2\)</span>.</p>
<p>The function below computes the un-normalized cumulative distribution function (i.e., without the scaling constant <span class="math inline">\(c(\mu, \theta)\)</span>) over the range <span class="math inline">\(x = 0,...,m\)</span> as follows:</p>
<ul>
<li>Compute <span class="math inline">\(f(x | \mu, \theta)\)</span> for <span class="math inline">\(x = 0,1,2,...\)</span>, without the scaling constant <span class="math inline">\(c(\mu, \theta)\)</span></li>
<li>Take <span class="math inline">\(F(0 | \mu, \theta) = f(0 | \mu, \theta)\)</span> and accumulating <span class="math inline">\(F(x | \mu, \theta) = F(x - 1 | \mu, \theta) + f(x | \mu, \theta)\)</span> for <span class="math inline">\(x = 0,...,m\)</span>.</li>
<li>Checking if <span class="math inline">\(f(x | \mu, \theta) / F(x | \mu, \theta)\)</span> is small (less than <span class="math inline">\(10^{-8}\)</span>), in which case accumulation stops at the value <span class="math inline">\(n\)</span>.</li>
<li>The normalized cumulative distribution function will then be <span class="math inline">\(F(x | \mu, \theta) / F(n | \mu, \theta)\)</span>.</li>
</ul>
<pre class="r"><code>stancode_cdf &lt;- &quot; 
functions {
  vector dpo_cdf(int max_value, real mu, real theta) {
    real d = exp(theta * (1 + log(mu)) - 1);
    real prob;
    int n = max_value + 1;
    vector[n] cdf;
    cdf[1] = sqrt(theta) * exp(-mu * theta);
    prob = cdf[1] * d;
    cdf[2] = cdf[1] + prob;
    for (i in 2:max_value) {
      prob = prob * d * exp((1 - theta) * (i - 1) * (log(i) - log(i - 1))) / (i^theta);
      cdf[i + 1] = cdf[i] + prob;
      if (prob / cdf[i + 1] &lt; 1e-8) {
        n = i + 1;
        break;
      }
    }
    return cdf / cdf[n];
  }
}
&quot;</code></pre>
<p>To check that this is accurate, I’ll again compare the Stan function to the corresponding function from <code>gamlss.dist</code>. If my function is accurate, the computed cdf values should be proportional to the cdf calculated from <code>gamlss.dist::pDPO()</code> and the ratio should be very close to 1.</p>
<pre class="r"><code>writeLines(stancode_cdf, &quot;DPO-cdf.stan&quot;)
expose_stan_functions(&quot;DPO-cdf.stan&quot;)

test_cdf &lt;- 
  expand_grid(
    mu = c(2, 5, 10, 20),
    theta = c(0.1, 0.2, 0.5, 1, 2, 5, 10),
  ) %&gt;%
  mutate(
    my_cdf = pmap(.l = list(max_value = 100, mu = mu, theta = theta), .f = dpo_cdf)
  ) %&gt;%
  unnest(my_cdf) %&gt;%
  filter(!is.nan(my_cdf)) %&gt;%
  group_by(mu, theta) %&gt;%
  mutate(
    q = row_number() - 1L,
    gamlss_cdf = pDPO(q = q, mu = mu, sigma = 1 / theta),
    ratio = my_cdf / gamlss_cdf
  )</code></pre>
<pre class="r"><code>ggplot(test_cdf, aes(factor(theta), ratio, color = factor(theta))) + 
  geom_boxplot() + 
  facet_wrap( ~ mu, labeller = &quot;label_both&quot;, ncol = 2) + 
  theme_minimal() + 
  labs(x = &quot;theta&quot;) + 
  theme(legend.position = &quot;none&quot;)</code></pre>
<p><img src="/post/Double-Poisson-in-Stan_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>Still on track here (although you might wonder, would I be sharing this post if I couldn’t get the function working?).</p>
</div>
<div id="quantile-function-and-sampler" class="section level1">
<h1>Quantile function and sampler</h1>
<p>The main other thing we need is a function for generating random samples from the double-Poisson. The <code>gamlss.dist</code> package has the function <code>rDPO()</code> for this purpose. It’s implemented using the standard inversion method, by calculating quantiles of the double-Poisson corresponding to a random sample from a uniform distribution. I’ll take the same approach.</p>
<p>The function below calculates quantiles by finding the minimum value of <span class="math inline">\(q \geq 0\)</span> such that <span class="math inline">\(F(q + 1 | \mu, \theta) \geq p\)</span> for a specified probability <span class="math inline">\(p \in [0, 1]\)</span>, It is vectorized over <span class="math inline">\(p\)</span> and solves for <span class="math inline">\(q\)</span> by starting with the smallest <span class="math inline">\(p\)</span> and continuing through the largest value.</p>
<pre class="r"><code>stancode_quantile &lt;- &quot; 
functions {
  vector dpo_cdf(int max_value, real mu, real theta) {
    real d = exp(theta * (1 + log(mu)) - 1);
    real prob;
    int n = max_value + 1;
    vector[n] cdf;
    cdf[1] = sqrt(theta) * exp(-mu * theta);
    prob = cdf[1] * d;
    cdf[2] = cdf[1] + prob;
    for (i in 2:max_value) {
      prob = prob * d * exp((1 - theta) * (i - 1) * (log(i) - log(i - 1))) / (i^theta);
      cdf[i + 1] = cdf[i] + prob;
      if (prob / cdf[i + 1] &lt; 1e-8) {
        n = i + 1;
        break;
      }
    }
    return cdf / cdf[n];
  }
  array[] int dpo_quantile(vector p, real mu, real theta, int max_value) {
    int N = rows(p);
    array[N] int qs;
    array[N] int indices = sort_indices_asc(p);
    vector[max_value + 1] cdf_vec = dpo_cdf(max_value, mu, theta);
    int j = 0;
    for (i in indices) {
      while (cdf_vec[j + 1] &lt; p[i]) {
        j += 1;
      }
      qs[i] = j;
    }
    return qs;
  }
}
&quot;</code></pre>
<p>If my quantile function is accurate, it should match the value computed from <code>gamlss.dist::qDPO()</code> exactly.</p>
<pre class="r"><code>writeLines(stancode_quantile, &quot;DPO-quantile.stan&quot;)
expose_stan_functions(&quot;DPO-quantile.stan&quot;)

test_quantile &lt;- 
  expand_grid(
    mu = c(2, 5, 10, 20),
    theta = c(0.1, 0.2, 0.5, 1, 2, 5, 10),
  ) %&gt;%
  mutate(
    p = map(1:n(), ~ runif(100)),
    my_q = pmap(.l = list(p = p, mu = mu, theta = theta, max_value = 5000), .f = dpo_quantile),
    gamlss_q = pmap(.l = list(p = p, mu = mu, sigma = 1 / theta), .f = qDPO)
  ) %&gt;%
  unnest(c(p, my_q, gamlss_q)) %&gt;%
  mutate(
    diff = my_q - gamlss_q
  )</code></pre>
<pre class="r"><code>ggplot(test_quantile, aes(factor(theta), diff, color = factor(theta))) + 
  geom_boxplot() + 
  facet_wrap( ~ mu, labeller = &quot;label_both&quot;, ncol = 2) + 
  theme_minimal() + 
  labs(x = &quot;theta&quot;) + 
  theme(legend.position = &quot;none&quot;)</code></pre>
<p><img src="/post/Double-Poisson-in-Stan_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Phew, still got it!</p>
<p>The last piece of the puzzle is to write a sampler by generating random points from a uniform distribution, then computing the double-Poisson quantiles of these random points. My implementation is as follows:</p>
<pre class="r"><code>stancode_qr &lt;- &quot;
functions {
  vector dpo_cdf(int max_value, real mu, real theta) {
    real d = exp(theta * (1 + log(mu)) - 1);
    real prob;
    int n = max_value + 1;
    vector[n] cdf;
    cdf[1] = sqrt(theta) * exp(-mu * theta);
    prob = cdf[1] * d;
    cdf[2] = cdf[1] + prob;
    for (i in 2:max_value) {
      prob = prob * d * exp((1 - theta) * (i - 1) * (log(i) - log(i - 1))) / (i^theta);
      cdf[i + 1] = cdf[i] + prob;
      if (prob / cdf[i + 1] &lt; 1e-8) {
        n = i + 1;
        break;
      }
    }
    return cdf / cdf[n];
  }
  array[] int dpo_quantile(vector p, real mu, real theta, int max_value) {
    int N = rows(p);
    array[N] int qs;
    array[N] int indices = sort_indices_asc(p);
    vector[max_value + 1] cdf_vec = dpo_cdf(max_value, mu, theta);
    int j = 0;
    for (i in indices) {
      while (cdf_vec[j + 1] &lt; p[i]) {
        j += 1;
      }
      qs[i] = j;
    }
    return qs;
  }
  array[] int dpo_rng(int n, real mu, real theta, int max_value) {
    vector[n] p;
    for (i in 1:n) {
      p[i] = uniform_rng(0,1);
    }
    array[n] int x = dpo_quantile(p, mu, theta, max_value);
    return x;
  }
}
&quot;</code></pre>
<p>To check this function, I’ll generate some large samples from the double-Poisson with a few different parameter sets. If the sampler is working properly, the empirical cumulative distribution should line up closely with the cumulative distribution computed using <code>gamlss.dist::pDPO()</code>.</p>
<pre class="r"><code>writeLines(stancode_qr, &quot;DPO-rng.stan&quot;)
expose_stan_functions(&quot;DPO-rng.stan&quot;)

test_rng &lt;- 
  expand_grid(
    mu = c(2, 5, 10, 20),
    theta = c(0.1, 0.2, 0.5, 1, 2, 5, 10),
  ) %&gt;%
  mutate(
    x = pmap(.l = list(n = 10000, mu = mu, theta = theta, max_value = 5000), .f = dpo_rng),
    tb = map(x, ~ as.data.frame(table(.x)))
  ) %&gt;%
  dplyr::select(-x) %&gt;%
  group_by(mu, theta) %&gt;%
  unnest(tb) %&gt;%
  mutate(
    .x = as.integer(levels(.x))[.x],
    Freq_cum = cumsum(Freq),
    gamlss_F = 10000 * pDPO(q = .x, mu = mu, sigma = 1 / theta)
  )</code></pre>
<pre class="r"><code>ggplot(test_rng, aes(gamlss_F, Freq_cum, color = factor(theta))) + 
  geom_abline(slope = 1, color = &quot;blue&quot;, linetype = &quot;dashed&quot;) + 
  geom_point() + geom_line() +  
  facet_grid(theta ~ mu, labeller = &quot;label_both&quot;) + 
  theme_minimal() + 
  labs(x = &quot;Theoretical cdf (gamlss.dist)&quot;, y = &quot;Empirical cdf (my function)&quot;) + 
  theme(legend.position = &quot;none&quot;) </code></pre>
<p><img src="/post/Double-Poisson-in-Stan_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Looks pretty good, no?</p>
</div>
<div id="using-the-custom-distribution-functions" class="section level1">
<h1>Using the custom distribution functions</h1>
<p>To finish out my tests of these functions, I’ll demonstrate their use in an actual estimation problem. I’ll generate data based on a simple generalized linear model with a single predictor <span class="math inline">\(X\)</span>, where the outcome <span class="math inline">\(Y\)</span> follows a double-Poisson distribution conditional on <span class="math inline">\(X\)</span>. The data-generating process is:</p>
<p><span class="math display">\[
\begin{aligned}
X &amp;\sim N(0, 1) \\
Y|X &amp;\sim DPO(\mu(X), \theta) \\
\log \mu(X) &amp;= 2 + 0.3 \times X
\end{aligned}
\]</span>
To make things interesting, I’ll set the dispersion parameter to <span class="math inline">\(1 / \theta = 0.7\)</span> so that the outcome is <em>under</em>-dispersed relative to the Poisson.</p>
<p>The following code generates a large sample from the data-generating process. To keep things R-centric, I use <code>gamlss.dist::rDPO</code> to generate the outcome.</p>
<pre class="r"><code>set.seed(20230913)
N &lt;- 500
X &lt;- rnorm(N)
mu &lt;- exp(2 + 0.3 * X)
theta_inv &lt;- 0.7
Y &lt;- rDPO(N, mu = mu, sigma = theta_inv)
dat &lt;- data.frame(X = X, Y = Y)</code></pre>
<p>Here’s what the sample looks like, along with a smoothed regression estimated using a basic cubic spline:</p>
<pre class="r"><code>ggplot(dat, aes(X, Y)) + 
  geom_point(alpha = 0.1) + 
  geom_smooth(method = &#39;gam&#39;, formula = y ~ s(x, bs = &quot;cs&quot;)) + 
  theme_minimal()</code></pre>
<p><img src="/post/Double-Poisson-in-Stan_files/figure-html/unnamed-chunk-6-1.png" width="576" />
Surely the simplest, quickest, and dirtiest way to estimate such a regression is with a generalized linear model, using the “quasi-Poisson” family to allow for non-unit dispersion. In R:</p>
<pre class="r"><code>quasi_fit &lt;- glm(Y ~ X, family = quasipoisson(link = &quot;log&quot;), data = dat)
summary(quasi_fit)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Y ~ X, family = quasipoisson(link = &quot;log&quot;), data = dat)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -2.79512  -0.59001   0.01096   0.53804   2.54536  
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  1.97015    0.01435  137.25   &lt;2e-16 ***
## X            0.32611    0.01364   23.92   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for quasipoisson family taken to be 0.7122746)
## 
##     Null deviance: 779.58  on 499  degrees of freedom
## Residual deviance: 373.24  on 498  degrees of freedom
## AIC: NA
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>This approach recovers the data-generating parameters quite well, with a dispersion estimate of 0.712 compared to the true dispersion parameter of 0.7.</p>
<p>Now let me fit the same generalized linear model but assuming that the outcome follows a true Poisson distribution (with unit dispersion). I’ll fit the model in a Bayesian framework with the <code>brms</code> package.</p>
<pre class="r"><code>Poisson_fit &lt;- 
  brm(
    Y ~ X, family = poisson(link = &quot;log&quot;),
    data = dat, 
    warmup = 500, 
    iter = 2000, 
    chains = 2, 
    cores = 2,
    seed = 20230913
  )</code></pre>
<pre><code>## Compiling Stan program...</code></pre>
<pre><code>## Start sampling</code></pre>
<pre class="r"><code>summary(Poisson_fit)</code></pre>
<pre><code>##  Family: poisson 
##   Links: mu = log 
## Formula: Y ~ X 
##    Data: dat (Number of observations: 500) 
##   Draws: 2 chains, each with iter = 2000; warmup = 500; thin = 1;
##          total post-warmup draws = 3000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     1.97      0.02     1.94     2.00 1.00     1994     1629
## X             0.33      0.02     0.29     0.36 1.00     1578     1830
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<p>This specification recovers the intercept and slope parameters very well indeed, but doesn’t provide any estimate of dispersion.</p>
<p>As an alternative, I’ll also fit the model using the negative binomial distribution, which is a generalization of the Poisson that allows for over-dispersion (but not under-dispersion):</p>
<pre class="r"><code>negbin_fit &lt;- 
  brm(
    Y ~ X, family = negbinomial(link = &quot;log&quot;),
    data = dat, 
    warmup = 500, 
    iter = 2000, 
    chains = 2, 
    cores = 2,
    seed = 20230913
  )</code></pre>
<pre><code>## Compiling Stan program...</code></pre>
<pre><code>## Start sampling</code></pre>
<pre class="r"><code>summary(negbin_fit)</code></pre>
<pre><code>##  Family: negbinomial 
##   Links: mu = log; shape = identity 
## Formula: Y ~ X 
##    Data: dat (Number of observations: 500) 
##   Draws: 2 chains, each with iter = 2000; warmup = 500; thin = 1;
##          total post-warmup draws = 3000
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept     1.97      0.02     1.94     2.00 1.00     2203     2065
## X             0.33      0.02     0.30     0.36 1.00     2468     1885
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## shape   266.72    119.30   109.20   557.32 1.00     2262     2409
## 
## Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
</div>
<div id="colophon" class="section level1">
<h1>Colophon</h1>
<pre><code>## R version 4.2.2 (2022-10-31 ucrt)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19045)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_United States.utf8 
## [2] LC_CTYPE=English_United States.utf8   
## [3] LC_MONETARY=English_United States.utf8
## [4] LC_NUMERIC=C                          
## [5] LC_TIME=English_United States.utf8    
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] loo_2.5.1           bayesplot_1.9.0     brms_2.18.0        
##  [4] Rcpp_1.0.10         rstan_2.26.23       StanHeaders_2.26.27
##  [7] gamlss.dist_6.0-3   MASS_7.3-57         forcats_0.5.1      
## [10] stringr_1.5.0       dplyr_1.1.2         purrr_1.0.2        
## [13] readr_2.1.2         tidyr_1.3.0         tibble_3.2.1       
## [16] ggplot2_3.4.0       tidyverse_1.3.1    
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.4.2         backports_1.4.1      RcppEigen_0.3.3.9.2 
##   [4] plyr_1.8.8           igraph_1.3.5         splines_4.2.2       
##   [7] crosstalk_1.2.0      TH.data_1.1-2        rstantools_2.2.0    
##  [10] inline_0.3.19        digest_0.6.30        htmltools_0.5.4     
##  [13] fansi_1.0.4          magrittr_2.0.3       BH_1.78.0-0         
##  [16] checkmate_2.1.0      tzdb_0.3.0           modelr_0.1.8        
##  [19] RcppParallel_5.1.5   matrixStats_0.62.0   xts_0.12.1          
##  [22] sandwich_3.0-1       timechange_0.2.0     prettyunits_1.1.1   
##  [25] colorspace_2.1-0     rvest_1.0.2          haven_2.5.0         
##  [28] xfun_0.34            callr_3.7.2          crayon_1.5.2        
##  [31] jsonlite_1.8.4       survival_3.4-0       zoo_1.8-10          
##  [34] glue_1.6.2           gtable_0.3.1         emmeans_1.7.3       
##  [37] distributional_0.3.1 pkgbuild_1.3.1       abind_1.4-5         
##  [40] scales_1.2.1         mvtnorm_1.1-3        DBI_1.1.2           
##  [43] miniUI_0.1.1.1       xtable_1.8-4         stats4_4.2.2        
##  [46] DT_0.23              htmlwidgets_1.6.2    httr_1.4.3          
##  [49] threejs_0.3.3        posterior_1.3.1      ellipsis_0.3.2      
##  [52] pkgconfig_2.0.3      farver_2.1.1         sass_0.4.5          
##  [55] dbplyr_2.1.1         utf8_1.2.3           tidyselect_1.2.0    
##  [58] labeling_0.4.2       rlang_1.1.1          reshape2_1.4.4      
##  [61] later_1.3.0          munsell_0.5.0        cellranger_1.1.0    
##  [64] tools_4.2.2          cachem_1.0.6         cli_3.6.1           
##  [67] generics_0.1.3       broom_0.8.0          ggridges_0.5.3      
##  [70] evaluate_0.18        fastmap_1.1.0        yaml_2.3.5          
##  [73] processx_3.7.0       knitr_1.40           fs_1.6.1            
##  [76] nlme_3.1-157         mime_0.12            xml2_1.3.3          
##  [79] compiler_4.2.2       shinythemes_1.2.0    rstudioapi_0.13     
##  [82] reprex_2.0.1         bslib_0.4.2          stringi_1.7.12      
##  [85] highr_0.9            ps_1.6.0             blogdown_1.10       
##  [88] Brobdingnag_1.2-9    lattice_0.20-45      Matrix_1.5-1        
##  [91] markdown_1.7         shinyjs_2.1.0        tensorA_0.36.2      
##  [94] vctrs_0.6.3          pillar_1.9.0         lifecycle_1.0.3     
##  [97] jquerylib_0.1.4      bridgesampling_1.1-2 estimability_1.3    
## [100] httpuv_1.6.8         QuickJSR_1.0.5       R6_2.5.1            
## [103] bookdown_0.26        promises_1.2.0.1     gridExtra_2.3       
## [106] codetools_0.2-18     colourpicker_1.1.1   gtools_3.9.3        
## [109] assertthat_0.2.1     withr_2.5.0          shinystan_2.6.0     
## [112] multcomp_1.4-23      mgcv_1.8-41          parallel_4.2.2      
## [115] hms_1.1.3            grid_4.2.2           coda_0.19-4         
## [118] rmarkdown_2.18       shiny_1.7.4          lubridate_1.9.2     
## [121] base64enc_0.1-3      dygraphs_1.1.1.6</code></pre>
</div>
