---
title: Variance component estimates in meta-analysis with mis-specified sampling correlation
authors:
- admin
date: '2021-11-20'
draft: true
number_sections: true
codefolding_nobutton: true
slug: variance-components-with-misspecified-correlation
categories: []
tags:
  - meta-analysis
  - dependent effect sizes
  - distribution theory
  - hierarchical models
header:
  caption: ''
  image: ''
---

$$
\def\Pr{{\text{Pr}}}
\def\E{{\text{E}}}
\def\Var{{\text{Var}}}
\def\Cov{{\text{Cov}}}
$$

In a recent paper with Beth Tipton, we proposed [new working models](/publication/rve-meta-analysis-expanding-the-range/) for meta-analyses involving dependent effect sizes. The central idea of our approach is to use a working model that captures the main features of the effect size data, such as by allowing for both between- and within-study heterogeneity in the true effect sizes (rather than only between-study heterogeneity). Doing so will lead to more precise estimates of overall average effects or, in models that include predictor variables, more precise estimates of meta-regression coefficients. Further, one can combine this working model with robust variance estimation methods to provide protection against the possibility that some of the model's assumptions could be mis-specified. 

In order to estimate these new working models, the analyst must first make some assumption about the degree of correlation between effect size estimates that come from the same sample. In typical applications, it can be difficult to obtain good empirical information about the correlation between effect size estimates, and so it is common to impose some simplifying assumptions and use rough guesses about the degree of correlation. There's a sense that this might not matter much---particularly because robust variance estimation should protect the inferences if the assumptions about the correlation are wrong. However, I still wonder about the extent to which these assumptions about the correlation structure matter for anything. 

There's a few reasons to wonder about how much the correlation matters. One is that the analyst might actually care about the variance component estimates from the working model, if they're substantively interested in the extent of heterogeneity or if they're trying to make predictions about the distribution of effect sizes that could be expected in a new study. Compared to earlier working models, the variance component estimates of the models that we proposed in the paper seem to be relatively more sensitive to the assumed correlation. Second, one alternative analytic strategy that's been proposed (and applied) for meta-analysis of dependent effect sizes is to use a multi-level meta-analysis (MLMA) model. The MLMA is a special case of the correlated-and-hierarchical effects model that we described in the paper, the main difference being that MLMA _ignores_ the possibility of correlation between effect size estimates, or equivalently, assumes that the correlation is zero. Thus, MLMA is one specific way that this correlation assumption might be mis-specified. There's some simulation evidence that inferences based on MLMA may be robust (even without using robust variance estimation), but it's not clear how general this robustness property might be.

In this post, I'm going to look at the implications of using a mis-specified assumption about the sampling correlation for the variance components in the correlated-and-hierarchical effects working model. As in [my previous post on weights in multivariate meta-analysis](/weighting-in-multivariate-meta-analysis/), I'm going to mostly limit consideration to the simple (but important!) case of an intercept-only model, without any further predictors of effect size, to see what can be learned about how the variance components can go wrong. 

# The correlated-and-hierarchical effects (CHE) model

Consider a meta-analytic dataset with effect size estimates $T_{ij}$, where $i = 1,...,k_j$ indexes effect size estimates within study $j$ and $j$ indexes studies, for $j = 1,...,J$. Say that effect size estimate $T_{ij}$ has sampling variance $\sigma^2_{ij}$, and there is some sampling correlation between effect sizes $h$ and $i$ within study $j$, denoted $\phi_{hij}$. 
The correlated-and-hierarchical effects (or CHE) model describes the distribution of effect sizes using random effects to capture between-study heterogeneity (as in the basic random effects model) and within-study heterogeneity in true effect sizes. In hierarchical notation, the model is
$$
\begin{align}
T_{ij} &= \theta_j + \nu_{ij} + e_{ij} \\
\theta_j &= \mu + \eta_j
\end{align}
$$
where $\Var(e_{ij}) = \sigma^2_{ij}$, $\Var(\nu_{ij}) = \omega^2$ is the within-study variance, and $\Var(\eta_j) = \tau^2$ is the between-study variance. 
To simplify things, let us also assume that the effect size estimates from a given study $j$ all have equal sampling variance, so $\sigma^2_{1j} = \sigma^2_{2j} = \cdots = \sigma^2_{k_jj} = \sigma^2_j$, and that there is a common correlation between any pair of effect size estimates from the same study, so $\Cov(e_{hj}, e_{ij}) = \phi \sigma^2_j$ for some correlation $\phi$. 

Typically, the analyst would estimate this working model using restricted maximum likelihood (REML) estimation to obtain estimates of the variance components $\tau^2$ and $\omega^2$, after specifying a value of $\phi$. With an adequately large sample of studies, the REML estimators should be close-to-unbiased and accurate. But what if the assumed correlation is wrong? Let's suppose that the analyst estimates (via REML) the CHE working model but uses the assumption that there is a common correlation between effect size estimates of $\rho$, which is not necessarily equal to the true correlation $\phi$. What are the consequences for estimating $\tau^2$ and $\omega^2$?

# Mis-specified REML

To figure out what's going on here, we need to know something about how REML estimators behave under mis-specified models. For starters, I'll work with a more general case than the CHE model described above. Suppose that we have a vector of multi-variate normal outcomes $\mathbf{T}_j$ for $j = 1,...,J$, explained by a set of covariates $\mathbf{X}_j$, and with true variance-covariance matrix $\boldsymbol\Phi_j$:
$$
\mathbf{T}_j \ \sim \ N\left( \mathbf{X}_j \beta, \boldsymbol\Phi_j \right)
$$
However, suppose that we posit a variance structure $\boldsymbol\Omega_j(\boldsymbol\theta)$, which is a function of a $v$-dimensional variance component parameter $\boldsymbol\theta$, and where $\boldsymbol\Phi_j$ is not necessarily conformable to $\boldsymbol\Omega_j(\boldsymbol\theta)$. Let $\mathbf{T}$ and $\mathbf{X}$ denote the full vector of outcomes and the full (stacked) predictor matrix for $j = 1,...,J$, and let $\boldsymbol\Phi$ and $\boldsymbol\Omega$ denote the corresponding block-diagonal variance-covariance matrices.

We estimate $\boldsymbol\theta$ by REML, which maximizes the log likelihood
$$
2 l_R(\boldsymbol\theta) = c -\log \left|\boldsymbol\Omega_j(\boldsymbol\theta)\right| - \log \left|\mathbf{X}' \boldsymbol\Omega^{-1}_j(\boldsymbol\theta) \mathbf{X}\right| - \mathbf{T}'\mathbf{Q}\mathbf{T},
$$
where $\mathbf{Q} = \boldsymbol\Omega^{-1}_j(\boldsymbol\theta) - \boldsymbol\Omega^{-1}_j(\boldsymbol\theta) \mathbf{X} \left(\mathbf{X}' \boldsymbol\Omega^{-1}_j(\boldsymbol\theta) \mathbf{X}\right)^{-1} \mathbf{X}'\boldsymbol\Omega^{-1}_j(\boldsymbol\theta)$. Equivalently, the REML estimators solve the score equations
$$
\frac{\partial l_R(\boldsymbol\theta)}{\partial \theta_q} = 0, \qquad \text{for} \qquad q = 1,...,v.
$$

Under mis-specification, the REML estimators converge (as $J$ increases) to the values that minimize the Kullback-Liebler divergence between the posited model and the true data-generating process. Equivalently, they converge to the values that solve 
$$
\E\left[\frac{\partial l_R(\boldsymbol\theta)}{\partial \theta_q}\right] = 0, \qquad \text{for} \qquad q = 1,...,v,
$$
where the expectation is taken under the true data-generating process. For the restricted log-likelihood given above, 
$$
\frac{\partial l_R(\boldsymbol\theta)}{\partial \theta_q} = -\text{tr}\left(\boldsymbol\Omega^{-1} \boldsymbol{\dot\Omega}_q\right) + \text{tr}\left[\left(\mathbf{X}' \boldsymbol\Omega^{-1} \mathbf{X}\right)^{-1} \mathbf{X}' \boldsymbol\Omega^{-1} \boldsymbol{\dot\Omega}_q \boldsymbol\Omega^{-1} \mathbf{X}\right] +   \mathbf{T}'\mathbf{Q}\boldsymbol{\dot\Omega}_q\mathbf{Q}\mathbf{T},
$$
where $\boldsymbol{\dot\Omega}_q = \partial \boldsymbol\Omega / \partial \theta_q$ and the dependence of $\boldsymbol\Omega$ on $\boldsymbol\theta$ is suppressed. Furthermore, the final term in the above is a quadratic form in $\mathbf{T}$, with expectation given by 
$$
\E\left(\mathbf{T}'\mathbf{Q}\boldsymbol{\dot\Omega}_q\mathbf{Q}\mathbf{T}\right) = \text{tr}\left(\mathbf{Q}\boldsymbol{\dot\Omega}_q\mathbf{Q} \boldsymbol\Phi\right).
$$
Therefore, the REML estimator of $\boldsymbol\theta$ converges to values that satisfy the equalities
$$
0 = \text{tr}\left(\mathbf{Q}\boldsymbol{\dot\Omega}_q\mathbf{Q} \boldsymbol\Phi\right) - \text{tr}\left(\boldsymbol\Omega^{-1} \boldsymbol{\dot\Omega}_q\right) + \text{tr}\left[\left(\mathbf{X}' \boldsymbol\Omega^{-1} \mathbf{X}\right)^{-1} \mathbf{X}' \boldsymbol\Omega^{-1} \boldsymbol{\dot\Omega}_q \boldsymbol\Omega^{-1} \mathbf{X}\right]
$$
for $q = 1,...,v$.

# Back to CHE

Let me now jump back to the special case of the CHE model for a meta-analysis with no predictors. Let $\tau_*^2$ and $\omega_*^2$ denote the variance components in the true data-generating process. Let $\tilde\tau^2$ and $\tilde\omega^2$ denote the asymptotic limits of the REML estimators under the mis-specified model. In this model, $\mathbf{X}_j = \mathbf{1}_j$, 
$$
\begin{aligned}
\boldsymbol\Phi_j &= \left(\tau_*^2 + \phi \sigma_j^2\right) \mathbf{1}_j \mathbf{1}_j' + \left(\omega_*^2 + (1 - \phi) \sigma_j^2\right) \mathbf{I}_j \\
\boldsymbol\Omega_j &= \left(\tilde\tau^2 + \rho \sigma_j^2\right) \mathbf{1}_j \mathbf{1}_j' + \left(\tilde\omega^2 + (1 - \rho) \sigma_j^2\right) \mathbf{I}_j \\
\boldsymbol\Omega_j^{-1} &= \frac{1}{\tilde\omega^2 + (1 - \rho)\sigma_j^2}\left[\mathbf{I}_j - \frac{\tilde\tau^2 + \rho \sigma_j^2}{k_j \tilde\tau^2 + k_j \rho \sigma_j^2 + \tilde\omega^2 + (1 - \rho)\sigma_j^2} \mathbf{1}_j \mathbf{1}_j' \right] \\
\left[\boldsymbol{\dot\Omega}_{\tau^2}\right]_j &= \mathbf{1}_j \mathbf{1}_j' \\
\left[\boldsymbol{\dot\Omega}_{\omega^2}\right]_j &= \mathbf{I}_j.
\end{aligned}
$$
Let $\displaystyle{\tilde{w}_j = \frac{k_j}{k_j \tilde\tau^2 + k_j \rho \sigma_j^2 + \tilde\omega^2 + (1 - \rho)\sigma_j^2}}$ and $\displaystyle{\tilde{W} = \sum_{j=1}^J \tilde{w}_j}$. Then we have that
$$
\begin{aligned}
\text{tr}\left(\boldsymbol\Omega^{-1} \boldsymbol{\dot\Omega}_{\tau^2}\right) &= \sum_{j=1}^J \mathbf{1}_j'\boldsymbol\Omega^{-1}\mathbf{1}_j = \tilde{W} \\
\text{tr}\left(\boldsymbol\Omega^{-1}  \boldsymbol{\dot\Omega}_{\omega^2}\right) &= \sum_{j=1}^J \tilde{w}_j \left(1 + \frac{(k_j - 1)(\tilde\tau^2 + \rho \sigma_j^2)}{\tilde\omega^2 + (1 - \rho)\sigma_j^2}\right) \\
\text{tr}\left[\left(\mathbf{X}' \boldsymbol\Omega^{-1} \mathbf{X}\right)^{-1} \mathbf{X}' \boldsymbol\Omega^{-1} \boldsymbol{\dot\Omega}_{\tau^2} \boldsymbol\Omega^{-1} \mathbf{X}\right] &= \frac{1}{\tilde{W}} \sum_{j=1}^J \tilde{w}_j^2 \\
\text{tr}\left[\left(\mathbf{X}' \boldsymbol\Omega^{-1} \mathbf{X}\right)^{-1} \mathbf{X}' \boldsymbol\Omega^{-1} \boldsymbol{\dot\Omega}_{\omega^2} \boldsymbol\Omega^{-1} \mathbf{X}\right] &= \frac{1}{\tilde{W}} \sum_{j=1}^J \frac{\tilde{w}_j^2}{k_j}.
\end{aligned}
$$
Finally, letting $\displaystyle{w^*_j = \frac{k_j}{k_j \tau_*^2 + k_j \phi \sigma_j^2 + \omega_*^2 + (1 - \phi)\sigma_j^2}}$, the quadratic forms can be written as
$$
\begin{aligned}
\text{tr}\left(\mathbf{Q}\boldsymbol{\dot\Omega}_{\tau^2}\mathbf{Q} \boldsymbol\Phi\right) &= \sum_{j=1}^J \mathbf{1}_j' \boldsymbol\Omega^{-1}_j \boldsymbol\Phi_j \boldsymbol\Omega^{-1}_j \mathbf{1}_j - \frac{2}{\tilde{W}} \sum_{j=1}^J \mathbf{1}_j'  \boldsymbol\Omega^{-1}_j \mathbf{1}_j \mathbf{1}_j' \boldsymbol\Omega^{-1}_j \boldsymbol\Phi_j \boldsymbol\Omega^{-1}_j \mathbf{1}_j + \frac{1}{\tilde{W}^2} \left[\sum_{j=1}^J \left( \mathbf{1}_j'  \boldsymbol\Omega^{-1}_j \mathbf{1}_j \right)^2\right]\left(\sum_{j=1}^J \mathbf{1}_j' \boldsymbol\Omega^{-1}_j \boldsymbol\Phi_j \boldsymbol\Omega^{-1}_j \mathbf{1}_j \right) \\
&= \sum_{j=1}^J \frac{\tilde{w}_j^2}{w^*_j} - \frac{2}{\tilde{W}} \sum_{j=1}^J \frac{\tilde{w}_j^3}{w^*_j} + \frac{1}{\tilde{W}^2}\left(\sum_{j=1}^J \tilde{w}_j^2 \right)\left(\sum_{j=1}^J \frac{\tilde{w}_j^2}{w^*_j}\right)
\end{aligned}
$$
and
$$
\begin{aligned}
\text{tr}\left(\mathbf{Q}\boldsymbol{\dot\Omega}_{\omega^2}\mathbf{Q} \boldsymbol\Phi\right) &= \text{tr}\left(\mathbf{Q} \boldsymbol\Phi \mathbf{Q}\right) \\
&= \text{tr}\left(\boldsymbol\Omega^{-1} \boldsymbol\Phi \boldsymbol\Omega^{-1}\right) - \frac{2}{W} \mathbf{1}' \boldsymbol\Omega^{-1} \boldsymbol\Phi \boldsymbol\Omega^{-1} \boldsymbol\Omega^{-1} \mathbf{1} + \frac{1}{W^2} \mathbf{1}' \boldsymbol\Omega^{-1} \boldsymbol\Phi \boldsymbol\Omega^{-1} \mathbf{1} \mathbf{1}' \boldsymbol\Omega^{-1} \boldsymbol\Omega^{-1} \mathbf{1} \\
&= \sum_{j=1}^J \text{tr}\left(\boldsymbol\Omega_j^{-1} \boldsymbol\Phi_j \boldsymbol\Omega_j^{-1}\right) - \frac{2}{\tilde{W}} \sum_{j=1}^J \frac{\tilde{w}_j^3}{k_j^3} \mathbf{1}_j' \boldsymbol\Phi_j \mathbf{1}_j + \frac{1}{\tilde{W}^2} \left(\sum_{j=1}^J \frac{\tilde{w}_j^2}{k_j^2}\mathbf{1}_j' \boldsymbol\Phi_j \mathbf{1}_j \right)\left(\sum_{j=1}^J \frac{\tilde{w}_j^2}{k_j} \right) \\
&= \sum_{j=1}^J \frac{1}{\left[\tilde\omega^2 + (1 - \rho)\sigma_j^2 \right]^2}\left[\frac{k_j}{w^*_j} + (k_j - 1)\left[\omega_*^2 + (1 - \phi)\sigma_j^2\right] - \frac{\left(\tilde\tau^2 + \rho \sigma_j^2\right)\tilde{w}_j k_j}{w^*_j} - \frac{\left(\tilde\tau^2 + \rho \sigma_j^2\right)\left[\tilde\omega^2 + (1 - \rho)\sigma_j^2 \right]\tilde{w}_j^2}{w^*_j} \right] - \frac{2}{\tilde{W}} \sum_{j=1}^J \frac{\tilde{w}_j^3}{k_j w^*_j} + \frac{1}{\tilde{W}^2} \left(\sum_{j=1}^J \frac{\tilde{w}^2}{w^*_j}\right) \left(\sum_{j=1}^J \frac{\tilde{w}^2_j}{k_j} \right).
\end{aligned}
$$

It follows that the REML estimators converge to the values $\tilde\tau^2$ and $\tilde\omega^2$ that solve 
$$
0 = \sum_{j=1}^J \frac{\tilde{w}_j^2}{w^*_j} - \frac{2}{\tilde{W}} \sum_{j=1}^J \frac{\tilde{w}_j^3}{w^*_j} + \frac{1}{\tilde{W}^2}\left(\sum_{j=1}^J \tilde{w}_j^2 \right)\left(\sum_{j=1}^J \frac{\tilde{w}_j^2}{w^*_j}\right) - \tilde{W} + \frac{1}{\tilde{W}} \sum_{j=1}^J \tilde{w}_j^2
$$
and
$$
\begin{aligned}
0 &= \sum_{j=1}^J \frac{1}{\left[\tilde\omega^2 + (1 - \rho)\sigma_j^2 \right]^2}\left[\frac{k_j}{w^*_j} + (k_j - 1)\left[\omega_*^2 + (1 - \phi)\sigma_j^2\right] - \frac{\left(\tilde\tau^2 + \rho \sigma_j^2\right)\tilde{w}_j k_j}{w^*_j} - \frac{\left(\tilde\tau^2 + \rho \sigma_j^2\right)\left[\tilde\omega^2 + (1 - \rho)\sigma_j^2 \right]\tilde{w}_j^2}{w^*_j} \right]  \\
& \qquad \qquad \qquad - \frac{2}{\tilde{W}} \sum_{j=1}^J \frac{\tilde{w}_j^3}{k_j w^*_j} + \frac{1}{\tilde{W}^2} \left(\sum_{j=1}^J \frac{\tilde{w}^2}{w^*_j}\right) \left(\sum_{j=1}^J \frac{\tilde{w}^2_j}{k_j} \right) - \sum_{j=1}^J \tilde{w}_j \left(1 + \frac{(k_j - 1)(\tilde\tau^2 + \rho \sigma_j^2)}{\tilde\omega^2 + (1 - \rho)\sigma_j^2}\right) + \frac{1}{\tilde{W}} \sum_{j=1}^J \frac{\tilde{w}_j^2}{k_j}.
\end{aligned}
$$
These are complicated non-linear equations but they can be solved numerically.

```{r}

CHE_score <- function(x, tausq, omegasq, phi, rho, k_j, sigmasq_j) {
  trs_j <- exp(x[1]) + rho * sigmasq_j
  ors_j <- exp(x[2]) + (1 - rho) * sigmasq_j
  w_j <- k_j / (k_j * trs_j + ors_j)
  W <- sum(w_j)
  
  tausq_ps_j <- tausq + phi * sigmasq_j
  omegasq_ps_j <- omegasq + (1 - phi) * sigmasq_j
  wj_star <- k_j / (k_j * tausq_ps_j + omegasq_ps_j)

  Quad_tausq <- sum(w_j^2 / wj_star) - 2 * sum(w_j^3 / wj_star) / W + sum(w_j^2) * sum(w_j^2 / wj_star) / W^2
  score_tausq <- Quad_tausq - W + sum(w_j^2) / W
  
  Quad_omegasq_a <- sum((k_j / wj_star 
                         + (k_j - 1) * omegasq_ps_j 
                         - trs_j * k_j * w_j / wj_star
                         - trs_j * ors_j * w_j^2 / wj_star) / ors_j^2)

  Quad_omegasq_b <- 2 * sum(w_j^3 / (k_j * wj_star)) / W

  Quad_omegasq_c <- sum(w_j^2 / k_j) * sum(w_j^2 / wj_star) / W^2

  Quad_omegasq <- Quad_omegasq_a - Quad_omegasq_b + Quad_omegasq_c
  
  score_omegasq_b <- sum(w_j * (1 + (k_j - 1) * trs_j / ors_j))
  
  score_omegasq_c <- sum(w_j^2/ k_j) / W
  
  score_omegasq <- Quad_omegasq - score_omegasq_b + score_omegasq_c
  
  c(tau_sq = score_tausq, omega_sq = score_omegasq)
}

find_tausq_omegasq <- function(tau, omega, phi, rho, k_j, sigmasq_j, full = FALSE) {
  require(rootSolve)
  
  res <- multiroot(CHE_score, start = log(c(tau^2, omega^2)), 
                   tausq = tau^2, omegasq = omega^2, 
                   phi = phi, rho = rho, 
                   k_j = k_j, sigmasq_j = sigmasq_j)
  
  names(res$root) <- c("tau","omega")
  res$root <- sqrt(exp(res$root))
  if (full) return(res)
  
  exp(res$root)
  
}

J <- 100
k_j <- 1 + rpois(J, 3)
sigmasq_j <- 4 / pmax(rgamma(J, 2, scale = 30), 20)

CHE_score(log(c(0.2^2, 0.02^2)), 
          tausq = 0.1^2, omegasq = 0.05^2, 
          phi = 0.4, rho = 0.8, 
          k_j = k_j, sigmasq_j = sigmasq_j)


find_tausq_omegasq(tau = 0.1, omega = 0.05, 
                   phi = 0.6, rho = 0.5, 
                   k_j = k_j, sigmasq_j = sigmasq_j, full = TRUE)

```


## Completely balanced designs

The equalities simplify a bit in the special case that the sample of studies is completely balanced, such that $k_1 = k_2 = \cdots = k_J$ and $\sigma_1^2 = \sigma_2^2 = \cdots = \sigma_J^2$.