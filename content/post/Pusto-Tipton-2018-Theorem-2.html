---
title: Corrigendum to Pustejovsky and Tipton (2018)
subtitle: Theorem 2 is incorrect as stated
authors:
- admin
date: '2022-09-28'
codefolding_show: 'show'
slug: Pusto-Tipton-2018-Theorem-2
bibliography: [RVE-references.bib]
csl: apa.csl
link-citations: true
categories: []
tags:
  - robust variance estimation
  - econometrics
  - matrix algebra
header:
  caption: ''
  image: ''
---



<p><span class="math display">\[
\def\Pr{{\text{Pr}}}
\def\E{{\text{E}}}
\def\Var{{\text{Var}}}
\def\Cov{{\text{Cov}}}
\def\bm{\mathbf}
\def\bs{\boldsymbol}
\]</span>
In my <a href="/publication/rve-in-fixed-effects-models/">2018 paper with Beth Tipton</a>, published in the <em>Journal of Business and Economic Statistics</em>, we considered how to do cluster-robust variance estimation in fixed effects models estimated by weighted (or unweighted) least squares. A careful reader, <a href="https://eeecon.uibk.ac.at/~pfaffermayr/">Dr. Michael Pfaffermayr</a>, recently alerted us to a problem with Theorem 2 in the paper, which concerns a computational short cut for a certain cluster-robust variance estimator in models with cluster-specific fixed effects. The theorem is incorrect as stated, and we are currently working on issuing a correction for the published version of the paper. In the interim, this post details the problem with Theorem 2. I’ll first review the CR2 variance estimator, then describe the assertion of the theorem, and then provide a numerical counter-example demonstrating that the assertion is not correct as stated.</p>
<div id="a-fixed-effects-model" class="section level3">
<h3>A fixed effects model</h3>
<p>For data that can be grouped into <span class="math inline">\(m\)</span> clusters of observations, we considered the model
<span class="math display" id="eq:regression">\[
\bm{y}_i = \bm{R}_i \bs\beta + \bm{S}_i \bs\gamma + \bm{T}_i \bs\mu + \bs\epsilon_i, \tag{1}
\]</span>
where <span class="math inline">\(\bm{y}_i\)</span> is an <span class="math inline">\(n_i \times 1\)</span> vector of responses for cluster <span class="math inline">\(i\)</span>, <span class="math inline">\(\bm{R}_i\)</span> is an <span class="math inline">\(n_i \times r\)</span> matrix of focal predictors, <span class="math inline">\(\bm{S}_i\)</span> is an <span class="math inline">\(n_i \times s\)</span> matrix of additional covariates that vary across multiple clusters, and <span class="math inline">\(\bm{T}_i\)</span> is an <span class="math inline">\(n_i \times t\)</span> matrix encoding cluster-specific fixed effects, all for <span class="math inline">\(i = 1,...,m\)</span>. The cluster-specific fixed effects satisfy <span class="math inline">\(\bm{T}_h \bm{T}_i&#39; = \bm{0}\)</span> for <span class="math inline">\(h \neq i\)</span>. Interest centers on inference for the coefficients on the focal predictors <span class="math inline">\(\bs\beta\)</span>.</p>
<p>We considered estimation of Model <a href="#eq:regression">(1)</a> by weighted least squares (WLS), possibly under a working model for the distribution of <span class="math inline">\(\bs\epsilon_i\)</span>. Let <span class="math inline">\(\bm{W}_1,...,\bm{W}_m\)</span> be a set of symmetric weight matrices used for WLS estimation. Sometimes, these weight matrices may be diagonal, consisting of sampling weights for each observation. Other times, the weight matrices may involve off-diagonal terms as well. Consider a working model <span class="math inline">\(\Var\left(\bs\epsilon_i | \bm{R}_i, \bm{S}_i, \bm{T}_i\right) = \sigma^2 \bs\Phi_i\)</span> where <span class="math inline">\(\bs\Phi_i\)</span> is a symmetric <span class="math inline">\(n_i \times n_i\)</span> matrix that may be a function of a low-dimensional, estimable parameter. Based on this working model, the weight matrices might be taken as <span class="math inline">\(\bm{W}_i = \bs{\hat\Phi}_i^{-1}\)</span>, where <span class="math inline">\(\bs{\hat\Phi}_i\)</span> is an estimate of <span class="math inline">\(\bs\Phi_i\)</span>.</p>
</div>
<div id="the-cr2-variance-estimator" class="section level3">
<h3>The CR2 variance estimator</h3>
<p>In the paper, we provide a generalization of the bias-reduced linearization estimator introduced by <span class="citation">McCaffrey et al. (<a href="#ref-McCaffrey2001generalizations" role="doc-biblioref">2001</a>)</span> and <span class="citation">Bell &amp; McCaffrey (<a href="#ref-Bell2002bias" role="doc-biblioref">2002</a>)</span> that can be applied to Model <a href="#eq:regression">(1)</a>. The variance estimator is effectively a generalization of the HC2 correction for heteroskedasticity-robust standard errors, but that works for models with within-cluster dependence and cluster-specific fixed effects, and so we refer to it the “CR2” estimator.</p>
<p>In order to define the CR2 variance estimator and explain the issue with Theorem 2, I’ll need to lay down a bit more notation. Let <span class="math inline">\(N = \sum_{i=1}^m n_i\)</span> be the total sample size. Let <span class="math inline">\(\bm{U}_i = \left[ \bm{R}_i \ \bm{S}_i \right]\)</span> be the set of predictors that vary across clusters and <span class="math inline">\(\bm{X}_i = \left[ \bm{R}_i \ \bm{S}_i \ \bm{T}_i \right]\)</span> be the full set of predictors. Let <span class="math inline">\(\bm{R}\)</span>, <span class="math inline">\(\bm{S}\)</span>, <span class="math inline">\(\bm{T}\)</span>, <span class="math inline">\(\bm{U}\)</span>, and <span class="math inline">\(\bm{X}\)</span> denote the stacked versions of the cluster-specific matrices (i.e., <span class="math inline">\(\bm{R} = \left[\bm{R}_1&#39; \ \bm{R}_2&#39; \ \cdots \ \bm{R}_m&#39;\right]&#39;\)</span>, etc.). Let <span class="math inline">\(\bm{W} = \bigoplus_{i=1}^m \bm{W}_i\)</span> and <span class="math inline">\(\bs\Phi = \bigoplus_{i=1}^m \bs\Phi_i\)</span>. For a generic matrix <span class="math inline">\(\bm{Z}\)</span>, let <span class="math inline">\(\bm{M}_{Z} = \left(\bm{Z}&#39;\bm{W}\bm{Z}\right)^{-1}\)</span> and <span class="math inline">\(\bm{H}_{\bm{Z}} = \bm{Z} \bm{M}_{\bm{Z}}\bm{Z}&#39;\bm{W}\)</span>. Let <span class="math inline">\(\bm{C}_i\)</span> be the <span class="math inline">\(n_i \times N\)</span> matrix that selects the rows of cluster <span class="math inline">\(i\)</span> from the full set of observations, such that <span class="math inline">\(\bm{X}_i = \bm{C}_i \bm{X}\)</span>. These operators provide an easy way to define absorbed versions of the predictors. Specifically, let <span class="math inline">\(\bm{\ddot{S}} = \left(\bm{I} - \bm{H}_{\bm{T}}\right) \bm{S}\)</span> be the covariates after absorbing (i.e., partialling out) the cluster-specific effects, let <span class="math inline">\(\bm{\ddot{U}} = \left(\bm{I} - \bm{H}_{\bm{T}}\right) \bm{U}\)</span> be an absorbed version of the focal predictors and the covariates, and let <span class="math inline">\(\bm{\ddot{R}} = \left(\bm{I} - \bm{H}_{\bm{\ddot{S}}}\right)\left(\bm{I} - \bm{H}_{\bm{T}}\right) \bm{R}\)</span> be the focal predictors after absorbing the covariates and the cluster-specific fixed effects.</p>
<p>With this notation established, the CR2 variance estimator has the form
<span class="math display">\[
\bm{V}^{CR2} = \bm{M}_{\bm{\ddot{R}}} \left(\sum_{i=1}^m \bm{\ddot{R}}_i&#39; \bm{W}_i \bm{A}_i \bm{e}_i \bm{e}_i&#39; \bm{A}_i \bm{W}_i \bm{\ddot{R}}_i \right) \bm{M}_{\bm{\ddot{R}}},
\]</span>
where <span class="math inline">\(\bm{\ddot{R}}_i = \bm{C}_i \bm{\ddot{R}}\)</span> is the cluster-specific matrix of absorbed focal predictors, <span class="math inline">\(\bm{e}_i\)</span> is the vector of weighted least squares residuals from cluster <span class="math inline">\(i\)</span>, and <span class="math inline">\(\bm{A}_1,...,\bm{A}_m\)</span> are a set of adjustment matrices that correct the bias of the residual cross-products.
The adjustment matrices are calculated as follows. Let <span class="math inline">\(\bm{D}_i\)</span> be the upper-right Cholesky factorization of <span class="math inline">\(\bm{\Phi}_i\)</span> and define the matrices
<span class="math display" id="eq:B-matrix">\[
\bm{B}_i = \bm{D}_i \bm{C}_i \left(\bm{I} - \bm{H}_{\bm{X}}\right) \bs\Phi \left(\bm{I} - \bm{H}_{\bm{X}}\right)&#39;\bm{C}_i&#39; \bm{D}_i&#39;
\tag{2}
\]</span>
for <span class="math inline">\(i = 1,...,m\)</span>. The adjustment matrices are then calculated as
<span class="math display" id="eq:A-matrix">\[
\bm{A}_i = \bm{D}_i&#39; \bm{B}_i^{+1/2} \bm{D}_i,
\tag{3}
\]</span>
where <span class="math inline">\(\bm{B}_i^{+1/2}\)</span> is the symmetric square root of the Moore-Penrose inverse of <span class="math inline">\(\bm{B}_i\)</span>.
Theorem 1 in the paper shows that, if the working model <span class="math inline">\(\bs\Phi\)</span> is correctly specified and some conditions on the rank of <span class="math inline">\(\bm{U}\)</span> are satisfied, then the CR2 estimator is exactly unbiased for the sampling variance of the weighted least squares estimator of <span class="math inline">\(\bs\beta\)</span>. Across multiple simulation studies, it’s been observed that the CR2 estimator also works well and outperforms alternative sandwich estimators even when the working model is not correctly specified.</p>
</div>
<div id="theorem-2" class="section level3">
<h3>Theorem 2</h3>
<p>The adjustment matrices given in <a href="#eq:A-matrix">(3)</a> can be expensive to compute directly because the <span class="math inline">\(\bm{B}_i\)</span> matrices involve computing a “residualized” version of the <span class="math inline">\(N \times N\)</span> matrix <span class="math inline">\(\bs\Phi\)</span> involving the full set of predictors <span class="math inline">\(\bm{X}\)</span>—including the cluster-specific fixed effects <span class="math inline">\(\bm{T}_1,...,\bm{T}_m\)</span>. Theorem 2 considered whether one can take a computational short cut by omitting the cluster-specific fixed effects from the calculation of the <span class="math inline">\(\bm{B}_i\)</span> matrices. Specifically, define the modified matrices
<span class="math display" id="eq:B-modified">\[
\bm{\tilde{B}}_i = \bm{D}_i \bm{C}_i \left(\bm{I} - \bm{H}_{\bm{\ddot{U}}}\right) \bs\Phi \left(\bm{I} - \bm{H}_{\bm{\ddot{U}}}\right)&#39;\bm{C}_i&#39; \bm{D}_i&#39;
\tag{4}
\]</span>
and
<span class="math display" id="eq:A-modified">\[
\bm{\tilde{A}}_i = \bm{D}_i&#39; \bm{\tilde{B}}_i^{+1/2} \bm{D}_i,
\tag{5}.
\]</span>
Theorem 2 claims that if the weight matrices are inverse of the working model, such that <span class="math inline">\(\bm{W}_i = \bs\Phi_i^{-1}\)</span> for <span class="math inline">\(i = 1,...,m\)</span>, then <span class="math inline">\(\bm{\tilde{B}}_i^{+1/2} = \bm{B}_i^{+1/2}\)</span> and hence <span class="math inline">\(\bm{\tilde{A}}_i = \bm{A}_i\)</span>. The implication is that the cluster-specific fixed effects can be ignored when calculating the adjustment matrices. However, the claimed equivalence does not actually hold.</p>
<p>Here is a simple numerical example that contradicts the assertion of Theorem 2. I first create a predictor matrix consisting of 4 clusters, a single focal predictor, and cluster-specific fixed effects.</p>
<pre class="r"><code>set.seed(20220926)
m &lt;- 4                                             # number of clusters
ni &lt;- 2 + rpois(m, 3.5)                            # cluster sizes
N &lt;- sum(ni)                                       # total sample size
id &lt;- factor(rep(LETTERS[1:m], ni))                # cluster ID
R &lt;- rnorm(N)                                      # focal predictor
dat &lt;- data.frame(R, id)                           # create raw data frame
X &lt;- model.matrix(~ R + id + 0, data = dat)        # full predictor matrix
Ui &lt;- tapply(R, id, \(x) x - mean(x))              # absorbed version of R
U &lt;- unsplit(Ui, id)</code></pre>
<p>Consider a model estimated by ordinary least squares, where the assumed working model is homoskedastic and independent errors, so <span class="math inline">\(\bs\Phi_i = \bm{I}_i\)</span>, an <span class="math inline">\(n_i \times n_i\)</span> identity matrix (with no parameters to estimate). In this case, the adjustment matrices simplify considerably, to
<span class="math display">\[
\bm{A}_i = \left(\bm{I}_i - \bm{X}_i \bm{M}_{X} \bm{X}_i&#39; \right)^{+1/2} \qquad \text{and} \qquad \bm{\tilde{A}}_i = \left(\bm{I}_i - \bm{\ddot{U}}_i \bm{M}_{\ddot{U}} \bm{\ddot{U}}_i&#39; \right)^{+1/2}.
\]</span>
I calculate these directly as follows:</p>
<pre class="r"><code>matrix_power &lt;- function(x, p) {
  eig &lt;- eigen(x, symmetric = TRUE)
  val_p &lt;- with(eig, ifelse(values &gt; 10^-12, values^p, 0))
  with(eig, vectors %*% (val_p * t(vectors)))
}

MX &lt;- solve(crossprod(X))
B &lt;- 
  by(X, id, as.matrix) |&gt;
  lapply(\(x) diag(nrow(x)) - x %*% MX %*% t(x))
A &lt;- lapply(B, matrix_power, p = -1/2)
  
MU &lt;- 1 / crossprod(U)
Btilde &lt;- lapply(Ui, \(x) diag(length(x)) - x %*% MU %*% t(x))
Atilde &lt;- lapply(Btilde, matrix_power, p = -1/2)</code></pre>
<p>Here are the adjustment matrices based on the full predictor matrix <span class="math inline">\(\bm{X}\)</span>:</p>
<pre class="r"><code>print(A, digits = 3)</code></pre>
<pre><code>## $A
##        [,1]   [,2]   [,3]   [,4]   [,5]
## [1,]  0.853 -0.198 -0.207 -0.191 -0.257
## [2,] -0.198  0.800 -0.200 -0.200 -0.202
## [3,] -0.207 -0.200  0.801 -0.201 -0.192
## [4,] -0.191 -0.200 -0.201  0.802 -0.210
## [5,] -0.257 -0.202 -0.192 -0.210  0.860
## 
## $B
##        [,1]   [,2]   [,3]
## [1,]  0.668 -0.338 -0.330
## [2,] -0.338  0.683 -0.345
## [3,] -0.330 -0.345  0.675
## 
## $C
##        [,1]   [,2]   [,3]   [,4]   [,5]   [,6]
## [1,]  0.873 -0.206 -0.163 -0.105 -0.233 -0.166
## [2,] -0.206  0.873 -0.171 -0.229 -0.100 -0.167
## [3,] -0.163 -0.171  0.834 -0.160 -0.173 -0.167
## [4,] -0.105 -0.229 -0.160  0.931 -0.271 -0.166
## [5,] -0.233 -0.100 -0.173 -0.271  0.946 -0.168
## [6,] -0.166 -0.167 -0.167 -0.166 -0.168  0.833
## 
## $D
##        [,1]   [,2]   [,3]
## [1,]  0.797 -0.342 -0.455
## [2,] -0.342  0.667 -0.325
## [3,] -0.455 -0.325  0.780</code></pre>
<p>Compare the above with the adjustment matrices based on the absorbed predictors only:</p>
<pre class="r"><code>print(Atilde, digits = 3)</code></pre>
<pre><code>## $A
##          [,1]      [,2]     [,3]      [,4]     [,5]
## [1,]  1.05313  0.001860 -0.00742  0.008995 -0.05657
## [2,]  0.00186  1.000065 -0.00026  0.000315 -0.00198
## [3,] -0.00742 -0.000260  1.00104 -0.001257  0.00790
## [4,]  0.00900  0.000315 -0.00126  1.001523 -0.00958
## [5,] -0.05657 -0.001980  0.00790 -0.009576  1.06022
## 
## $B
##          [,1]     [,2]     [,3]
## [1,]  1.00139 -0.00478  0.00339
## [2,] -0.00478  1.01642 -0.01163
## [3,]  0.00339 -0.01163  1.00824
## 
## $C
##           [,1]      [,2]      [,3]      [,4]     [,5]      [,6]
## [1,]  1.039180 -0.039378  4.00e-03  0.061921 -0.06632  5.94e-04
## [2,] -0.039378  1.039577 -4.02e-03 -0.062234  0.06665 -5.97e-04
## [3,]  0.003999 -0.004019  1.00e+00  0.006320 -0.00677  6.07e-05
## [4,]  0.061921 -0.062234  6.32e-03  1.097861 -0.10481  9.39e-04
## [5,] -0.066317  0.066651 -6.77e-03 -0.104808  1.11225 -1.01e-03
## [6,]  0.000594 -0.000597  6.07e-05  0.000939 -0.00101  1.00e+00
## 
## $D
##          [,1]     [,2]    [,3]
## [1,]  1.13078 -0.00914 -0.1216
## [2,] -0.00914  1.00064  0.0085
## [3,] -0.12165  0.00850  1.1131</code></pre>
<p>The matrices differ:</p>
<pre class="r"><code>all.equal(A, Atilde)</code></pre>
<pre><code>## [1] &quot;Component \&quot;A\&quot;: Mean relative difference: 0.6073885&quot;
## [2] &quot;Component \&quot;B\&quot;: Mean relative difference: 0.7403564&quot;
## [3] &quot;Component \&quot;C\&quot;: Mean relative difference: 0.5671847&quot;
## [4] &quot;Component \&quot;D\&quot;: Mean relative difference: 0.6682793&quot;</code></pre>
<p>Thus, Theorem 2 is incorrect as stated. (I have yet to identify the mis-step in the proof as given in the supplementary materials of the paper.)</p>
</div>
<div id="further-thoughts" class="section level3">
<h3>Further thoughts</h3>
<p>For this particular model specification, it is interesting to note that <span class="math inline">\(\bm{\tilde{A}}_i = \bm{A}_i + \bm{T}_i \bm{M}_{\bm{T}} \bm{T}_i&#39;\)</span>. Because <span class="math inline">\(\bm{\ddot{U}}_i&#39; \bm{T}_i = \bm{0}\)</span>, it follows that
<span class="math display">\[
\bm{\ddot{U}}_i&#39; \bm{\tilde{A}}_i = \bm{\ddot{U}}_i&#39; \left(\bm{A}_i + \bm{T}_i \bm{M}_{\bm{T}} \bm{T}_i&#39; \right) = \bm{\ddot{U}}_i&#39; \bm{A}_i.
\]</span>
This holds in the numerical example:</p>
<pre class="r"><code>UiAtilde &lt;- mapply(\(u, a) t(u) %*% a, u = Ui, a = Atilde, SIMPLIFY = FALSE)
UiA &lt;- mapply(\(u, a) t(u) %*% a, u = Ui, a = A, SIMPLIFY = FALSE)
all.equal(UiAtilde, UiA)</code></pre>
<pre><code>## [1] TRUE</code></pre>
<p>Thus, although the exact statement of Theorem 2 is incorrect, the substantive implication actually still holds. For this particular example, computing the CR2 variance estimator using the short-cut adjustment matrices <span class="math inline">\(\bm{\tilde{A}}_1,...,\bm{\tilde{A}}_m\)</span> is equivalent to computing the CR2 variance estimator using the full model adjustment matrices <span class="math inline">\(\bm{A}_1,...,\bm{A}_m\)</span>. However, I have not yet been able to work out the general conditions under which this equivalence holds. It may require stricter conditions than those assumed in Theorem 2.</p>
</div>
<div id="references" class="section level3 unnumbered">
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent" line-spacing="2">
<div id="ref-Bell2002bias" class="csl-entry">
Bell, R. M., &amp; McCaffrey, D. F. (2002). <span class="nocase">Bias reduction in standard errors for linear regression with multi-stage samples</span>. <em>Survey Methodology</em>, <em>28</em>(2), 169–181.
</div>
<div id="ref-McCaffrey2001generalizations" class="csl-entry">
McCaffrey, D. F., Bell, R. M., &amp; Botts, C. H. (2001). <span class="nocase">Generalizations of biased reduced linearization</span>. <em>Proceedings of the Annual Meeting of the American Statistical Association</em>.
</div>
</div>
</div>
